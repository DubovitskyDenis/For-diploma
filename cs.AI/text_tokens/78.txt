reinforc
learn
approach
real
time
strategi
game
battl
citi
harshit
sethya
amit
patelb
acto
gymtrekk
fit
privat
limit
mumbai
india
email
bassist
professor
depart
comput
scienc
engin
rgukt
iiit
nuzvid
india
email
paper
propos
reinforc
learn
algorithm
gener
reward
function
propos
method
use
sarsa
algorithm
generalis
reward
function
train
forcement
learn
agent
evalu
perform
propos
algorithm
two
strategi
game
call
battlec
two
main
advantag
approach
compar
work
rt
ignor
concept
simul
often
game
speciﬁc
usual
hard
code
type
rt
game
system
learn
interact
oppon
quickli
chang
strategi
accord
oppon
need
human
trace
use
previou
work
keyword
reinforc
learn
machin
learn
real
time
strategi
artiﬁci
intellig
introduct
exist
good
artiﬁci
intellig
techniqu
background
game
one
major
factor
fun
abil
commerci
comput
game
although
appli
success
sever
game
chess
backgammon
checker
come
game
script
usual
use
simul
artiﬁci
genc
chess
backgammon
etc
seem
work
game
decis
made
well
search
space
huge
contain
true
learn
tradit
plan
approach
diﬃcult
case
rt
game
variou
factor
like
huge
decis
space
adversari
domain
observ
real
time
mean
decid
best
action
game
continu
run
state
chang
taneous
real
time
strategi
game
today
game
develop
compani
start
show
interest
rt
game
unlik
turn
base
strategi
game
one
abil
take
one
time
real
time
strategi
game
movement
construct
combat
occur
real
time
typic
rt
game
screen
contain
map
area
consist
game
world
build
unit
terrain
usual
sever
player
rt
game
player
variou
game
entiti
call
particip
unit
structur
control
player
player
need
save
asset
destroy
asset
oppon
player
make
use
control
entiti
use
rt
game
battlec
game
evalu
snapshot
two
rt
game
call
battlec
given
figur
battlec
game
battlec
multidirect
shooter
video
game
play
use
two
basic
tion
move
fire
player
control
tank
must
destroy
enemi
tank
enemi
base
also
protect
base
player
move
tank
four
direct
left
right
ﬁre
bullet
whichev
direct
tank
last
move
base
static
tank
three
type
obstacl
brick
wall
destroy
ﬁring
type
wall
marbl
tank
cant
destroy
ﬁring
water
wall
harshit
sethi
amit
patel
bodi
tank
ﬁre
tank
cant
pa
obstacl
brick
wall
destroy
tank
destroy
tank
pa
figur
snapshot
game
snapshot
battlec
game
game
strategi
game
player
goal
remain
aliv
destroy
rest
player
four
basic
action
game
harvest
gather
resourc
gold
wood
build
build
build
rack
blacksmith
tower
etc
train
produc
troop
archer
footman
catapult
knight
tack
attack
enemi
paper
structur
follow
apart
introduct
ﬁve
tion
highlight
review
relat
work
section
discu
reinforc
learn
techniqu
game
line
variou
learn
algorithm
use
forcement
learn
section
outlin
plement
detail
relat
propos
inforc
learn
algorithm
aliz
reward
function
two
game
battlec
game
section
discu
experiment
result
relat
propos
work
battlec
conclud
section
relat
work
one
major
work
use
onlin
base
plan
techniqu
real
time
egi
game
publish
base
plan
revis
case
base
plan
strateg
domain
involv
plan
plan
system
call
introduc
play
rt
game
introduc
set
algorithm
use
learn
plan
repres
one
human
stration
anoth
work
author
us
address
issu
plan
acquisit
plan
execut
leav
plan
execut
plan
adapt
author
summar
work
plore
use
ﬁrst
order
induct
learn
foil
algorithm
learn
rule
use
repres
oppon
strategi
author
improv
use
inform
relat
sensor
game
refer
work
paper
capabl
learn
play
rt
game
observ
human
demonstr
ing
human
trace
make
plan
play
game
priorit
plan
accord
back
game
feedback
decid
ing
rule
depend
sensor
game
reinforc
learn
approach
real
time
strategi
game
like
battl
citi
drawback
case
base
learn
proach
mention
requir
expert
demonstr
make
plan
train
done
learn
take
place
cover
larg
state
space
would
requir
larg
number
rule
plan
base
plorat
optim
solut
follow
man
stefan
wender
us
ment
learn
citi
site
select
base
strategi
game
civil
tion
strategi
game
game
battl
citi
real
time
game
stefan
wender
us
reinforc
learn
citi
site
select
egi
game
civil
civil
strategi
game
similar
game
real
time
multi
agent
game
paper
aim
away
hard
code
simul
propos
learn
approach
base
reinforc
learn
wherein
sensor
inform
current
use
select
best
action
reinforc
learn
use
caus
advantag
previou
strategi
speciﬁc
cut
need
ualli
specifi
rule
agent
learn
simpli
play
game
human
player
even
agent
larg
state
space
combin
function
tor
neural
network
approxim
evalu
function
agent
alway
explor
optim
solut
reach
goal
appli
wide
mani
ﬁeld
robot
board
game
turn
base
game
singl
agent
game
great
result
hardli
ever
rt
game
reinforc
learn
reinforc
learn
ﬁeld
chine
learn
deal
map
situat
action
maxim
numer
reward
learner
know
action
take
form
machin
learn
instead
must
discov
action
give
reward
appli
interest
challeng
case
action
may
aﬀect
immedi
reward
also
next
situat
subsequ
reward
compar
reinforc
learn
rt
game
environ
player
learn
interact
environ
observ
interact
fundament
way
human
anim
learn
human
perform
tion
observ
result
action
environ
way
act
environ
observ
sult
assign
reward
penalti
state
pair
accord
desir
result
state
figur
reinforc
learn
tectur
reinforc
learn
reinforc
learn
architectur
architectur
two
main
characterist
one
learn
play
harshit
sethi
amit
patel
learnt
experi
initi
rlearner
knowledg
game
random
action
observ
result
state
use
sensor
inform
game
give
feedback
form
reward
use
calcul
pair
action
previou
state
accord
desir
current
state
pair
known
deﬁn
polici
everi
action
polici
updat
state
action
pair
polici
use
predict
best
action
play
game
agent
learn
play
give
feedback
whole
process
go
till
end
game
basic
compon
reinforc
learn
contain
ﬁve
basic
ponent
list
set
environ
state
set
action
rule
transit
state
rule
determin
scalar
immedi
reward
transit
reward
function
rule
describ
agent
observ
valu
function
reward
function
scalar
valu
repres
degre
state
action
desir
known
reward
scalar
reward
assign
tion
particular
transit
result
state
game
result
state
abl
safe
posit
scalar
valu
reward
assign
action
otherwis
state
safe
undesir
neg
scalar
valu
neg
reward
assign
action
use
type
reward
function
condit
reward
function
generalis
reward
function
valu
function
valu
function
use
map
state
pair
real
number
valu
state
repres
term
reward
achiev
start
state
execut
particular
polici
estim
good
particular
action
given
state
return
tion
expect
two
type
valu
function
valu
state
polici
expect
return
start
follow
thereaft
valu
take
action
state
polici
expect
turn
start
take
action
thereaft
follow
polici
two
method
deﬁn
valu
function
mont
carlo
method
method
agent
would
need
wait
ﬁnal
reward
receiv
pair
valu
updat
ﬁnal
reward
receiv
path
taken
reach
ﬁnal
state
would
need
trace
back
valu
updat
state
visit
time
reward
time
constant
paramet
tempor
diﬀer
method
use
estim
valu
function
step
estim
ﬁnal
reward
culat
state
valu
updat
everi
step
way
reﬂect
realist
assign
reward
action
compar
updat
action
end
directli
learn
noth
combin
dynam
program
mont
carlo
method
formula
relat
learn
given
observ
reward
time
reinforc
learn
approach
real
time
strategi
game
like
battl
citi
sensor
represent
tleciti
game
figur
snapshot
battlec
game
current
map
use
two
type
sensor
inform
assign
reward
battl
citi
game
explain
follow
enemyinlin
enemi
posit
directli
line
player
without
block
wall
sensor
repres
number
wall
block
enemi
player
sensor
repres
ber
enemi
posit
line
player
sensor
enemybaseinlin
sensor
inform
repres
way
instead
take
consider
posit
enemi
posit
taken
account
posit
rectli
line
player
without
block
wall
sensor
repres
ber
wall
block
player
sensor
sent
number
posit
line
player
sensor
sensor
inform
game
get
current
map
store
two
dimension
array
gold
wood
sensor
retriev
current
number
peasant
footman
entiti
enemi
player
retriev
entiti
state
updat
two
dimension
array
static
entiti
like
goldmin
posit
build
far
outlin
method
ing
sensor
inform
relat
two
strategi
game
battlec
action
select
polici
follow
action
select
polici
use
select
desir
action
ing
behavior
particular
polici
time
action
highest
estim
reward
sen
call
greediest
action
small
probabl
action
select
random
ensur
optim
action
cover
similar
best
action
select
probabl
rest
time
random
action
chosen
uniformli
softmax
one
drawback
method
select
random
action
probabl
case
worst
possibl
action
select
second
best
softmax
remedi
harshit
sethi
amit
patel
assign
rank
weight
action
accord
timat
worst
action
unlik
chosen
step
learn
rlearner
observ
input
game
state
rlearner
creat
new
polici
base
dimens
world
set
paramet
number
episod
rlearner
start
ing
start
run
epoch
option
run
epoch
individu
one
epoch
contain
follow
step
action
determin
decis
ing
function
action
perform
rlearner
receiv
scalar
reward
inforc
environ
ing
reward
function
inform
reward
given
state
action
pair
record
updat
accord
learn
algorithm
sarsa
propos
learn
algorithm
section
outlin
propos
ing
algorithm
integr
two
rt
game
battlec
also
provid
implement
detail
relat
select
paramet
reward
function
paramet
section
contain
inform
regard
reward
algorithm
paramet
use
two
game
battlec
learn
rate
learn
rate
determin
fraction
old
estim
updat
new
estim
stop
learn
anyth
plete
chang
previou
valu
new
one
discount
factor
discount
factor
determin
fraction
upcom
reward
valu
consid
evalu
ing
reward
ignor
mean
consid
current
upcom
reward
equal
weightag
explor
rate
action
tion
polici
one
polici
call
greedi
method
us
explor
rate
determin
ratio
explor
exploit
use
greedi
method
select
best
action
maintain
balanc
explor
exploit
reward
function
battlec
algorithm
reward
function
late
reward
perform
action
current
state
accord
result
action
ward
penalti
assign
step
get
posit
player
emi
enemi
base
map
step
game
winner
player
add
reward
total
reward
newreward
els
deduct
penalti
total
reward
step
enemi
line
deduct
penalti
total
reward
alway
tri
line
enemi
step
enemi
base
line
calcul
distanc
enemi
base
deduct
time
reward
add
total
reward
push
come
closer
enemi
base
step
give
gener
reward
function
make
quickli
tack
enemi
base
prevent
attack
enemi
reward
function
algorithm
step
get
sensor
relat
total
gold
total
wood
size
troop
reinforc
learn
approach
real
time
strategi
game
like
battl
citi
algorithm
calcreward
battlec
input
state
contain
posit
entiti
reward
penalti
sensorslist
contain
sensor
game
domain
gamest
contain
state
game
run
output
reward
layerx
null
layeri
null
enemyx
null
enemyy
null
enemybasex
null
enemybasey
null
winner
null
newreward
distanc
layerx
getpositionx
state
player
layeri
getpositioni
state
player
enemyx
getpositionx
state
enemi
enemyy
getpositioni
state
enemi
enemybasex
getpositionx
state
enemybas
enemybasey
getpositioni
state
enemybas
gamest
end
winner
getwinn
winner
player
newreward
newreward
reward
els
newreward
newreward
penalti
els
sensorlist
enemyinlin
newreward
newreward
penalti
sensorlist
enemybaseinlin
distanc
enemybasex
layerx
enemybasey
layeri
distanc
enemyx
layerx
enemyy
layeri
newreward
newreward
reward
distanc
newreward
newreward
distanc
newreward
newreward
distanc
return
newreward
player
enemi
step
game
winner
player
add
reward
total
reward
newreward
els
deduct
penalti
total
reward
step
gold
wood
player
greater
enemi
add
reward
total
reward
otherwis
deduct
penalti
total
reward
alway
tri
increas
gold
wood
compar
enemi
step
player
troop
bigger
enemi
troop
add
twice
reward
total
reward
newreward
els
deduct
twice
penalti
total
reward
push
attack
build
armi
increas
size
troop
compar
enemi
step
return
total
reward
experiment
result
previou
section
discuss
success
appli
reinforc
learn
two
strategi
game
call
battlec
section
outlin
mental
result
relat
reinforc
learn
battlec
battlec
evalu
perform
help
variou
map
well
harshit
sethi
amit
patel
algorithm
calcreward
input
state
contain
posit
entiti
reward
penalti
global
access
gamest
contain
state
game
run
output
reward
sensorslist
contain
sensor
game
domain
layerg
layerw
enemyg
enemyw
enemyt
rooplength
layert
rooplength
winner
null
newreward
layerg
layerw
enemyg
enemyw
enemyt
rooplength
layert
rooplength
gamest
end
winner
getwinn
winner
player
newreward
newreward
reward
els
newreward
newreward
penalti
els
layerg
enemyg
newreward
newreward
reward
els
newreward
newreward
penalti
layerw
enemyw
newreward
newreward
reward
els
newreward
newreward
penalti
layert
rooplength
enemyt
rooplength
newreward
newreward
els
newreward
newreward
return
newreward
two
type
oppon
call
map
observ
reinforc
learn
agent
game
play
ponent
ple
map
play
complex
map
play
plex
map
statist
perform
sarsa
variou
map
repres
form
graph
observ
perform
agent
sarsa
learn
algorithm
better
techniqu
also
train
sarsa
algorithm
take
le
time
win
game
perform
evalu
battlec
game
two
oppon
follow
three
diﬀer
map
select
random
action
alway
tough
compet
caus
alway
follow
oppon
ﬁre
clear
experiment
result
reinforc
learn
agent
sarsa
algorithm
perform
better
techniqu
like
onlin
case
base
reinforc
learn
approach
real
time
strategi
game
like
battl
citi
ing
base
statist
relat
perform
given
form
graph
statist
repres
use
two
type
graph
one
time
millisecond
taken
win
game
versu
episod
sent
number
episod
repres
time
millisecond
number
game
versu
episod
also
resent
number
episod
resent
total
number
game
till
episod
map
map
size
refer
figur
tal
state
space
map
total
combin
player
emi
map
marbl
wall
tank
destroy
ﬁring
advantag
tank
hide
oppon
attack
oppon
enter
side
figur
map
follow
figur
map
map
complex
map
refer
figur
among
perform
uation
size
structur
map
search
space
contain
mani
brick
wall
water
bodi
brick
wall
destroy
ﬁring
size
water
bodi
make
diﬃcult
complex
map
figur
map
random
harshit
sethi
amit
patel
figur
map
time
versu
episod
graph
refer
figur
plot
refer
figur
ing
time
win
game
strategi
vari
everi
episod
map
ter
bodi
diﬃcult
learn
strategi
win
quickli
perform
strategi
close
case
sarsa
perform
well
win
game
compar
figur
map
random
figur
map
follow
figur
map
random
reinforc
learn
approach
real
time
strategi
game
like
battl
citi
still
avail
also
send
catapult
attack
enemi
build
rack
start
two
ant
start
harvest
gold
wood
build
barrack
train
footman
two
train
footman
start
attack
figur
map
follow
map
relat
complex
battlec
evalu
proach
variou
map
sever
player
experi
built
agent
game
use
rel
reward
tion
sarsa
approach
discuss
earlier
learn
play
game
call
simpl
map
refer
figur
ing
two
approach
sarsa
pair
valu
updat
play
learn
discuss
earlier
also
learn
play
use
updat
play
game
well
anoth
type
call
build
barrack
start
two
peasant
vest
gold
two
harvest
wood
start
build
catapult
nonstop
also
attack
time
increas
number
peasant
start
build
second
barrack
also
look
goldmin
gold
figur
snapshot
game
map
gow
experi
use
three
type
map
refer
figur
accord
culti
level
perform
experi
ﬁve
game
two
wherein
two
approach
sarsa
map
comparison
statist
given
tabl
observ
sarsa
win
game
previou
approach
perform
almost
better
sarsa
also
sarsa
give
best
result
tabl
show
result
comparison
lyze
result
shown
tabl
see
map
sarsa
drawn
game
map
lost
found
quick
attack
agent
abl
produc
enough
number
harshit
sethi
amit
patel
tabl
comparison
sarsa
map
approach
epoch
epoch
epoch
epoch
epoch
sarsa
sarsa
sarsa
gow
gow
gow
sarsa
sarsa
sarsa
gow
gow
gow
lost
lost
draw
lost
lost
draw
lost
draw
draw
lost
lost
lost
lost
lost
draw
draw
lost
draw
lost
draw
draw
draw
lost
lost
draw
lost
lost
lost
lost
lost
lost
draw
troop
defend
enemi
ing
agent
basic
tri
ﬁnd
way
enter
wall
tree
map
shown
result
drawn
mean
resourc
like
wood
gold
player
enemi
got
ﬁnish
peasant
left
side
anyth
without
gold
wood
compar
previou
research
strategi
use
play
game
plan
adapt
ule
use
switch
strategi
research
quickli
switch
strategi
play
even
though
use
simpl
map
train
conclus
paper
propos
reinforc
learn
model
strategi
game
order
achiev
end
make
use
two
inforc
learn
algorithm
sarsa
learn
idea
get
best
action
use
one
algorithm
make
use
trace
gener
player
previou
work
strategi
game
use
line
case
base
learn
human
trace
form
portant
compon
learn
process
propos
method
make
use
previou
knowledg
like
trace
therefor
follow
unsupervis
approach
research
regard
get
best
action
use
two
algorithm
sarsa
come
reinforc
learn
without
trace
gener
player
propos
previou
work
line
case
base
learn
ing
anoth
major
contribut
work
reward
function
reward
culat
two
type
reward
function
call
condit
gener
reward
function
sensor
inform
relat
game
use
culat
reward
reward
valu
ther
use
two
algorithm
sarsa
algorithm
make
polici
accord
reward
pair
agent
choos
action
ing
polici
evalu
approach
success
two
diﬀer
game
domain
tleciti
observ
ment
learn
perform
better
previou
reinforc
learn
approach
real
time
strategi
game
like
battl
citi
santiago
page
aaai
press
ument
http
villar
may
page
marc
ponsen
pieter
spronck
improv
adapt
game
evolutionari
ing
comput
game
artiﬁci
genc
design
educ
page
bhaskara
marthi
stuart
russel
david
latham
carlo
guestrin
concurr
erarch
reinforc
learn
strategi
game
civil
tional
joint
confer
artiﬁci
genc
edinburgh
scotland
page
pranay
game
simul
learner
univers
hyderabad
thesi
harshit
sethi
founder
chief
technolog
oﬃcer
gymtrekk
fit
privat
limit
mumbai
dia
receiv
master
artiﬁci
degre
ligenc
univers
hyderabad
assist
current
amit
patel
professor
rajiv
gandhi
univers
knowledg
technolog
iiit
nuzvid
krishna
obtain
bachelor
technolog
uttar
pradesh
technic
univers
receiv
master
degre
artiﬁci
intellig
univers
hyderabad
hyderabad
proach
term
learn
time
win
ratio
particular
sarsa
algorithm
take
lesser
time
learn
start
win
quickli
complex
map
refer
sutton
barto
reinforc
learn
introduct
book
publish
mit
press
kati
long
genter
use
ﬁrst
order
induct
learn
altern
simul
game
arﬁcial
intellig
si
georgia
institut
technolog
page
may
kati
long
genter
santiago
ashwin
ram
learn
oppon
strategi
ﬁrst
order
induct
flair
confer
page
llanso
santiago
win
ram
mmpm
gener
platform
plan
research
iccbr
workshop
reason
comput
game
page
juli
stefan
wender
ian
watson
use
inforc
learn
citi
site
select
strategi
game
tion
comput
intellig
game
page
janet
kolodn
introduct
base
reason
artiﬁci
intellig
review
page
santi
kinshuk
mishra
neha
sugandh
ashwin
ram
base
plan
comput
genc
page
santiago
long
genter
kati
ashwin
ram
learn
human
demonstr
base
plan
workshop
coloc
ijcai
page
neha
sugandh
santiago
win
ram
plan
adapt
strategi
game
associ
advanc
artiﬁci
intellig
