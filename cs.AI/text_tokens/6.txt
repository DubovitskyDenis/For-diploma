artiﬁci
intellig
final
project
angrier
bird
bayesian
reinforc
learn
imanol
arrieta
bernardo
lar
abstract
train
reinforc
learner
play
simpliﬁ
version
game
angri
bird
learner
provid
game
state
manner
similar
output
could
produc
comput
vision
algorithm
improv
efﬁcienc
regular
linear
function
approxim
systemat
explor
random
least
squar
valu
iter
rlsvi
algorithm
sampl
polici
posterior
distribut
optim
polici
larger
space
efﬁcient
explor
becom
increasingli
import
evidenc
faster
learn
rlsvi
keyword
reinforc
learn
rlsvi
explor
manag
scienc
engin
stanford
univers
california
unit
state
content
introduct
model
algorithm
linear
function
approxim
random
least
squar
valu
iter
featur
extractor
pig
posit
valu
pig
posit
indic
nest
pig
posit
counter
npp
nest
pig
posit
shift
counter
npp
nest
pig
posit
counter
obstacl
nppo
result
discuss
comparison
featur
extractor
rlsvi
regular
learn
success
conclus
futur
work
refer
introduct
angri
bird
wildli
success
internet
franchis
center
around
origin
mobil
game
player
shoot
bird
slingshot
hit
target
point
angri
bird
larg
determinist
game
complex
physic
engin
govern
ﬂight
collis
impact
appear
almost
realist
optim
gameplay
could
achiev
optim
highli
complex
function
describ
jectori
plan
instead
train
sever
reinforc
learn
algorithm
play
game
compar
ﬁnal
perform
learn
speed
oracl
simul
physic
engin
actual
seem
produc
random
result
sequenc
move
guarante
yield
exact
outcom
simul
gameplay
adapt
project
recreat
angri
bird
python
use
pygam
work
adapt
base
code
need
design
api
expos
game
markov
decis
ce
mdp
larg
follow
convent
class
framework
python
port
use
simpliﬁ
version
origin
angri
bird
game
includ
level
one
standard
type
bird
one
type
target
one
type
beam
level
end
whenev
either
target
bird
screen
target
destroy
agent
advanc
next
level
otherwis
agent
lose
go
back
ﬁrst
level
level
score
calcul
accord
three
paramet
number
bird
left
shatter
beam
column
destroy
target
agent
lose
incur
arbitrari
penalti
somewhat
reduc
space
allow
ativ
straightforward
believ
similar
algorithm
model
would
also
work
complex
version
game
model
game
state
expos
api
compos
low
inform
state
represent
fulli
describ
relev
game
situat
player
perspect
therefor
sufﬁc
interfac
learner
game
note
part
game
state
could
tain
comput
vision
algorithm
principl
direct
interact
game
mechan
would
essari
number
bird
left
ammunit
number
target
pig
absolut
posit
game
map
angrier
bird
bayesian
reinforc
learn
figur
angri
bird
gameplay
stationari
slingshot
left
use
hit
target
right
beam
structur
destroy
allow
complex
consequ
interact
chosen
action
number
obstacl
beam
absolut
sition
game
map
game
score
level
achiev
player
far
simpliﬁ
game
mechan
make
game
fulli
learner
alway
wait
ject
motionless
take
next
action
sumption
allow
work
absolut
posit
set
possibl
action
pair
angl
distanc
scribe
extens
aim
slingshot
therebi
bird
launch
momentum
direct
acceler
learn
allow
shoot
forward
within
constraint
allow
possibl
extens
angl
compar
discret
action
space
vari
level
granular
base
discret
use
differ
evenli
space
action
algorithm
reinforc
learn
artiﬁci
intellig
describ
way
learn
algorithm
act
unknown
unspeciﬁ
decis
process
guid
mere
reward
punish
desir
unwant
outcom
spectiv
reinforc
learn
close
mirror
human
intuit
learn
condit
low
algorithm
master
system
would
otherwis
hard
imposs
specifi
standard
algorithm
reinforc
learn
effect
algorithm
reli
errat
explor
possibl
action
implement
variat
updat
belief
distribut
optim
cie
sampl
polici
distribut
impli
systemat
explor
sinc
polici
seem
unattract
high
conﬁdenc
introduc
two
algorithm
discu
variou
iter
ﬁnd
optim
featur
extractor
linear
function
approxim
expos
algorithm
approxim
valu
function
qopt
order
deriv
optim
polici
unknown
decis
process
order
learn
optim
behavior
unknown
game
need
explor
system
choos
viousli
unseen
action
exploit
knowledg
thu
gather
choos
action
promis
best
outcom
given
previou
experi
balanc
efﬁcient
explor
exploit
main
challeng
reinforc
learn
popular
method
literatur
use
method
probabl
algorithm
would
ignor
previou
experi
pick
action
random
method
tend
work
well
practic
whenev
action
need
increas
payoff
complic
howev
complex
set
action
need
taken
order
get
higher
reward
tend
take
exponenti
time
ﬁnd
reward
see
aid
gener
angri
bird
state
space
use
linear
function
approxim
deﬁn
featur
extractor
calcul
featur
given
pair
allow
exploit
step
updat
weight
vector
perform
fashion
similar
stochast
gradient
descent
follow
standard
algorithm
random
least
squar
valu
iter
osband
propos
random
least
squar
valu
ation
rlsvi
algorithm
differ
mentat
two
point
instead
use
gradient
descent
learn
enc
onlin
fashion
rlsvi
store
full
histori
state
action
reward
new
state
tupl
deriv
exact
optim
polici
given
data
given
hyperparamet
qualiti
linear
approxim
bayesian
least
squar
employ
estim
distribut
around
optim
polici
rive
learner
sampl
polici
distribut
replac
explor
speciﬁc
rlsvi
model
valu
function
follow
reward
max
successor
arbitrari
discount
factor
hyperparamet
given
memori
state
action
reward
new
state
ple
use
bayesian
least
squar
deriv
expect
valu
optim
polici
cov
covari
optim
polici
detail
fairli
found
note
given
assumpt
linear
approxim
weight
vector
fulli
speciﬁ
therebi
polici
learner
ﬁnalli
pick
polici
sampl
ˆwt
take
action
ˆwt
predict
highest
reward
instead
take
entir
random
action
probabl
algorithm
thu
alway
sampl
random
polici
converg
optim
mean
data
accumul
varianc
shrink
therefor
le
time
may
expect
wast
polici
highli
unlik
success
keep
algorithm
errat
polici
chang
observ
made
yet
initi
uninform
prior
furthermor
inspect
show
almost
diagon
save
comput
therefor
cide
sampl
use
varianc
individu
weight
ignor
covari
weight
osband
propos
rlsvi
context
episod
learn
suggest
updat
polici
end
episod
ﬁxed
number
action
help
angrier
bird
bayesian
reinforc
learn
steadi
algorithm
unfortun
game
simul
comput
expens
rel
small
number
action
simul
forc
learn
continu
context
featur
extractor
design
featur
extractor
follow
two
premis
ﬁrst
given
valu
function
approxim
linear
clear
interact
featur
would
linear
particular
separ
nent
locat
would
work
second
want
give
away
littl
domain
knowledg
game
possibl
task
algorithm
understand
play
angri
bird
without
previous
concept
target
obstacl
two
premis
effect
impos
constraint
strategi
reliabl
way
algorithm
detect
whether
obstacl
front
behind
target
regardless
learner
develop
impress
perform
seen
later
one
reason
may
level
simul
rel
simpl
iter
follow
ﬁve
idea
form
ing
featur
due
rel
complex
space
featur
space
fairli
big
quickli
span
eral
thousand
featur
turn
problem
rlsvi
memori
comput
requir
grew
intract
rewrit
algorithm
run
spars
trice
offer
help
ultim
found
reason
perform
featur
pig
posit
valu
ﬁrst
attempt
use
round
target
posit
featur
valu
everi
pig
given
state
would
featur
valu
featur
repeat
everi
action
would
action
taken
allow
differ
weight
differ
action
combin
ultim
impli
learn
best
combin
includ
interact
term
hope
captur
rel
distanc
target
slingshot
establish
linear
function
would
allow
fast
gener
target
posit
unfortun
hindsight
surprisingli
approach
fail
quickli
produc
practic
learn
pig
posit
indic
next
approach
indic
variabl
ﬁne
grid
pig
posit
creat
separ
featur
everi
possibl
pig
posit
includ
action
taken
creat
impract
larg
featur
space
yet
work
rel
well
howev
clearli
success
seri
action
found
algorithm
would
reliabl
complet
level
target
move
pixel
howev
algorithm
would
retrain
complet
angrier
bird
bayesian
reinforc
learn
nest
pig
posit
counter
npp
order
solv
problem
develop
nest
mechan
would
gener
unobserv
state
achiev
deﬁn
nest
grid
game
screen
exempliﬁ
figur
three
grid
progress
smaller
squar
grid
count
number
target
contain
solv
gener
issu
maintain
nice
properti
previou
featur
extractor
larger
grid
help
gener
ﬁner
one
provid
way
rememb
level
alreadi
observ
figur
npp
nppo
grid
structur
progress
smaller
grid
creat
squar
within
squar
extract
speciﬁc
locat
inform
allow
gener
nest
pig
posit
shift
counter
npp
one
issu
npp
especi
larger
squar
target
close
squar
boundari
captur
adequ
improv
creat
copi
grid
shift
diagon
half
squar
size
everi
target
lay
exactli
two
squar
allow
judg
whether
target
left
right
within
squar
npp
therefor
featur
set
two
set
three
lap
squar
grid
surpris
npp
perform
wors
simpler
npp
assum
due
much
bigger
featur
space
fact
could
learn
algorithm
full
converg
due
comput
limit
nest
pig
posit
counter
obstacl
nppo
final
tri
address
issu
obstacl
describ
want
give
away
inform
repres
ﬁxed
relationship
target
obstacl
therefor
ad
counter
obstacl
way
use
target
hope
algorithm
would
learn
prefer
area
high
target
obstacl
ratio
case
npp
surpris
ad
inform
obstacl
detriment
learn
ce
inde
ad
obstacl
inform
stop
learn
altogeth
npp
suspect
nppo
may
work
well
given
time
converg
bloat
featur
space
result
discuss
somewhat
crude
featur
extractor
npp
provid
best
result
rslvi
inde
outperform
regular
rlsvi
particularli
impress
abil
clear
level
almost
speed
untrain
human
player
could
simul
gameplay
converg
result
comput
limit
game
engin
use
intend
big
simul
even
speed
signiﬁcantli
graphic
disabl
comparison
featur
extractor
compar
ﬁve
differ
featur
extractor
learn
baselin
algorithm
display
ﬁgure
noteworthi
start
high
baselin
score
improv
much
due
fact
learner
quickli
master
level
gener
follow
level
consequ
tend
get
stuck
achiev
rel
constant
game
score
ﬁrst
level
contrast
npp
learn
master
easi
level
equal
fast
carri
harder
level
better
yield
higher
total
score
per
attempt
npp
show
similar
slope
npp
support
explan
sheer
number
featur
slow
ing
process
howev
npp
clearli
outperform
npp
within
simul
time
frame
somewhat
surpris
complet
failur
nppo
improv
time
rlsvi
regular
explor
space
accord
posterior
belief
inde
produc
better
result
train
algorithm
featur
extractor
npp
compar
move
averag
game
score
per
tempt
rlsvi
achiev
higher
score
overal
simul
time
frame
learn
quickli
comparison
two
algorithm
given
ﬁgure
note
implement
rlsvi
requir
signiﬁcantli
memori
comput
regular
sinc
requir
keep
complet
histori
observ
featur
matrix
oper
larg
data
block
learn
success
neither
human
player
oracl
score
algorithm
manag
level
angrier
bird
bayesian
reinforc
learn
figur
move
averag
differ
featur
extractor
dysfunct
posit
valu
featur
extractor
exclud
display
move
averag
game
score
next
attempt
number
attempt
made
attempt
deﬁn
number
shot
level
fail
success
use
one
attempt
explor
record
follow
attempt
without
explor
order
distort
result
explor
behavior
unit
point
figur
rlsvi
faster
achiev
greater
reward
display
move
averag
game
score
next
attempt
number
attempt
made
attempt
deﬁn
number
shot
level
fail
success
use
one
attempt
explor
record
follow
attempt
without
explor
order
distort
result
explor
behavior
unit
point
model
number
trial
ﬁnish
provid
game
engin
use
one
attempt
regular
rlsvi
howev
outperform
human
player
shall
remain
unnam
given
shame
beat
crude
algorithm
term
maximum
point
achiev
singl
attempt
term
est
level
reach
tabl
summar
maximum
score
attain
highest
level
reach
differ
player
model
featur
algorithm
human
npp
nppo
rlsvi
maximum
perform
level
tabl
maximum
level
score
achiev
regular
rlsvi
outperform
untrain
human
player
score
npp
compar
number
attempt
requir
pa
given
level
provid
interest
insight
especi
later
level
rlsvi
explor
prof
efﬁcient
pass
level
almost
number
attempt
requir
human
player
featur
regular
requir
time
mani
attempt
seen
tabl
algorithm
human
npp
nppo
rlsvi
featur
level
level
level
tabl
averag
number
trial
need
ﬁnish
level
rlsvi
learn
quickli
later
level
pass
level
impress
short
explor
period
npp
conclus
futur
work
rlsvi
bayesian
approach
space
explor
seem
like
promis
rather
intuit
way
navig
unknown
system
previou
work
shown
bayesian
belief
updat
sampl
explor
highli
efﬁcient
ﬁnding
conﬁrm
rlsvi
beat
baselin
algorithm
somewhat
embarrassingli
human
oracl
main
constraint
work
simul
speed
ite
algorithm
converg
could
achiev
would
like
explor
possibl
featur
especi
comput
vein
would
interest
train
deep
neural
network
function
allow
learn
complex
interact
effect
particular
target
obstacl
promis
result
combin
bayesian
explor
deep
reinforc
learn
shown
final
may
interest
explor
differ
angrier
bird
bayesian
reinforc
learn
jerom
friedman
trevor
hasti
robert
tibshirani
element
statist
learn
volum
springer
seri
statist
springer
berlin
volodymyr
mnih
koray
kavukcuoglu
david
silver
alex
graf
ioanni
antonogl
daan
wierstra
martin
riedmil
play
atari
deep
reinforc
learn
arxiv
preprint
itamar
arel
cong
liu
urbanik
kohl
forcement
system
network
trafﬁc
signal
control
intellig
transport
system
iet
bution
assumpt
within
rlsvi
assum
nois
normal
distribut
therefor
sampl
normal
distribut
around
expect
optim
polici
may
particularli
prone
get
stuck
local
minimum
use
bootstrap
method
lieu
bayesian
date
may
prove
power
improv
rlsvi
algorithm
explor
appendix
work
found
nal
code
angri
bird
python
found
http
acknowledg
would
like
thank
teach
team
ing
quarter
refer
estevaofon
angri
bird
python
github
itori
http
januari
fork
http
angrybird
ian
osband
benjamin
van
roy
zheng
wen
aliz
explor
via
random
valu
function
arxiv
preprint
richard
sutton
andrew
barto
reinforc
learn
introduct
volum
mit
press
bridg
ian
osband
dan
russo
benjamin
van
roy
efﬁcient
reinforc
learn
via
posterior
sampl
advanc
neural
inform
process
system
page
ian
osband
benjamin
van
roy
bootstrap
arxiv
preprint
son
sampl
deep
explor
richard
dearden
nir
friedman
stuart
russel
page
bayesian
michael
kearn
satind
singh
forcement
learn
polynomi
time
machin
ing
lesli
pack
kaelbl
michael
littman
drew
moor
reinforc
learn
survey
nal
artiﬁci
intellig
research
page
christoph
jch
watkin
peter
dayan
machin
learn
