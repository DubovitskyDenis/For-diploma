robust
robot
plan
uncertainti
min
chen
emilio
frazzoli
david
hsu
wee
sun
lee
partial
observ
markov
decis
process
pomdp
provid
principl
gener
model
plan
uncertainti
howev
solv
gener
pomdp
putat
intract
worst
case
paper
introduc
subclass
pomdp
hidden
state
variabl
constant
chang
determinist
show
equival
set
fulli
observ
markov
decis
process
index
hidden
paramet
use
model
varieti
interest
robot
task
develop
simpl
bayesian
reinforc
learn
algorithm
solv
model
algorithm
perform
well
model
state
outperform
pomdp
algorithm
show
algorithm
suitabl
condit
introduct
time
must
exploit
imperfect
robot
control
sensor
nois
unexpect
environ
chang
contribut
uncertainti
pose
signiﬁc
challeng
robust
robot
plan
robot
must
explor
order
gain
inform
reduc
uncertainti
inform
achiev
task
object
partial
observ
markov
decis
process
pomdp
provid
principl
gener
framework
balanc
explor
exploit
optim
found
applic
mani
robot
task
rang
navig
manipul
interact
howev
solv
pomdp
exactli
comput
intract
worst
case
rapid
progress
efﬁcient
approxim
pomdp
algorithm
recent
year
remain
challeng
scale
larg
pomdp
complex
dynam
complex
pomdp
lie
system
ic
partial
observ
particularli
conﬂuenc
two
introduc
factor
model
restrict
partial
observ
state
variabl
constant
chang
determinist
may
appear
restrict
power
enough
model
varieti
interest
robot
task
chen
hsu
lee
depart
comput
scienc
nation
univers
singapor
singapor
singapor
frazzoli
laboratori
inform
decis
system
massachusett
institut
technolog
cambridg
usa
chen
support
allianc
research
ogi
smart
graduat
fellowship
frazzoli
support
singapor
nrf
smart
futur
urban
mobil
irg
hsu
support
industri
robot
program
grant
lee
support
aoard
grant
view
conclus
contain
herein
author
interpret
necessarili
repres
ofﬁcial
polici
endors
either
express
impli
air
forc
research
laboratori
govern
unknown
system
paramet
robot
link
mass
unknown
priori
swing
standup
conﬁgur
unknown
type
autonom
drive
robot
vehicl
encount
human
driver
unknown
behavior
uncontrol
trafﬁc
intersect
collabor
task
human
accord
unknown
human
prefer
unknown
goal
assist
agent
help
human
cook
one
sever
dish
without
know
human
intent
advanc
robot
pose
object
task
requir
robot
gather
inform
unknown
quantiti
noisi
observ
achiev
task
object
time
fact
belong
special
case
hidden
variabl
remain
constant
throughout
mainli
focu
special
case
interestingli
famou
tiger
problem
appear
semin
paper
pomdp
also
belong
special
case
small
modiﬁc
tiger
agent
stand
front
two
close
door
tiger
behind
one
door
agent
object
open
door
without
tiger
pomdp
model
state
unknown
tiger
posit
agent
three
action
open
left
door
open
right
door
listen
produc
observ
produc
noisi
observ
tiger
left
tiger
right
correct
probabl
listen
cost
agent
open
door
tiger
get
reward
otherwis
incur
penalti
perform
well
agent
must
decid
optim
number
listen
action
take
open
action
tiger
toy
problem
captur
essenc
robust
plan
uncertainti
trade
gather
inform
exploit
inform
achiev
task
object
origin
tiger
repeat
game
agent
open
door
game
reset
tiger
go
behind
two
door
equal
probabl
chang
game
game
termin
agent
open
door
game
singl
state
variabl
tiger
posit
remain
unchang
game
thu
admit
model
repeat
game
pomdp
equival
set
markov
decis
process
mdp
index
hidden
paramet
key
idea
equival
transform
combin
pomdp
state
observ
form
expand
mdp
state
captur
pomdp
uncertainti
observ
uncertainti
mdp
transit
dynam
tiger
exampl
form
two
mdp
index
null
end
null
end
tiger
left
tiger
right
fig
pomdp
model
tiger
transform
set
two
mdp
node
label
pair
repres
pomdp
state
observ
start
state
null
observ
label
accordingli
special
termin
state
label
end
edg
label
tripl
repres
action
probabl
reach
next
state
action
reward
tiger
posit
left
right
fig
mdp
state
pair
consist
pomdp
state
observ
exampl
mdp
tiger
left
repres
true
tiger
posit
agent
receiv
observ
agent
take
action
probabl
transit
new
state
receiv
observ
see
section
iii
detail
gener
construct
equival
enabl
develop
onlin
algorithm
bayesian
ment
learn
hidden
paramet
valu
known
problem
would
simpli
becom
mdp
algorithm
gather
inform
unknown
hidden
paramet
robot
must
explor
maintain
belief
probabl
distribut
hidden
paramet
follow
intern
reward
approach
bayesian
modiﬁ
mdp
reward
function
order
encourag
explor
time
step
onlin
algorithm
solv
intern
reward
mdp
choos
action
updat
belief
porat
new
observ
receiv
algorithm
simpl
implement
perform
well
task
state
outperform
pomdp
algorithm
furthermor
suitabl
condit
relat
work
pomdp
plan
huge
literatur
see
brief
review
focu
onlin
search
algorithm
time
step
onlin
algorithm
perform
search
pute
best
action
current
belief
robot
execut
action
algorithm
updat
belief
base
observ
receiv
process
repeat
new
belief
next
time
step
onlin
search
algorithm
scale
focus
current
belief
rather
possibl
belief
robot
may
encount
sinc
onlin
algorithm
recomput
best
action
scratch
step
natur
handl
unexpect
environ
chang
without
addit
head
pomcp
despot
fastest
onlin
pomdp
algorithm
avail
today
employ
idea
sampl
futur
conting
pomcp
perform
mont
carlo
tree
search
mct
low
overhead
scale
larg
pomdp
extrem
poor
perform
mct
sometim
overli
greedi
despot
sampl
ﬁxed
number
futur
conting
determinist
advanc
perform
heurist
search
result
search
tree
substanti
improv
perform
bound
also
ﬂexibl
easili
incorpor
domain
knowledg
despot
success
implement
autonom
drive
crowd
also
crucial
compon
system
humanitarian
robot
autom
technolog
challeng
hratc
demin
task
instead
solv
gener
pomdp
take
differ
approach
identifi
structur
properti
enabl
simpler
efﬁcient
algorithm
mix
observ
bayesian
like
markov
decis
process
momdp
also
factor
model
howev
place
restrict
partial
observ
state
variabl
fact
equival
gener
pomdp
everi
pomdp
repres
momdp
vice
versa
hidden
goal
markov
decis
process
hgmdp
hidden
paramet
markov
decis
process
relat
lite
restrict
partial
observ
static
hidden
variabl
work
hgmdp
reli
myopic
heurist
plan
unlik
perform
well
task
need
explor
work
focu
mainli
learn
hidden
structur
data
sever
approach
bayesian
intern
reward
approach
among
success
simpl
perform
well
practic
intern
reward
method
divid
two
main
categori
bayesian
optim
gorithm
optim
respect
true
mdp
provid
strong
theoret
guarante
may
suffer
explor
empir
bayesian
optim
algorithm
optim
respect
optim
bayesian
polici
simpli
tri
achiev
high
expect
total
reward
particular
bayesian
explor
bonu
beb
algorithm
achiev
lower
sampl
complex
mdp
algorithm
howev
beb
requir
dirichlet
prior
hidden
paramet
algorithm
inspir
beb
construct
explor
bonu
differ
allow
arbitrari
discret
prior
use
featur
practic
iii
deﬁnit
special
class
pomdp
terminist
assumpt
partial
observ
variabl
speciﬁc
partial
observ
variabl
lite
static
determinist
dynam
formal
introduc
tupl
set
fulli
observ
state
hidden
paramet
ﬁnite
number
possibl
valu
state
space
cross
product
fulli
observ
state
hidden
paramet
set
action
set
observ
transit
function
speciﬁ
probabl
ing
state
agent
take
action
state
accord
determinist
assumpt
observ
function
speciﬁ
probabl
receiv
observ
take
action
reach
state
reward
function
speciﬁ
reward
receiv
agent
take
action
state
discount
factor
state
unknown
agent
tain
belief
probabl
distribut
state
step
agent
take
action
receiv
new
observ
belief
updat
accord
bay
rule
solut
polici
map
belief
state
action
valu
polici
expect
reward
respect
initi
belief
γtr
denot
state
action
time
optim
polici
highest
valu
belief
state
correspond
optim
valu
function
satisﬁ
bellman
equat
max
equival
transform
set
mdp
section
show
import
properti
lite
model
equival
collect
mdp
index
mdp
model
paramet
tupl
set
state
set
action
transit
function
reward
function
discount
factor
theorem
let
model
equal
collect
mdp
index
proof
proof
theorem
show
equival
ﬁrst
reduc
direct
easi
simpli
treat
part
state
lite
model
remain
part
becom
part
lite
model
without
chang
interest
direct
reduc
let
ﬁrst
consid
case
valu
main
constant
given
model
becom
mdp
model
paramet
consist
dpθi
fig
graphic
model
left
mdp
model
paramet
right
follow
element
state
space
null
null
simpli
mean
observ
receiv
set
action
ident
action
model
transit
function
speciﬁ
probabl
reach
state
take
action
state
sition
observ
probabl
function
lite
model
reward
function
speciﬁ
reward
receiv
agent
take
action
state
discount
factor
graphic
model
fig
show
relationship
model
correspond
mdp
model
paramet
sinc
hidden
paramet
ﬁnite
number
valu
lite
reduc
collect
mdp
index
show
simpl
extens
allow
handl
case
valu
hidden
variabl
chang
minist
key
intuit
determinist
dynam
hidden
variabl
introduc
addit
uncertainti
model
given
initi
valu
hidden
variabl
histori
time
step
valu
hidden
variabl
predict
use
determinist
function
thu
given
initi
valu
hidden
variabl
determinist
function
model
reduc
mdp
model
state
compar
static
case
augment
histori
valu
fulli
captur
rest
mdp
model
similar
static
case
particular
set
action
ident
lite
model
transit
function
reward
tion
discount
factor
sinc
ﬁnite
number
valu
reduc
collect
mdp
index
algorithm
part
present
efﬁcient
model
base
brl
algorithm
solut
brl
problem
polici
map
tupl
belief
state
action
valu
polici
belief
state
given
bellman
equat
mean
reward
function
mean
transit
function
second
line
follow
fact
belief
updat
determinist
optim
bayesian
valu
function
max
optim
action
maxim
right
hand
size
like
optim
polici
origin
lite
problem
optim
bayesian
polici
choos
action
base
affect
next
state
also
base
affect
next
belief
howev
optim
bayesian
polici
comput
intract
instead
explor
updat
belief
step
algorithm
explor
explicitli
modifi
reward
function
word
state
action
pair
reward
bonu
base
much
inform
reveal
reward
bonu
use
algorithm
motiv
observ
belief
get
updat
whenev
inform
hidden
paramet
reveal
thu
use
diverg
two
belief
measur
amount
inform
gain
reward
bonu
deﬁn
formal
follow
deﬁnit
belief
updat
measur
inform
gain
diverg
base
reward
bonu
deﬁn
expect
diverg
current
belief
next
belief
constant
tune
factor
updat
belief
observ
time
step
algorithm
solv
intern
reward
mdp
choos
action
greedili
respect
follow
valu
function
max
reward
bonu
term
deﬁn
deﬁnit
part
ident
equat
except
belief
updat
equat
solv
use
standard
valu
iter
algorithm
time
complex
work
interest
problem
larg
state
space
thu
use
uct
onlin
mdp
solver
achiev
onlin
perform
detail
algorithm
describ
algorithm
algorithm
maximum
step
end
arg
maxa
executeact
pdatebelief
end
analysi
initi
valu
greedili
choos
action
updat
belief
although
algorithm
greedi
algorithm
actual
perform
polynomi
number
time
step
section
present
theoret
result
bound
sampl
complex
algorithm
unless
state
otherwis
proof
lemma
section
defer
appendix
clean
analysi
assum
reward
function
bound
sampl
complex
sampl
complex
measur
number
sampl
need
algorithm
perform
optim
start
deﬁnit
sampl
complex
state
action
pair
deﬁnit
given
initi
belief
target
accuraci
reward
bonu
tune
factor
deﬁn
sampl
complex
function
visit
time
start
belief
correspond
reward
bonu
visit
new
belief
le
declar
known
sampl
time
ceas
updat
belief
sampl
known
state
action
pair
follow
assumpt
theorem
hold
true
gener
assumpt
essenti
say
earlier
tri
pair
inform
gain
give
concret
exampl
illustr
assumpt
lemma
assumpt
reward
bonu
monoton
decreas
state
action
pair
timestep
theorem
let
present
central
theoret
result
bound
sampl
complex
algorithm
respect
optim
bayesian
polici
sampl
complex
let
denot
polici
follow
algorithm
time
let
correspond
state
belief
probabl
least
algorithm
optim
bayesian
polici
time
step
word
algorithm
act
polynomi
number
time
step
although
algorithm
primari
design
discret
prior
theorem
appli
mani
prior
distribut
appli
two
simpl
special
class
provid
concret
sampl
complex
bound
first
show
case
independ
dirichlet
prior
reward
bonu
monoton
decreas
sampl
complex
pair
bound
polynomi
function
case
also
satisﬁ
assumpt
lemma
independ
dirichlet
prior
let
number
time
visit
known
reward
function
independ
dirichlet
prior
transit
dynam
pair
monoton
decreas
rate
sampl
complex
function
strength
algorithm
lie
abil
handl
discret
prior
use
simpl
exampl
discret
prior
unknown
determinist
mdp
show
advantag
state
follow
lemma
intuit
behind
lemma
quit
simpl
sampl
state
action
pair
agent
know
effect
without
nois
lemma
discret
prior
determinist
mdp
let
discret
prior
determinist
mdp
sampl
complex
function
proof
theorem
key
intuit
prove
algorithm
quickli
achiev
time
step
rithm
respect
bayesian
polici
valu
optim
decay
zero
given
enough
sampl
proof
theorem
follow
standard
ment
previou
result
ﬁrst
show
close
valu
act
accord
optim
bayesian
polici
assum
probabl
escap
known
set
small
use
hoeffd
bound
show
escap
probabl
larg
polynomi
number
time
step
begin
proof
follow
lemma
ﬁrst
lemma
essenti
say
solv
intern
reward
mdp
use
current
mean
belief
state
addit
explor
bonu
deﬁnit
lead
valu
function
bayesian
polici
lemma
optimist
let
valu
function
algorithm
valu
function
bayesian
polici
follow
deﬁnit
gener
known
mdp
bayesian
set
mdp
whose
dynam
transit
function
reward
function
equal
mean
mdp
pair
known
set
pair
valu
take
pair
equal
current
valu
estim
deﬁnit
given
current
belief
set
valu
estim
pair
set
known
pair
deﬁn
known
action
mdp
follow
addit
state
action
agent
return
probabl
receiv
reward
ﬁnal
lemma
show
intern
reward
mdp
known
mdp
low
error
set
known
pair
lemma
accuraci
fix
histori
time
step
let
belief
state
set
known
pair
mkt
known
mdp
greedi
polici
respect
current
belief
arg
maxa
mkt
readi
prove
theorem
proof
proof
theorem
let
mkt
describ
lemma
let
see
lemma
mkt
let
denot
event
pair
gener
execut
start
time
step
mkt
mkt
mkt
ﬁrst
inequ
follow
fact
equal
unless
occur
bound
sinc
limit
reward
bonu
still
maintain
optim
second
inequ
follow
deﬁnit
third
inequ
follow
lemma
last
inequ
follow
lemma
fact
precis
optim
polici
ternal
reward
mdp
time
suppos
otherwis
hoeffd
inequ
happen
time
step
probabl
notat
suppress
logarithm
factor
experi
evalu
algorithm
experiment
compar
sever
state
art
algorithm
pomdp
literatur
pomcp
despot
two
success
onlin
pomdp
planner
scale
larg
pomdp
qmdp
myopic
ofﬂin
solver
wide
use
efﬁcienc
sarsop
state
art
ofﬂin
pomdp
solver
help
calibr
best
perform
achiev
pomdp
moder
size
mean
mdp
common
myopic
approxim
bayesian
plan
explor
sarsop
pomcp
despot
use
softwar
provid
author
slight
modiﬁc
pomcp
make
strictli
follow
time
limit
plan
algorithm
mean
mdp
mdp
need
solv
step
use
onlin
mdp
solver
uct
similar
paramet
set
use
pomcp
reward
bonu
scalar
use
algorithm
typic
much
smaller
one
requir
theorem
common
trend
intern
reward
algorithm
tune
ofﬂin
use
plan
ﬁrst
appli
algorithm
two
benchmark
lem
pomdp
literatur
demonstr
scale
abil
algorithm
larger
pomdp
rocksampl
robot
move
grid
contain
rock
may
good
bad
probabl
initi
step
robot
move
adjac
cell
sens
rock
robot
sampl
rock
grid
contain
rock
sampl
rock
give
reward
rock
good
otherwis
move
sampl
produc
observ
sens
produc
observ
set
good
bad
accuraci
decreas
exponenti
robot
distanc
rock
increas
robot
reach
termin
state
pass
east
edg
map
discount
factor
hidden
paramet
properti
rock
remain
constant
thu
problem
model
battleship
ship
place
random
grid
subject
constraint
ship
may
place
adjac
diagon
adjac
anoth
ship
ship
differ
size
goal
ﬁnd
sink
ship
initi
agent
know
conﬁgur
ship
step
agent
ﬁre
upon
one
cell
grid
receiv
observ
ship
hit
otherwis
receiv
observ
reward
per
step
termin
reward
hit
everi
cell
everi
ship
illeg
ﬁre
twice
cell
discount
factor
hidden
paramet
conﬁgur
ship
remain
constant
thu
problem
also
model
result
rocksampl
battleship
shown
tabl
algorithm
except
qmdp
sarsop
ofﬂin
algorithm
run
real
time
second
per
step
result
sarsop
replic
result
test
averag
run
mean
problem
size
larg
algorithm
short
rocksampl
short
battleship
see
tabl
algorithm
achiev
similar
perform
state
art
ofﬂin
solver
problem
size
small
howev
size
problem
increas
ofﬂin
solver
start
fail
algorithm
outperform
onlin
algorithm
final
show
robot
arm
grasp
task
origin
amazon
pick
challeng
simul
view
shown
fig
goal
robot
arm
grasp
cup
shelf
quickli
robustli
robot
know
conﬁgur
exactli
movement
determinist
howev
due
sensor
limit
initi
posit
cup
uncertain
gripper
tabl
perform
comparison
grasp
cup
simul
model
evalu
state
action
observ
pomcp
despot
mean
mdp
continu
continu
return
success
rate
return
success
rate
tactil
sensor
insid
ﬁnger
give
posit
read
inner
part
ﬁnger
get
touch
cup
robot
need
move
around
local
cup
grasp
soon
possibl
usual
model
pomdp
problem
howev
model
algorithm
achiev
much
better
perform
compar
solv
pomdp
introduc
plan
model
task
restrict
movement
gripper
plane
shown
fig
divid
plane
region
rel
gripper
cup
region
gripper
get
touch
cup
move
along
cup
region
gripper
get
touch
cup
move
along
cup
region
gripper
sens
cup
move
singl
direct
gripper
move
along
axi
step
size
reward
movement
region
region
region
gripper
close
open
ﬁnger
reward
pick
cup
give
reward
pick
success
otherwis
compar
algorithm
pomcp
despot
mean
mdp
sinc
qmdp
sarsop
support
continu
state
space
algorithm
test
via
model
evalu
simul
model
evalu
mean
use
plan
model
examin
polici
simul
mean
comput
best
action
use
plan
model
execut
simul
next
state
observ
obtain
result
model
evalu
simul
port
tabl
time
use
onlin
plan
algorithm
second
per
step
run
trial
model
evalu
trial
simul
see
algorithm
achiev
higher
return
success
rate
set
compar
algorithm
conclus
introduc
subclass
pomdp
hidden
variabl
either
static
chang
terminist
equival
set
mdp
index
hidden
paramet
exploit
equival
develop
simpl
onlin
algorithm
bayesian
reinforc
learn
preliminari
experi
suggest
algorithm
outperform
pomdp
solver
larg
model
make
promis
tool
robot
plan
uncertainti
current
implement
experi
algorithm
kinova
mico
robot
object
tabl
perform
comparison
rocksampl
battleship
state
action
observ
qmdp
sarsop
pomcp
despot
mean
mdp
kearn
singh
reinforc
learn
polynomi
time
machin
learn
vol
kocsi
bandit
base
plan
machin
learn
ecml
springer
kolter
explor
polynomi
time
proceed
annual
intern
confer
machin
learn
koval
pollard
srinivasa
polici
decomposit
planar
contact
manipul
uncertainti
int
robot
research
kurniawati
hsu
lee
sarsop
efﬁcient
base
pomdp
plan
approxim
optim
reachabl
belief
space
proc
robot
scienc
system
nikolaidi
ramakrishnan
shah
efﬁcient
model
learn
demonstr
collabor
task
proc
int
conf
interact
ong
png
hsu
lee
plan
tainti
robot
task
mix
observ
intern
journal
robot
research
vol
papadimitri
tsitsikli
complex
markov
decis
process
mathemat
oper
research
vol
poupart
vlassi
hoey
regan
analyt
solut
discret
bayesian
reinforc
learn
proceed
intern
confer
machin
learn
acm
rohmer
singh
frees
versatil
abl
robot
simul
framework
intellig
robot
system
iro
intern
confer
ieee
ross
pineau
paquet
onlin
plan
algorithm
pomdp
artiﬁci
intellig
research
vol
roy
thrun
coastal
navig
mobil
robot
advanc
neural
inform
process
system
mit
press
vol
seiler
kurniawati
singh
onlin
approxim
solver
pomdp
continu
action
space
proc
ieee
int
conf
robot
autom
silver
veness
plan
larg
pomdp
advanc
neural
inform
process
system
smith
simmon
pomdp
algorithm
prove
analysi
implement
proc
conf
uncertainti
artiﬁci
intellig
somani
hsu
lee
despot
onlin
pomdp
plan
regular
advanc
neural
inform
process
system
sondik
optim
control
partial
observ
markov
ce
dissert
stanford
univers
stanford
california
usa
sorg
singh
lewi
reward
approxim
bayesian
reinforc
learn
uai
proceed
confer
uncertainti
artiﬁci
intellig
strehl
littman
increment
learner
formal
guarante
uai
proceed
confer
uncertainti
artiﬁci
intellig
thrun
burgard
fox
probabilist
robot
mit
press
simul
view
simpliﬁ
model
fig
robot
arm
grasp
task
tion
interest
import
investig
extens
handl
larg
observ
action
space
refer
asmuth
littman
nouri
wingat
bayesian
sampl
approach
explor
reinforc
learn
proceed
confer
uncertainti
artiﬁci
intellig
bai
cai
hsu
lee
onlin
pomdp
plan
autonom
drive
crowd
proc
ieee
int
conf
robot
autom
bai
hsu
lee
plan
learn
proc
ieee
int
conf
robot
autom
bandyopadhyay
frazzoli
hsu
lee
ru
motion
plan
algorithm
foundat
robot
int
workshop
algorithm
foundat
robot
wafr
konidari
hidden
paramet
markov
decis
process
semiparametr
regress
approach
discov
latent
task
parametr
arxiv
preprint
fern
natarajan
judah
tadep
model
assist
proc
aaai
conf
artiﬁci
intellig
hsiao
kaelbl
grasp
pomdp
proc
ieee
int
conf
robot
autom
kaelbl
littman
cassandra
plan
act
partial
observ
stochast
domain
artiﬁci
intellig
vol
techin
proof
proof
lemma
proof
let
denot
let
denot
accord
deﬁnit
dirichlet
bution
reward
bonu
term
describ
sampl
complex
proof
lemma
ﬁrst
introduc
notat
use
proof
denot
step
histori
belief
state
time
step
follow
deﬁnit
diverg
reward
function
transit
function
deﬁnit
denot
set
mean
reward
tion
set
mean
transit
function
given
belief
suppos
belief
chang
diverg
denot
base
deﬁnit
introduc
regret
step
histori
belief
updat
step
deﬁnit
given
step
histori
belief
updat
regret
ith
action
deﬁn
deﬁn
γie
total
regret
histori
follow
deﬁnit
measur
extra
valu
reward
bonu
term
use
intern
reward
deﬁnit
given
polici
step
histori
reward
bonu
ith
action
deﬁn
γib
total
extra
valu
reward
bonu
next
lemma
go
bound
regret
use
extra
valu
reward
bonu
lemma
given
step
histori
let
regret
updat
belief
deﬁn
deﬁnit
let
extra
valu
reward
bonu
term
deﬁn
deﬁnit
constant
tun
factor
reward
bonu
proof
begin
proof
show
diverg
reward
function
bound
reward
bonu
chosen
properli
ﬁrst
inequ
follow
fact
bound
triangl
inequ
second
inequ
also
follow
triangl
inequ
bsi
bsi
third
inequ
follow
monoton
assumpt
reward
bonu
similarli
show
diverg
transit
function
bound
reward
bonu
bsi
final
go
show
regret
bound
extra
valu
reward
bonu
total
γie
γib
second
inequ
transform
equat
low
third
inequ
transform
equat
follow
deﬁnit
sinc
arbitrari
equat
histori
min
appli
equat
repeatedli
step
get
min
second
inequ
follow
lemma
prof
lemma
proof
lemma
proof
consid
sequenc
state
action
state
follow
ﬁrst
state
action
pair
gener
suppos
belief
γir
mkt
ﬁrst
inequ
follow
fact
diverg
reward
function
transit
bound
reward
bonu
valu
second
inequ
follow
γib
readi
prove
lemma
proof
proof
lemma
let
see
lemma
initi
belief
state
consid
state
let
new
belief
form
updat
step
max
max
min
min
min
min
ﬁrst
inequ
transform
equat
follow
max
min
max
mkt
