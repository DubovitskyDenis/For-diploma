hierarch
markov
decis
problem
extend
version
supplementari
materi
ander
jonsson
depart
inform
commun
technolog
universitat
pompeu
fabra
roc
boronat
barcelona
spain
abstract
present
hierarch
reinforc
learn
framework
formul
task
hierarchi
special
type
markov
decis
process
bellman
tion
linear
analyt
solut
problem
type
call
mdp
lmdp
ing
properti
exploit
hierarch
set
efﬁcient
learn
optim
valu
function
task
composition
propos
hierarch
approach
also
seen
novel
altern
solv
lmdp
larg
state
space
deriv
hierarch
version
call
algorithm
learn
differ
task
taneous
show
empir
signiﬁcantli
form
learn
method
two
classic
hierarch
reinforc
learn
domain
taxi
domain
autonom
guid
vehicl
task
introduct
hierarch
reinforc
learn
hrl
gener
framework
address
reinforc
ing
problem
exploit
task
action
structur
problem
consid
polici
tempor
extend
action
typic
involv
reduc
subset
state
compon
exampl
maxq
approach
dietterich
decompos
markov
decis
process
mdp
valu
function
hierarchi
smaller
mdp
valu
function
target
mdp
correspond
addit
combin
valu
function
smaller
mdp
anoth
exampl
option
approach
differ
task
learn
simultan
onlin
fashion
sutton
precup
hrl
method
also
use
explain
human
anim
behavior
botvinick
niv
barto
independ
class
stochast
optim
control
lem
introduc
action
cost
tion
restrict
way
make
bellman
equat
linear
thu
efﬁcient
solvabl
todorov
kappen
class
problem
known
crete
set
mdp
lmdp
continu
set
control
alli
control
kappen
opper
optim
control
comput
class
problem
equival
minim
origin
lmdp
formul
consid
singl
action
chang
stochast
law
environ
tern
interpret
adopt
work
sider
stochast
polici
determinist
action
lmdp
mani
interest
properti
exampl
optim
trol
law
lmdp
linearli
combin
deriv
composit
optim
control
law
efﬁcient
todorov
also
power
iter
method
use
solv
lmdp
equival
popular
belief
propag
algorithm
use
probabilist
infer
dynam
graphic
el
kappen
opper
optim
valu
function
lmdp
learn
efﬁcient
use
polici
learn
algorithm
oper
directli
state
space
instead
product
space
state
action
todorov
continu
set
reduc
familiar
quadrat
energi
cost
wide
use
robot
plicat
exampl
applic
includ
robot
igat
kinjo
uchib
doya
motor
skill
inforc
learn
theodor
buchli
schaal
class
problem
also
relev
disciplin
cognit
scienc
decis
make
theori
ton
ortega
braun
howev
eral
applic
lmdp
problem
leng
mainli
due
curs
dimension
yadkori
matsubara
kappen
todorov
paper
propos
combin
hrl
lmdp
framework
formul
reinforc
ing
problem
hierarchi
lmdp
surprisingli
despit
lmdp
introduc
alreadi
ten
year
ago
unifi
framework
combin
methodolog
pose
yet
beneﬁt
combin
one
hand
hrl
problem
express
way
beneﬁt
properti
lmdp
enjoy
exampl
one
use
efﬁcient
altern
hrl
method
anoth
exampl
task
tional
composit
task
learn
cost
given
optim
solut
differ
compos
task
use
task
sever
termin
state
show
later
hand
lmdp
also
eﬁt
hrl
framework
exampl
address
curs
dimension
altern
way
viousli
mention
approach
simultan
learn
hrl
paper
organ
follow
review
hrl
lmdp
section
main
contribut
work
hierarch
formul
lmdp
present
section
empir
illustr
beneﬁt
two
benchmark
section
conclud
work
tion
preliminari
section
introduc
preliminari
notat
ﬁrst
deﬁn
mdp
explain
idea
hind
maxq
decomposit
ﬁnalli
describ
solvabl
mdp
satisfi
mdp
mdp
consist
set
state
set
action
transit
probabl
distribut
pair
expect
reward
function
aim
learn
optim
polici
map
state
action
maxim
expect
futur
reward
mdp
usual
solv
deﬁn
valu
function
estim
expect
futur
reward
state
undiscount
case
optim
valu
obtain
solv
bellman
optim
equat
max
max
bound
optim
valu
function
consid
ﬁrst
exit
problem
deﬁn
set
termin
state
function
deﬁn
ﬁnal
reward
termin
state
altern
valu
function
one
deﬁn
function
estim
expect
futur
reward
pair
bellman
optim
equat
solv
global
use
algorithm
valu
iter
polici
tion
howev
larg
state
space
feasibl
tern
algorithm
make
local
updat
valu
tion
onlin
arguabl
popular
onlin
algorithm
mdp
watkin
given
transit
state
state
take
tion
receiv
reward
make
low
updat
estim
optim
function
max
learn
rate
gener
mdp
includ
action
take
one
complet
case
figur
task
graph
taxi
domain
bellman
optim
equat
becom
max
max
durat
action
expect
reward
appli
last
step
probabl
transit
step
deﬁn
get
max
treat
solv
mdp
maxq
decomposit
maxq
decomposit
dietterich
decompos
mdp
ﬁnite
set
task
root
task
solv
lent
solv
task
consist
termin
set
action
set
task
primit
subtask
primit
task
correspond
action
origin
mdp
deﬁn
alway
ble
termin
one
time
step
everywher
task
appli
state
state
termin
state
produc
correspond
action
set
action
task
maxq
deﬁn
task
graph
task
node
edg
node
action
task
avoid
inﬁnit
recurs
task
graph
acycl
figur
show
simpliﬁ
task
graph
taxi
domain
commonli
use
illustr
maxq
decomposit
aim
maxq
decomposit
learn
archic
polici
separ
polici
individu
task
task
deﬁn
valu
function
state
estim
expect
cumul
reward
termin
reward
associ
appli
action
state
task
equal
valu
rootpickupnavig
putdownnorthsoutheastwest
henc
bellman
optim
equat
decompos
max
probabl
transit
appli
possibl
composit
action
primit
task
correspond
action
origin
mdp
valu
expect
immedi
reward
use
learn
polici
contribut
valu
function
dietterich
propos
onlin
algorithm
maxq
decomposit
call
learn
gorithm
maintain
two
valu
function
task
estim
ˆvi
valu
function
deﬁn
estim
expect
cumul
reward
includ
estim
deﬁn
polici
estim
ˆvi
pass
reward
parent
task
learn
achiev
recurs
iti
polici
local
optim
respect
dietterich
also
show
use
state
abstract
simplifi
learn
maxq
decomposit
mdp
mdp
lmdp
ﬁrst
introduc
todorov
origin
formul
plicit
action
control
consist
chang
predeﬁn
uncontrol
probabl
distribut
next
state
tern
interpret
view
result
probabl
distribut
stochast
polici
determinist
action
todorov
idea
transform
discret
optim
problem
action
continu
optim
problem
transit
probabl
convex
calli
tractabl
formal
lmdp
consist
set
state
uncontrol
transit
probabl
distribut
state
expect
reward
function
given
state
next
state
distribut
deﬁn
set
next
state
problem
lmdp
also
subset
termin
state
control
lmdp
probabl
distribut
next
state
given
next
state
reward
appli
control
state
satisfi
log
reward
associ
state
genc
penal
control
niﬁcantli
differ
typic
random
walk
act
temperatur
paramet
larg
valu
high
temperatur
lead
solut
tic
sinc
deviat
random
dynam
penal
convers
small
valu
low
temperatur
result
determinist
polici
sinc
term
domin
immedi
cost
lmdp
sens
place
determinist
polici
deﬁn
stochast
action
stochast
polici
deﬁn
determinist
action
follow
unless
otherwis
state
next
state
alway
drawn
distribut
deﬁn
bellman
optim
equat
max
log
max
input
satisfi
max
given
state
set
consist
control
bound
valu
absens
discount
factor
termin
state
sorb
introduc
obtain
log
obtain
diverg
side
introduc
normal
term
min
log
min
sert
bellman
equat
min
log
log
term
achiev
minimum
tion
equal
optim
polici
exponenti
bellman
equat
give
write
equat
matrix
form
ωπz
diagon
matrix
term
diagon
transit
probabl
matrix
rive
distribut
unlik
bellman
optim
equat
system
linear
equat
sinc
equat
linear
solv
tor
problem
use
exampl
power
iter
method
altern
todorov
propos
line
learn
algorithm
lmdp
call
lar
mdp
idea
low
trajectori
record
transit
perform
increment
updat
valu
function
sinc
lmdp
explicit
action
transit
consist
state
next
state
reward
record
transit
tain
estim
optim
valu
estim
updat
transit
learn
rate
naiv
sampl
transit
passiv
namic
essenti
amount
random
walk
lead
slow
learn
better
altern
use
tanc
sampl
guid
explor
sampl
transit
inform
distribut
natur
choic
estim
optim
polici
deriv
result
follow
correct
updat
rule
todorov
wˆa
wˆa
note
import
weight
wˆa
requir
ce
passiv
dynam
lmdp
reward
origin
formul
lmdp
reward
depend
develop
hierarch
framework
base
lmdp
account
fact
task
may
accumul
differ
amount
reward
henc
reward
depend
current
state
also
next
state
section
extend
lmdp
reward
expect
ward
function
deﬁn
pair
state
reward
appli
control
state
log
bellman
equat
becom
max
log
max
let
yield
normal
min
log
yield
log
result
polici
exponenti
bellman
equat
give
written
matrix
form
entri
equal
solv
equat
either
appli
power
ation
method
trivial
extend
lmdp
reward
tion
still
triplet
differ
reward
depend
next
state
well
current
state
compar
target
valu
see
updat
rule
tion
caus
tend
toward
optim
valu
use
uncontrol
distribut
sampl
transit
updat
import
sampl
equat
also
directli
appli
lmdp
ward
hierarch
lmdp
section
formal
framework
hierarch
lmdp
base
maxq
decomposit
assum
exist
underli
lmdp
set
task
root
task
task
consist
termin
set
set
subtask
state
absorb
produc
reward
clariti
present
ﬁrst
assum
task
determinist
state
termin
uniqu
state
later
show
extend
hierarch
lmdp
determinist
task
maxq
decomposit
sinc
action
nal
mdp
includ
primit
task
action
set
task
contain
subset
action
analog
hierarch
lmdp
task
contain
subset
allow
transit
origin
lmdp
transit
abil
accord
intuit
optim
control
task
view
stochast
polici
select
determinist
task
determinist
next
state
associ
task
lmdp
reward
task
primit
state
let
subset
next
state
origin
lmdp
also
present
let
set
subtask
applic
termin
state
clearli
primit
given
state
passiv
dynam
immedi
reward
task
lmdp
deﬁn
term
transit
due
transit
due
maxq
decomposit
reward
associ
appli
subtask
state
equal
valu
task
composition
task
subsect
extend
deﬁnit
hierarch
lmdp
task
associ
task
lmdp
import
differ
subtask
one
termin
state
primit
subtask
transit
dress
omit
clariti
thu
deﬁn
passiv
dynam
immedi
reward
primit
subtask
one
termin
state
denot
nal
state
subtask
deﬁn
counterpart
tion
multipl
termin
state
transit
probabl
subtask
state
termin
state
valu
function
express
use
composition
optim
control
law
lmdp
todorov
describ
low
note
differ
immedi
transit
probabl
subtask
total
transit
probabl
subtask
still
distribut
among
possibl
termin
state
ing
termin
state
task
ﬁne
separ
task
new
task
ident
termin
state
differ
reward
task
goal
zero
remain
termin
state
neg
optim
valu
individu
task
use
sition
origin
task
multipl
termin
state
express
weight
sum
individu
task
particular
composit
optim
polici
todorov
consid
optim
polici
mix
weight
compos
task
uniform
equal
sinc
task
assign
termin
state
valu
function
equat
given
log
figur
lmdp
task
graph
taxi
domain
function
tion
associ
subtask
uniform
probabl
transit
probabl
portion
produc
reward
origin
lmdp
task
valu
function
estim
pect
cumul
reward
termin
deﬁn
immedi
reward
task
write
bellman
optim
equat
max
log
task
graph
hierarch
lmdp
deﬁn
maxq
decomposit
acycl
figur
show
lmdp
task
graph
taxi
domain
compar
figur
primit
action
longer
appear
task
new
sink
node
navig
correspond
primit
task
hierarch
lmdp
deﬁnit
implicitli
consid
follow
assumpt
differ
maxq
formul
requir
hierarch
lmdp
first
assum
termin
state
subtask
tualli
exclus
overlap
next
state
reason
lmdp
allow
one
transit
two
state
differ
reward
happen
optim
polici
tiﬁabl
sinc
one
collaps
transit
one
determin
result
immedi
reward
problem
anoth
differ
lmdp
task
need
equival
action
correspond
markov
chain
aperiod
need
vergenc
method
final
unlik
maxq
decomposit
valu
function
equat
includ
term
due
differ
control
uncontrol
dynam
reward
also
includ
subtask
dent
term
consequ
valu
function
root
task
includ
term
task
although
introduc
approxim
trol
rel
import
term
adjust
valu
rootpickupnavig
putdownnorthsoutheastwest
transit
probabl
equat
deﬁn
cursiv
state
deﬁn
hierarch
lmdp
framework
determinist
task
note
individu
task
still
determinist
purpos
avoid
termin
state
differ
hierarch
learn
algorithm
aim
hierarch
lmdp
learn
estim
erarch
control
polici
ˆan
individu
control
polici
ˆai
task
similar
maxq
decomposit
two
main
altern
learn
hierarch
polici
learn
individu
polici
ˆai
separ
fashion
learn
polici
simultan
use
hierarch
ecut
fashion
implement
algorithm
ﬁrst
type
ward
sinc
individu
task
lmdp
ing
power
iter
method
sinc
task
solv
reward
known
ﬁxed
solv
implement
algorithm
second
type
similar
learn
start
root
task
sampl
subtask
execut
use
current
estim
polici
execut
termin
possibl
appli
subtask
along
way
termin
return
control
root
task
anoth
subtask
sampl
use
continu
reach
absorb
state
process
valu
function
estim
task
updat
use
learn
task
differ
learn
two
estim
valu
function
one
estim
ˆvi
optim
valu
tion
exclud
anoth
timat
includ
estim
deﬁn
polici
ˆai
ˆvi
pass
reward
parent
task
hierarch
mdp
aim
learn
separ
polici
individu
task
sinc
gorithm
possibl
use
transit
record
one
task
learn
polici
anoth
task
ing
known
converg
faster
sutton
precup
section
describ
algorithm
learn
describ
subsect
use
import
sampl
improv
explor
let
sition
sampl
use
estim
polici
ˆaj
task
consid
updat
estim
valu
ˆzi
anoth
task
even
though
sampl
distribut
ˆaj
differ
estim
polici
ˆai
consid
updat
equat
ˆzi
ˆzi
ˆzi
wˆai
see
updat
rule
correct
simpli
substitut
express
wˆai
ˆai
ˆzi
ˆzi
ˆzi
ˆai
ˆzi
ˆzi
ˆzi
ˆzi
ˆzi
ˆzi
word
instead
move
ˆzi
direct
ˆzi
updat
rule
move
ˆzi
direct
ˆzi
precis
desir
valu
ˆzi
particular
import
weight
share
differ
task
learn
lmdp
transit
cost
updat
rule
use
substitut
express
wˆai
ˆai
lead
slightli
differ
result
ˆzi
ˆzi
ˆzi
ˆai
ˆzi
ˆzi
ˆzi
ˆzi
ˆzi
ˆzi
recal
expect
ˆzi
result
fact
observ
reward
expect
reward
may
differ
equal
expect
lmdp
transit
cost
ˆzi
desir
valu
ˆzi
state
abstract
hierarch
lmdp
appli
form
state
abstract
maxq
decomposit
dietterich
common
form
state
abstract
project
max
node
irrelev
form
state
abstract
sume
state
factor
domain
state
variabl
max
node
irrelev
identiﬁ
state
variabl
irrelev
given
task
impli
valu
state
variabl
remain
complet
task
irrelev
state
variabl
ignor
learn
valu
function
dietterich
identiﬁ
condit
safe
perform
state
abstract
one
condit
leaf
figur
taxi
problem
result
primit
task
gate
figur
taxi
problem
result
abstract
task
root
irrelev
appli
hierarch
lmdp
sinc
tion
longer
includ
leaf
task
graph
condit
result
distribut
irrelev
appli
hierarch
lmdp
two
state
transit
probabl
respect
given
task
need
estim
singl
valu
ˆvi
state
experi
evalu
propos
framework
two
task
monli
use
hierarch
mdp
taxi
domain
etterich
autonom
vehicl
guid
task
agv
ghavamzadeh
mahadevan
compar
follow
method
use
naiv
sampl
random
walk
without
correct
term
equat
import
sampl
without
learn
equat
import
sampl
task
learn
equat
without
learn
learn
variant
evalu
use
task
lmdp
describ
section
compar
variant
task
lmdp
construct
tradit
mdp
follow
methodolog
todorov
result
tradit
mdp
guarante
optim
valu
function
origin
lmdp
follow
todorov
use
dynam
learn
rate
cay
optim
separ
algorithm
current
trial
paramet
also
optim
best
perform
compar
perform
calcul
iter
differ
learn
optim
valu
function
comput
exactli
task
consid
detail
ment
describ
supplementari
materi
taxi
domain
taxi
domain
deﬁn
grid
four
distinguish
locat
passeng
one
four
locat
passeng
wish
transport
one
three
locat
also
taxi
must
navig
passeng
locat
pick
passeng
navig
destin
locat
put
passeng
use
variant
taxi
domain
dietterich
much
larger
state
space
grid
decompos
taxi
domain
shown
figur
like
dietterich
appli
state
abstract
form
project
navig
task
ignor
passeng
locat
destin
result
state
space
size
navig
full
task
respect
primit
task
navig
contain
state
tion
associ
navig
action
north
south
east
west
idl
action
four
primit
task
one
locat
corner
grid
correspond
lmdp
similar
grid
exampl
todorov
passiv
dynam
random
walk
term
zero
nal
state
correspond
corner
elsewher
figur
show
perform
differ
learn
method
primit
task
domain
best
result
obtain
import
sampl
learn
variant
perform
variant
mainli
unlik
need
maxim
oper
valu
todorov
remark
learn
still
perform
better
learn
naiv
perform
ter
greedi
particular
task
random
explor
still
use
learn
locat
one
corner
grid
full
task
compos
four
possibl
tion
subtask
plu
transit
result
appli
origin
pickup
putdown
action
idl
transit
figur
show
result
full
task
sinc
one
task
learn
pli
case
random
explor
converg
slowli
domain
primit
task
domain
task
figur
agv
problem
result
primit
task
igat
figur
agv
problem
comparison
term
throughtput
curv
indic
also
advantag
import
sampl
le
pronounc
primit
task
result
conclud
propos
tension
outperform
ing
method
domain
autonom
guid
vehicl
agv
domain
second
domain
consid
variant
agv
main
ghavamzadeh
mahadevan
problem
agv
transport
part
machin
hous
differ
part
arriv
warehous
uncertain
time
part
load
warehous
deliv
speciﬁc
machin
process
sembl
machin
termin
avg
pick
assembl
bring
unload
locat
warehous
state
space
full
problem
nine
compon
three
compon
posit
agv
gle
one
type
part
agv
carri
ﬁve
repres
differ
part
avail
pick
warehous
assembl
locat
convert
overal
problem
task
allow
new
part
arriv
warehous
task
assembl
part
deliv
unload
station
detail
see
supplementari
materi
import
featur
problem
avg
navig
use
transit
correspond
itiv
action
forward
stay
unlik
taxi
domain
signiﬁcantli
constrain
trajectori
requir
navig
one
locat
warehous
similar
taxi
domain
ﬁne
six
primit
navig
task
navig
six
dropoff
pickup
locat
warehous
appli
state
abstract
form
project
task
ignor
locat
part
assembl
figur
show
result
differ
learn
method
navig
task
similar
taxi
domain
although
learn
form
compar
better
naiv
perform
compar
wors
latter
result
explain
need
guid
explor
navig
domain
sinc
total
number
state
larg
also
appli
state
abstract
form
result
distribut
irrelev
overal
task
sinc
navig
task
alway
termin
predict
state
necessari
maintain
valu
function
dropoff
pickup
locat
also
implement
onlin
algorithm
similar
learn
instead
use
valu
function
subtask
estim
transit
reward
execut
subtask
termin
record
reward
along
way
reward
accumul
subtask
use
observ
immedi
reward
abstract
task
perform
measur
throughput
ber
assembl
deliv
unload
station
per
time
step
figur
show
rel
perform
import
sampl
variant
omit
naiv
sinc
throughput
random
walk
constant
time
number
time
step
includ
primit
transit
includ
navig
subtask
ﬁgure
show
verg
quickli
suboptim
polici
compar
learn
illustr
beneﬁt
hierarch
lmdp
conclus
present
framework
hierarch
ment
learn
combin
maxq
decomposit
formul
task
mdp
work
illustr
two
domain
erarch
algorithm
outperform
method
hierarch
mdp
domain
primit
task
domain
task
hierarch
markov
decis
problem
supplementari
materi
ander
jonsson
depart
inform
commun
technolog
universitat
pompeu
fabra
roc
boronat
barcelona
spain
experiment
setup
compar
variant
task
lmdp
construct
tradit
mdp
follow
methodolog
todorov
state
ﬁne
symbol
action
transit
probabl
tion
match
optim
comput
use
power
iter
method
origin
lmdp
also
ﬁne
mani
symbol
action
number
possibl
state
follow
transit
probabl
obtain
circular
shift
reward
symbol
action
λkl
result
tional
mdp
guarante
optim
valu
function
origin
lmdp
follow
todorov
use
dynam
learn
rate
decay
constant
optim
separ
algorithm
rent
trial
paramet
also
optim
best
perform
compar
perform
calcul
iter
differ
learn
optim
valu
function
taxi
domain
taxi
domain
deﬁn
grid
four
distinguish
locat
passeng
one
four
locat
passeng
wish
transport
one
three
locat
also
taxi
must
navig
passeng
locat
pick
passeng
navig
destin
locat
put
passeng
use
variant
taxi
domain
dietterich
much
larger
state
space
grid
shown
figur
state
space
compos
horizont
vertic
coordin
taxi
locat
locat
passeng
differ
pickup
locat
passeng
taxi
like
dietterich
appli
state
tion
form
project
navig
task
ignor
passeng
locat
destin
result
state
space
size
navig
full
figur
taxi
domain
use
experi
color
valu
function
composit
task
navig
task
respect
primit
task
navig
compos
state
transit
correspond
navig
action
name
north
south
east
west
idl
action
four
primit
task
one
locat
corner
grid
correspond
lmdp
similar
grid
exampl
passiv
dynam
random
walk
term
zero
termin
state
correspond
corner
elsewher
term
scalabl
bottleneck
algorithm
numer
precis
requir
comput
exact
timal
valu
function
precis
strongli
depend
absolut
differ
maximum
minimum
valu
matrix
order
obtain
good
composit
valu
function
locationsdestin
locat
part
type
avail
warehous
part
type
avail
warehous
convert
overal
problem
task
allow
new
part
arriv
warehous
task
assembl
part
deliv
unload
station
result
task
approxim
state
bottleneck
algorithm
numer
cision
error
calcul
optim
valu
function
use
power
iter
method
refer
bartlett
chen
malek
markov
decis
problem
control
cost
applic
crowdsourc
intern
confer
chine
learn
icml
botvinick
niv
barto
botvinick
niv
barto
hierarch
organ
ior
neural
foundat
reinforc
learn
spectiv
cognit
dietterich
dietterich
state
abstract
maxq
hierarch
reinforc
learn
advanc
neural
inform
process
system
nip
dietterich
dietterich
hierarch
forcement
learn
maxq
valu
function
posit
journal
artiﬁci
intellig
research
friston
friston
schwartenbeck
ald
moutoussi
behren
dolan
anatomi
choic
activ
infer
agenc
tier
human
neurosci
ghavamzadeh
mahadevan
ghavamzadeh
hierarch
averag
reward
mahadevan
journal
machin
learn
reinforc
learn
research
kappen
opper
kappen
opper
optim
control
graphic
model
infer
problem
machin
learn
kappen
kappen
linear
theori
trol
nonlinear
stochast
system
physic
review
letter
kinjo
uchib
doya
kinjo
uchib
doya
evalu
linearli
solvabl
markov
decis
process
dynam
model
learn
mobil
robot
navig
task
frontier
neurorobot
matsubara
kappen
matsubara
kappen
latent
back
leibler
control
system
use
confer
probabilist
graphic
model
uncertainti
artiﬁci
intellig
uai
auai
press
ortega
braun
ortega
braun
thermodynam
theori
figur
agv
domain
use
experi
timat
one
need
set
sufﬁcient
small
mention
todorov
increas
requir
numer
precis
although
obtain
correct
polici
larger
grid
grid
start
numer
lem
sinc
threshold
requir
check
converg
power
iter
method
autonom
guid
vehicl
agv
domain
second
domain
consid
variant
agv
main
ghavamzadeh
mahadevan
problem
agv
transport
part
machin
hous
differ
part
arriv
warehous
uncertain
time
part
load
warehous
deliv
speciﬁc
machin
process
sembl
machin
termin
avg
pick
assembl
bring
unload
locat
warehous
simpliﬁ
origin
problem
reduc
ber
machin
set
process
time
machin
make
task
fulli
observ
see
ure
state
space
full
problem
follow
ponent
coordin
agv
posit
coordin
agv
posit
orient
agv
right
left
num
part
input
buffer
machin
num
part
output
buffer
machin
num
part
input
buffer
machin
num
part
output
buffer
machin
cost
proceed
royal
societi
london
volum
royal
societi
sutton
precup
sutton
precup
learn
tempor
abstract
tion
proceed
intern
confer
machin
learn
icml
morgan
kaufman
theodor
buchli
schaal
theodor
buchli
schaal
gener
path
integr
journal
control
approach
reinforc
learn
machin
learn
research
todorov
todorov
markov
decis
problem
advanc
neural
inform
cess
system
nip
todorov
todorov
composition
optim
control
law
advanc
neural
inform
cess
system
nip
todorov
todorov
efﬁcient
comput
optim
action
proceed
nation
academi
scienc
unit
state
america
todorov
todorov
eigenfunct
imat
method
optim
control
lem
proceed
ieee
symposium
tive
dynam
program
reinforc
learn
watkin
watkin
learn
delay
reward
dissert
king
colleg
bridg
