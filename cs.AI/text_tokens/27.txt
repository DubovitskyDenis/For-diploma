toward
resolv
unidentiﬁ
invers
reinforc
learn
kareem
amin
univers
michigan
amkareem
satind
singh
univers
michigan
baveja
abstract
consid
set
invers
reinforc
learn
irl
learner
extend
abil
tive
select
multipl
environ
observ
agent
havior
environ
ﬁrst
demonstr
learner
experi
transit
dynam
ﬁxed
set
state
action
exist
algorithm
reconstruct
agent
reward
function
fullest
extent
theoret
possibl
requir
small
logarithm
number
experi
trast
result
known
irl
singl
ﬁxed
environ
name
true
reward
function
dament
unidentiﬁ
extend
set
realist
case
learner
may
select
transit
dynam
rather
restrict
ﬁxed
set
environ
may
tri
connect
problem
maxim
inform
deriv
experi
activ
submodular
function
maxim
demonstr
greedi
algorithm
near
optim
logarithm
factor
final
empir
valid
algorithm
environ
inspir
behavior
psycholog
introduct
invers
reinforc
learn
irl
ﬁrst
introduc
russel
concern
problem
fer
unknown
reward
function
agent
behav
optim
markov
decis
process
basic
mulat
problem
ask
given
known
optim
agent
polici
deduc
reward
function
make
optim
mdp
irl
seen
number
applic
ment
autonom
system
autonom
vehicl
oper
even
cooper
human
agent
might
great
diﬃcultli
describ
incent
howev
problem
fundament
almost
studi
involv
behavior
model
consid
mental
psychologist
attempt
understand
intern
motiv
subject
say
mous
consid
market
observ
user
behavior
websit
hope
understand
potenti
consum
valu
variou
oﬀer
note
russel
fundament
complic
goal
irl
imposs
identifi
act
reward
function
agent
behavior
eral
may
inﬁnit
mani
reward
function
tent
observ
polici
ﬁxed
environ
use
terminolog
environ
refer
mdp
without
reward
function
sinc
true
reward
function
fundament
abl
much
previou
work
irl
concern
develop
heurist
prefer
certain
ward
better
explan
behavior
other
contrast
make
sever
major
contribut
ward
directli
resolv
issu
unidentiﬁ
irl
paper
ﬁrst
contribut
separ
caus
unidentiﬁ
three
class
trivial
reward
tion
assign
constant
reward
pair
make
behavior
optim
agent
constant
reward
execut
polici
includ
observ
reward
function
behavior
invari
certain
arithmet
oper
final
behavior
press
observ
polici
may
suﬃcient
distinguish
two
possibl
reward
function
ration
observ
behavior
observ
behavior
could
optim
reward
function
refer
ﬁrst
two
case
unidentiﬁ
sentat
unidentiﬁ
third
experiment
unidentiﬁ
second
contribut
demonstr
represent
unidentiﬁ
unavoid
tal
unidentiﬁ
contrast
previou
method
demonstr
latter
elimin
plete
case
moreov
manner
make
precis
section
argu
way
represent
unidentiﬁ
superﬁci
elimin
experiment
unidentiﬁ
one
arriv
fullest
possibl
character
agent
reward
tion
one
hope
third
contribut
develop
slightli
richer
model
irl
suppos
learner
observ
agent
behav
optim
number
environ
learner
choos
notic
mani
vate
exampl
reason
assum
learner
inde
power
one
ask
oper
vehicl
drive
multipl
terrain
iment
psychologist
might
observ
mous
across
ber
environ
experiment
nize
dynam
maze
one
key
result
right
choic
environ
learner
elimin
experiment
unidentiﬁ
studi
repeat
experiment
irl
two
set
one
learner
omnipot
restrict
environ
present
agent
anoth
restrict
type
environ
learner
present
show
former
case
experiment
unidentiﬁ
elimin
small
number
environ
latter
case
cast
problem
budget
explor
show
number
environ
simpl
greedi
algorithm
approxim
maxim
inform
reveal
environ
close
relat
work
prior
work
irl
mostli
focus
infer
agent
reward
function
data
acquir
ﬁxed
viron
consid
set
learner
activ
select
multipl
environ
explor
use
observ
obtain
environ
infer
agent
reward
studi
model
agent
make
activ
select
environ
irl
set
novel
best
knowledg
previou
applic
activ
learn
irl
sider
set
singl
environ
learner
queri
agent
action
state
inform
reward
prior
work
use
data
collect
multipl
exogen
ﬁxed
environ
predict
agent
behavior
also
applic
method
mdp
adapt
ple
environ
nevertheless
work
attempt
resolv
ambigu
inher
recov
true
reward
irl
describ
irl
pose
problem
result
work
ultim
consid
object
mimick
predict
agent
optim
behavior
perfectli
reason
object
interest
set
identiﬁc
goal
among
mani
reason
may
learner
explicitli
desir
interpret
model
agent
behavior
learner
desir
transfer
learn
reward
function
new
set
econom
literatur
problem
infer
agent
util
behavior
long
studi
head
util
prefer
elicit
model
analyz
markovian
environ
assum
ﬁxed
environ
learner
ask
certain
type
queri
bound
queri
ite
whether
reward
instead
interest
case
learner
make
infer
agent
behavior
extern
sourc
inform
manipul
environ
agent
act
set
preliminari
denot
environ
tupl
ﬁnite
set
state
agent
ﬁnd
ﬁnite
set
action
avail
agent
collect
transit
dynam
repres
matrix
denot
agent
probabl
transit
state
state
select
action
agent
discount
factor
repres
agent
reward
function
vector
indic
undiscount
payout
rive
state
note
joint
choic
markovian
environ
reward
function
ﬁxe
mdp
polici
map
slight
abus
notat
repres
matrix
take
action
chosen
state
let
opt
denot
set
polici
optim
maxim
agent
expect
reward
mdp
consid
repeat
tation
set
suppos
learner
abl
select
sequenc
sequenti
observ
satisfi
opt
unknown
agent
reward
function
call
experi
goal
experiment
output
ward
estim
approxim
true
reward
function
mani
set
assumpt
learner
rectli
observ
agent
full
polici
strong
realist
assumpt
learner
observ
tori
denot
sequenc
pair
drawn
accord
distribut
induc
agent
play
polici
environ
refer
former
feedback
model
polici
observ
set
latter
trajectori
observ
set
fundament
theorem
irl
follow
rewrit
bellman
equat
associ
optim
polici
singl
mdp
note
compon
vector
γpπ
correspond
action
polici
reward
state
theorem
russel
let
arbitrari
environ
opt
γpπ
key
theorem
ici
observ
set
set
reward
function
sistent
observ
optim
polici
precis
satisfi
set
linear
constraint
furthermor
constraint
comput
environ
polici
thu
object
make
recur
refer
set
reward
function
consist
experi
denot
γpπ
rmin
rmax
sinc
intersect
linear
constraint
ﬁne
convex
polytop
fact
later
mic
import
immedi
corollari
theorem
given
sequenc
experi
set
reward
consist
precis
also
think
trajectori
induc
partial
polici
state
visit
trajectori
lar
let
denot
domain
say
two
polici
consist
denot
thu
given
set
reward
consist
tion
precis
γpπ
rmin
rmax
given
sequenc
deﬁn
trajectori
set
state
action
space
inequ
read
tion
hold
standard
hold
compon
identif
section
give
nuanc
tion
mean
identifi
reward
function
argu
multipl
type
uncertainti
volv
identifi
categor
tation
unidentiﬁ
experiment
unidentiﬁ
furthermor
argu
ﬁrst
type
way
ﬁcial
ought
ignor
second
type
elimin
begin
deﬁnit
let
reward
tion
deﬁn
state
space
say
behavior
equival
environ
also
deﬁn
agent
whose
reward
function
behav
ident
agent
whose
reward
function
definit
two
reward
vector
deﬁn
behavior
equival
denot
set
action
transit
dynam
discount
deﬁn
environ
opt
opt
behavior
equival
deﬁn
equival
relat
vector
let
denot
equival
class
deﬁn
manner
intuit
behavior
equival
induc
ident
optim
polici
everi
singl
environ
therefor
realli
diﬀer
reward
function
simpli
diﬀer
represent
incent
observ
behavior
equival
class
variant
multipl
scale
posit
scalar
translat
constant
intuit
easi
see
ad
reward
everi
state
reward
function
aﬀect
agent
simpli
background
reward
agent
get
free
similarli
scale
posit
constant
simpli
chang
unit
use
repres
reward
agent
care
whether
reward
repres
dollar
cent
prove
formal
follow
theorem
theorem
let
denot
vector
compon
equal
proof
first
consid
deﬁn
statement
theorem
fix
environ
action
arbitrari
polici
begin
claim
γpπ
woodburi
formula
matrix
invers
tell
γpπ
γpπ
furthermor
matrix
therefor
γpπ
γpπ
γpπ
γpπ
sinc
must
reward
function
arbitrari
ronment
consid
opt
theorem
know
opt
γpπ
occur
γpπ
sinc
posit
scalar
final
conclud
opt
γpπ
last
condit
impli
opt
theorem
sinc
choic
arbitrari
deﬁnit
conclud
proof
thu
argu
one
reason
reward
function
identiﬁ
trivial
one
classic
irl
problem
consist
represent
reward
function
uncount
number
function
name
behavior
ident
howev
distinguish
tween
function
irrelev
whether
agent
true
reward
function
simpli
matter
unit
use
repres
reward
light
observ
conveni
ical
element
equival
class
stant
reward
function
take
canonic
resent
otherwis
note
way
rem
translat
max
min
care
take
canonic
represent
min
max
min
canonic
consist
behavior
equival
state
follow
theorem
whose
proof
found
appendix
consequ
theorem
use
notat
interchang
refer
equival
class
uniqu
canon
element
theorem
canonic
represent
next
consid
issu
reward
sinc
irl
problem
ﬁrst
formul
serv
singl
experi
ever
determin
agent
reward
function
constant
reward
function
algebra
reason
fact
alway
solut
linear
system
intuit
reason
fact
optim
polici
agent
whose
reward
therefor
consid
agent
whose
true
reward
even
polici
observ
set
furthermor
appear
multipl
experiment
sequenc
experi
also
remain
consid
agent
whose
true
reward
function
crucial
consequ
irl
algorithm
guarante
identifi
necessarili
tiﬁe
reward
function
agent
trivial
reward
function
allow
behav
ili
therefor
may
choos
behav
consist
reward
irl
algorithm
tee
identiﬁc
trivial
reward
therefor
misidentifi
agent
whose
true
reward
lead
follow
revis
deﬁnit
tiﬁcat
account
call
represent
unidentiﬁ
definit
say
irl
algorithm
succe
identiﬁc
observ
behavior
get
subtract
everi
state
divid
figur
observ
agent
behavior
ronment
set
reward
consist
observ
behavior
depict
shade
region
previou
work
concern
design
select
rule
pick
point
region
depict
red
circl
amount
experiment
remov
represent
unidentiﬁ
set
depict
darker
shade
region
nevertheless
ad
constraint
induc
second
experi
disprov
origin
remov
experiment
unidentiﬁ
agent
true
reward
algorithm
output
whenev
notic
deﬁnit
accomplish
two
thing
first
excus
algorithm
decis
sent
word
assert
salient
task
irl
comput
member
liter
secondli
true
reward
function
constant
demand
algorithm
identifi
tation
decis
howev
agent
realli
reward
function
algorithm
allow
output
anyth
word
algorithm
allow
behav
arbitrarili
agent
behav
also
note
deﬁnit
relax
give
notion
approxim
identiﬁc
state
definit
say
irl
algorithm
reward
function
observ
behavior
agent
true
reward
algorithm
output
whenev
even
deﬁnit
may
attain
singl
experi
may
contain
multipl
behavior
class
call
phenonmenon
experiment
tiﬁabl
due
fact
experi
may
simpli
insuﬃci
distinguish
next
section
observ
sourc
uncertainti
reward
function
decreas
multipl
experiment
depict
figur
see
tion
detail
word
distinguish
tation
unidentiﬁ
experiment
unidentiﬁ
formal
resolv
latter
concret
exampl
given
figur
pict
squar
repres
state
ﬁgure
thick
line
repres
impenetr
wall
agent
polici
depict
arrow
circl
indic
agent
decid
stay
grid
locat
goal
learner
infer
reward
state
ure
depict
agent
polici
take
shortest
path
locat
label
ing
locat
one
explan
behavior
depict
comment
practic
matter
one
usual
interest
ration
behavior
agent
believ
figur
agent
polici
ﬁxed
environ
agent
move
one
four
direct
stay
locat
repres
black
circl
thick
purpl
line
repres
impass
wall
experi
reveal
get
requir
step
get
requir
fewer
agent
prefer
figur
agent
larg
reward
state
zero
reward
everi
state
howev
equal
possibl
explan
state
also
give
posit
reward
smaller
exist
shortest
path
also
pass
agent
take
depict
figur
without
addit
format
two
explan
distinguish
exampl
experiment
unidentiﬁ
nevertheless
resolv
addit
tion
observ
agent
environ
pict
figur
learner
infer
inde
reward
state
final
observ
agent
behavior
environ
figur
reveal
agent
prefer
travel
state
get
requir
step
get
requir
step
fewer
sequent
observ
allow
learner
relat
agent
reward
state
agent
reward
state
omnipot
experiment
set
consid
repeat
experiment
set
environ
avail
select
iment
complet
unrestrict
formal
ronment
select
experiment
belong
class
contain
environ
everi
feasibl
set
transit
dynam
call
nipot
experiment
set
describ
algorithm
omnipot
menter
set
use
log
periment
omnipot
experiment
extrem
power
result
demonstr
guarante
tain
repeat
irl
set
far
stronger
avail
standard
irl
set
thermor
clariﬁ
distinct
experiment
unidentiﬁ
represent
unidentiﬁ
figur
typic
environ
second
phase
rithm
dot
line
repres
transit
action
solid
line
repres
transit
action
omnipot
identiﬁc
algorithm
algorithm
proce
two
stage
volv
simpl
binari
search
ﬁrst
stage
identifi
state
smin
smax
smin
rmin
smax
rmax
second
stage
identiﬁ
αsrmin
rmax
throughout
gorithm
make
use
two
agent
action
denot
therefor
describ
algorithm
assum
environ
select
algorithm
fulli
determin
choic
fact
omnipot
experiment
ting
one
reduc
set
make
remain
action
equival
either
ﬁrst
address
task
identifi
smax
suppos
two
candid
smax
key
idea
ﬁrst
stage
algorithm
give
agent
lute
choic
two
state
set
set
agent
select
reveal
agent
select
reveal
test
conduct
distinct
pair
state
singl
experi
thu
given
candid
smax
singl
experi
narrow
set
candid
guarante
one
remain
state
satisﬁ
rmax
log
experi
identifi
singl
state
smax
satisﬁ
smax
conduct
analog
procedur
identiﬁ
state
smin
smin
smax
identiﬁ
take
remain
state
consid
environ
transit
dynam
parameter
typic
environ
phase
depict
figur
environ
set
smin
smax
sink
smin
smin
smax
smax
smin
smin
smax
smax
remain
smin
αsi
smax
αsi
take
action
state
repres
probabl
gambl
best
worst
state
nalli
also
set
take
action
state
repres
receiv
sure
ing
agent
reveal
αsrmin
rmax
choic
reveal
αsrmin
rmax
thu
binari
search
conduct
independ
order
determin
srmin
approxim
rmax
algorithm
succe
summar
follow
theorem
proof
theorem
straightforward
analysi
binari
search
possibl
set
transit
namic
set
arbitrarili
theorem
let
deﬁn
let
smin
smax
smin
smax
identiﬁ
describ
true
reward
function
canon
form
takeaway
set
problem
ing
identiﬁc
irl
circumv
repeat
experiment
thought
even
polici
vation
irl
question
fundament
ever
see
repeat
experiment
fact
possibl
identifi
arbitrari
precis
deﬁn
sens
result
inform
believ
unrealist
imagin
learner
trarili
inﬂuenc
environ
agent
next
section
develop
theori
repeat
experiment
learner
restrict
select
environ
restrict
subset
possibl
transit
dynam
restrict
experiment
set
consid
set
experiment
restrict
univers
environ
choos
need
contain
everi
possibl
transit
dynam
sumption
requir
execut
binari
search
algorithm
previou
section
best
experiment
could
ever
hope
tri
everi
environ
give
experiment
avail
inform
agent
reward
function
thu
interest
imiz
inform
gain
experiment
minim
number
experi
conduct
tice
observ
agent
may
expens
hard
come
even
small
budget
experi
learner
would
like
select
environ
maxim
reduc
experiment
unidentiﬁ
sequenc
experi
observ
know
consist
observ
sequenc
thu
valu
repeat
tation
allow
learner
select
environ
inform
possibl
contrast
note
previou
work
irl
larg
focus
design
heurist
select
problem
pick
ﬁxed
set
equal
possibl
reward
function
thu
interest
make
small
irl
tradit
focus
select
exogen
ﬁxed
deﬁn
mean
small
review
preexist
method
select
gener
select
heurist
standard
set
given
viron
observ
polici
learner
must
make
select
among
one
reward
tic
suggest
motiv
idea
given
state
reward
function
maxim
ferenc
observ
action
state
action
give
strongest
planat
behavior
observ
agent
thu
reason
linear
select
criterion
maxim
sum
diﬀer
across
state
ad
regular
term
encourag
select
reward
function
also
spars
put
togeth
standard
select
heurist
irl
select
maxim
γpπ
min
two
natur
candid
gener
lection
rule
repeat
experiment
set
instead
singl
experi
experiment
counter
sequenc
observ
ﬁrst
sum
environ
state
pair
minimum
diﬀer
action
select
agent
action
second
sum
state
take
minimum
environ
action
pair
one
could
make
argument
motiv
ultim
object
heurist
howev
argu
strong
algorithm
reason
prefer
ter
object
particular
former
object
grow
dimension
environ
ad
quickli
result
intract
dimens
object
latter
equat
howev
remain
maxim
min
select
rule
set
generaliz
repeat
tation
set
includ
heurist
inﬁnit
state
ting
trajectori
heurist
well
approach
alreadi
adapt
multipl
environ
due
space
straint
discu
foundat
approach
goal
simpli
emphas
dichotomi
adapt
irl
method
data
gather
multipl
environ
howev
data
gener
problem
best
select
environ
begin
latter
problem
focu
next
section
adapt
experiment
given
univers
candid
environ
ask
select
small
number
environ
environ
maxim
inform
must
ﬁrst
decid
mean
propos
set
experi
either
polici
jectori
set
natur
object
minim
mass
result
space
possibl
reward
respect
measur
distribut
lebesgu
measur
uniform
distribut
correspond
natur
goal
reduc
volum
much
possibl
thu
deﬁn
volµ
ﬁnd
conveni
cast
maxim
problem
therefor
also
deﬁn
volµ
equat
standard
form
requir
translat
min
constraint
thu
number
constraint
grow
number
experi
demonstr
experiment
result
tractabl
solver
upper
bound
volum
rmax
goal
maxim
object
sever
desir
properti
first
foremost
reduc
volum
elimin
space
possibl
reward
function
experiment
tiﬁabl
secondli
repeat
experiment
set
fundament
activ
learn
set
think
true
unknown
function
label
ment
either
correspond
polici
trajectori
thu
volum
oper
correspond
reduc
version
space
possibl
reward
furthermor
see
later
section
object
monoton
modular
function
assumpt
activ
learn
literatur
allow
prove
guarante
greedi
algorithm
final
normal
think
lebesgu
measur
vol
volum
euclidean
space
uniform
distribut
rmin
ever
choic
make
object
quit
gener
exampl
make
uniform
vol
spond
count
number
reward
respect
metric
mani
set
natur
come
discret
space
corner
hypercub
readili
model
correct
choic
fact
thought
simpli
prior
rmax
readi
describ
simpl
algorithm
adapt
select
environ
attempt
ili
maxim
depict
algorithm
algorithm
greedi
environ
select
input
arg
max
min
min
observ
polici
end
return
order
state
perform
guarante
rithm
use
fact
submodular
decreas
function
subset
environ
observ
pair
set
possibl
observ
lemma
submodular
function
proof
given
set
compon
use
denot
union
singleton
set
let
set
possibl
observ
trajectori
trajectori
set
polici
polici
set
let
space
possibl
environ
fix
deﬁnit
vol
establish
submodular
sinc
trari
second
equal
zero
also
monoton
vol
perform
algorithm
function
mani
experi
attempt
thu
analysi
must
polici
observ
trajectori
observ
figur
plot
display
error
predict
vector
polici
observ
set
bar
indic
standard
error
plot
display
trajectori
set
take
account
let
determinist
algorithm
deploy
experi
perform
depend
true
reward
polici
observ
say
sequenc
experi
consist
choos
environ
observ
quenc
experi
ther
trajectori
polici
consist
denot
set
consist
experi
best
manc
algorithm
guarante
experi
optn
maxan
minr
submodular
allow
prove
greedi
environ
select
need
slightli
experi
logarithm
factor
attain
optn
theorem
return
greedi
environ
lection
algorithm
satisﬁ
optn
proof
theorem
us
mani
techniqu
use
guillori
work
interact
set
cover
technic
reason
state
theorem
directli
corollari
result
assum
ﬁnite
hypothesi
class
wherea
inﬁnit
space
sibl
reward
nevertheless
proof
easili
adapt
set
full
proof
given
appendix
final
note
line
comput
exactli
without
parametr
assumpt
class
ment
space
reward
practic
scribe
next
section
approxim
exact
mizat
sampl
environ
reward
optim
sampl
set
experiment
analysi
deploy
techniqu
discuss
set
strate
maxim
inde
eﬀect
fy
imagin
agent
trajectori
set
one
would
replac
mizat
opt
line
algorithm
minim
consist
opt
drop
grid
world
experiment
would
like
infer
agent
reward
space
grid
ine
experiment
power
construct
wall
agent
environ
altern
fer
environ
maze
motiv
valu
repeat
experiment
recal
figur
restrict
environ
learner
learner
exampl
make
action
caus
agent
travel
bottom
corner
maze
top
corner
howev
learner
modifi
dynam
environ
far
construct
maze
wall
evalu
algorithm
grid
size
agent
reward
given
vector
rmax
rmax
taken
follow
simul
randomli
assign
state
reward
rmax
assign
state
reward
remain
state
give
reward
agent
discount
rate
taken
goal
learner
mine
state
reward
determin
latter
state
yield
reward
former
figur
display
main
experiment
result
four
diﬀer
algorithm
polici
observ
set
figur
trajectori
set
error
repres
algorithm
predict
error
bar
repres
standard
error
simul
figur
horizont
line
display
best
sult
achiev
without
repeat
experiment
learner
select
singl
environ
observ
ici
stuck
whatev
experiment
iti
exist
scenario
select
accord
classic
irl
heurist
given
section
choic
sinc
perform
method
depend
ment
use
choic
randomli
gener
diﬀer
environ
ment
select
evalu
approach
motiv
one
might
think
agent
mous
reward
correspond
food
pellet
variou
shini
object
mous
cage
simul
best
error
among
diﬀer
algorithm
display
horizont
line
immedi
see
experiment
biliti
use
singl
environ
make
diﬃcult
distinguish
actual
reward
function
err
best
choic
greater
remain
algorithm
describ
greater
detail
conduct
repeat
experiment
algorithm
us
diﬀer
rule
select
new
environ
round
given
sequenc
ment
polici
pair
gener
algorithm
solv
end
round
done
choic
algorithm
besid
greedi
algorithm
previou
section
implement
two
algorithm
conduct
repeat
experi
randuniform
round
select
maze
uniformli
random
space
possibl
maze
wall
present
probabl
note
randuniform
tend
select
maze
roughli
half
wall
present
thu
also
consid
randvari
round
select
maze
diﬀer
distribut
maze
drawn
gener
process
first
row
column
select
number
uniform
distribut
wall
along
row
column
respect
creat
probabl
respect
although
probabl
particular
wall
present
still
correl
creat
variabl
maze
allow
entir
row
spars
popul
wall
implement
algorithm
greedi
previou
tion
approxim
maxim
line
rithm
approxim
done
sampl
ment
distribut
use
randvari
polici
observ
set
sampl
ﬁrst
drawn
consist
set
use
sampler
mcmc
method
uniformli
sampl
dimension
convex
set
polynomi
time
sampl
also
use
estim
volum
jectori
set
ﬁrst
sampl
trajectori
ment
use
arbitrari
proxi
examin
result
see
greedi
converg
niﬁcantli
quicker
either
approach
round
experiment
polici
tion
set
greedi
attain
error
best
approach
attain
greedi
requir
round
reach
similar
error
note
perform
greedi
seem
continu
improv
adapt
approach
appear
stagnat
could
due
fact
certain
number
round
adapt
approach
receiv
inform
abl
environ
typic
sampl
tribut
order
make
progress
must
receiv
new
inform
contrast
greedi
design
tive
select
environ
final
greedi
run
select
sequenc
ment
result
observ
select
use
thu
regular
paramet
free
paramet
greedi
took
equal
result
figur
conclud
mental
analyz
sensit
greedi
choic
paramet
well
randuniform
randvari
also
select
accord
increas
eventu
optim
take
set
begin
occur
begin
see
patholog
behavior
figur
problem
occur
standard
irl
one
approach
select
larg
lambda
transit
henc
choic
howev
even
signiﬁcantli
smaller
result
qualit
similar
figur
figur
ﬁnd
long
larg
result
sensit
choic
figur
result
repeat
experiment
algorithm
use
larg
small
regular
paramet
conclus
provid
number
contribut
work
first
separ
caus
unidentiﬁ
irl
problem
two
class
represent
experiment
argu
represent
unidentiﬁ
superﬁci
lead
redeﬁn
problem
identiﬁc
irl
accord
deﬁnit
previou
work
distinguish
two
class
demonstr
algorithm
design
elimin
iment
unidentiﬁ
provid
formal
guarante
along
way
deriv
new
model
irl
learner
observ
behavior
multipl
environ
model
believ
interest
right
also
key
elimin
experiment
unidentiﬁ
give
algorithm
power
learner
serv
agent
behavior
environ
show
algorithm
agent
reward
deﬁn
state
observ
behavior
log
environ
weaken
learner
model
realist
set
learner
might
restrict
type
ronment
may
choos
may
abl
elicit
small
number
demonstr
agent
deriv
simpl
adapt
greedi
algorithm
select
nearli
optim
respect
reduc
volum
possibl
reward
function
set
environ
valu
solut
found
greedi
algorithm
parabl
optim
algorithm
us
logarithm
factor
fewer
number
experi
final
implement
algorithm
simpl
maze
viron
nevertheless
demonstr
valu
inat
experiment
unidentiﬁ
signiﬁcantli
form
method
attempt
perform
irl
singl
environ
refer
abbeel
coat
quigley
applic
reinforc
learn
aerobat
helicopt
ﬂight
advanc
neural
inform
process
system
abbeel
apprenticeship
learn
via
invers
reinforc
learn
proceed
intern
confer
machin
learn
page
acm
chajewska
koller
parr
make
ration
decis
use
adapt
util
elicit
page
coat
abbeel
learn
control
multipl
demonstr
proceed
intern
confer
machin
learn
page
acm
approach
apprenticeship
learn
advanc
neural
inform
process
system
page
von
neumann
morgenstern
theori
game
econom
behavior
anniversari
commemor
edit
princeton
univers
press
ziebart
maa
bagnel
dey
maximum
entropi
invers
reinforc
learn
aaai
page
appendix
proof
theorem
theorem
coat
abbeel
apprenticeship
canonic
represent
learn
helicopt
control
commun
acm
golovin
kraus
adapt
submodular
new
approach
activ
learn
stochast
optim
colt
page
guillori
bilm
interact
submodular
set
cover
proceed
intern
confer
machin
learn
lope
melo
montesano
activ
learn
reward
estim
invers
reinforc
learn
machin
learn
knowledg
discoveri
databas
page
springer
mix
fast
mathemat
program
russel
algorithm
invers
reinforc
learn
icml
page
ramachandran
amir
bayesian
invers
reinforc
learn
urbana
ratliﬀ
bagnel
zinkevich
maximum
margin
plan
proceed
intern
confer
machin
learn
page
acm
regan
boutili
reward
elicit
markov
decis
process
proceed
confer
uncertainti
artiﬁci
intellig
page
auai
press
regan
boutili
robust
polici
comput
mdp
use
nondomin
polici
aaai
regan
boutili
elicit
addit
reward
function
markov
decis
process
ijcai
joint
confer
artiﬁci
intellig
volum
page
rothkopf
dimitrakaki
prefer
elicit
invers
reinforc
learn
machin
learn
knowledg
discoveri
databas
page
springer
smart
kaelbl
eﬀect
reinforc
learn
mobil
robot
robot
autom
proceed
icra
ieee
intern
confer
volum
page
ieee
sy
schapir
one
proof
deﬁnit
canonic
represent
reward
function
attain
scale
translat
therefor
theorem
canonic
therefor
direct
suppos
respect
ize
theorem
thu
prove
theorem
suﬃcient
argu
behavior
equival
forward
show
behavior
equival
thu
focu
case
consid
three
case
first
suppos
state
diﬀer
state
without
loss
gener
pose
furthermor
let
sider
environ
two
action
action
determinist
transit
state
state
action
determininst
transit
state
state
let
polici
alway
take
howev
opt
tion
opt
mean
therefor
polici
opt
thu
opt
opt
behavior
equival
next
suppos
diﬀer
state
analag
previou
case
suppos
without
loss
gener
state
deﬁn
environ
way
previou
case
time
opt
let
exist
sinc
opt
final
suppos
share
maxim
minim
reward
state
exist
let
state
let
state
without
loss
gener
suppos
let
environ
two
action
let
real
number
everi
state
action
transit
state
probabi
state
remain
probabl
everi
state
action
transtion
state
determinist
reward
take
action
state
either
reward
function
action
give
reward
conclud
proof
thu
opt
πap
opt
proof
greedi
perform
given
set
compon
use
denot
union
singleton
set
begin
redeﬁn
volµ
volµ
upper
bound
volµ
rmax
let
set
possibl
observ
trajectori
trajectori
set
polici
polici
set
let
space
possibl
environ
wwe
ﬁrst
establish
inde
submodular
lemma
submodular
function
proof
fix
deﬁnit
vol
vol
establish
submodular
sinc
arbitrari
second
equal
also
monoton
let
denot
set
function
map
environ
observ
overload
suppos
environ
label
accord
consid
algorithm
know
select
fewest
number
environ
given
algorithm
deﬁn
gener
identiﬁc
cost
identiﬁ
possibl
label
strategi
particular
gicα
max
min
recal
deﬁnit
main
bodi
optn
maxan
min
min
largest
algorithm
guarante
make
environ
environ
sistent
label
let
algorithm
fy
max
lemma
gicoptn
proof
fix
consid
two
case
first
pose
exist
inconsist
label
deﬁntion
optn
sinc
lemma
proven
otherwis
must
sistent
label
deﬁnit
optn
run
label
provid
guarante
result
sequenc
environ
satisfi
optn
wit
given
environ
true
reward
let
denot
set
possibl
observ
either
polici
trajectori
set
lemma
optn
ist
environ
min
min
proof
suppos
everi
environ
exist
optn
let
deﬁn
arg
min
optn
deﬁnit
gic
min
gicoptn
exist
set
environ
gicoptn
optn
monoton
know
optn
let
howev
despit
optn
repeatedli
pli
submodular
appli
equat
impli
optn
optn
γoptn
optn
establish
contradict
prove
main
theorem
theorem
return
greedi
environ
lection
algorithm
satisﬁ
optn
proof
let
denot
subsequ
consist
ﬁrst
environ
observ
pair
optn
noth
prove
otherwis
appli
lemma
deﬁnit
algorithm
know
optn
impli
optn
optn
use
fact
conclud
optn
optn
exp
appli
lemma
substitut
complet
proof
