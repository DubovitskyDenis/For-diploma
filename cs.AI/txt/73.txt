random
forest
based
approach
concept
drift
handling
aleksei
zhukov1,2
denis
sidorov1,2
aoife
foley3
energy
systems
institute
ras
irkutsk
russia
dsidorov
isem.irk.ru
irkutsk
state
university
irkutsk
russia
zhukovalex13
gmail.com
queens
university
belfast
belfast
a.foley
qub.ac.uk
abstract
concept
drift
potential
smart
grid
analysis
socio-economic
behaviour
consumers
governed
laws
physics
likewise
also
applications
wind
power
forecast-
ing
paper
present
decision
tree
ensemble
classiﬁcation
method
based
random
forest
algorithm
concept
drift
weighted
majority
voting
ensemble
aggregation
rule
employed
based
ideas
accuracy
weighted
ensemble
awe
method
base
learner
weight
case
computed
sample
evaluation
using
base
learners
accuracy
intrinsic
proximity
measure
random
forest
algo-
rithm
exploits
temporal
weighting
samples
ensemble
pruning
forgetting
strategy
present
results
empirical
comparison
method
оriginal
random
forest
incorporated
replace-the-
looser
forgetting
andother
state-of-the-art
concept-drﬁt
classiﬁers
like
awe2
keywords
machine
learning
decision
tree
concept
drift
ensemble
learn-
ing
classiﬁcation
random
forest
introduction
ensemble
methods
classiﬁcation
brieﬂy
ensembles
employ
various
learn-
ing
algorithms
obtain
better
predictive
accuracy
comparing
individual
classiﬁers
ensembles
much
used
research
research
de-
voted
stationary
environments
complete
datasets
available
learning
classiﬁers
transfer
functions
dynamical
systems
changing
time
goes
real
world
applications
e.g
power
engineering
learning
algorithms
supposed
work
dynamic
environments
data
continuously
generated
form
stream
necessarily
equally
spaced
time
intervals
data
stream
processing
commonly
relies
single
scans
training
data
implies
restrictions
memory
time
changes
caused
dynamic
environments
e.g
consumer
behaviour
future
smart
grids
categorised
sudden
gradual
concept
drift
subject
appearance
novel
classes
stream
rate
changing
deﬁnitions
classes
one
generally
eﬀective
ensemble
classiﬁer
random
forest
algorithm
employs
bagging
principles
random
subspace
method
build
highly
decorrelated
ensemble
decision
trees
attempts
adapt
random
forest
handling
concept-drift
approach
fully
discovered
objective
paper
propose
novel
approach
classiﬁcation
adapt
concept
drifts
novel
classifcation
method
applied
consumer
behaviour
future
smart
grids
predict
classify
random
behaviour
humans
interact
smart
appliances
home
charging
discharging
electric
vehicles
likrwise
methods
used
forecast
windpower
indeed
solar
power
propose
compute
base
learner
weight
samples
evaluation
using
base
learners
accuracy
intrinsic
proximity
measure
random
forest
noted
concept
drifting
related
non-stationary
dynami-
cal
systems
modelling
completely
supervised
learning
employed
using
special
set
input
test
signals
details
concerning
integral
dynamical
models
theory
applications
refer
monograph
bibli-
ography
paper
organised
ﬁve
sections
section
brief
review
ensemble
streaming
classiﬁers
based
random
forest
given
section
delivers
detailed
presentation
proposed
algorithm
section
experiments
machine
learning
tracking
tasks
provided
finally
paper
concludes
section
main
results
discussed
related
work
use
drifting
concepts
huge
datasets
analysis
unfamiliar
machine
learning
systems
identiﬁcation
communities
work
restrict
considering
decision
tree
ensamble
classiﬁcation
methods
let
brieﬂy
discuss
methods
related
proposal
employed
experiments
detailed
overview
results
area
in-
cluding
online
incremental
ensembles
readers
may
refer
monograph
review
bayesian
logistic
regression
used
handle
drifting
con-
cepts
terms
dynamical
programming
concept
drifting
handling
close
methodology
on-line
random
forest
algorithm
ideas
on-line
bagging
extremely
randomised
forests
on-line
decision
tree
growing
proce-
dure
employed
accuracy
weighted
ensemble
awe
approach
proposed
main
idea
train
new
classiﬁer
incoming
dataset
use
evaluate
existing
classiﬁers
ensemble
incorporate
idea
approach
massive
online
analysis
moa
framework
one
popular
benchmarks
testing
online
classiﬁcation
clusterization
regression
algorithms
written
java
java
software
contains
state
art
clas-
siﬁers
concept
drift
handling
sea
online
bagging
analysis
ensemble
methods
decision
tree
base
learners
interesting
topic
addressed
paper
recursive
nature
decision
trees
makes
on-line
learning
diﬃcult
task
due
hard
splitting
rule
errors
corrected
tree
proximity
driven
streaming
random
forest
paper
authors
clearly
demonstrated
classiﬁer
ensemble
outperform
single
classiﬁer
presence
concept
drifts
base
classiﬁers
ensemble
adherence
weighted
error
similar
current
testing
samples
propose
another
approach
exploits
random
forest
properties
produce
novel
algorithm
capable
handling
concept
drift
following
questions
need
answered
adapt
original
random
forest
data
streaming
deﬁne
sample
similarity
metric
choose
base
classiﬁer
weighting
function
choose
forgetting
strategy
questions
considered
following
subsections
3.1
streaming
classiﬁer
based
random
forest
methodologically
ensemble
approaches
allow
concept-drift
handled
following
ways
base
classiﬁer
adaptation
changing
training
dataset
bootstrap
rsm
ensemble
aggregation
rule
changing
chang-
ing
structure
ensemble
pruning
growing
paper
propose
proximity
driven
streaming
random
forest
pdsrf
exploit
combina-
tions
approaches
besides
methods
already
incorporated
original
random
forest
contrary
conventional
algorithms
use
weighted
majority
voting
aggregation
rule
ensemble
allows
adapt
entire
classiﬁer
changing
weights
base
learners
order
obtain
classiﬁers
weight
estimation
store
samples
purpose
use
sliding
windows
approach
used
periodicaly
updated
ran-
dom
forest
length
window
ﬁxed
estimated
cross-validation
random
forest
uses
unpruned
cart
trees
instances
try
atributes
chosen
decision
tree
node
average
compu-
tational
cost
ensemble
building
tryn
log2n
number
trees
unsiﬃcient
online
applications
reduce
comlexity
use
randomization
approach
proposed
extremely
randomized
trees
implementation
split
set
consists
randomly
generated
splits
best
one
choosen
minimization
gini-index
measure
tryn
logn
cost
complexity
achieved
3.2
sample
similarity
metric
employ
assumption
base
classiﬁers
make
similar
errors
sim-
ilar
samples
even
concept-drift
conventional
random
forest
exploy
called
proximity
measure
uses
tree
structure
obtain
similarity
following
way
two
diﬀerent
sample
terminal
node
prox-
imity
increased
one
end
proximities
normalised
dividing
number
trees
3.3
base
classiﬁer
weighting
function
following
awe
approach
proposed
use
error
rate
produce
weights
classiﬁers
new
block
testing
error
i-th
classiﬁer
small
parameter
3.4
forgetting
strategy
one
main
problems
concept-drifting
learning
select
proper
for-
getting
strategy
forgetting
rate
classiﬁer
adaptive
enough
handle
changes
case
diﬀerent
strategies
appropriate
diﬀerent
types
drift
example
sudden
gradual
drifts
paper
focus
gradual
changes
propose
two
diﬀerent
ways
handle
concept-drift
temporal
sample
weighting
ensemble
pruning
technique
data
forgetting
temporal
sample
weighting
use
sample
weighing
decrease
inﬂuence
old
samlpes
learn
new
trees
exp
−αt
sample
weighting
rate
selected
experiment
cross-validation
knowledge
forgetting
ensemble
growing
pruning
part
apply
classic
replace-the-looser
approach
discard
trees
high
error
new
block
samples
3.5
algorithm
predict
sample
algorithm
first
use
stored
window
ﬁnd
similar
items
using
speciﬁed
similarity
metric
second
evaluate
current
ensemble
similar
examples
compute
weights
adherence
errors
similar
samples
every
chunk
algorithm
tests
trees
choose
poorest
base
learner
replace
new
one
trained
new
block
data
process
iterative
ensemble
error
new
block
samples
higher
speciﬁed
threshold
input
data
stream
chunks
sequentially
produce
training
examples
data
stream
chunks
blocksize
number
nearest
neighbours
classiﬁer
weighting
function
output
class
probability
vector
nearest
ﬁnd
nearest
samples
cache
using
proximity
metric
trees
ensemble
get
average
error
nearest
base
classiﬁer
probability
class
probability
vector
nearest
samples
add
weighted
end
end
return
probability
vector
algorithm
pdsrf
prediction
algorithm
experimental
evaluation
experiment
evaluate
algorithm
various
publicly
available
datasets
like
covertype
compared
popular
concept-
drift
classiﬁers
compare
algorithm
sea
hoeﬀding
adaptive
tree
online
bagging
awe
implemented
moa
algo-
rithm
implemented
natively
c++
according
testing
method-
ology
extremely
randomized
trees
used
base
learner
adherence
methodology
classication
accuracy
calculated
using
data
block
evaluation
method
exploits
test-then-train
approach
data
block
evaluation
method
reads
incoming
samples
without
processing
form
data
block
size
new
data
block
ﬁrst
used
test
existing
classiﬁer
updates
classiﬁer
proposed
proximity
driven
streaming
random
forest
tested
blocksize
300
windowsize
1500
number
nearest
neighbours
ensemble
consists
decision
trees
order
make
results
interpreteble
also
test
оriginal
random
forest
incorporated
replace-the-
looser
forgetting
method
pdsrf
ave2
buﬀered
mean
accuracy
0.54
0.63
0.81
table
cover
type
dataset
mean
accuracy
results
4.1
datasets
follow
literature
adaptive
ensemble
classiﬁers
select
publicly
available
benchmark
datasets
concept
drift
paper
proposed
method
tested
cover
type
dataset
jock
blackard
colorado
state
university
cover
type
dataset
contains
forest
cover
type
30×30
meter
cells
obtained
forest
service
region
resource
information
system
ris
data
contains
581
012
instances
attributes
used
benchmark
several
papers
data
stream
classiﬁcation
4.2
results
proposed
approach
shows
results
similar
mentioned
aue2
next
comparison
ref
fig
original
random
forest
incorporated
replace-the-looser
forgetting
proposed
proximity
driven
streaming
random
forest
presented
table
mean
accuracy
shown
epochs
fig
original
random
forest
incorporated
replace-the-looser
blue
pdsrf
red
accuracy
remark
sake
space
report
complete
tests
paper
details
complete
results
approach
evaluation
readers
may
refer
http
//mmwind.github.io/pdsrf
discussion
conclusions
comparison
concept-drift
approaches
like
online
random
forest
awe
approach
needs
computational
resources
thus
time
training
prediction
stages
proposed
approach
highly
parallelisable
implemented
using
gpgpu
must
noted
proposed
approach
eﬃciently
applied
gradual
concept
drifts
pdsrf
senisible
parameters
changes
param-
eters
must
accurately
tuned
shown
proposed
approach
signiﬁcantly
exceeds
original
random
forest
incorporated
replace-the-looser
forget-
ting
although
presented
results
show
accuracy
lower
awe2
approach
promising
directions
random
forest
used
unsupervised
allows
work
missing
data
issue
smart
grid
datasets
wind
power
forecasting
telecommunications
signals
data
recording
100
robust.s
acknowledgment
work
partly
funded
rsf
grant
14-19-
00054.
references
zhukov
kurbatsky
tomin
sidorov
panasetsky
foley
ensemble
methods
classiﬁcation
power
systems
security
assessment
arxiv
artiﬁcial
intelligence
cs.ai
arxiv:1601.01675
2016
1–6
tomin
zhukov
sidorov
kurbatsky
panasetsky
spiryaev
random
forest
based
model
preventing
large-scale
emergencies
power
sys-
tems
international
journal
artiﬁcial
intelligence
2015
211–228
sidorov
modeling
nonlinear
time-dependent
dynamical
systems
volterra
series
identiﬁcation
application
sib
ind
mat
2000
182–194
sidorov
modelling
non-linear
dynamic
systems
volterra
series
attractors
signals
synergetics
workshop
2000
pabst
science
publ
usa-
germany
2002
276–282
sidorov
integral
dynamical
models
singularities
signals
control
singa-
pore
world
scientiﬁc
publ
2015
wang
fan
p.s.
han
mining
concept-drifting
data
streams
using
ensemble
classiﬁers
proceedings
sigkdd
august
24–27
2003
washington
usa
2003
226–235
gama
knowledge
discovery
data
streams
singapore
crc
press
publ
2010
kuncheva
classiﬁer
ensembles
changing
environment
roli
kittler
windeatt
eds
multiple
classiﬁer
systems
2004
5th
intl
workshop
springer-verlag
2004
1–15
turkov
krasotkina
mottl
dynamic
programming
bayesian
logistic
regression
learning
concept
drift
pattern
recognition
machine
intelligence
5th
international
conference
premi
2013
kolkata
india
december
10-14
2013.
proceedings
springer
berlin
heidelberg
berlin
heidelberg
2013
190–195
10.
saﬀari
leistner
santner
godec
bischof
on-line
random
computer
vision
workshops
iccv
workshops
2009
ieee
12th
forests
international
conference
ieee
2009
1393–1400
11.
bifet
holmes
kirkby
pfahringer
moa
massive
online
analysis
journal
machine
learning
research
2010
1601–1604
12.
wang
fan
p.s.
han
mining
concept-drifting
data
streams
using
proceedings
ninth
acm
sigkdd
international
ensemble
classiﬁers
conference
knowledge
discovery
data
mining
acm
2003
226–235
13.
breiman
bagging
predictors
machine
learning
1996
123–140
14.
t.k
random
subspace
method
constructing
decision
forests
pattern
analysis
machine
intelligence
ieee
transactions
aug
1998
832–
844
15.
zhukov
kurbatsky
tomin
sidorov
panasetsky
spiryaev
random
forest
based
model
emergency
state
monitoring
power
systems
mathematical
method
pattern
recognition
book
abstract
17th
all-russian
conference
interneational
participation
svetlogorsk
torus
press
2015
274
16.
breiman
random
forests
machine
learning
2001
5–32
17.
breiman
friedman
olshen
stone
steinberg
colla
cart
classiﬁcation
regression
trees
wadsworth
belmont
156
1983
18.
geurts
ernst
wehenkel
extremely
randomized
trees
machine
learning
2006
3–42
19.
blake
c.l.
merz
c.j
uci
repository
machine
learning
databases
1998
20.
street
w.n.
kim
streaming
ensemble
algorithm
sea
large-scale
clas-
siﬁcation
proceedings
seventh
acm
sigkdd
international
conference
knowledge
discovery
data
mining
kdd
new
york
usa
acm
2001
377–382
21.
bifet
gavald
adaptive
learning
evolving
data
streams
ad-
vances
intelligent
data
analysis
viii
8th
international
symposium
intel-
ligent
data
analysis
ida
2009
lyon
france
august
september
2009.
proceedings
springer
berlin
heidelberg
berlin
heidelberg
2009
249–260
22.
oza
online
bagging
boosting
systems
man
cybernetics
2005
ieee
international
conference
volume
oct
2005
2340–2345
vol
23.
brzezinski
mining
data
streams
concept
drift
diss
thesis
dept
computing
science
management
poznan
university
technology
2010
24.
brzezinski
stefanowski
reacting
diﬀerent
types
concept
drift
accuracy
updated
ensemble
algorithm
neural
networks
learning
systems
ieee
transactions
jan
2014
81–94
