artiﬁcial
intelligence
221
final
project
angrier
birds
bayesian
reinforcement
learning
imanol
arrieta
ibarra1
bernardo
ramos1
lars
roemheld1
abstract
train
reinforcement
learner
play
simpliﬁed
version
game
angry
birds
learner
provided
game
state
manner
similar
output
could
produced
computer
vision
algorithms
improve
efﬁciency
regular
ε-greedy
q-learning
linear
function
approximation
systematic
exploration
randomized
least
squares
value
iteration
rlsvi
algorithm
samples
policy
posterior
distribution
optimal
policies
larger
state-action
spaces
efﬁcient
exploration
becomes
increasingly
important
evidenced
faster
learning
rlsvi
keywords
reinforcement
learning
q-learning
rlsvi
exploration
1department
management
science
engineering
stanford
university
california
united
states
contents
introduction
model
algorithms
2.1
q-learning
linear
function
approximation
2.2
randomized
least
squares
value
iteration
2.3
feature
extractors
pig
position
values
pig
position
indicator
nested
pig
position
counter
npp
nested
pig
positions
shifted
counters
npps
nested
pig
positions
counters
obstacles
nppo
results
discussion
3.1
comparison
feature
extractors
3.2
rlsvi
vs.
regular
q-learning
3.3
learning
success
conclusions
future
work
references
introduction
angry
birds
wildly
successful
internet
franchise
centered
around
original
mobile
game
players
shoot
birds
slingshot
hit
targets
points
angry
birds
largely
deterministic
game
complex
physics
engine
governs
ﬂight
collision
impact
appear
almost
realistically
optimal
gameplay
could
achieved
optimizing
highly
complex
function
describes
tra-
jectory
planning
instead
train
several
reinforcement
learning
algorithms
play
game
compare
ﬁnal
performance
learning
speeds
human-play
oracle
1in
simulation
physics
engine
actually
seemed
produce
semi-
random
results
sequence
moves
guaranteed
yield
exact
outcomes
simulate
gameplay
adapted
open-source
project
recreate
angry
birds
python
using
pygame
frame-
work
adapting
base
code
needs
designed
api
exposed
game
markov
decision
pro-
cess
mdp
largely
following
conventions
class
framework
python
port
used
simpliﬁed
version
original
angry
birds
game
includes
levels
one
standard
type
bird
one
type
target
one
type
beam
level
ends
whenever
either
targets
birds
screen
targets
destroyed
agent
advances
next
level
otherwise
agent
loses
goes
back
ﬁrst
level
level
score
calculated
according
three
parameters
number
birds
left
shattered
beams
columns
destroyed
targets
agent
loses
incurs
arbitrary
penalty
somewhat
reduced
state-action
space
allowed
rel-
atively
straightforward
proof-of-concept
believe
similar
algorithms
models
would
also
work
complex
versions
game
model
game
state
exposed
api
composed
fol-
lowing
information
state
representation
fully
describes
relevant
game
situation
player
perspective
therefore
sufﬁces
interface
learner
game
note
parts
game
state
could
ob-
tained
computer
vision
algorithms
principle
direct
interaction
game
mechanics
would
nec-
essary
number
birds
left
ammunition
number
targets
pigs
absolute
positions
game
map
angrier
birds
bayesian
reinforcement
learning
2/6
figure
angry
birds
gameplay
stationary
slingshot
left
used
hit
targets
right
beam
structures
destroyed
allowing
complex
consequences
interactions
chosen
actions
number
obstacles
beams
absolute
po-
sitions
game
map
game
score
level
achieved
player
far
simpliﬁed
game
mechanics
making
game
fully
turn-based
learner
always
waits
ob-
jects
motionless
taking
next
action
as-
sumption
allowed
work
absolute
positions
set
possible
actions
pair
angle
distance
de-
scribing
extension
aiming
slingshot
thereby
bird
launching
momentum
direction
some-
accelerate
learning
allowed
shooting
forwards
within
constraint
allowed
possible
extensions
angles
compare
discretizations
action
space
varying
levels
granularity
base
discretization
used
different
evenly
spaced
actions
algorithms
reinforcement
learning
sub-ﬁeld
artiﬁcial
intelligence
describes
ways
learning
algorithm
act
unknown
unspeciﬁed
decision
processes
guided
merely
rewards
punishments
desirable
unwanted
outcomes
re-
spectively
reinforcement
learning
closely
mirrors
human
intuition
learning
conditioning
al-
lows
algorithms
master
systems
would
otherwise
hard
impossible
specify
standard
algorithm
reinforcement
learning
ε-greedy
q-learning
effective
algorithm
relies
erratic
exploration
possible
actions
implemented
variation
updates
belief
distribution
optimal
poli-
cies
samples
policies
distribution
implies
systematic
exploration
since
policies
seem
unattractive
high
conﬁdence
re-tried
introducing
two
algorithms
discuss
various
iterations
ﬁnd
optimal
feature
extractors
2.1
q-learning
linear
function
approximation
exposed
q-learning
model-free
algorithm
approximates
state-action
value
function
qopt
order
derive
optimal
policy
unknown
decision
process
order
learn
optimal
behavior
unknown
game
q-learner
needs
explore
system
choosing
pre-
viously
unseen
actions
exploit
knowledge
thus
gathered
choosing
action
promises
best
outcomes
given
previous
experience
balancing
efﬁcient
exploration
exploitation
main
challenge
reinforcement
learning
popular
method
literature
use
ε-greedy
methods
probability
algorithm
would
ignore
previous
experience
pick
action
random
method
tends
work
well
practice
whenever
actions
needed
increase
payoff
complicated
however
complex
sets
actions
need
taken
order
get
higher
reward
ε-greedy
tends
take
exponential
time
ﬁnd
rewards
see
aid
generalization
angry
birds
state
space
used
linear
function
approximation
deﬁned
feature
extractor
calculates
features
given
state-action
pair
allows
exploitation
step
update
weights
vector
performed
fashion
similar
stochastic
gradient
descent
following
standard
algorithm
2.2
randomized
least
squares
value
iteration
osband
propose
randomized
least
squares
value
iter-
ation
rlsvi
algorithm
differs
imple-
mentation
q-learning
two
points
instead
using
gradient
descent
learn
experi-
ence
online
fashion
rlsvi
stores
full
history
state
action
reward
new
state
tuples
derive
exact
optimal
policy
given
data
given
hyperparameters
quality
linear
approximation
bayesian
least
squares
employed
estimate
distribution
around
optimal
policy
de-
rived
learner
samples
policy
distribution
replaces
ε-greedy
exploration
speciﬁcally
rlsvi
models
state-action
value
function
follows
reward
max
at+1
successor
at+1
arbitrary
discount
factor
hyperparameter
given
memory
state
action
reward
new
state
tu-
ples
use
bayesian
least
squares
derive
¯wt
expected
value
optimal
policy
cov
covariance
optimal
policy
details
fairly
straight-forward
found
note
given
assumption
linear
approximation
weights
vector
fully
speciﬁes
q-function
thereby
policy
learner
ﬁnally
picks
policy
sampling
ˆwt
¯wt
taking
action
ˆwt
predicts
highest
reward
instead
taking
entirely
random
actions
probability
algorithm
thus
always
samples
random
policies
converges
optimal
means
data
accumulated
variances
shrink
therefore
less
time
may
expected
wasted
policies
highly
unlikely
successful
keep
algorithm
erratic
policy
changes
observations
made
yet
initialize
uninformed
prior
furthermore
inspection
showed
almost
diagonal
save
computation
therefore
de-
cided
sample
using
variances
individual
weights
ignoring
covariance
weights
osband
propose
rlsvi
context
episodic
learning
i.e
suggest
updating
policy
end
episodes
ﬁxed
number
actions
helps
angrier
birds
bayesian
reinforcement
learning
3/6
steady
algorithm
unfortunately
game
simulation
computationally
expensive
relatively
small
number
actions
simulated
forced
learn
continuous
context
2.3
feature
extractors
designing
feature
extractors
followed
two
premises
ﬁrst
given
value
function
approximation
linear
clear
interaction
features
would
linear
particular
separating
compo-
nents
locations
would
work
second
wanted
give
away
little
domain
knowledge
game
possible
task
algorithm
understand
play
angry
birds
without
previously
concept
targets
obstacles
two
premises
effectively
impose
constraint
strategies
reliable
way
algorithm
detect
-for
example-
whether
obstacle
front
behind
target
regardless
learner
developed
impressive
performance
seen
later
one
reason
may
levels
simulator
relatively
simple
iterated
following
ﬁve
ideas
form
mean-
ingful
features
due
relatively
complex
state-action
space
feature
space
fairly
big
quickly
spanning
sev-
eral
thousand
features
turned
problem
rlsvi
memory
computational
requirements
grew
intractable
rewriting
algorithm
run
sparse
ma-
trices
offered
help
ultimately
found
reasonable
performance
best-functioning
features
2.3.1
pig
position
values
ﬁrst
attempt
used
rounded
target
positions
feature
values
every
pig
given
state
would
feature
values
features
repeated
every
action
would
non-zero
action
taken
allows
different
weights
different
position-
action
combinations
ultimately
implies
learning
best
position-action
combinations
including
interaction
term
hoped
capture
relative
distance
target
slingshot
establish
linear
function
would
allowed
fast
generalization
target
positions
unfortunately
hindsight
surprisingly
approach
failed
quickly
produced
practically
learning
2.3.2
pig
position
indicator
next
approach
indicator
variable
ﬁne
grid
pig
positions
created
separate
feature
every
possible
pig
position
included
action
taken
created
impractically
large
feature
space
yet
worked
relatively
well
however
clearly
over-ﬁt
successful
series
actions
found
algorithm
would
reliably
complete
level
target
moved
pixels
however
algorithm
would
retrained
completely
angrier
birds
bayesian
reinforcement
learning
4/6
2.3.3
nested
pig
position
counter
npp
order
solve
over-ﬁtting
problem
developed
nested
mechanism
would
generalize
unobserved
states
achieve
deﬁned
nested
grids
game
screen
exempliﬁed
figure
three
grids
progressively
smaller
square
grid
count
number
targets
contained
solved
generalization
issue
maintaining
nice
properties
previous
feature
extractor
larger
grids
helped
generalize
ﬁner
ones
provided
way
remember
level
already
observed
figure
npp
nppo
grid
structure
progressively
smaller
grids
create
squares
within
squares
extracting
speciﬁc
location
information
allowing
generalization
2.3.4
nested
pig
positions
shifted
counters
npps
one
issue
npp
especially
larger
squares
targets
close
square
boundary
captured
adequately
improve
created
copy
grid
shifted
diagonally
half
square
size
every
target
lay
exactly
two
squares
allowing
judge
whether
target
left
right
within
square
npps
therefore
feature
set
two
sets
three
over-
lapping
square
grids
surprise
npps
performed
worse
simpler
npp
assume
due
much
bigger
feature
space
fact
could
learn
algorithms
full
convergence
due
computational
limits
2.3.5
nested
pig
positions
counters
obstacles
nppo
finally
tried
address
issue
obstacles
described
want
give
away
gameplay-related
information
representing
ﬁxed
relationship
targets
obstacles
therefore
added
counters
obstacles
way
used
targets
hoping
algorithm
would
learn
prefer
areas
high
targets
obstacles
ratio
case
npps
surprised
adding
information
obstacles
detrimental
learning
suc-
cess
indeed
adding
obstacle
information
stopped
learning
altogether
npps
suspect
nppo
may
work
well
given
time
converge
bloated
feature
space
results
discussion
somewhat
crude
feature
extractor
npp
provided
best
results
rslvi
indeed
outperform
regular
ε-greedy
q-learner
rlsvi
particularly
impressive
ability
clear
levels
almost
speed
untrained
human
player
could
simulate
gameplay
convergence
result
computational
limitations
game
engine
used
intended
big
simulations
even
speed
signiﬁcantly
graphics
disabled
3.1
comparison
feature
extractors
compared
ﬁve
different
feature
extractors
learning
baseline
algorithm
displayed
ﬁgure
noteworthy
starts
high
baseline
score
improve
much
due
fact
learner
quickly
masters
level
generalize
following
levels
consequence
tends
get
stuck
achieve
relatively
constant
game
scores
ﬁrst
levels
contrast
npp
learns
master
easy
levels
equally
fast
carries
harder
levels
better
yielding
higher
total
scores
per
attempt
npps
shows
similar
slope
npp
supporting
explanation
sheer
number
features
slows
learn-
ing
process
however
npps
clearly
outperformed
npp
within
simulation
time
frame
somewhat
surprised
complete
failure
nppo
improve
time
3.2
rlsvi
vs.
regular
q-learning
exploring
state-action
space
according
posterior
beliefs
indeed
produce
better
results
ε-greedy
q-learner
trained
algorithms
feature
extractor
npp
compared
moving
average
game
score
per
at-
tempt
rlsvi
achieved
higher
scores
overall
simulated
time
frame
learned
quickly
comparison
two
algorithms
given
ﬁgure
noted
implementation
rlsvi
required
signiﬁcantly
memory
computation
regular
q-learner
since
required
keeping
complete
history
observed
features
matrix
operations
large
data
blocks
3.3
learning
success
neither
human
player
-our
oracle
score
comparison-
algorithms
managed
levels
angrier
birds
bayesian
reinforcement
learning
5/6
figure
moving
average
different
feature
extractors
dysfunctional
position
values
feature
extractor
excluded
display
moving
average
game
score
next
attempts
number
attempts
made
attempt
deﬁned
number
shots
level
failed
successively
used
one
attempt
explore
recorded
following
attempt
without
exploration
order
distort
results
exploration
behavior
y-axis
units
104
points
figure
rlsvi
faster
achieves
greater
rewards
q-learning
display
moving
average
game
score
next
attempts
number
attempts
made
attempt
deﬁned
number
shots
level
failed
successively
used
one
attempt
explore
recorded
following
attempt
without
exploration
order
distort
results
exploration
behavior
y-axis
units
104
points
model
number
trials
ﬁnish
provided
game
engine
used
one
attempt
regular
q-learner
rlsvi
however
outperformed
human
player
shall
remain
unnamed
given
shamefully
beat
crude
algorithms
terms
maximum
points
achieved
single
attempt
terms
high-
est
level
reached
table
summarizes
maximum
score
attained
highest
level
reached
different
players
model
features
algorithm
human
q-learning
q-learning
npp
q-learning
nppo
rlsvi
maximum
performance
level
table
maximum
level
scores
achieved
regular
q-learner
rlsvi
outperform
untrained
human
player
score
210000
170000
495000
120000
550000
npp
comparing
number
attempts
required
pass
given
level
provides
interesting
insights
especially
later
levels
rlsvi
exploration
proves
efﬁcient
passes
levels
almost
number
attempts
required
human
player
features
regular
q-learning
required
times
many
attempts
seen
table
algorithm
human
q-learning
q-learning
npp
q-learning
nppo
rlsvi
features
level
level
level
table
average
number
trials
needed
ﬁnish
level
rlsvi
learns
quickly
later
levels
passes
levels
impressively
short
exploration
periods
npp
conclusions
future
work
rlsvi
bayesian
approach
state-action
space
exploration
seems
like
promising
rather
intuitive
way
navigating
unknown
systems
previous
work
shown
bayesian
belief
updating
sampling
exploration
highly
efﬁcient
ﬁndings
conﬁrm
rlsvi
beat
baseline
q-learning
algorithm
somewhat
embarrassingly
human
oracle
main
constraint
work
simulation
speed
lim-
iting
algorithm
convergence
could
achieve
would
liked
explore
possible
features
especially
computation
vein
would
interesting
train
deep
neural
network
function
allow
learning
complex
interaction
effects
particular
targets
obstacles
promising
results
combining
bayesian
exploration
deep
reinforcement
learning
shown
finally
may
interesting
explore
different
distri-
angrier
birds
bayesian
reinforcement
learning
6/6
jerome
friedman
trevor
hastie
robert
tibshirani
elements
statistical
learning
volume
springer
series
statistics
springer
berlin
2001
volodymyr
mnih
koray
kavukcuoglu
david
silver
alex
graves
ioannis
antonoglou
daan
wierstra
martin
riedmiller
playing
atari
deep
reinforcement
learning
arxiv
preprint
arxiv:1312.5602
2013
itamar
arel
cong
liu
urbanik
kohls
rein-
forcement
learning-based
multi-agent
system
network
trafﬁc
signal
control
intelligent
transport
systems
iet
:128–135
2010.
bution
assumptions
within
rlsvi
assuming
least-squares
noise
normally
distributed
therefore
sampling
normal
distribution
around
expected
optimal
policies
may
particularly
prone
get
stuck
local
minima
using
bootstrapping
methods
lieu
closed-form
bayesian
up-
dates
may
prove
powerful
improvement
rlsvi
algorithm
explored
appendix
work
found
github.com/imanolarrieta/angrybirds
origi-
nal
codes
angry
birds
python
found
https
//github.com/estevaofon/angry-birds-python
acknowledgments
would
like
thank
cs221
teaching
team
inspir-
ing
quarter
references
estevaofon
angry
birds
python
github
repos-
itory
https
//github.com/estevaofon/
angry-birds-python
january
2015.
forked
https
//github.com/imanolarrieta/
angrybirds
ian
osband
benjamin
van
roy
zheng
wen
gener-
alization
exploration
via
randomized
value
functions
arxiv
preprint
arxiv:1402.0635
2014
richard
sutton
andrew
barto
reinforcement
learning
introduction
volume
mit
press
cam-
bridge
1998
ian
osband
dan
russo
benjamin
van
roy
efﬁcient
reinforcement
learning
via
posterior
sampling
advances
neural
information
processing
systems
pages
3003–3011
2013
ian
osband
benjamin
van
roy
bootstrapped
thomp-
arxiv
preprint
son
sampling
deep
exploration
arxiv:1507.00300
2015
richard
dearden
nir
friedman
stuart
russell
aaai/iaai
pages
761–768
bayesian
q-learning
1998
michael
kearns
satinder
singh
near-optimal
rein-
forcement
learning
polynomial
time
machine
learn-
ing
2-3
:209–232
2002
leslie
pack
kaelbling
michael
littman
an-
drew
moore
reinforcement
learning
survey
jour-
nal
artiﬁcial
intelligence
research
pages
237–285
1996
christopher
jch
watkins
peter
dayan
q-learning
machine
learning
3-4
:279–292
1992
