value
iteration
networks
aviv
tamar
garrett
thomas
sergey
levine
pieter
abbeel
dept
electrical
engineering
computer
sciences
berkeley
abstract
introduce
value
iteration
network
vin
fully
differentiable
neural
net-
work
planning
module
embedded
within
vins
learn
plan
suitable
predicting
outcomes
involve
planning-based
reasoning
policies
reinforcement
learning
key
approach
novel
differentiable
approximation
value-iteration
algorithm
represented
con-
volutional
neural
network
trained
end-to-end
using
standard
backpropagation
evaluate
vin
based
policies
discrete
continuous
path-planning
domains
natural-language
based
search
task
show
learning
explicit
planning
computation
vin
policies
generalize
better
new
unseen
domains
introduction
last
decade
deep
convolutional
neural
networks
cnns
revolutionized
supervised
learning
tasks
object
recognition
action
recognition
semantic
segmentation
recently
cnns
applied
reinforcement
learning
tasks
visual
observations
atari
games
robotic
manipulation
imitation
learning
tasks
neural
network
trained
represent
policy
mapping
observation
system
state
action
goal
representing
control
strategy
good
long-term
behavior
typically
quantiﬁed
minimization
sequence
time-dependent
costs
sequential
nature
decision
making
inherently
different
one-step
decisions
supervised
learning
general
requires
form
planning
however
recent
deep
works
employed
architectures
similar
standard
networks
used
supervised
learning
tasks
typically
consist
cnns
feature
extraction
fully
connected
layers
map
features
probability
distribution
actions
networks
inherently
reactive
particular
lack
explicit
planning
computation
success
reactive
policies
sequential
problems
due
learning
algorithm
essentially
trains
reactive
policy
select
actions
good
long-term
consequences
training
domain
understand
planning
nevertheless
important
ingredient
policy
consider
grid-world
navigation
task
depicted
figure
left
agent
observe
map
domain
required
navigate
obstacles
target
position
one
hopes
training
policy
solve
several
instances
problem
different
obstacle
conﬁgurations
policy
would
generalize
solve
different
unseen
domain
figure
right
however
show
experiments
standard
cnn-based
networks
easily
trained
solve
set
maps
generalize
well
new
tasks
outside
set
understand
goal-directed
nature
behavior
observation
suggests
computation
learned
reactive
policies
different
planning
required
solve
new
task1
1in
principle
enough
training
data
covers
possible
task
conﬁgurations
rich
enough
policy
representation
reactive
policy
learn
map
task
optimal
policy
practice
often
expensive
offer
data-efﬁcient
approach
exploiting
ﬂexible
prior
planning
computation
underlying
behavior
30th
conference
neural
information
processing
systems
nips
2016
barcelona
spain
work
propose
nn-based
policy
effectively
learn
plan
model
termed
value-iteration
network
vin
differen-
tiable
planning
program
embedded
within
structure
key
approach
observation
classic
value-iteration
planning
algo-
rithm
may
represented
speciﬁc
figure
two
instances
grid-world
domain
type
cnn
embedding
network
task
move
goal
obstacles
module
inside
standard
feed-forward
classiﬁ-
cation
network
obtain
model
learn
parameters
planning
computation
yields
useful
predictions
block
differentiable
whole
network
trained
using
standard
backpropagation
makes
policy
simple
train
using
standard
algorithms
straightforward
integrate
nns
perception
control
connections
planning
algorithms
recurrent
nns
previously
explored
ilin
work
builds
related
ideas
results
broadly
applicable
policy
representation
approach
different
model-based
requires
system
identiﬁcation
map
observations
dynamics
model
solved
policy
many
applications
including
robotic
manipulation
locomotion
accurate
system
identiﬁcation
difﬁcult
modelling
errors
severely
degrade
policy
performance
domains
model-free
approach
often
preferred
since
vin
policy
trained
model
free
without
requiring
explicit
system
identiﬁcation
addition
effects
modelling
errors
vins
mitigated
training
network
end-to-end
similarly
methods
demonstrate
effectiveness
vins
within
standard
algorithms
various
problems
among
require
visual
perception
continuous
control
also
natural
language
based
decision
making
webnav
challenge
training
policy
learns
map
observation
planning
computation
relevant
task
generate
action
predictions
based
resulting
plan
demonstrate
leads
policies
generalize
better
new
unseen
task
instances
background
section
provide
background
planning
value
iteration
cnns
policy
representations
sequel
shall
show
cnns
implement
particular
form
planning
computation
similar
value
iteration
algorithm
used
policy
value
iteration
standard
model
sequential
decision
making
planning
markov
decision
process
mdp
mdp
consists
states
actions
reward
function
transition
kernel
cid:48
encodes
probability
next
state
given
current
state
action
policy
a|s
prescribes
action
distribution
state
goal
mdp
ﬁnd
policy
obtains
high
rewards
long
term
formally
value
state
policy
expected
discounted
sum
rewards
starting
state
t=0
γtr
discount
factor
executing
policy
denotes
expectation
trajectories
states
actions
actions
selected
according
states
evolve
according
transition
kernel
cid:48
optimal
value
function
maxπ
maximal
long-term
return
possible
popular
algorithm
calculating
state
policy
said
optimal
value
iteration
vn+1
maxa
cid:80
well
known
value
function
converges
optimal
policy
may
derived
arg
maxa
convolutional
neural
networks
cnns
nns
particular
architecture
proved
useful
computer
vision
among
domains
cnn
comprised
stacked
convolution
max-pooling
layers
input
convolution
layer
dimensional
signal
typically
image
channels
horizontal
pixels
verti-
cal
pixels
output
cid:48
-channel
convolution
image
kernels
cid:48
scalar
activation
function
max-pooling
cid:48
cid:48
cid:48
layer
selects
channel
pixel
maximum
value
among
neighbors
maxi
cid:48
cid:48
cid:48
cid:48
typically
neighbors
chosen
image
hmaxpool
cid:80
cid:48
cid:48
cid:48
jxl
cid:48
cid:48
cid:16
cid:80
cid:48
cid:17
dataset
optimal
observations
corresponding
cid:8
cid:9
patch
around
pixel
max-pooling
image
down-sampled
constant
factor
com-
monly
resulting
output
signal
cid:48
channels
m/d
horizontal
pixels
n/d
vertical
pixels
cnns
typically
trained
using
stochastic
gradient
descent
sgd
backpropagation
computing
gradients
reinforcement
learning
imitation
learning
mdps
state
space
large
continuous
mdp
transitions
rewards
known
advance
planning
algorithms
applied
cases
policy
learned
either
expert
supervision
trial
error
learning
algorithms
cases
different
policy
representations
focus
work
similar
additionally
state-of-the-art
algorithms
agnostic
policy
representation
require
differentiable
performing
gradient
descent
algorithm-speciﬁc
loss
function
therefore
paper
commit
speciﬁc
learning
algorithm
consider
policy
let
denote
observation
state
policy
speciﬁed
parametrized
function
a|φ
mapping
observations
probability
actions
policy
parameters
example
policy
could
represented
neural
network
denoting
network
weights
goal
tune
parameters
policy
behaves
well
sense
a|φ
a|φ
optimal
policy
mdp
deﬁned
section
state
actions
i=1
...
generated
expert
learning
policy
becomes
instance
supervised
learning
optimal
action
available
instead
agent
act
world
observe
rewards
state
transitions
actions
effect
algorithms
use
observations
improve
value
policy
value
iteration
network
model
section
introduce
general
policy
representation
embeds
explicit
planning
module
stated
earlier
motivation
representation
natural
solution
many
tasks
path
planning
described
involves
planning
model
domain
let
denote
mdp
domain
design
policy
assume
unknown
mdp
optimal
plan
contains
useful
information
optimal
policy
original
task
however
emphasize
assume
know
advance
idea
equip
policy
ability
learn
solve
add
solution
element
policy
hypothesize
lead
policy
automatically
learns
useful
plan
denote
cid:48
|¯s
states
actions
rewards
transitions
facilitate
connection
let
depend
observation
namely
later
learn
functions
part
policy
learning
process
example
grid-world
domain
described
let
state
action
spaces
true
grid-world
reward
function
map
image
domain
high
reward
goal
negative
reward
near
obstacle
encode
deterministic
movements
grid-world
depend
observation
rewards
transitions
necessarily
true
rewards
transitions
task
optimal
plan
still
follow
trajectory
avoids
obstacles
reaches
goal
similarly
optimal
plan
mdp
speciﬁed
standard
planning
algorithm
used
obtain
value
function
next
section
shall
show
using
particular
implementation
planning
advantage
differentiable
simple
implement
within
framework
section
however
focus
use
planning
result
within
policy
approach
based
two
important
observations
ﬁrst
vector
values
encodes
information
optimal
plan
thus
adding
vector
additional
features
policy
sufﬁcient
extracting
information
optimal
plan
however
additional
property
optimal
decision
¯π∗
state
depend
cid:48
cid:48
|¯s
cid:48
subset
values
since
¯π∗
arg
max¯a
therefore
mdp
local
connectivity
structure
grid-world
example
states
cid:48
|¯s
small
subset
terminology
form
attention
sense
given
label
prediction
action
subset
input
features
value
function
relevant
attention
known
improve
learning
performance
reducing
effective
number
network
parameters
learning
therefore
second
element
network
attention
module
outputs
vector
attention
cid:80
modulated
values
finally
vector
added
additional
features
reactive
policy
πre
a|φ
full
network
architecture
depicted
figure
left
returning
grid-world
example
particular
state
reactive
policy
needs
query
values
states
neighboring
order
select
correct
action
thus
attention
module
case
could
return
vector
subset
neighboring
states
figure
planning-based
models
left
general
policy
representation
adds
value
function
features
planner
reactive
policy
right
module
cnn
representation
algorithm
let
denote
parameters
policy
namely
parameters
πre
note
fact
function
therefore
policy
written
form
a|φ
similarly
standard
policy
form
section
could
back-propagate
function
potentially
could
train
policy
using
standard
algorithms
like
standard
policy
representation
easy
design
functions
differentiable
provide
several
examples
experiments
back-propagating
gradient
planning
algorithm
trivial
following
propose
novel
interpretation
approximate
algorithm
particular
form
cnn
allows
conveniently
treat
planning
module
another
back-propagating
train
whole
policy
end-to-end
3.1
module
introduce
module
encodes
differentiable
planning
computation
starting
point
algorithm
main
observation
iteration
may
seen
passing
previous
value
function
reward
function
convolution
layer
max-pooling
layer
analogy
channel
convolution
layer
corresponds
q-function
speciﬁc
action
convolution
kernel
weights
correspond
discounted
transition
probabilities
thus
recurrently
applying
convolution
layer
times
iterations
effectively
performed
following
idea
propose
network
module
depicted
figure
inputs
module
reward
image
dimensions
purpose
clarity
follow
cnn
formulation
explicitly
assume
state
space
maps
2-dimensional
grid
however
approach
extended
general
discrete
state
spaces
example
graph
report
wikinav
experiment
section
4.4.
reward
fed
convolutional
layer
¯rl
cid:48
cid:48
channel
layer
corresponds
particular
action
layer
max-pooled
along
actions
channel
produce
next-iteration
value
function
layer
¯vi
max¯a
next-iteration
value
function
layer
stacked
reward
fed
back
convolutional
layer
max-pooling
layer
times
perform
iterations
value
iteration
module
simply
architecture
capability
performing
approximate
computation
nevertheless
representing
form
makes
learning
mdp
parameters
reward
function
natural
backpropagating
network
similarly
standard
cnn
modules
also
composed
hierarchically
treating
value
one
module
additional
input
another
module
report
idea
supplementary
material
channels
linear
activation
function
¯q¯a
cid:48
cid:48
cid:80
3.2
value
iteration
networks
ingredients
differentiable
planning-based
policy
term
value
iteration
network
vin
vin
based
general
planning-based
policy
deﬁned
module
planning
algorithm
order
implement
vin
one
specify
state
recurrencerewardqprev
valuenew
value
moduleprv
action
spaces
planning
module
reward
transition
functions
attention
function
refer
vin
design
tasks
show
experiments
relatively
straightforward
select
suitable
design
tasks
may
require
thought
however
emphasize
important
point
reward
transitions
attention
deﬁned
parametric
functions
trained
whole
policy2
thus
rough
design
speciﬁed
ﬁne-tuned
end-to-end
training
vin
design
chosen
implementing
vin
straightforward
simply
form
cnn
networks
experiments
required
several
lines
theano
code
next
section
evaluate
vin
policies
various
domains
showing
learning
plan
achieve
better
generalization
capability
experiments
section
evaluate
vins
policy
representations
various
domains
additional
experiments
investigating
hierarchical
vins
well
technical
implementation
details
discussed
supplementary
material
source
code
available
https
//github.com/avivt/vin
goal
experiments
investigate
following
questions
vins
effectively
learn
planning
computation
using
standard
algorithms
planning
computation
learned
vins
make
better
reactive
policies
generalizing
new
domains
additional
goal
point
several
ideas
designing
vins
various
tasks
exhaustive
list
ﬁts
domains
hope
motivate
creative
designs
future
work
4.1
grid-world
domain
ﬁrst
experiment
domain
synthetic
grid-world
randomly
placed
obstacles
observation
includes
position
agent
also
image
map
obstacles
goal
position
figure
shows
two
random
instances
grid-world
size
16.
conjecture
learning
optimal
policy
several
instances
domain
vin
policy
would
learn
planning
computation
required
solve
new
unseen
task
simple
domain
optimal
policy
easily
calculated
using
exact
note
however
interested
evaluating
whether
policy
trained
using
learn
plan
following
results
policies
trained
using
standard
supervised
learning
demonstrations
optimal
policy
supplementary
material
report
additional
experiments
show
similar
ﬁndings
design
vin
task
following
guidelines
described
planning
mdp
grid-world
similar
true
mdp
reward
mapping
cnn
mapping
image
input
reward
map
grid-world
thus
potentially
learn
discriminate
obstacles
non-obstacles
goal
assign
suitable
reward
transitions
deﬁned
convolution
kernels
block
exploiting
fact
transitions
grid-world
local3
recurrence
chosen
proportion
grid-world
size
ensure
information
ﬂow
goal
state
state
attention
module
chose
trivial
approach
selects
values
block
current
state
i.e.
ﬁnal
reactive
policy
fully
connected
network
maps
probability
actions
compare
vins
following
reactive
policies
cnn
network
devised
cnn-based
reactive
policy
inspired
recent
impressive
results
dqn
convolution
layers
fully
connected
output
network
trained
predict
values
network
outputs
probability
actions
terms
related
since
arg
maxa
fully
convolutional
network
fcn
problem
setting
domain
similar
semantic
segmentation
pixel
image
assigned
semantic
label
action
case
therefore
devised
fcn
inspired
state-of-the-art
semantic
segmentation
algorithm
convolution
layers
ﬁrst
layer
ﬁlter
spans
whole
image
properly
convey
information
goal
every
state
table
present
average
prediction
loss
model
evaluated
held-out
test-set
maps
random
obstacles
goals
initial
states
different
problem
sizes
addition
map
full
trajectory
initial
state
predicted
iteratively
rolling-out
next-states
2vins
fundamentally
different
inverse
methods
transitions
required
known
3note
transitions
deﬁned
way
depend
state
interestingly
shall
see
network
learned
plan
successful
trajectories
nevertheless
appropriately
shaping
reward
figure
grid-world
domains
best
viewed
color
two
random
instances
synthetic
gridworld
vin-predicted
trajectories
ground-truth
shortest
paths
random
start
goal
positions
image
mars
domain
points
elevation
sharper
10◦
colored
red
points
calculated
matching
image
elevation
data
shown
available
learning
algorithm
note
difﬁculty
distinguishing
obstacles
non-obstacles
vin-predicted
purple
line
cross
markers
shortest-path
ground
truth
blue
line
trajectories
random
start
goal
positions
domain
prediction
loss
0.004
0.05
0.11
vin
success
traj
diff
rate
99.6
0.001
99.3
0.089
0.086
pred
loss
0.02
0.10
0.13
cnn
succ
traj
rate
diff
97.9
0.006
87.6
0.06
74.2
0.078
pred
loss
0.01
0.07
0.09
fcn
succ
traj
rate
diff
97.3
0.004
88.3
0.05
76.6
0.08
table
performance
grid-world
domain
top
comparison
reactive
policies
domain
sizes
vin
networks
signiﬁcantly
outperform
standard
reactive
networks
note
performance
gap
increases
dramatically
problem
size
predicted
network
trajectory
said
succeed
reached
goal
without
hitting
obstacles
trajectory
succeeded
also
measured
difference
length
optimal
trajectory
average
difference
average
success
rate
reported
table
clearly
vin
policies
generalize
domains
outside
training
set
visualization
reward
mapping
see
supplementary
material
shows
negative
obstacles
positive
goal
small
negative
constant
otherwise
resulting
value
function
gradient
pointing
towards
direction
goal
around
obstacles
thus
useful
planning
computation
learned
vins
also
signiﬁcantly
outperform
reactive
networks
performance
gap
increases
dramatically
problem
size
importantly
note
prediction
loss
reactive
policies
comparable
vins
although
success
rate
signiﬁcantly
worse
shows
standard
case
overﬁtting/underﬁtting
reactive
policies
rather
vin
policies
structure
focus
prediction
errors
less
important
parts
trajectory
reactive
policies
make
distinction
learn
easily
predictable
parts
trajectory
yet
fail
complete
task
vins
effective
depth
larger
depth
reactive
policies
one
may
wonder
whether
deep
enough
network
would
learn
plan
principle
cnn
fcn
depth
potential
perform
computation
vin
however
much
parameters
requiring
much
training
data
evaluate
untying
weights
recurrent
layers
vin
results
reported
supplementary
material
show
untying
weights
degrades
performance
stronger
effect
smaller
sizes
training
data
4.2
mars
rover
navigation
experiment
show
vins
learn
plan
natural
image
input
demonstrate
path-planning
overhead
terrain
images
mars
landscape
domain
represented
128
128
image
patch
deﬁned
grid-world
state
considered
obstacle
terrain
corresponding
image
patch
contained
elevation
angle
degrees
evaluated
using
external
elevation
data
base
example
domain
terrain
image
depicted
figure
mdp
shortest-path
planning
case
similar
grid-world
domain
section
4.1
vin
design
similar
deeper
cnn
reward
mapping
processing
image
policy
trained
predict
shortest-path
directly
terrain
image
emphasize
elevation
data
part
input
must
inferred
needed
terrain
image
0.35
0.59
0.30
0.39
vin
cnn
network
train
error
test
error
figure
continuous
control
domain
top
aver-
age
distance
goal
training
test
domains
vin
cnn
policies
bottom
trajectories
predicted
vin
cnn
test
domains
training
vin
achieved
success
rate
84.8
put
rate
context
compare
best
performance
achievable
without
access
elevation
data
90.3
make
comparison
trained
cnn
classify
whether
patch
obstacle
classiﬁer
trained
using
image
data
vin
network
labels
true
obstacle
classiﬁcations
elevation
map
reiterate
vin
access
ground-truth
obstacle
labels
training
testing
success
rate
planner
uses
obstacle
map
generated
classiﬁer
raw
image
90.3
showing
obstacle
identiﬁcation
raw
image
indeed
challenging
thus
success
rate
vin
trained
without
obstacle
labels
ﬁgure
planning
process
quite
remarkable
4.3
continuous
control
consider
path
planning
domain
continuous
states
continuous
actions
solved
using
therefore
vin
naively
applied
instead
construct
vin
perform
high-level
planning
discrete
coarse
grid-world
rep-
resentation
continuous
domain
shall
show
vin
learn
plan
high-
level
plan
also
exploit
plan
within
low-level
continuous
control
policy
moreover
vin
policy
results
better
generalization
reactive
policy
consider
domain
figure
red-colored
particle
needs
navigated
green
goal
us-
ing
horizontal
vertical
forces
gray-colored
obstacles
randomly
positioned
domain
apply
elastic
force
friction
contacted
domain
presents
non-trivial
control
problem
agent
needs
plan
feasible
trajectory
obstacles
use
bounce
also
control
particle
mass
inertia
follow
state
obser-
vation
consists
particle
continuous
position
velocity
static
downscaled
image
obstacles
goal
position
domain
principle
observation
sufﬁcient
devise
rough
plan
particle
follow
previous
experiments
investigate
whether
policy
trained
several
instances
domain
different
start
state
goal
obstacle
positions
would
generalize
unseen
domain
training
chose
guided
policy
search
gps
algorithm
unknown
dynamics
suitable
learning
policies
continuous
dynamics
contacts
used
publicly
available
gps
code
mujoco
physical
simulation
generated
200
random
training
instances
evaluate
performance
different
test
instances
distribution
vin
design
similar
grid-world
cases
important
modiﬁcations
attention
module
selects
patch
value
centered
around
current
discretized
position
map
ﬁnal
reactive
policy
3-layer
fully
connected
network
2-dimensional
continuous
output
controls
addition
due
limited
number
training
domains
pre-trained
vin
transition
weights
correspond
discounted
grid-world
transitions
reasonable
prior
weights
2-d
task
emphasize
even
initialization
initial
value
function
meaningless
since
reward
map
yet
learned
compare
cnn-based
reactive
policy
inspired
state-of-the-art
results
cnn
layers
image
processing
followed
3-layer
fully
connected
network
similar
vin
reactive
policy
figure
shows
performance
trained
policies
measured
ﬁnal
distance
target
vin
clearly
outperforms
cnn
test
domains
also
plot
several
trajectories
policies
test
domains
showing
vin
learned
sensible
generalization
task
4.4
webnav
challenge
previous
experiments
planning
aspect
task
corresponded
navigation
consider
general
domain
webnav
language
based
search
task
graph
webnav
agent
needs
navigate
links
website
towards
goal
web-page
speciﬁed
short
4-sentence
query
state
web-page
agent
observe
average
word-
embedding
features
state
possible
next
states
cid:48
linked
pages
features
query
based
select
link
follow
search
performed
wikipedia
website
report
experiments
wikipedia
schools
website
simpliﬁed
wikipedia
designed
children
6000
pages
292
links
per
page
nn-based
policy
proposed
ﬁrst
learns
mapping
hidden
state
vector
action
selected
according
cid:48
exp
cid:0
cid:62
cid:48
cid:1
essence
policy
reactive
relies
word
embedding
features
state
contain
meaningful
information
path
goal
indeed
property
naturally
holds
encyclopedic
website
structured
tree
categories
sub-categories
sub-sub-categories
etc
sought
explore
whether
planning
based
vin
lead
better
performance
task
intuition
plan
simpliﬁed
model
website
help
guide
reactive
policy
difﬁcult
queries
therefore
designed
vin
plans
small
subset
graph
contains
1st
2nd
level
categories
graph
word-embedding
features
designing
vin
requires
different
approach
grid-world
vins
described
earlier
challenging
aspect
deﬁne
meaningful
mapping
nodes
true
graph
nodes
smaller
vin
graph
reward
mapping
chose
weighted
similarity
measure
query
features
features
nodes
small
graph
thus
intuitively
nodes
similar
query
high
reward
transitions
ﬁxed
based
graph
connectivity
smaller
vin
graph
known
though
different
true
graph
attention
module
also
based
weighted
similarity
measure
features
possible
next
states
cid:48
features
node
simpliﬁed
graph
reactive
policy
part
vin
similar
policy
described
note
training
vin
end-to-end
effectively
learning
exploit
small
graph
better
planning
true
large
graph
vin
policy
baseline
reactive
policy
trained
supervised
learning
random
trajectories
start
root
node
graph
similarly
policy
said
succeed
query
correct
predictions
along
path
within
top-4
predictions
training
vin
policy
performed
mildly
better
baseline
2000
held-out
test
queries
starting
root
node
achieving
1030
successful
runs
vs.
1025
baseline
however
tested
policies
harder
task
starting
random
position
graph
vins
signiﬁcantly
outperformed
baseline
achieving
346
successful
runs
vs.
304
baseline
4000
test
queries
results
conﬁrm
indeed
navigating
tree
categories
root
features
state
contain
meaningful
information
path
goal
making
reactive
policy
sufﬁcient
however
starting
navigation
different
state
reactive
policy
may
fail
understand
needs
ﬁrst
back
root
switch
different
branch
tree
results
indicate
strategy
better
represented
vin
remark
still
room
improvements
webnav
results
e.g.
better
models
reward
attention
functions
better
word-embedding
representations
text
conclusion
outlook
introduction
powerful
scalable
methods
opened
range
new
problems
deep
learning
however
recent
works
investigate
policy
architectures
speciﬁcally
tailored
planning
uncertainty
current
theory
benchmarks
rarely
investigate
generalization
properties
trained
policy
work
takes
step
direction
exploring
better
generalizing
policy
representations
vin
policies
learn
approximate
planning
computation
relevant
solving
task
shown
computation
leads
better
generalization
diverse
set
tasks
ranging
simple
gridworlds
amenable
value
iteration
continuous
control
even
navigation
wikipedia
links
future
work
intend
learn
different
planning
computations
based
simulation
optimal
linear
control
combine
reactive
policies
potentially
develop
solutions
task
motion
planning
acknowledgments
research
funded
part
siemens
onr
pecase
award
army
research
ofﬁce
mast
program
nsf
career
award
1351028
partially
funded
viterbi
scholarship
technion
partially
funded
darpa
ppaml
program
contract
fa8750-14-c-0011
references
bellman
dynamic
programming
princeton
university
press
1957
bertsekas
dynamic
programming
optimal
control
vol
athena
scientiﬁc
4th
edition
2012
ciresan
meier
schmidhuber
multi-column
deep
neural
networks
image
classiﬁcation
computer
vision
pattern
recognition
pages
3642–3649
2012
deisenroth
rasmussen
pilco
model-based
data-efﬁcient
approach
policy
search
icml
2011
duan
chen
houthooft
schulman
abbeel
benchmarking
deep
reinforcement
learning
continuous
control
arxiv
preprint
arxiv:1604.06778
2016
farabet
couprie
najman
lecun
learning
hierarchical
features
scene
labeling
ieee
transactions
pattern
analysis
machine
intelligence
:1915–1929
2013
finn
zhang
tan
mccarthy
scharff
levine
guided
policy
search
code
implementation
2016.
software
available
rll.berkeley.edu/gps
fukushima
neural
network
model
mechanism
pattern
recognition
unaffected
shift
position-
neocognitron
transactions
iece
j62-a
:658–665
1979
giusti
machine
learning
approach
visual
perception
forest
trails
mobile
robots
ieee
robotics
automation
letters
2016
guo
singh
lee
lewis
wang
deep
learning
real-time
atari
game
play
using
ofﬂine
monte-carlo
tree
search
planning
nips
2014
guo
singh
lewis
lee
deep
learning
reward
design
improve
monte
carlo
tree
search
atari
games
arxiv:1604.07095
2016
ilin
kozma
werbos
efﬁcient
learning
cellular
simultaneous
recurrent
neural
networks-the
case
maze
navigation
problem
adprl
2007
joseph
geramifard
roberts
roy
reinforcement
learning
misspeciﬁed
model
classes
icra
2013
kaelbling
lozano-pérez
hierarchical
task
motion
planning
international
conference
robotics
automation
icra
pages
1470–1477
2011.
ieee
krizhevsky
sutskever
hinton
imagenet
classiﬁcation
deep
convolutional
neural
lecun
bottou
bengio
haffner
gradient-based
learning
applied
document
recognition
proceedings
ieee
:2278–2324
1998
levine
abbeel
learning
neural
network
policies
guided
policy
search
unknown
levine
finn
darrell
abbeel
end-to-end
training
deep
visuomotor
policies
jmlr
networks
nips
2012.
dynamics
nips
2014
2016
long
shelhamer
darrell
fully
convolutional
networks
semantic
segmentation
ieee
conference
computer
vision
pattern
recognition
pages
3431–3440
2015
mnih
badia
mirza
graves
lillicrap
harley
silver
kavukcuoglu
asynchronous
methods
deep
reinforcement
learning
arxiv
preprint
arxiv:1602.01783
2016
mnih
kavukcuoglu
silver
rusu
veness
bellemare
graves
riedmiller
fidjeland
ostrovski
human-level
control
deep
reinforcement
learning
nature
518
7540
:529–533
2015
neu
szepesvári
apprenticeship
learning
using
inverse
reinforcement
learning
gradient
methods
uai
2007
nogueira
cho
webnav
new
large-scale
task
natural
language
based
sequential
decision
making
arxiv
preprint
arxiv:1602.02261
2016
ross
gordon
bagnell
reduction
imitation
learning
structured
prediction
no-regret
online
learning
aistats
2011
schmidhuber
on-line
algorithm
dynamic
reinforcement
learning
planning
reactive
environments
international
joint
conference
neural
networks
ieee
1990
schulman
levine
abbeel
jordan
moritz
trust
region
policy
optimization
icml
sutton
barto
reinforcement
learning
introduction
mit
press
1998
theano
development
team
theano
python
framework
fast
computation
mathematical
expres-
sions
arxiv
e-prints
abs/1605.02688
may
2016
tieleman
hinton
lecture
6.5.
coursera
neural
networks
machine
learning
2012
todorov
erez
tassa
mujoco
physics
engine
model-based
control
intelligent
robots
systems
iros
2012
ieee/rsj
international
conference
pages
5026–5033
ieee
2012
watter
springenberg
boedecker
riedmiller
embed
control
locally
linear
latent
dynamics
model
control
raw
images
nips
2015
kiros
cho
courville
salakhudinov
zemel
bengio
show
attend
tell
neural
image
caption
generation
visual
attention
icml
2015
2015
visualization
learned
reward
value
figure
plot
learned
reward
value
function
gridworld
task
learned
reward
negative
obstacles
positive
goal
slightly
negative
constant
otherwise
resulting
value
function
peak
goal
gradient
pointing
towards
direction
goal
around
obstacles
plot
clearly
shows
block
learned
useful
planning
computation
figure
visualization
learned
reward
value
function
left
sample
domain
center
learned
reward
domain
right
resulting
value
function
block
domain
weight
sharing
vins
effective
depth
larger
depth
reactive
policies
one
may
wonder
whether
deep
enough
network
would
learn
plan
principle
cnn
fcn
depth
potential
perform
computation
vin
however
much
parameters
requiring
much
training
data
evaluate
untying
weights
recurrent
layers
vin
results
table
show
untying
weights
degrades
performance
stronger
effect
smaller
sizes
training
data
training
data
100
pred
loss
0.06
0.05
0.05
vin
traj
succ
rate
diff
98.2
0.106
99.4
0.018
99.3
0.089
vin
untied
weights
pred
traj
succ
rate
loss
diff
91.9
0.094
0.09
95.2
0.078
0.07
0.05
95.6
0.068
table
performance
grid-world
domain
evaluation
effect
module
shared
weights
relative
data
size
gridworld
reinforcement
learning
demonstrate
value
iteration
network
trained
using
reinforcement
learning
methods
achieves
favorable
generalization
properties
compared
standard
convolutional
neural
networks
cnns
overall
setup
experiment
follows
train
policies
parameterized
vins
policies
parameterized
convolutional
networks
set
randomly
generated
gridworld
maps
way
described
test
performance
held-out
set
test
maps
generated
way
set
training
maps
disjoint
training
set
mdp
one
would
expect
gridworld
environment
states
positions
map
actions
movements
left
right
rewards
reaching
goal
falling
hole
−0.01
otherwise
encourage
policy
ﬁnd
shortest
path
transitions
deterministic
structure
networks
vins
used
similar
described
main
body
paper
value-iteration
recurrences
approximate
values
every
state
action
map
attention
selects
current
state
converted
network
vin
cnn
90.9
82.5
86.9
33.1
table
results
performance
test
maps
probability
distribution
actions
using
softmax
function
use
maps
maps
convolutional
networks
structure
adapted
accommodate
size
maps
8×8
maps
use
ﬁlters
ﬁrst
layer
100
ﬁlters
second
layer
size
layers
followed
max-pool
end
fully
connected
hidden
layer
100
hidden
units
followed
fully-connected
layer
outputs
converted
probabilities
using
softmax
function
network
maps
similar
uses
three
convolutional
layers
100
100
ﬁlters
respectively
ﬁrst
two
max-pooled
followed
two
fully-connected
hidden
layers
200
100
hidden
units
respectively
connecting
outputs
performing
softmax
training
curriculum
ensure
policies
simply
memorizing
speciﬁc
maps
randomly
select
map
episode
maps
far
difﬁcult
others
agent
learns
best
stands
reasonable
chance
reaching
goal
thus
found
beneﬁcial
begin
training
easiest
maps
gradually
progress
difﬁcult
maps
idea
curriculum
training
consider
curriculum
training
way
address
exploration
problem
completely
untrained
agent
dropped
challenging
map
moves
randomly
stands
approximately
zero
chance
reaching
goal
thus
learning
useful
reward
even
random
policy
consistently
reach
goals
nearby
learn
something
useful
process
e.g
move
toward
goal
policy
knows
solve
tasks
difﬁculty
easily
learn
solve
tasks
difﬁculty
compared
completely
untrained
policy
strategy
well-aligned
formal
education
structured
effectively
learn
calculus
without
knowing
basic
algebra
environments
obvious
difﬁculty
metric
fortunately
gridworld
task
deﬁne
difﬁculty
map
length
shortest
path
start
state
goal
state
natural
start
difﬁculty
start
state
goal
state
adjacent
ramp
difﬁculty
one
level
certain
threshold
success
reached
experiments
use
average
discounted
return
assess
progress
increase
difﬁculty
level
average
discounted
return
iteration
exceeds
35.
rule
chosen
empirically
takes
account
fact
higher
difﬁculty
levels
difﬁcult
learn
networks
trained
using
trust
region
policy
optimization
trpo
algorithm
using
publicly
available
code
rllab
benchmark
testing
testing
ignore
exact
rewards
measure
simply
whether
agent
reaches
goal
map
test
set
run
episode
noting
policy
succeeds
reaching
goal
proportion
successful
trials
trials
reported
network
see
table
maps
used
number
training
iterations
types
networks
make
comparison
fair
possible
maps
became
clear
convolutional
network
struggling
allowed
twice
many
training
iterations
vin
yet
still
failed
achieve
even
remotely
similar
level
performance
test
maps
see
left
image
figure
posit
vin
learns
plan
cnn
simply
follows
reactive
policy
though
cnn
policy
performs
reasonably
well
smaller
domains
scale
larger
domains
vin
see
right
image
figure
technical
details
experiments
report
full
technical
details
used
training
networks
figure
results
performance
vin
cnn
test
maps
left
performance
maps
function
amount
training
right
success
rate
test
maps
increasing
difﬁculty
d.1
grid-world
domain
training
set
consists
5000
random
grid-world
instances
shortest-path
trajectories
calculated
using
optimal
planning
algorithm
random
start-state
random
goal-state
instance
total
trajectories
state
trajectory
produce
-sized
observation
image
simage
ﬁrst
channel
simage
encodes
obstacle
presence
obstacle
otherwise
second
channel
encodes
goal
position
goal
otherwise
full
observation
vector
simage
addition
state
produce
label
encodes
action
one
directions
optimal
shortest-path
policy
would
take
state
design
vin
task
follows
state
space
chosen
grid-world
similar
true
state
space
s.4
reward
space
represented
map
chose
reward
mapping
cnn
simage
input
one
layer
150
kernels
size
second
layer
one
ﬁlter
output
thus
maps
image
obstacles
goal
reward
image
transitions
deﬁned
convolution
kernels
block
exploit
fact
transitions
grid-world
local
note
transitions
deﬁned
way
depend
state
interestingly
shall
see
network
learned
rewards
transitions
nevertheless
enable
successfully
plan
task
attention
module
since
one-to-one
mapping
agent
position
chose
trivial
approach
selects
values
block
state
real
mdp
i.e.
ﬁnal
reactive
policy
fully
connected
softmax
output
layer
weights
πre
·|ψ
exp
cid:0
cid:62
cid:1
trained
several
neural-network
policies
based
multi-class
logistic
regression
loss
function
using
stochastic
gradient
descent
rmsprop
step
size
implemented
theano
library
compare
policies
vin
network
used
vin
model
section
described
channels
layer
block
recurrence
set
relative
problem
size
domains
16×
domains
28×
domains
guideline
choosing
values
keep
network
small
guaranteeing
goal
information
ﬂow
every
state
map
cnn
network
devised
cnn-based
reactive
policy
inspired
recent
impressive
results
dqn
convolution
layers
100
100
100
kernels
size
max-pooling
ﬁrst
third
layers
ﬁnal
layer
fully
connected
maps
softmax
actions
represent
current
state
added
simage
channel
encodes
current
position
current
state
otherwise
4for
particular
conﬁguration
obstacles
true
grid-world
domain
captured
state
space
obstacles
encoded
mdp
transitions
notation
general
obstacle
conﬁguration
obstacle
positions
also
encoded
state
vin
able
learn
policy
general
obstacle
conﬁguration
planning
state
space
also
taking
account
observation
map
fully
convolutional
network
fcn
problem
setting
domain
similar
semantic
segmentation
pixel
image
assigned
semantic
label
action
case
therefore
devised
fcn
inspired
state-of-the-art
semantic
segmentation
algorithm
convolution
layers
ﬁrst
layer
ﬁlter
spans
whole
image
properly
convey
information
goal
every
state
ﬁrst
convolution
layer
150
ﬁlters
size
span
whole
image
convey
information
goal
every
pixel
second
layer
150
ﬁlters
size
third
layer
ﬁlters
size
produce
output
sized
10×
similarly
layer
vin
similarly
attention
mechanism
vin
values
correspond
current
state
pixel
passed
fully
connected
softmax
output
layer
d.2
mars
domain
consider
problem
autonomously
navigating
surface
mars
rover
mars
science
laboratory
msl
lockwood
2006
long-distance
trajectories
msl
limited
ability
climbing
high-degree
slopes
path-planning
algorithm
therefore
avoid
navigating
high-slope
areas
experiment
plan
trajectories
avoid
slopes
degrees
using
overhead
terrain
images
high
resolution
imaging
science
experiment
hirise
mcewen
al.
2007
hirise
data
consists
grayscale
images
mars
terrain
matching
elevation
data
accurate
tens
centimeters
used
image
33.3km
6.3km
area
49.96
degrees
latitude
219.2
degrees
longitude
10.5
meters
pixel
resolution
domain
128×
128
image
patch
deﬁned
16×
grid-world
state
considered
obstacle
corresponding
image
patch
contained
angle
degrees
evaluated
using
additional
elevation
data
example
domain
terrain
image
depicted
figure
mdp
shortest-path
planning
case
similar
grid-world
domain
section
4.1
vin
design
similar
deeper
cnn
reward
mapping
processing
image
goal
train
network
predicts
shortest-path
trajectory
directly
terrain
image
data
emphasize
ground-truth
elevation
data
part
input
elevation
therefore
must
inferred
needed
terrain
image
vin
design
follows
model
section
4.1.
case
however
instead
feeding
obstacle
map
feed
raw
terrain
image
accordingly
modify
reward
mapping
additional
cnn
layers
processing
image
ﬁrst
kernels
size
max-pooling
second
kernels
size
max-pooling
resulting
tensor
concatenated
goal
image
passed
third
layer
150
kernels
size
fourth
layer
one
ﬁlter
output
state
inputs
output
labels
remain
grid-world
experiments
emphasize
whole
network
trained
end-to-end
without
pre-training
input
ﬁlters
table
present
results
training
map
10k
image-patch
dataset
random
trajectories
per
patch
evaluated
held-out
test
set
patches
figure
shows
instance
input
image
obstacles
shortest-path
trajectory
trajectory
predicted
method
put
84.8
success
rate
context
compare
best
performance
achievable
without
access
elevation
data
make
comparison
trained
cnn
classify
whether
patch
obstacle
classiﬁer
trained
using
image
data
vin
network
labels
true
obstacle
classiﬁcations
elevation
map
reiterate
vin
network
access
ground-truth
obstacle
classiﬁcation
labels
training
testing
training
classiﬁer
standard
binary
classiﬁcation
problem
performance
represents
best
obstacle
identiﬁcation
possible
cnn
domain
best-achievable
shortest-path
prediction
deﬁned
shortest
path
obstacle
map
generated
classiﬁer
raw
image
results
optimal
predictor
reported
table
90.3
success
rate
shows
obstacle
identiﬁcation
raw
image
indeed
challenging
thus
success
rate
vin
network
trained
without
obstacle
labels
ﬁgure
planning
process
quite
remarkable
d.3
continuous
control
training
chose
guided
policy
search
gps
algorithm
unknown
dynamics
suitable
learning
policies
continuous
dynamics
contacts
used
publicly
available
gps
code
mujoco
physical
simulation
gps
works
learning
time-
varying
ilqg
controllers
domain
ﬁtting
controllers
single
policy
using
pred
loss
0.089
traj
succ
rate
diff
84.8
0.016
90.3
0.0089
vin
best
achievable
table
performance
vins
mars
domain
comparison
performance
planner
used
obstacle
predictions
trained
labeled
obstacle
data
shown
upper
bound
performance
demonstrates
difﬁculty
identifying
obstacles
raw
image
data
remarkably
vin
achieved
close
performance
without
access
labeled
data
obstacles
supervised
learning
process
repeated
several
iterations
special
cost
function
used
enforce
agreement
trajectory
distribution
ilqg
controllers
refer
full
algorithm
details
task
ran
iterations
ilqg
cost
quadratic
distance
goal
followed
one
iteration
policy
ﬁtting
allows
cleanly
compare
vins
policies
without
gps-speciﬁc
effects
vin
design
similar
grid-world
cases
state
space
grid-world
transitions
convolution
kernels
block
similar
grid-world
section
4.1.
however
made
important
modiﬁcations
attention
module
selects
patch
value
centered
around
current
discretized
position
map
ﬁnal
reactive
policy
3-layer
fully
connected
network
2-dimensional
continuous
output
controls
addition
due
limited
number
training
domains
pre-trained
vin
transition
weights
correspond
discounted
grid-world
transitions
example
transitions
action
north-west
would
top
left
corner
zeros
otherwise
training
end-to-end
reasonable
prior
weights
2-d
task
emphasize
even
initialization
initial
value
function
meaningless
since
reward
map
yet
learned
reward
mapping
cnn
simage
input
one
layer
150
kernels
size
second
layer
one
ﬁlter
output
d.4
webnav
webnav
recently
proposed
goal-driven
web
navigation
benchmark
webnav
web
pages
links
website
form
directed
graph
agent
presented
query
text
consists
sentences
target
page
hops
away
starting
page
goal
agent
navigate
target
page
starting
page
via
clicking
links
per
page
choose
agent
receives
reward
reaching
target
page
via
path
longer
hops
evaluation
convenience
experiment
agent
receive
reward
reaches
destination
via
shortest
path
makes
task
much
harder
measure
top-1
top-4
prediction
accuracy
well
average
reward
baseline
vin
model
every
page
valid
transitions
cid:48
cid:48
every
web
page
every
query
text
utilize
bag-of-words
model
pretrained
word
embedding
provided
produce
feature
vectors
agent
choose
valid
actions
cid:48
cid:48
based
current
baseline
method
uses
single
tanh-layer
neural
net
parametrized
compute
ﬁnal
baseline
policy
computed
via
hidden
vector
tanh
cid:20
πbsl
cid:48
exp
cid:0
cid:62
cid:48
cid:1
cid:48
cid:21
cid:19
cid:18
design
vin
task
follows
ﬁrstly
selected
smaller
website
approximate
graph
choose
states
query
page
compute
reward
¯s|q
tanh
parameters
diagonal
matrix
vector
transition
since
graph
remains
unchanged
ﬁxed
attention
module
cid:63
compute
cid:63
cid:80
cid:17
cid:63
wrφ
wπφ
cid:16
¯s∈
sigmoid
cid:62
cid:62
parameters
diagonal
moreover
compute
coefﬁcient
based
query
state
using
tanh-layer
neural
net
parametrized
cid:16
cid:17
network
top-1
test
err
top-4
test
err
avg
reward
bsl
vin
52.019
50.562
24.424
26.055
0.27779
0.30389
table
performance
full
wikipedia
dataset
cid:18
cid:20
cid:21
cid:19
finally
combine
module
baseline
method
vin
tanh
model
simply
adding
outputs
two
networks
together
addition
experiments
reported
main
text
performed
experiments
full
wikipedia
using
wikipedia
schools
graph
vin
planning
report
preliminary
results
full
wikipedia
website
full
wikipedia
dataset
consists
779169
training
queries
million
training
samples
20004
testing
queries
76664
testing
samples
4.8
million
pages
maximum
300
links
per
page
use
whole
wikischool
website
approximate
graph
set
vin
accelerate
training
ﬁrstly
train
module
obtained
case
jointly
train
whole
model
results
shown
tab
vin
achieves
1.5
better
prediction
accuracy
baseline
interestingly
1.5
prediction
accuracy
enhancement
vin
achieves
2.5
better
success
rate
baseline
note
agent
success
making
consecutive
correct
predictions
indicates
provide
useful
high-level
planning
information
d.5
additional
technical
comments
runtime
domains
different
samples
domain
share
com-
putation
since
observation
therefore
single
computation
required
samples
domain
using
gpu
code
theano
vins
much
slower
baselines
language
task
however
since
theano
support
convolutions
graphs
sparse
operations
gpu
vins
considerably
slower
implementation
hierarchical
modules
number
iterations
required
vin
depends
problem
size
consider
example
grid-world
goal
located
steps
away
state
least
iterations
required
convey
reward
information
goal
state
clearly
action
prediction
obtained
less
iterations
state
unaware
goal
location
therefore
unacceptable
convey
reward
information
faster
reduce
effective
propose
perform
multiple
levels
resolution
term
model
hierarchical
network
hvin
due
similarity
hierarchical
planning
algorithms
hvin
copy
input
down-sampled
factor
ﬁrst
fed
module
termed
high-level
module
down-sampling
offers
speedup
information
transmission
map
price
reduced
accuracy
value
layer
high-level
module
up-sampled
added
additional
input
channel
input
standard
module
thus
high-level
module
learns
mapping
down-sampled
image
features
suitable
reward-shaping
nominal
module
full
hvin
model
depicted
figure
model
easily
extended
include
multiple
levels
hierarchy
table
shows
performance
hvin
module
grid-world
task
compared
vin
results
reported
main
text
used
down-sampling
layer
similarly
standard
vin
convolution
kernels
150
channels
hidden
layer
down-sampled
image
standard
image
channels
layer
block
similarly
vin
networks
recurrence
set
relative
problem
size
taking
account
down-
sampling
factor
domains
domains
domains
comparison
respective
values
standard
vins
hvins
demonstrated
better
performance
larger
map
attribute
improved
information
transmission
hierarchical
module
figure
hierarchical
network
copy
input
ﬁrst
fed
convolution
layer
downsampled
signal
fed
module
produce
coarse
value
function
corresponding
upper
level
hierarchy
value
function
up-sampled
added
additional
channel
reward
layer
standard
module
lower
level
hierarchy
domain
prediction
loss
0.004
0.05
0.11
vin
rate
99.6
99.3
success
trajectory
prediction
success
trajectory
hierarchical
vin
diff
0.001
0.089
0.086
loss
0.005
0.03
0.05
rate
99.3
98.1
diff
0.0
0.007
0.037
table
hvin
performance
grid-world
domain
observationrewardrhierarchical
networkvimodulerewardk
recurrenceqnew
valuedownsampleup-samplehigh-level
blockreward
map
