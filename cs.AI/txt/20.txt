proactive
message
passing
memory
factor
networks
proactive
message
passing
memory
factor
networks
patrick
eschenfeldt
operations
research
center
massachusetts
institute
technology
cambridge
02139
usa
dan
schmidt
disney
research
boston
cambridge
02142
usa
stark
draper
dept
electrical
computer
engineering
university
toronto
m5s
3g4
canada
jonathan
yedidia
disney
research
boston
cambridge
02142
usa
editor
tbd
peschen
mit.edu
dan.schmidt
disneyresearch.com
stark.draper
utoronto.ca
yedidia
disneyresearch.com
abstract
introduce
new
type
graphical
model
call
memory
factor
network
mfn
show
use
mfns
model
structure
inherent
many
types
data
sets
also
introduce
associated
message-passing
style
algorithm
called
proactive
message
passing
pmp
performs
inference
mfns
pmp
comes
convergence
guarantees
eﬃcient
comparison
competing
algorithms
variants
belief
propagation
specialize
mfns
pmp
number
distinct
types
data
discrete
continuous
labelled
inference
problems
interpolation
hypothesis
testing
provide
examples
discuss
approaches
eﬃcient
implementation
keywords
machine
learning
optimization
pattern
recognition
models
vision
scene
understanding
image
restoration
introduction
motivations
paper
introduce
memory
factor
networks
mfns
proactive
message
passing
pmp
algorithm
objective
combine
capability
message
passing
algorithms
make
large-scale
inferences
highly
eﬃcient
manner
ability
machine
learning
algorithms
generalize
experience
factor
graphs
message
passing
kschischang
al.
2001
loeliger
al.
2007
koller
friedman
2009
yedidia
al.
2005
sudderth
freeman
2008
proved
extremely
eﬀective
combination
one
faced
inference
task
wherein
global
problem
structure
decomposes
large
set
local
constraints
applications
error
correction
decoding
richardson
urbanke
2008
local
constraints
january
2016
analog
devices
lyric
labs
cambridge
02142
eschenfeldt
schmidt
draper
yedidia
simple
characterization
makes
local
inference
computationally
simple
message-passing
algorithms
iterate
making
local
inferences
combining
local
decisions
global
estimate
however
many
settings
local
problem
structure
known
ahead
time
engineered
error
correcting
codes
rather
must
learned
instance
number
examples
may
provided
initial
task
deduce
underlying
problem
structure
exemplars
following
one
make
inferences
based
problem
structure
learned
tasks—of
deducing
problem
structure
making
inferences
based
structure—are
central
many
areas
data
analysis
statistics
machine
learning
mfns
set
variables
describe
problem
conﬁguration
collection
memory
factors
learned
examples
encode
possibly
soft
constraints
overlapping
subsets
variables
high-level
goal
pmp
ﬁnd
conﬁguration
variables
minimally
conﬂicting
local
problem
structure
represented
memory
factors
evidence
obtained
world
conﬂicts
occur
constraints
overlap
general
satisﬁed
exactly
description
somewhat
generic
one
key
innovation
mfns
pmp
design
memory
factors
encode
learned
local
problem
structure
manner
easily
accessible
inference
guiding
philosophy
one
joint
design
graphical
model
message
passing
algorithm
make
necessary
computations
eﬃcient
analogy
biology
need
memories
easily
accessible
thinking
thus
engineer
detailed
statistical
structure
problem
engineer
way
store
knowledge
statistical
structure
order
make
easy
exploit
inference
tasks
framework
memories
correspond
learned
local
problem
structure
encoded
memory
factors
second
key
innovation
pmp
simple
methodology
producing
local
inferences
one
guarantees
global
convergence
learned
problem
structure
encoded
memory
factors
extremely
rich—in
comparison
factor
nodes
used
graphical
models
belief
propagation
typically
applied
to—a
simple
methodology
producing
local
inference
important
maintain
computational
tractability
pmp
accomplishes
intuitive
voting
mechanism
iteration
considers
current
messages
coming
neighboring
memory
factors
order
make
choices
reduce
global
objective
pmp
thus
proactive
sense
considers
eﬀect
possible
local
choice
global
situation
deciding
choice
implement
proactivity
ensures
global
objective
decreases
iteration
guarantees
pmp
converge
locally
optimum
conﬁguration
order
ensure
pmp
converges
good
local
optimum
use
powerful
heuristic
scheduling
factor
updates
one
derived
novel
notion
factor
conﬁdence.
general
idea
combining
message-passing
inference
algorithm
graphical
model
built
using
example-based
learning
approach
introduced
freeman
vista
vision
image/scene
training
approach
freeman
al.
2000
main
application
considered
using
vista
approach
example-based
super-resolution
freeman
al.
2002
vista
approach
used
markov
random
ﬁeld
graphical
mod-
els
built
dynamically
response
inference
problem
encountered
probable
example
patches
included
markov
random
ﬁeld
node
proactive
message
passing
memory
factor
networks
inference
algorithms
used
belief
propagation
eﬃcient
one-pass
algorithms
one
think
paper
providing
formal
generalization
vista
idea
whereas
vista
work
primarily
concerned
ﬁnding
good
solution
particular
application
super-resolution
primary
concern
carefully
deﬁne
demonstrate
general
approach
learning-based
inference
based
memory
factor
network
data
structure
proactive
message-passing
algorithm
believe
ap-
proach
applied
wide
range
problems
going
beyond
encountered
computer
vision
rest
paper
structured
follows
section
introduce
memory
factor
networks
section
describe
proactive
message
passing
algorithm
section
specialize
discussions
section
speciﬁc
types
variable
nodes
integer
real
label
cost
functions
absolute
diﬀerence
quadratic
diﬀerence
indicator
section
turn
memory
factors
describe
two
distinct
types
memory
factors
ﬁrst
consider
memory
tables
memory
factor
consists
database
fragments
observed
exemplars
consider
subspace
factors
memory
factor
constrains
local
problem
structure
reside
learned
low-
dimensional
subspace
section
provide
illustrative
applications
include
face
reconstruction
missing
noisy
data
music
reconstruction
handwritten
digit
classiﬁcation
examples
compare
contrast
performance
memory
tables
subspace
factors
make
concluding
remarks
section
memory
factor
networks
memory
factor
network
mfn
contains
variables
constraints
subsets
variables
constraints
may
encode
problem
structure
may
induced
observations
capture
two
types
constraints
memory
factor
evidence
factor
nodes
respectively
seek
conﬁguration
variables
minimizes
cost
function
variables
induced
factors
following
use
notation
denote
index
set
cardinality
interconnection
variables
factors
described
edge-weighted
bipartite
graph
graph
cost
function
formulation
described
following
sets
set
variable
nodes
indexed
values
discuss
various
choices
alphabet
real
integer
binary
label
distinct
variables
diﬀerent
alphabets
set
factor
nodes
indexed
mentioned
factors
either
memory
factors
indexed
evidence
factors
indexed
set
edges
induced
neighborhood
structure
network
slight
always
resolvable
context
equivocation
notation
write
denote
set
factor
variable
nodes
neighboring
variable
node
factor
node
note
evidence
factor
typically
attached
single
variable
typically
|na|
eschenfeldt
schmidt
draper
yedidia
set
votes
always
restrict
weights
associated
variable
equal
weights
corresponding
vote
weights
ﬁxed
real
numbers
normally
mfn
used
inference
degrees
freedom
memory
factor
network
values
variable
nodes
votes
factor
nodes
changes
state
world
reﬂected
changes
evidence
factors
mfn
learning
acquisition
memories
reﬂected
adjustments
memory
factors
inference
time
reﬂected
factor
possible
votes
often
reason
refer
set
variables
votes
weights
associated
single
factor
use
refer
vector
variables
neighboring
factor
use
similarly
deﬁned
denote
vote
vector
weight
vector
occasionally
need
refer
say
set
variables
set
write
refer
set
also
need
refer
set
votes
regarding
variable
subset
neighbors
denote
set
perhaps
edge
weights
description
standard
quite
neatly
ﬁts
formalism
normal
factor
graph
due
forney
forney
2001
map
forney
normal
representation
variable
factor
nodes
would
correspond
factors
former
equality
constraints
votes
would
correspond
variables
objective
minimize
global
cost
function
factored
based
graphical
structure
problem
include
two
types
local
costs
mismatch
costs
selection
costs
mismatch
costs
penalize
diﬀerence
variable
setting
factor
vote
variable
contribution
mismatches
global
cost
additive
variable
factor
indices
contribution
weighted
particular
vote
weight
make
mismatch
costs
function
variable
index
factor
index
reason
weighting
votes
pertaining
particular
variable
compatible
sense
e.g.
therefore
measure
mismatch
apply
selection
costs
make
various
choices
factor
vote
vector
less
costly
thus
function
pertinent
factor
index
unlike
mismatch
costs
selection
costs
general
additive
factor
indices
variable
indices
denote
mismatch
costs
use
denote
selection
costs
use
cid:34
cid:88
a∈if
cid:88
i∈na
cid:35
detail
later
sections
consider
various
forms
types
cost
functions
example
local
mismatch
costs
may
use
absolute
diﬀerence
squared
diﬀerence
v|2
indicator
function
cid:54
discrete
set
selection
costs
i∈na
belong
discrete
set
lie
certain
subspace
design
limit
vector
space
practice
selection
costs
arise
result
training
phase
examples
shown
system
likely
frequent
conﬁgurations
learned
later
system
trying
understand
new
sample
world
tries
minimize
overall
proactive
message
passing
memory
factor
networks
cost
thus
interpreting
new
sample
terms
well
variables
made
match
evidence
sample
well
variables
match
structure
learned
training
examples
proactive
message
passing
eﬀorts
minimize
introduce
proactive
message
passing
pmp
algorithm
proactive
message
passing
works
sequence
iterative
subproblems
attempt
minimize
pmp
algorithm
understood
analogy
idealized
political
convention
objective
convention
determine
party
platform
party
platform
consists
number
stances
issues
issue
corresponds
one
variable
analogy
stance
value
variable
delegate
corresponds
one
factor
delegate
concerned
generally
small
subset
issues
close
convention
want
identify
optimal
vector
stances
results
minimal
dissatisfaction
amongst
delegates
dissatisfaction
measured
analogy
political
convention
exact
hope
help
elucidate
pmp
algorithm
develop
intuition
convention
sequence
rounds
balloting
round
corresponds
one
iteration
algorithm
round
delegates
vote
issues
concern
others
abstain
voting
delegates
vote
initially
ones
conﬁdent
opinions
delegates
temporarily
abstain
delay
voting
become
suﬃciently
conﬁdent
opinions
pmp
factor
votes
votes
variables
interest
never
abstains
round
balloting
delegates
reconsider
opinions
may
changed
based
recent
set
votes
cast
forming
opinions
delegate
takes
account
current
votes
non-abstaining
delegates
issue
interest
delegate
opinions
formed
correspond
votes
cast
would
reduce
maximally
cost
function
derived
algorithm
termed
proactive
delegate
preemptively
evaluates
change
votes
would
impact
dissatisfaction
delegates
voted
set
issues
next
round
balloting
subset
delegates
chosen
change
votes
cast
initial
set
votes
delegates
chosen
cast
new
votes
previously
voted
leave
votes
unchanged
subset
chosen
set
delegates
conﬁdent
opinions
delegates
least
likely
opinions
change
future
due
delegates
votes
cost
function
structured
eventually
delegates
must
cast
votes
abstain
convention
ends
delegates
votes
align
suﬃciently
opinions
one
wants
change
votes
3.1
pmp
speciﬁcation
put
mathematical
framework
around
intuition
described
description
factor
current
opinion
value
variable
neigh-
boring
opinion
may
diﬀer
vote
already
cast
opinion
vector
one-to-one
correspondence
vote
vector
denoted
iterative
subproblems
pmp
works
correspond
rounds
bal-
loting
iteration
subset
factors
abstain
voting
one
part
eschenfeldt
schmidt
draper
yedidia
objective
subproblem
modiﬁcation
original
objective
considers
participating
non-abstaining
factors
cid:34
cid:88
a∈if\a
cid:88
i∈na
cid:35
connect
original
objective
pmp
minimizes
cost
tuple
|a|
ﬁrst
element
tuple
count
abstaining
factors
second
compare
tuples
assert
lexicographic
order
either
ordering
global
optimum
tuple
minimizes
pmp
algorithm
feature
iteration
cost
decrease
ultimately
factors
abstain
although
algorithm
may
converge
local
optimum
rather
global
one
feature
guaranteed
convergence
least
local
optimum
shared
message-passing
algorithms
operating
factor
graphs
containing
cycles
yedidia
al.
2005
addition
abstaining
set
pmp
works
three
sets
factors
vote-
changing
set
reacting
set
dissatisﬁed
set
iteration
ballot
set
factors
delegates
change
votes
set
factors
vote-changing
set
factor
voted
previously
removed
abstaining
set
note
never
add
factors
therefore
cardinality
abstaining
set
|a|
decreases
monotonically
iteration
count
reacting
set
set
factors
neighbor
variables
connected
set
factors
recently
changed
votes
∪a∈v
∪i∈nani\
set
factors
based
recent
change
votes
might
change
opinions
variables
issues
dissatisﬁed
set
contains
factors
whose
recomputed
opinion
vector
fact
match
vote
vector
yet
given
chance
cast
votes
match
latest
opinions
note
abstaining
set
plays
fundamental
role
algorithm
role
reactive
dissatisﬁed
sets
purely
reduce
computation
algorithm
could
deﬁned
without
tracking
latter
two
sets
number
redundant
computations
would
repeated
iteration
initialize
algorithm
start
set
factors
want
vote
ﬁrst
iteration
one
natural
choice
set
set
evidence
nodes
set
votes
factors
equal
observations
set
abstaining
set
if\v
reacting
set
∪a∈v
∪i∈nani\
dissatisﬁed
set
note
use
selection
mismatch
costs
non-
negative
therefore
elements
cost
tuple
associated
pmp
non-negative
given
iteration
factor
node
stops
abstaining
ﬁrst
element
tuple
decreases
reducing
cost
hand
abstaining
set
change
second
element
tuple
change
since
design
choice
new
votes
decrease
abstaining
set
change
second
element
tuple
least
small
previous
iteration
thus
cost
tuple
monotonically
decreasing
overall
proactive
message
passing
algorithm
summarized
algorithm
key
step
algorithm
optimal
opinion
vector
factor
computed
using
step
opinion
vector
chosen
factor
minimize
sum
proactive
message
passing
memory
factor
networks
selection
cost
factor
mismatch
costs
cid:80
mismatch
costs
cid:88
cid:88
i∈na
ni\
i∈na
¯oi
factor
factors
neighbor
set
variables
factor
neighbors
proactively
choose
value
neighboring
variables
optimize
combined
costs
second
third
terms
given
current
state
factors
votes
proactivity
reminiscent
cavity
method
introduced
statistical
mechanics
study
spin
glasses
m´ezard
al.
1987
another
step
highlighted
choose
conﬁdent
factor
change
votes
simply
want
greedily
choose
factor
reduce
overall
cost
might
two
diﬀerent
choices
values
variables
connected
factor
reduce
overall
cost
similar
large
amount
committing
one
prematurely
could
lead
convergence
poor
local
optimum
particular
easily
happen
factors
initially
isolated
factors
including
evidence
nodes
could
reduce
overall
cost
signiﬁcantly
selecting
memory
set
variable
values
consistent
one
examples
learned
training
voting
early
way
would
lead
premature
commitments
leading
ultimately
convergence
poor
local
optima
instead
want
prioritize
factors
one
choice
variable
values
signiﬁcantly
better
choice—what
call
conﬁdent
factors
factors
typically
share
variables
several
factors
already
voting
given
votes
one
choice
memory
would
reduce
overall
cost
much
3.2
message-passing
formulation
equation
written
general
form
want
manipulate
form
reveals
message
passing
interpretation
particular
want
obtain
messages
factor
node
variable
nodes
connected
informing
factor
preferences
rest
network
variable
node
form
suﬃcient
statistic
end
begin
deﬁning
˜xi
optimal
choice
based
votes
variable
non-abstaining
neighbors
factor
i.e.

cid:88
b∈n

˜xi
arg
min
ni\
deﬁned
set
non-abstaining
memory
factors
neighboring
variable
excluding
factor
eschenfeldt
schmidt
draper
yedidia
algorithm
proactive
message
passing
vote
changing
set
cid:54
factor
reacting
set
compute
opinion
vector
factor
opinion
vector
chosen
mini-
mize
combination
selection
cost
opinion
vector
aggregate
dissatisfaction
induced
choice
dissatisfaction
measured
sum
mismatch
costs
appropriately
weighted
computed
respect
op-
timal
choice
variable
settings
vector
possible
opinion
vector
following
vector
dimension
alphabet
va.
χa
cid:88
i∈na
min
ψi
¯oi
cid:88
ni\

arg
min
set
optimizers
one
matches
previous
vote
vector
set
va.
cid:39
else
factor
opinion
vector
approximately
current
vote
vector
factor
satisﬁed
set
note
speciﬁcation
cid:39
cid:48
cid:48
problem
speciﬁc
factor
dissatisﬁed
set
compute
conﬁdence
factor
computation
problem
speciﬁc
end
end
identify
factor
change
votes
sequential
update
rule
would
update
single
conﬁdent
factor
using
arg
maxa∈d
ties
broken
random
simultaneous
voting
rules
also
possible
|v|
example
updating
ﬁxed
fraction
conﬁdent
factors
factor
set
factor
vote
vector
equal
opinion
vector
end
recompute
abstaining
set
removing
new
voters
a\v
recompute
dissatisﬁed
set
removing
factors
changed
votes
d\v
recent
update
votes
∪a∈v
∪i∈nani\
determine
reacting
set
factors
whose
opinion
vectors
may
change
due
end
deﬁne
output
set
i|i
∪a∈if\ana
problems
stage
always
equal
equal
i∈na
cid:80
a∈if\a
cid:80
optimal
compute
arg
minxo
return
˜xo
variables
output
set
xi|i
iv\o
return
unknown
settings
variable
given
votes
set
˜xo
ﬂag
proactive
message
passing
memory
factor
networks
next
rewrite
innermost
argument
ψi
¯oi
ψi
¯oi
cid:88
cid:88
b∈n
b∈n
min
min

˜xi
˜xi
˜xi
cid:88
b∈n
˜xi

substitute
result
back
drop
last
term
function
either
yielding
following
modiﬁed
form
optimization
stated
χa
cid:88
i∈na
ψi
¯oi
min
cid:88
b∈n
arg
min
˜xi


many
reasonable
choices
mismatch
cost
functions
variable
alphabets
minimization
requires
small
amount
summary
information
think
summary
information
external
vote
weight
vectors
cid:88
message
passed
variable
factor
rewrite
cid:40
cid:41
arg
min
¯oi
i∈na
¯oi
interpreted
incremental
cost
factor
casting
vote
variable
always
possible
reexpress
simply
setting
equal
set
transformation
thus
real
interest
dimension
cardinality
message
small
easy
compute
incremental
cost
section
present
number
examples
useful
mismatch
cost
functions
variable
alphabets
meet
criteria
i.e.
3.3
parallel
updates
simultaneous
voting
transformation
enables
perform
opinion
computation
relatively
eﬃciently
still
accounts
overwhelming
fraction
total
running
time
pmp
particularly
set
candidate
opinions
minimization
runs
large
luckily
computation
entirely
independent
factor
aﬀect
state
external
factor
lines
3–9
algorithm
executed
parallel
factor
update
must
happen
separate
loop
thus
pmp
naturally
parallelizable
available
number
cores
diﬀerent
form
parallelism
actually
changes
behavior
algorithm
multiple
factors
change
votes
simultaneously
simplest
version
pmp
single
conﬁdent
factor
changes
vote
iteration
version
easily
guarantee
convergence
iteration
one
memory
factor
changes
eschenfeldt
schmidt
draper
yedidia
vote
vector
make
guaranteed
non-increasing
change
overall
objective
memory
factors
able
adjust
change
computing
new
opinions
contrast
could
let
multiple
factors
example
conﬁdent
percent
simultaneously
change
votes
|v|
principal
positive
eﬀect
modi-
ﬁcation
reduction
total
number
times
computation
executed
course
algorithm
see
note
non-simultaneous
version
factor
enters
reacting
set
every
time
nearby
factor
votes
one
neighboring
variables
simultaneous
voting
two
nearby
factors
change
vote
simultaneously
factor
react
total
rather
twice
since
every
factor
reacting
set
participates
opinion
updates
reducing
number
times
factor
enters
reacting
set
reduces
total
number
opinion
updates
algo-
rithm
consumes
less
cpu
time
even
single
core
computer
simultaneous
voting
also
signiﬁcantly
reduces
overall
number
iterations
required
pmp
converge
shall
see
later
terms
quality
optimum
found
clear
whether
simultaneous
voting
superior
inferior
serial
voting
simultaneous
voting
potential
advantage
make
pmp
less
sensitive
whims
single
factor
conﬁdent
factor
disagrees
next
four
conﬁdent
factors
letting
vote
likely
move
system
non-optimal
direction
factors
may
change
opinions
based
ﬁrst
factor
votes
may
get
chance
express
majority
opinion
hand
ﬁve
factors
vote
simultaneously
system
move
way
consistent
majority
empirically
found
pmp
converged
slightly
better
optima
mnist
handwritten
digit
classiﬁcation
task
simultaneous
voting
used
see
section
6.3
hand
simultaneous
voting
slightly
degraded
empirical
performance
pmp
restoration
corrupted
images
see
section
6.4
simultaneous
voting
introduce
complication
pmp
longer
true
cost
must
monotonically
non-increasing
iteration
updating
factors
may
share
variables
change
one
factor
makes
vote
vector
could
alter
opinion
another
factor
practice
however
observe
using
procedure
cost
function
increases
extremely
rarely
possible
recover
guarantee
convergence
following
simple
modiﬁcation
observe
whether
changes
vote
vectors
particular
iteration
result
cost
increase
due
interference
vote-changing
factors
cost
increase
observed
roll
back
vote
changes
iteration
return
serial
procedure
allowing
single
conﬁdent
factor
change
votes
practice
rollback
occurs
rarely
mnist
task
fewer
simultaneous
votes
retracted
types
variables
objective
functions
section
consider
several
options
alphabet
cost
variable
particularly
consider
diﬀerent
cases
minimization
˜xi
arg
min
cid:88
b∈b
proactive
message
passing
memory
factor
networks
minimization
form
used
ﬁxed
set
votes
pmp
computing
ﬁnal
variable
settings
return
computing
opinion
factor
via
version
given
message-passing
interpretation
thus
great
importance
minimization
tractable
indeed
choices
alphabets
cost
functions
obtain
explicit
solutions
allows
write
simple
message-passing
form
per
discussion
sec
3.2.
messages
come
neighboring
variable
nodes
summarize
votes
associated
weights
memory
factors
neighboring
variables
4.1
real
variables
quadratic
cost
first
consider
case
local
cost
functions
quadratic
thus
quadratic
local
cost
function
becomes
˜xi
arg
min
cid:88
b∈b
cid:80
cid:80
b∈b
b∈b
cost
functional
take
derivative
respect
set
result
equal
zero
ﬁnd
minimizing
value
˜xi
words
minimizing
weighted
combination
votes
note
result
extends
complex
case
v|2
show
appendix
apply
result
update
opinion
vectors
factor
ˆwi
ˆwi
¯oi
˜xi
becomes
cid:34
arg
min
˜xi
i∈na
cid:88
cid:80
cid:80
weighted
average
votes
except
one
factor
sum
weights
non-abstaining
factors
note
ˆwi
equation
tells
setting
message
˜xi
ˆwi
tells
value
˜xi
rest
network
prefers
strength
ˆwi
preference
b∈n
cid:35
cid:80
b∈n
b∈n
cid:88
ˆwi
b∈ni\a
eschenfeldt
schmidt
draper
yedidia
4.2
integer
variables
linear
cost
consider
situation
mismatch
cost
absolute
diﬀerence
i.e.
consider
setting
weights
identical
argue
taking
median
value
votes
minimizes
cost
median
actually
set
values
example
four
votes
values
median
would
set
generally
median
set
integers
least
half
votes
values
greater
lower
end
range
least
half
votes
values
least
large
upper
end
range
thus
cid:0
cid:1
˜xi
mmed
cid:100
n/2
cid:101
cid:100
n/2
cid:101
note
median
set
summarized
smallest
largest
elements
mmed
course
possible
set
contains
single
element
case
argue
minimizing
˜xi
must
element
median
set
mmed
first
show
elements
median
set
equal
cost
set
fact
multiple
elements
even-degree
variable
node
case
half
elements
equal
least
element
set
half
least
equal
largest
element
set
assumption
identical
weights
used
change
value
˜xi
among
elements
median
set
mismatch
cost
increase
half
votes
amount
decreases
half
thus
votes
balanced
range
median
set
conﬁrm
elements
median
set
equal
cost
next
understand
choice
˜xi
outside
median
set
incurs
greater
cost
consider
case
˜xi
set
equal
largest
value
median
set
increase
˜xi
mismatch
cost
|˜xi
would
increase
least
cid:100
n/2
cid:101
due
fact
least
cid:100
n/2
cid:101
elements
values
equal
lowest
value
median
set
least
one
element
value
equal
largest
value
median
set
mismatch
cost
elements
values
larger
largest
value
median
set
could
decrease
cid:100
n/2
cid:101
cost
increases
take
˜xi
anything
larger
largest
element
median
set
similar
logic
holds
choose
˜xi
smaller
smallest
element
median
set
solution
message-passing
opinion
update
becomes
cid:35
cid:34
arg
min
¯oi
arg
min
cid:34
cid:88
i∈na
cid:35
cid:88
cid:1
cid:0
¯oi
i∈na
smallest
largest
element
mmed
i.e.
set
non-abstaining
memory
factor
neighboring
variable
exclusive
futher
function
deﬁned
z
distance
interval
proactive
message
passing
memory
factor
networks
case
summary
message
variable
passes
memory
factor
pair
integers
message
suﬃcient
factor
determine
impact
choosing
particular
opinion
variable
global
cost
function
impact
summed
across
variables
memory
factor
neighbors
finally
note
weights
assumption
weights
variable
e.g
diﬀerent
weights
may
associated
diﬀerent
variables
appearing
equation
subject
4.3
labels
histograms
consider
situation
relevant
hypothesis
testing
setting
valid
values
correspond
labels
generally
consider
set
labels
ﬁnite
|xi|
integers
enforce
restriction
weights
connected
variable
must
equal
setting
take
local
cost
functions
indicators
cid:54
minimize
cost
picking
˜xi
one
vote
values
occurs
frequently
pick
less
frequently
observed
vote
value
cost
would
increase
decrease
number
votes
thus
optimal
pick
˜xi
element
mode
set
denote
mmod
median
set
set
may
singleton
may
multiple
distinct
elements
thus
write
˜xi
optimal
message-passing
form
opinion
update
becomes
cid:34
˜xi
mmod
cid:88
i∈na
arg
min
cid:0
cid:1
cid:0
¯oi
cid:54
mmod
cid:1
cid:1
cid:35
cid:0
binary
vector
length
|xi|
non-zero
coordinates
indicate
possible
vote
values
elements
mode
set
setting
summary
message
4.4
mixing
variable
types
factor
neighbors
variables
diﬀerent
types
real
integer
label
opinion
update
problem
becomes
combination
problem
types
described
situation
sum
split
sums
diﬀerent
types
variables
example
corresponding
complex
variables
corresponding
integer
variables
re-express
arg
min
i∈b
ˆwi
ˆwi
|¯oi
¯xi|2
¯oi
cid:35
cid:88
i∈c
cid:34
cid:88
appropriate
summary
messages
update
rules
use
variable
node
follow
discussion
eschenfeldt
schmidt
draper
yedidia
types
memory
factors
section
describe
two
ways
construct
memory
factors—a
simple
approach
using
memory
tables
somewhat
complicated
perhaps
scalable
approach
using
reduction
lower
dimensional
subspace
5.1
memory
table
factors
memory
table
simply
database
exemplars
exemplar
encoding
valid
con-
ﬁguration
variables
neighbor
factor
question
memory
factors
memory
tables
trivial
train
given
number
exemplars
learn
local
structure
simply
store
exemplar
think
stored
local
snapshots
memory.
formally
memory
table
corresponding
factor
database
generally
memory
vector
length
|na|
jth
distinct
memories
element
element
vector
corresponds
particular
variable
refers
variable
jth
element
neighbors
factor
memory
table
corresponding
factor
perhaps
conveniently
thought
|na|
array
variable
values
note
evidence
factors
thought
special
case
memory
table
factors
single
dynamic
memory
corresponding
current
state
evidence
selection
cost
use
pmp
memory
table
restricts
factor
opinions
thus
votes
memories
table
accomplished
using
selection
cost
otherwise
s.t
cid:26
choice
simpliﬁes
pmp
algorithm
outside
arg
min
restricts
optimization
opinions
exist
memory
table
selection
cost
zero
conﬁdence
memory
table
factor
diﬀerence
total
cost
speciﬁed
best
second-best
distinct
sets
opinions
corresponding
memories
intuitively
memory
table
factor
high
conﬁdence
choice
memory
correct
even
memory
costly
next
best
alternative
much
worse
therefore
makes
sense
factor
cast
votes
allow
less
conﬁdent
factors
react
decision
algorithm
requires
specify
relationship
cid:39
denoting
whether
factor
opinion
vector
suﬃciently
close
vote
vector
want
change
votes
since
set
possible
vote
vectors
memory
table
factor
ﬁnite
simply
use
equality
comparison
cid:39
5.2
subspace
factors
another
particular
form
learned
structure
utilize
memory
factors
reduction
lower
dimensional
subspace
subset
thereof
refer
memory
factors
enforce
dimensionality
reduction
subspace
factors.
particular
consider
linear
subspaces
take
form
transformations
hidden
variables
visible
proactive
message
passing
memory
factor
networks
variables
subspace
factor
represented
using
matrix
rn×p
subspace
y|y
variables
complex
use
complex
matrices
hidden
variables
furthermore
often
restrict
operate
subset
subspace
example
later
provide
example
wij
restricts
subspace
factor
positive
cone
y|y
abuse
terminology
continue
refer
memory
factors
subspace
factors
subspace
factors
may
learned
data
variety
methods
e.g
nonnegative
matrix
factorization
case
cone
mentioned
apply
sparsity
assumptions
hidden
variables
generally
enforce
subspace
factor
indeed
provides
low-dimensional
representation
visible
variables
constrained
lie
low
dimensional
subspace
spanned
columns
context
mfn
let
suppose
particular
memory
factor
subspace
factor
maps
hidden
variables
require
set
neighboring
variables
include
visible
variables
represented
subspace
factor
variables
represented
mfn
presumed
interest
problem
paper
assume
precisely
set
visible
variables
subspace
factor
thus
|na|
note
restricts
variables
alphabet
restriction
could
lifted
allowing
subspace
factors
visible
variables
diﬀerent
alphabets
subspace
factors
possible
consider
visible
variables
selection
cost
votes
subspace
factor
use
otherwise
s.t
waz
cid:40
cost
simply
requires
vote
subspace
subset
thereof
deﬁned
factor
allows
hidden
variables
take
value
domain
selection
cost
either
zero
inﬁnite
replace
opinion
update
optimization
feasibility
constraint
problem
becomes
ψi
¯oi
cid:88
b∈n


arg
min
s.t
¯o=waz
z∈z
reasonable
constraint
variety
variable
types
examples
make
use
case
real
complex
using
notation
section
real
variables
quadratic
cost
problem
ˆwi
¯oi
˜xi
arg
min
i∈na
ˆwi
subject
waz
convex
quadratic
program
small
straightforward
solve
note
linear
restrictions
included
alphabet
without
altering
nature
problem
cid:88
i∈na
min
cid:88
eschenfeldt
schmidt
draper
yedidia
one
example
restrict
variables
non-negative
i.e.
case
subsequently
provide
examples
entries
constrained
non-negative
case
problem
becomes
cid:88
ˆwi
¯oi
¯xi
arg
min
subject
i∈na
ˆwi
waz
also
extend
problem
complex
variables
cost
v|2
deﬁne
conﬁdence
subspace
factor
cid:32
cid:88
i∈na
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
|¯oi
¯xi|2
cid:33
|ni
|na|
system
parameter
choose
balance
term
representing
mismatches
opinion
incoming
messages
term
representing
number
votes
messages
represent
intuitively
factor
conﬁdent
lot
information
rest
system
opinion
closely
matches
information
scale
score
total
number
adjacent
variables
penalty
variables
votes
relative
impact
subspace
factors
variables
total
finally
choose
parameter
deﬁne
relationship
cid:39
cid:39
cid:107
cid:107
cid:107
cid:107
cid:96
norm
means
subspace
factors
satisﬁed
small
diﬀerence
opinion
vector
vote
vector
prevents
pmp
making
inﬁnite
number
decreasingly
small
updates
applications
section
describe
number
applications
illustrate
basic
mechanisms
abilities
memory
factor
networks
also
compare
behavior
mfns
using
memory
tables
using
subspace
factors
6.1
face
reconstruction
ﬁrst
inference
application
reconstruction
missing
noisy
data
two-
dimensional
color
image
use
fei
fei
dataset
400
pixel
images
showing
single
face
manually
aligned
facial
features
pixel
locations
image
use
320
images
training
set
hold
testing
describe
memory
factor
network
memory
table
version
subspace
factor
version
represents
images
used
variety
tasks
similar
images
set
variables
interested
red
green
blue
pixel
values
pixel
position
image
additional
set
variables
representing
gray
proactive
message
passing
memory
factor
networks
value
pixel
added
examples
variables
therefore
nonnegative
real
numbers
also
normalized
one
enforce
explicitly
mfn
simply
truncate
one
rare
cases
mfn
returns
larger
value
6.1.1
factor
layout
factors
use
problem
cover
square
grids
pixels
particular
location
image
one
basic
structure
create
three
factors
representing
red
green
blue
channels
squares
pixels
image
squares
starting
every
pixels
factor
shares
pixel
square
three
neighboring
factors
type
corners
edges
follow
pattern
truncate
factors
necessary
structure
essentially
creates
three
parallel
factor
networks
representing
three
color
channels
since
none
factors
connected
pixels
adjacent
variables
multiple
colors
link
networks
together
introduce
factors
include
variables
three
colors
keep
overly
large
cover
4×4
pixel
squares
3·16
visible
variables
beyond
limiting
total
size
factor
structure
linked
factors
1/4
size
single
channel
factors
greatly
simpliﬁes
connections
factors
implementation
convenient
group
variables
sets
connections
sets
variables
factors
rather
directly
variables
factors
choice
size
linked
factors
sets
grids
single
channel
pixel
values
set
thus
connections
four
mono-colored
factors
one
factor
links
three
colors
also
consider
another
layout
grids
pixels
covered
single
factor
encoding
three
colors
overlapping
structure
single
channel
factors
greatly
increases
size
individual
factor
typical
factor
connected
192
variables
results
network
fewer
factors
knowledge
connections
colors
6.1.2
learning
memories
case
memory
tables
implementation
factors
simple
table
includes
appropriate
pixel
values
face
training
set
subspace
factors
must
learn
matrix
choosing
order
variables
represented
vector
since
pixel
values
nonnegative
choose
use
nonnegative
matrix
factorization
nmf
learning
step
precise
learn
given
matrix
subspace
factor
variables
training
set
images
choose
value
construct
matrix
ith
column
contains
pixel
values
appropriate
positions
ith
training
image
use
standard
nmf
algorithm
compute
approximate
factorization
rp×m
rn×p
implementation
use
nmf.jl
package
julia
programming
language
alternate
least
square
using
projected
gradient
descent
algorithm
matrix
desired
subspace
matrix
representing
hidden
variable
values
correspond
training
samples
note
eschenfeldt
schmidt
draper
yedidia
original
evidence
memory
table
soln
subspace
solution
figure
example
output
mfn
run
factors
color
channel
linked
factors
hidden
variables
subspace
factors
factors
cover
region
corners
missing
region
evidence
corners
updates
done
serial
fashion
evidence
given
weight
memory
factor
weight
memory
table
solution
mean
square
error
0.0111
subspace
solution
mean
square
error
0.0057.
approach
works
arbitrary
collection
scalar
variables
dependent
square
samples
subspace
matrices
particular
size
variables
particular
meaning
simply
need
transform
whatever
portion
image
interested
vector
scalars
stack
vectors
matrix
factorize
restriction
comes
choice
smaller
avoid
trivial
factorizations
generally
choose
much
smaller
subspace
sense
representing
key
features
image
segment
learned
training
set
6.1.3
missing
data
primary
example
problem
test
image
portion
image
erased
example
eyes
seen
fig
general
algorithm
able
make
reasonably
plausible
reconstructions
missing
pixels
based
pixels
see
memory
face
images
naturally
test
image
training
set
case
provide
network
evidence
nearby
sections
image
run
algorithm
generate
pixel
values
occluded
region
case
assume
evidence
erased
matches
ground
truth
weight
evidence
pixels
made
suﬃciently
large
dominate
factor
votes
determining
ﬁnal
variable
values
without
modiﬁcation
tendency
blur
distort
images
particularly
subspace
case
since
must
represented
dimensions
subspace
networks
tend
generate
average
set
eyes
two
eyes
match
surrounding
face
lack
signiﬁcant
notable
features
memory
tables
drawing
directly
wide
sample
eyes
likely
generate
unique-looking
eyes
may
may
match
also
lead
color
anomalies
variation
sorts
results
factor
votes
one
missing
data
instance
may
advantageous
allow
blurring
received
data
however
case
data
missing
random
locations
proactive
message
passing
memory
factor
networks
original
evidence
memory
table
soln
subspace
solution
figure
example
output
mfn
random
missing
evidence
run
factors
color
channel
linked
factors
hidden
variables
subspace
factors
factors
cover
entire
image
updates
done
using
simultaneous
voting
conﬁdent
subspace
factors
wanting
update
iteration
evidence
given
weight
memory
factors
given
weight
memory
table
solution
mean
square
error
0.0039
subspace
solution
mean
square
error
0.0039.
image
rather
given
section
data
loss
isolated
e.g
eyes
little
gained
correcting
data
provided
data
loss
spread
among
sections
image
data
present
treating
evidence
absolute
truth
result
anomalies
pixels
receive
smoothed
values
adjacent
pixels
keep
variable
original
cases
may
make
sense
simply
smooth
whole
image
usually
results
blurry
consistent-looking
face
rather
one
odd
blotches
see
fig
example
blurring
image
randomly
missing
evidence
another
example
missing
data
case
given
evidence
grayscale
pixel
values
rather
color
data
given
grayscale
image
face
would
like
color
requires
introduction
variables
gray
values
attached
subset
factors
ﬁnd
best
results
come
including
gray
factors
contain
three
color
channels
recall
gray
value
pixel
found
taking
linear
combination
rgb
values
use
cie
1931
standard
gray
0.212673
0.715152
0.072175
rather
treating
hard
constraint
however
mfn
treats
gray
values
simply
another
channel
trusting
learned
structure
maintain
appropriate
relationships
inference
problem
coloring
perfect
grayscale
image
add
post-processing
step
scales
rgb
pixel
values
match
gray
values
example
results
problem
seen
fig
colorization
see
signiﬁcant
diﬀerences
behavior
memory
tables
subspace
factors
given
strong
gray
evidence
clusters
memory
tables
often
choose
training
face
resulting
mfn
image
composites
relatively
small
number
images
post-processing
step
often
lead
misplaced
colors
example
pixels
part
neck
training
example
become
part
background
colored
image
subspace
factors
image
directly
mfn
closely
eschenfeldt
schmidt
draper
yedidia
original
evidence
memory
table
soln
subspace
solution
figure
example
output
mfn
run
linked
factors
gray
factors
cover
entire
image
updates
done
serial
fashion
gray
evidence
given
weight
100
factor
weight
subspace
factors
used
hidden
variables
mfn
color
output
scaled
match
gray
evidence
memory
table
solution
mean
square
error
0.0011
subspace
solution
mean
square
error
0.0015
original
evidence
memory
table
soln
subspace
solution
figure
example
output
mfn
noisy
evidence
run
factors
color
channel
linked
factors
hidden
variables
subspace
factor
factors
cover
entire
image
updates
done
using
simultaneous
voting
conﬁdent
factors
wanting
update
iteration
evidence
memory
factors
given
weight
memory
table
solution
mean
square
error
0.0060
subspace
solution
mean
square
error
0.0049.
resemble
grayscale
image
though
blurred
subspace
factors
also
often
leave
sections
colorized
image
gray
apparently
little
distinguish
color
options
whereas
memory
tables
always
choose
color
memories
6.1.4
noisy
data
one
case
rarely
ever
make
sense
privilege
given
evidence
case
noisy
data
known
pixel
values
perturbed
way
prefer
smoothed
result
given
applying
algorithm
generally
equal
weights
factors
evidence
even
extra
weight
factors
see
fig
example
results
case
noisy
data
proactive
message
passing
memory
factor
networks
table
mean
square
error
result
summary
tests
problems
described
fig
1-4
problem
type
eyes
removed
factor
type
mean
median
best
worst
subspace
0.0077
0.0068
0.0021
0.0210
eyes
removed
memory
table
0.0116
0.0107
0.0036
0.0346
random
dropped
evidence
subspace
0.0041
0.0039
0.0026
0.0061
random
dropped
evidence
memory
table
0.0055
0.0053
0.0023
0.0111
colorization
colorization
subspace
0.0020
0.0015
0.0004
0.0071
memory
table
0.0018
0.0016
0.0005
0.0047
noisy
evidence
subspace
0.0050
0.0049
0.0040
0.0064
noisy
evidence
memory
table
0.0058
0.0055
0.0035
0.0106
6.1.5
results
summary
ran
pmp
described
settings
problems
described
test
images
computing
mean
square
error
color-pixel
values
portion
image
used
mfn
statistics
summarizing
results
presented
table
images
best
worst
examples
problem
available
appendix
6.2
music
reconstruction
another
application
mfns
processing
audio
ﬁles
question
interest
may
reconstructing
missing
data
smoothing
noisy
data
design
decisions
diﬀerent
depending
application
ﬁrst
describe
general
process
use
transforming
audio
ﬁle
format
appropriate
mfn
data
set
use
second
long
clips
randomly
generated
music
downloaded
otomata
oto
142
training
samples
test
samples
clips
sample
rate
40k
memory
table
factors
limit
ram
usage
random
subset
training
samples
used
run
pmp
sample
included
independently
probability
0.3
6.2.1
creating
spectrogram
mfns
work
spectrogram
representation
audio
ﬁles
use
short-time
fourier
transform
stft
process
audio
samples
process
begins
splitting
audio
series
overlapping
frames
given
length
overlap
may
varied
changing
hop
size
determines
distance
starting
time
consecutive
frames
use
frame
length
50ms
hop
size
25ms
splitting
frames
data
frame
multiplied
window
function
use
hanning
window
windowed
frame
individually
fourier
transformed
fourier
transforms
matrix
complex
values
column
represents
frame
time
row
represents
signal
frequency
frequency
range
eschenfeldt
schmidt
draper
yedidia
resolution
determined
sample
rate
audio
length
frames
respectively
particular
number
frequencies
represented
equal
half
number
data
points
per
frame
plus
50ms
frames
audio
sampled
40k
1001
frequencies
represented
ranging
20k
rather
use
entire
matrix
decrease
size
grouping
frequencies
bins
given
number
frequencies
desired
number
bins
generally
use
400
compute
coeﬃcient
cid:88
cid:98
eja
cid:99
j=1
use
cid:98
eja
cid:99
number
frequencies
include
jth
bin
starting
ﬁrst
bin
taking
lowest
cid:98
cid:99
frequencies
bins
multiple
frequencies
simply
sum
values
included
frequencies
logarithmic
binning
scheme
reduces
size
network
work
maintaining
resolution
lower
frequencies
importance
original
signal
binned
matrix
prepared
build
memory
factor
network
covariance
matrix
cid:80
variables
complex
valued
learn
subspace
factors
follow
work
baldi
baldi
2012
use
pca
approach
speciﬁcally
matrix
formed
eigenvectors
greatest
magnitude
eigenvalues
training
vectors
note
notation
notation
baldi
memory
tables
continue
simply
store
training
examples
xtx∗
6.2.2
factor
layout
perceived
quality
sound
generally
depends
frequencies
present
sound
possibly
non-adjacent
frequencies
consider
memory
factors
cover
entire
frequency
spectrum
small
period
time
allows
factor
learn
entire
frequency
proﬁle
given
sound
often
includes
many
distant
frequencies
main
test
case
randomly
generated
music
also
note
absolute
time
position
little
meaning
priori
expectation
example
last
two
seconds
sample
diﬀerent
ﬁrst
two
seconds
instead
learning
diﬀerent
memory
factors
diﬀerent
parts
spectrogram
learn
single
subspace
matrix
set
memories
time-positions
training
examples
share
memory
factors
network
note
results
large
training
set
even
relatively
small
number
music
samples
note
time-independence
feature
particular
choice
audio
sig-
nal
would
hold
true
something
structured
time
like
short
piece
speech
examples
would
closely
resemble
face
example
speciﬁc
fea-
tures
generally
appear
time
position
possibly
frequency
position
every
sample
6.2.3
missing
music
analogously
removal
eyes
face
consider
inference
problem
ﬁlling
gaps
piece
music
situation
give
signiﬁcant
weight
evidence
proactive
message
passing
memory
factor
networks
original
evidence
memory
table
soln
subspace
solution
figure
example
output
mfn
music
missing
evidence
run
pixel
wide
factors
covering
entire
frequency
range
hidden
variables
subspace
factors
updates
done
serial
fashion
evidence
given
weight
memory
factors
given
weight
memory
table
solution
mean
square
error
16.258
subspace
solution
mean
square
error
7.130.
present
preferring
maintain
original
signal
possible
also
generally
advantageous
pick
relatively
wide
memory
factors
10-20
spectrogram
pixels
maximize
connection
newly
created
signal
original
signal
adjacent
example
spectrograms
reconstruction
problem
see
fig
constructing
missing
music
memory
tables
signiﬁcantly
better
properties
subspace
factors
testing
memory
table
network
selects
full
notes
sequences
notes
attempts
choose
best
surrounding
music
whereas
subspace
factors
generally
match
dominant
frequencies
surrounding
music
recreate
shape
notes
rather
creating
sort
ﬂat
buzz
often
volume
spike
believe
part
reason
behavior
subspace
factors
trained
variety
shifts
time
note
rather
forced
choose
shifts
able
combine
creating
smoother
blended
sections
spectrogram
quite
unlike
patterns
notes
training
set
6.2.4
noisy
music
another
problem
consider
removing
noise
music
sample
simulate
problem
add
normal
random
noise
test
sample
many
regions
spectrogram
value
zero
absence
noise
evidence
given
signiﬁcant
weight
relative
memory
factors
certain
degree
noise
still
present
end
algorithm
combat
eﬀect
run
pmp
twice
ﬁrst
large
weight
evidence
votes
factors
little
weight
evidence
taking
ﬁnal
votes
factors
initial
votes
second
run
allows
factors
take
advantage
evidence
allowing
dominate
factors
primarily
need
match
signal
underneath
noise
rather
coordinating
time
intervals
use
narrow
factors
pixels
noise
problems
example
noisy
music
problem
seen
fig
see
memory
tables
better
able
maintain
rich
structure
notes
though
subspace
factors
maintain
much
structure
music
attempting
create
eschenfeldt
schmidt
draper
yedidia
original
evidence
memory
table
soln
subspace
solution
figure
example
output
mfn
music
noisy
evidence
run
pixel
wide
factors
covering
entire
frequency
range
hidden
variables
subspace
factors
updates
done
serial
fashion
evidence
originally
given
weight
memory
factors
given
weight
evidence
weight
set
0.01
second
run
initialized
ﬁnal
factor
votes
ﬁrst
memory
table
solution
mean
square
error
10.386
subspace
solution
mean
square
error
1.212.
table
mean
square
error
result
summary
tests
problems
described
fig
problem
type
factor
type
mean
median
best
worst
sections
removed
subspace
8.248
7.270
3.712
20.366
sections
removed
memory
table
14.025
12.835
8.205
28.042
noisy
evidence
subspace
1.459
1.238
0.635
4.303
noisy
evidence
memory
table
13.100
10.399
4.932
42.328
stretches
better
able
exactly
match
main
structures
memory
tables
must
try
ﬁnd
matching
notes
among
memories
combining
notes
diﬀerent
training
samples
results
somewhat
disjointed
reconstruction
whereas
subspace
factors
somewhat
fuzzier
smoother
6.2.5
results
summary
ran
pmp
described
settings
two
problems
described
test
clips
computing
mean
square
error
spectrogram
pixel
values
statistics
summarizing
results
presented
table
images
best
worst
examples
problem
available
appendix
6.3
handwritten
digit
classiﬁcation
demonstrate
use
label
variables
mixed-variable
factors
using
memory
table
mfns
classify
handwritten
digits
use
mnist
dataset
lecun
consisting
60,000
training
examples
28×28
grayscale
images
correct
classiﬁcations
10,000
test
images
classiﬁcation
proactive
message
passing
memory
factor
networks
work
32×
images
original
images
centered
order
facilitate
hierarchy
pixel
label
variables
addition
322
level-0
variables
representing
image
pixels
162
hidden
level-1
variables
corresponding
lower-resolution
version
image
hidden
level-2
variables
corresponding
even
lower
resolution
version
hidden
level-3
variables
representing
entire
image
hierarchy
label
variables
exists
parallel
hierarchy
pixel
variables
152
level-0
variables
level-1
variables
level-2
variables
single
level-3
variable
characterizing
entire
image
pixel
variables
integer
variables
linear
cost
weights
label
variables
use
indicator
cost
described
section
4.3
weights
32.
label
variables
higher
weight
pixel
variables
fewer
natural
scale
mismatch
lower
hierarchy
two
useful
properties
firstly
allows
locally
inferred
labels
synthesized
one
global
label
read
classiﬁcation
entire
image
secondly
provides
fast
lane
information
propagate
one
part
image
remote
part
inferences
pixels
labels
ﬂow
hierarchy
faster
way
linearly
diﬀuse
network
network
hierarchy
constructed
following
manner
factor
level
connected
pixel
variables
label
variables
level
well
pixel
variables
label
variable
level
sample
set
variables
connected
single
memory
table
factor
shown
fig
training
example
presented
system
label
variables
set
correct
label
hidden
pixel
variables
set
average
pixel
values
corresponding
patch
image
next
lower
level
every
memory
memory
table
thus
ﬁlled
speciﬁed
values
testing
time
hidden
pixel
label
variables
connected
evidence
factors
acquire
values
result
voted
memory
table
factors
level-0
factor
thinks
group
pixels
characteristic
cast
votes
neighboring
level-1
pixel
variables
blurred
version
memory
cast
vote
neighboring
level-1
label
variable
factors
pay
mismatch
cost
disagreeing
pixel
variables
label
variables
thus
factor
sure
whether
looking
may
swayed
fact
neighboring
factor
cast
vote
label
algorithm
converged
classify
image
according
single
label
top
hierarchy
simple
memory
factor
network
suﬃcient
classify
10,000
test
images
mnist
data
set
96.15
accuracy
run
single
factor
voting
time
96.41
accuracy
using
simultaneous
votes
top
factors
see
section
3.3
average
single-vote
implementation
required
78.7
iterations
1148.9
opinion
updates
simultaneous-vote
implementation
required
16.9
iterations
687.2
opinion
updates
6.4
restoration
corrupted
images
natural
application
mfns
using
memory
tables
reconstruct
previously-seen
images
presented
versions
corrupted
noise
erasure
use
similar
network
face
reconstruction
application
data
eschenfeldt
schmidt
draper
yedidia
level
level
level
level
figure
variables
connected
sample
memory
table
factor
level
variables
high-resolution
representation
region
lower-resolution
representa-
tion
region
labels
correlated
subregions
high-resolution
image
one
label
correlated
entire
region
low-resolution
image
use
cifar-10
dataset
rgb
images
krizhevsky
2009
containing
50,000
images
provided
labeling
images
irrelevant
problem
50,000
images
read
one
previously-seen
images
selected
random
gaussian
noise
applied
every
color
channel
value
every
pixel
standard
deviation
40.
addition
randomly-generated
blob
144
pixels
completely
erased
center
image
providing
evidence
image
presented
mfn
restores
image
original
version
removing
applied
noise
ﬁlling
correct
pixels
erased
region
application
particularly
convenient
quantitative
metric
as-
sessing
quality
result
simply
measure
distance
computed
image
original
makes
natural
choice
evaluating
eﬀect
changes
algorithm
thus
assess
eﬀects
simultaneous
voting
ran
pmp
algorithm
1000
test
images
using
standard
serial
algorithm
simultaneous
voting
version
described
section
3.3
running
factors
every
iteration
simultaneous
voting
version
less
likely
reproduce
initial
image
exactly
average
error
comparable
serial
version
serial
version
restored
91.9
images
perfectly
average
total
error
across
color
channels
895.1
0.291
per
pixel
per
channel
simultaneous
voting
version
restored
83.1
images
perfectly
average
total
error
936.7
0.305
per
pixel
per
channel
proactive
message
passing
memory
factor
networks
conclusions
future
work
introduced
new
approach
combining
inference
learning
ex-
perience
memory
factor
networks
provide
simple
way
store
examples
learned
experience
proactive
message
passing
using
conﬁdence-based
scheme
prioritiz-
ing
factor
updates
gives
reliable
way
converge
good
optima
memory
factor
network
cost
function
consider
algorithms
applications
demonstrated
initial
foray
possibilities
raised
approach
thus
one
might
consider
factor
graphs
combined
memory
factor
nodes
conventional
factor
nodes
encoded
known
statistical
dependencies
constraints
pmp
algorithm
well-suited
mfns
one
might
nevertheless
consider
using
approaches
based
belief
propagation
variational
approaches
wainwright
jordan
2008
based
alternating
directions
method
multipliers
boyd
al.
2011
derbinsky
al.
2013
many
potential
applications
one
might
attack
approach
including
computer
vision
problems
inferring
depth
single
images
motion
videos
finally
important
open
question
well
approach
described
take
advantage
massive
amounts
data
often
available
halevy
al.
2009
appendix
derivation
opinion
update
real
variables
quadratic
cost
section
derive
messages
message-passing
version
pmp
real
variables
quadratic
costs
choices
specializes
wa
cid:88
min
cid:88


arg
min
¯oi
b∈n
ﬁrst
optimize
choice
taking
derivative
respect
i∈na
recall
deﬁnition
ni\
originally
made
cid:88
cid:80
deﬁne
ˆwi
cid:80
cid:80
cid:80
ˆwi

¯oi
b∈n
¯xi
arg
min
b∈n
b∈ni\a
¯oiwa
b∈ni
b∈n
second
term
ˆwi
get
multiplying
dividing
cid:88
b∈n
˜xi
arg
min
˜xi
¯xi
ˆwi
¯oi
ˆwi
cid:80
cid:80
b∈n
b∈n
cid:80
b∈n
ˆwi
χa
wb
eschenfeldt
schmidt
draper
yedidia
minimization
respect
solely
memory
factors
deﬁned
sec
3.2.
˜xi
denote
cost
associated
particular
variable
given
opinion
¯oi
cid:18
¯oi
ˆwi
˜xi
ˆwi
cid:19
cid:17
cid:19
˜xi
¯oi
¯oi
ˆwi
˜xi
ˆwivb
2¯oi
˜xi
¯o2
¯oi
ˆwi
¯oi
˜xi
ˆwivb
¯oi
cid:19
¯oi
¯xi
¯oi
¯oi
cid:18
b∈n
¯oi
ˆwi
ˆwi
ˆwi
ˆw2
ˆwi
ˆw2
cid:88
cid:16
cid:17
¯xi
˜xi
cid:16
ˆw2
b∈n
¯oi
cid:88
cid:19
cid:18
cid:88
cid:18
cid:88
recalling
cid:80
b∈n
b∈n
ˆw2
cid:80
b∈n
¯oi
ˆwi
ˆwi
ˆwi
ˆw2
˜xi
hence
2¯oi
˜xi
¯o2
¯o2
2˜xi¯oi
cid:1
cid:0
¯o2
constant
term
includes
terms
function
next
simplify
sums
ˆwi−wa
b∈n
ˆwi
ˆw2
¯oi
˜xi
ˆwi
ˆw2
¯oi
˜xi
ˆwi
ˆwi
ˆw2
ˆwi
noting
constant
dropped
adding
constant
make
cid:0
¯o2
second
equation
number
terms
cancel
substitute
result
cid:1
2˜xi¯oi
quadratic
form
yields
ﬁnal
form
optimization
arg
min
i∈na
ˆwi
ˆwi
¯oi
˜xi
cid:40
cid:88
cid:41
appendix
reconstruction
results
reader
reasonable
sense
range
possible
results
including
nature
artefacts
obtained
using
proactive
message
passing
memory
factor
networks
include
images
best
worst
solutions
variety
problems
type
factor
memory
table
subspace
ﬁrst
reconstructing
face
images
reconstructing
music
spectrograms
references
otomata
generative
musical
sequencer
http
//www.earslap.com/page/otomata.html
fei
face
database
http
//fei.edu.br/~cet/facedatabase.html
proactive
message
passing
memory
factor
networks
example
original
evidence
memory
ta-
ble
solution
subspace
so-
lution
best
memory
table
best
space
sub-
worst
mem-
ory
table
worst
space
sub-
mse
0.0036
mse
0.0021
mse
0.0036
mse
0.0021
mse
0.0346
mse
0.0177
figure
best
worst
examples
eye
reconstruction
mse
0.0138
mse
0.0210
eschenfeldt
schmidt
draper
yedidia
example
original
evidence
memory
ta-
ble
solution
subspace
so-
lution
best
memory
table
best
space
sub-
worst
mem-
ory
table
worst
space
sub-
mse
0.0023
mse
0.0027
mse
0.0034
mse
0.0026
mse
0.0111
mse
0.0056
figure
best
worst
examples
reconstructing
randomly
missing
evidence
mse
0.0086
mse
0.0061
proactive
message
passing
memory
factor
networks
example
original
evidence
memory
ta-
ble
solution
subspace
so-
lution
best
memory
table
best
space
sub-
worst
mem-
ory
table
worst
space
sub-
mse
0.0005
mse
0.0004
mse
0.0005
mse
0.0004
mse
0.0058
mse
0.0047
figure
best
worst
examples
colorization
mse
0.0030
mse
0.0071
eschenfeldt
schmidt
draper
yedidia
example
original
evidence
memory
ta-
ble
solution
subspace
so-
lution
best
memory
table
best
space
sub-
worst
mem-
ory
table
worst
space
sub-
mse
0.0035
mse
0.0041
mse
0.0081
mse
0.0040
mse
0.0106
mse
0.0056
figure
best
worst
examples
reconstructing
noisy
evidence
mse
0.0073
mse
0.0064
proactive
message
passing
memory
factor
networks
example
original
evidence
memory
table
so-
lution
subspace
solution
best
memory
table
best
sub-
space
worst
memory
table
worst
sub-
space
mse
8.205
mse
5.073
mse
10.341
mse
3.712
mse
28.042
mse
20.366
figure
best
worst
examples
reconstructing
music
missing
sections
mse
28.042
mse
20.366
eschenfeldt
schmidt
draper
yedidia
example
original
evidence
memory
table
so-
lution
subspace
solution
best
memory
table
best
sub-
space
worst
memory
table
worst
sub-
space
mse
4.932
mse
0.635
mse
4.932
mse
0.635
mse
42.328
mse
4.304
figure
best
worst
examples
reconstructing
music
noise
added
mse
42.328
mse
4.304
proactive
message
passing
memory
factor
networks
baldi
complex-valued
autoencoders
neural
networks
33:136–147
2012.
boyd
parikh
chu
peleato
eckstein
distributed
optimization
statistical
learning
via
alternating
direction
method
multipliers
foundations
trends
machine
learning
3:1–122
2011.
derbinsky
bento
elser
yedidia
improved
three-weight
message-
passing
algorithm
http
//arxiv.org/pdf/1305.1961.pdf
2013.
forney
codes
graphs
normal
realizations
ieee
trans
inform
theory
520–548
february
2001.
freeman
pasztor
carmichael
learning
low-level
vision
int
computer
vision
:25–47
2000.
freeman
jones
pasztor
example-based
super-resolution
ieee
computer
graphics
applications
22:56–65
2002.
halevy
norvig
pereira
unreasonable
eﬀectiveness
data
ieee
intel-
ligent
systems
:8–12
2009.
koller
friedman
probabilistic
graphical
models
principles
techniques
mit
press
2009.
krizhevsky
learning
multiple
layers
features
tiny
images
masters
thesis
university
toronto
2009.
kschischang
frey
loeliger
factor
graphs
sum-product
algo-
rithm
ieee
trans
inform
theory
47:498–519
february
2001.
lecun
cortes
burge
mnist
database
handwritten
digits
http
//yann.lecun.com/exdb/mnist/
loeliger
dauwels
korl
ping
kschischang
factor
graph
approach
model-based
signal
processing
proc
ieee
:1295–1322
2007.
m´ezard
parisi
virasoro
spin
glass
theory
beyond
world
scientiﬁc
1987.
richardson
urbanke
modern
coding
theory
cambridge
university
press
2008.
sudderth
freeman
signal
image
processing
belief
propagation
ieee
signal
process
mag.
25:114–141
march
2008.
wainwright
jordan
graphical
models
exponential
families
variational
inference
foundations
trends
machine
learning
1:1–305
2008.
yedidia
freeman
weiss
constructing
free
energy
approximations
generalized
belief
propagation
algorithms
ieee
trans
inform
theory
:2282–2312
2005
