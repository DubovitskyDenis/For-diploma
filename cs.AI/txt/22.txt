technical
report
str
2016-1
january
2016
arxiv:1601.05977
cs.ai
doi
10.13140/rg.2.1.3416.6809
singularity
controversy
part
lessons
learned
open
questions
conclusions
battle
legitimacy
debate
amnon
eden
sapience
project
2016
synopsis
report
seeks
inform
policy
makers
nature
merit
arguments
concerns
associated
potential
technological
singularity
part
describes
lessons
learned
investigation
subject
separating
argu-
ments
merit
fallacies
misconceptions
confuse
debate
undermine
rational
resolution
acm
computing
classification
system
ccs
2012
computing
methodologies~philosophi-
cal/theoretical
foundations
artificial
intelligence
social
professional
topics~codes
ethics
contents
motivation
studying
singularity
conclusions
drawn
conclusion
singularity
acceleration
discontinuity
superintelligence
conclusion
singularity
mean
conclusion
singularities
implausible
incoherent
singular
conclusion
pulp
singularities
scientific
hypotheses
conclusion
risks
arise
indifference
malevolence
conclusion
risk
essentially
like
risk
powerful
technology
conclusion
clear
artificial
intelligence
means
conclusion
debate
ended
barely
begun
controlled
prove
ais
becoming
intelligent
open
questions
question
question
question
summary
acknowledgements
bibliography
london
united
kingdom
sapience
project
thinktank
dedicated
study
disruptive
intelligent
computing
charter
identify
extrapolate
anticipate
disruptive
long-lasting
possibly
unintended
consequences
progressively
intelligent
computation
economy
society
syndicate
focus
reports
mitigation
strategies
board
vic
callaghan
university
essex
jack
copeland
university
canterbury
amnon
eden
sapience
project
jim
moor
dartmouth
college
david
pearce
bltc
research
steve
phelps
kings
college
london
anders
sandberg
future
humanity
institute
oxford
university
tony
willson
helmsman
services
singularity
controversy
part
lessons
learned
open
questions
motivation
lately
artificial
intelligence
receiving
unusual
attention
news
outlets
scream
cel-
ebrated
scientists
end
nigh
tell
prospects
robot
uprising
catastrophe
artificial
intelligence
may
bring
others
dismiss
concerns
speculative
cultish
apocalyptic
rapture
nerds
nonsense
american
think-tank
gone
far
calling
elon
musk
stephen
hawking
implication
computer
science
foremost
geniuses
historically
fear-mongering
luddites
innovation
killers
danger
techno-
logical
progress
america
safety
unusual
scientific
controversies
occupy
headlines
unusual
crit-
icism
concerns
delegitimises
debate
attacks
expressions
concern
risks
researched
used
create
largely
inaccurate
impression
experts
disagree
whether
subject
significance
strong
conflict-
ing
opinions
authority
figures
confusing
ask
right
fear
killer
robots
stop
research
perhaps
con-
cerns
public
safety
headline-grabbing
click-bait
claptrap
debate
legiti-
mate
prospects
major
disruptive
event
technological
singularity
around
2045
examined
scientists
decades
much
back
philosophers
unfortu-
nately
recent
debate
always
informed
misunderstandings
misdirection
misinformation
created
illusion
bitter
dispute
general
public
also
exposed
personal
assistants
siri
satellite
navigation
picture
tagging
game
bots
applications
singularity
blockbusters
popular
series
brought
new
old
ideas
general
public
trend
popular
media
arti-
cles
arnold
schwarzenegger
cyborg
face
terminator
depicts
dumbed
misin-
formed
often
mocked
perceptions
singularity
muddy
controversy
fur-
ther
reality
like
called
debates
climate
change
genetic
engineering
domain
ex-
perts
much
agreement
concerning
timeframe
human-level
disa-
greements
experts
exist
concern
scenarios
far
subtler
complex
robot
uprising
fairytale
dedicate
time
understand
complex
ques-
tions
arise
may
wrongly
considered
purely
technical
questions
fewer
yet
keep
barrage
publications
field
yet
truth
concerns
afford
policy
vacuum
first
report
series
singularity
controversy
written
inform
policy
decisions
makes
lessons
fellows
project
sapience
learned
studying
potential
technological
singularity
since
2009
including
publication
first
peer-re-
viewed
collection
superintelligence
singularity
hypotheses
volume
eden
2013
first
report
focuses
false
unfounded
arguments
repeatedly
used
dismiss
concerns
delegitimise
debate
believe
pernicious
misconcep-
tions
haunted
debate
undermine
rational
resolution
refuting
fallacies
therefore
first
step
sapience
project
studying
singularity
although
thoughts
singularity
may
appear
new
fact
ideas
long
philosophical
history
help
increase
awareness
deep
roots
singularitarian
thought
within
traditional
philosophy
may
useful
look
historical
ante-
cedents
many
philosophers
portrayed
cosmic
process
ascending
curve
positivity
time
quantities
intelligence
power
value
always
increasing
technological
versions
sometimes
invoked
broad
technical
progress
sometimes
focused
spe-
cific
outcomes
possible
recursive
self-improvement
artificial
intelligence
his-
torical
analysis
broad
range
paradigm
shifts
science
biology
history
technology
particular
computing
technology
suggest
accelerating
rate
progress
predictive
power
biological
evolution
cultural
evolution
technological
evolution
attempted
unified
titles
big
history
christian
2010
christian
2012
law
accelerating
returns
kurzweil
2004
consequently
john
von
neu-
mann
forecasted
arrival
essential
singularity
history
race
beyond
human
affairs
know
could
continue
statement
taken
coin
term
tech-
nological
singularity
current
use
notion
singularity
coincides
time
nature
alan
turing
1950
expectation
machines
exhibit
intelligence
par
average
human
2050.
john
irving
good
good
1965
vernor
vinge
1993
expect
take
form
'intelligence
explosion
process
ultra-intelligent
machines
design
ever
intelligent
machines
critics
technological
singularity
dismiss
claims
speculative
empirically
un-
sound
pseudo-scientific
many
valid
arguments
put
forth
2009
enquiry
produced
satisfying
results
remained
unclear
arguments
attack
actually
learned
accounts
technological
singularity
appear
disagree
causes
possible
consequences
timescale
even
nature
superintelligence
emerge
namely
machine
post-
human
event
period
technological
singularity
unique
others
absence
consensus
basic
questions
casts
doubt
whether
notion
singularity
coherent
clarify
issues
organised
conference
track
technological
singularity
accel-
eration
studies
european
conference
computing
philosophy
ecap
inviting
researchers
present
contributions
philosophical
computational
mathematical
scientific
points
view
eden
2009
july
2009
researchers
singularity
institute
later
renamed
machine
learning
research
institute
dartmouth
college
elsewhere
presented
work
venue
7th
ecap
autonomous
university
barcelona
october
2010
eden
2010
8th
ecap
technical
university
munich
discussions
sought
promote
debate
singularity
arguments
example
using
methods
probabilistic
risk
assessment
examining
plausibility
arms
race
scenarios
comparing
cold
war
scenarios
none
track
participants
argued
risks
ignored
main
conclusion
issues
deserve
clarification
appropriate
debate
merit
singularity
hypotheses
taken
place
yet
therefore
2010
called
prominent
experts
opposing
views
debate
address
collection
essays
eden
2010
entitled
singularity
hypothesis
scientific
philosophical
assessment
virtually
scientists
philosopher
studied
singularity
controversy
part
lessons
learned
open
questions
subject
invited
make
contribution
book
beginning
2012
essays
developed
collected
volume
promote
debate
invited
short
rebuttal
essay
known
critics
essays
appeared
volume
published
2013
eden
moor
2013
followed
short
rebuttal
rebuttals
unique
among
volumes
similar
topics
proven
crucial
contribution
debate
list
conclusions
conclusions
drawn
almost
decade
research
issues
led
following
conclusions
conclusion
singularity
acceleration
discontinuity
superintelligence
term
technological
singularity
contemporary
sense
traces
back
john
von
neumann
ulam
1958
popularized
vernor
vinge
vinge
1993
ray
kurzweil
kurzweil
2006
elaborated
eliezer
yudkowsky
yudkowsky
2007
robin
henson
hanson
2008
david
chalmers
chalmers
2010
differences
accounts
raised
doubt
whether
term
coherent
investiga-
tion
led
believe
despite
differences
careful
expositions
technological
singularity
occurring
mid-21st
century
uniquely
described
using
three
common
characteristics
superintelligence1
acceleration
discontinuity
superintelligence
underlies
singularity
scenarios
see
quantitative
measure
in-
telligence
least
measured
traditional
tests
wechsler
stanford-binet
becoming
meaningless
capturing
intellectual
capabilities
minds
orders
magnitude
intelligent
alternatively
may
say
graph
measuring
average
in-
telligence
beyond
singularity
terms
score
may
display
form
radical
dis-
continuity
superintelligence
emerges
remains
true
whether
particular
account
de-
scribes
scenario
see
conclusion
example
commonly
argued
arrival
human-level
may
soon
followed
artificial
superintelligence
ac-
counts
singularity
human
amplification
describe
superhuman
cognitive
capabilities
including
unbounded
memory
accelerating
recall
times
eradication
common
obstructions
intelligent
behaviour
limited
resources
disease
age
discontinuity
accounts
singularity
take
term
stand
turning-point
human
history
von
neumann
canonical
definition
essential
singularity
history
race
beyond
human
affairs
know
could
continue.
whether
taken
last
hours
e.g
hard
takeoff
foom
scenarios
period
spread
decades
e.g.
wave
toffler
1980
singularity
term
appears
originate
mathematical
singularities
hirshfeld
2011
discontinuities
gravitational
singularities
better
known
black
holes
seen
central
metaphor
gravitational
singular-
ity
theoretical
point
centre
black
holes
quantities
otherwise
definition
gave
eden
steinhart
2013
omits
superintelligence
taking
implied
ex-
position
singularity
immergence
superintelligence
artificial
posthuman
sapience
project
meaningful
e.g.
density
space-time
curvature
become
infinite
rather
meaningless
metaphor
useful
express
idea
superintelligence
stands
level
intelligence
traditional
measures
become
ineffective
meaningless
discontinuity
best
explained
using
event
horizon
surrounding
gravitational
singulari-
ties
understand
foresee
events
may
follow
singularity
way
peek
black
holes
crossing
event
horizon
boundary
space-time
marked
schwarzschild
radius
around
gravitational
singularity
beyond
events
inside
area
observed
outside
horizon
beyond
gravitational
pull
becomes
strong
nothing
escape
even
light
hence
black
point
return
emergence
superintelligence
marks
similar
point
event
horizon
even
tiny
increment
problem-solving
ability
group
coordination
left
apes
dust
sandberg
2014
discontinuity
ontological
epistemological
account
existence
acceleration
refers
rate
growth
quantity
intelligence
good
1965
com-
putations
per
second
per
fixed
dollar
kurzweil
2005
economic
measures
growth
rate
han-
son
1994
bostrom
2014
miller
2013
total
output
goods
services
toffler
1970
energy
rate
density
chaisson
2013
accounts
acceleration
describe
quantitative
measures
physical
biological
social
cultural
technological
processes
evolution
mile-
stones
paradigm
shifts
whose
timing
demonstrates
accelerating
pace
change
ex-
ample
carl
sagan
cosmic
calendar
1977
names
milestones
biological
evolution
emergence
eukaryotes
vertebrates
amphibians
mammals
primates
hominidae
homo
sapiens
show
accelerating
trend
authors
attempt
unify
accel-
eration
quantities
one
law
nature
adams
1904
kurzweil
2004
christian
2010
chaisson
2013
quantitatively
qualitatively
measured
acceleration
commonly
visualized
upwards-curved
mathematical
graph
projected
future
said
lead-
ing
discontinuity
see
conclusion
singularity
mean
conclude
singularity
accounts
three
common
seemingly
unique
denominators
nonetheless
also
become
abundantly
clear
singularity
hypotheses
refer
either
one
two
distinct
different
scenarios
one
scenario
singularity
artificial
super-
intelligence
refers
often
dystopian
emergence
human-level
smarter
superintelligent
agents
software-based
syn-
thetic
minds
within
four
decades
absent
precautions
scenario
sees
becoming
one
greatest
potential
threats
human
existence
sandberg
2014
worst
thing
hap-
pen
humanity
history
hawking
2014
hand
singularity
posthu-
man
superintelligence
however
refers
radically
different
usually
utopian
scenario
sees
emergence
superintelligent
posthumans
within
similar
timeframe
resulting
merger
biology
technology
kurzweil
2005
specifically
intelligence
amplifica-
tion
singularity
would
see
descendants
overcoming
disease
aging
hunger
present
sources
misery
humans
conclusion
singularity
ambiguous
term
either
stands
one
greatest
threats
human
existence
scenario
diamaterically
opposed
humanity
magnificent
transcendence
scenario
ambiguity
caused
endless
confusion
singularity
controversy
part
lessons
learned
open
questions
obscured
debate
helped
undermining
legitimacy
popular
media
gets
two
singularities
mixed
guardian
cadwalladr
2014
even
respected
magazine
under-
taken
debunk
myths
ars
technica
goodwins
2015
confuse
two
scenarios
singularity
hypotheses
volume
structured
accordingly
part
computer
scien-
tists
describe
uniquely
intelligent
behaviour
programs
possess
ability
learn
plausibility
intelligence
explosion
likelihood
singularity
artificial
su-
perintelligence
within
four
decades
part
computer
scientists
discussed
risks
artifi-
cial
superintelligence
described
research
friendly
essays
part
iii
volume
discussed
human
enhancement
plausibility
singularity
human
superintelligence
within
similar
timeframe
conclusion
singularities
implausible
incoherent
singular
part
singularity
hypotheses
volume
dedicated
critics
technological
sin-
gularity
many
view
religious
idea
proudfoot
2013
bringsjord
bringsjord
bello
2013
idea
artificial
superintelligence
emerging
2045
mockingly
de-
scribed
rapture
nerds
doctorow
stross
2013
yet
another
apocalyptic
fantasy
technology-infused
variation
doom-and-gloom
scenarios
originating
mysti-
cism
fiction
cults
commercial
interests
powerful
counterargument
singularity
hypotheses
offered
energy
density
rate
the-
ory
offered
physicist
eric
chaisson
chaisson
2013
chaisson
accepts
acceleration
superintelligence
two
three
premises
underlying
singularity
accounts
see
conclusion
singularity
acceleration
discontinuity
superintelligence
rejects
discontinuity
potent
part
chaisson
argument
shows
one
physical
quantity
energy
density
rate
define
demonstrate
evolution
physical
galaxies
biological
life
cultural
technological
aeroplanes
systems
thereby
unifying
forms
evolution
chaisson
work
explains
coincides
carl
sagan
sagan
1977
historians
be-
hind
big
history
christian
2010
christian
2012
nazaretyan
2013
law
accelerating
returns
kurzweil
2004
others
chaisson
theory
also
embraces
emergence
super-
intelligence
next
evolutionary
leap
forward
beyond
sentient
beings
chaisson
am-
bitious
acceleration
theory
also
underplays
premise
discontinuity
cosmic
scale
ar-
gues
reason
claim
step
important
past
emergence
increasingly
intricate
complex
systems.
conclusion
pulp
singularities
scientific
hypotheses
term
singularity
general
use
vastly
influenced
recent
spate
hollywood
blockbusters
popular
series
illustrate
use
term
philosophers
schneider
2009
scientists
hawking
2014
used
movies
illustrate
convey
un-
derstanding
singularity
indeed
science
fiction
stories
greatly
contributed
understanding
behav-
iour
risks
blockbusters
series
also
taken
artistic
freedom
drawing
in-
sapience
project
creasingly
irrational
scenarios
example
robot
uprising
scenario
film
termina-
tor2
depicts
malevolent
artificial
superintelligence
skynet
sets
killer
robots
chase
crowds
mercilessly
gun
similarly
ais
matrix
enslaved
human
race
farming
bodies3
highly
unlikely
purpose
generating
electricity4
unprecedented
popularity
cinematic
works
used
term
associated
tech-
nological
singularity
variation
golem
fairytale
human
race
strug-
gles
survive
army
hostile
shapeshifting
robots
thus
cultural
construct
singularity
meme
dawkins
2000
blackmore
2001
become
synonymous
pulp
fiction
malevolent
ais
robot
uprising
narrative
largely
ignoring
subtler
rational
ais
depicted
2001
space
odyssey
machina
many
expositions
singularity
hypotheses
discuss
risks
artificial
superintelligence
essayists
explicitly
reject
robot
uprising
scenario
nonetheless
critics
often
attack
plot
films
conflating
singularity
meme
scientific
hypotheses
confusion
remained
despite
numerous
headlines
mass
media
debunking
common
misconceptions
singularity
critics
even
gone
far
misattribute
plot
films
authors
taken
distance
singularity
meme5
confusion
often
derailed
debate
undertaken
debunk
several
fallacies
arising
confusion
discussing
risks
authors
scientifically
philosophically
sound
singularity
hy-
potheses
distance
singularity
meme
emphasising
dangers
indifferent
rather
aggressively
hostile
distinction
malevolent
indif-
ferent
separates
meme
scientific
hypotheses
explained
next
conclusion
conclusion
risks
arise
indifference
malevolence
nearly
singularity
hypotheses
scientifically
philosophically
informed
expo-
sitions
singularity
taken
pains
reject
robot
uprising
scenario
aggressive
malevolence
uniformly
dismissed
apocalyptic
fantasy
one
may
mistak-
enly
called
science
fiction
idea
spontaneous
emergence
class
hostile
ais
considered
anthropomorphic
irrational
hence
highly
implausible
theorists
argued
self-preservation
may
basic
drive
omohundro
2008
emergent
property
artificial
superintelligence
rather
explicitly
programmed
fea-
ture
bostrom
2014
rational
singularity
accounts
reject
idea
ais
somehow
spontaneously
human
traits
popping
existence
rather
singularity
hypotheses
argue
risks
associated
artificial
superintelligence
concern
indifference
malevolence
indeed
anthropomorphic
bias
recurring
theme
literature
safety
human
bias
explain
leads
expect
agent
undergo
sudden
unexplained
emergence
chronicles
sarah
connor
series
process
creating
detailed
simulation
reality
keep
human
body
generates
much
electricity
potato
people
cost
lot
farm
potatoes
guardian
cadwalladr
2014
ars
technica
goodwins
2015
omohundro
argument
however
makes
several
assumptions
premises
include
restricted
class
artificial
superintelligence
self-improving
utility-driven
etc
specific
circumstances
free
market
forces
etc
singularity
controversy
part
lessons
learned
open
questions
emotions
malevolence
capabilities
empathy
needs
autonomy
self-expression
fact
existential
risk
far
serious
comes
advanced
race
artificially
superintelligent
agents
lack
anthropomorphic
emotions
needs
there-
fore
occam
razor
strictly
terms
probability
theory
safely
ignore
assumption
develop
human
traits
spontaneously
fact
even
greater
risk
emerging
race
superintelligence
may
lethally
indifferent
human
race
regardless
whether
superintelligence
artificial
posthuman
alien
whatnot
see
one
need
witness
homo
sapiens
indiffer-
ent
welfare
less
intelligent
species
farm
pigs
cramped
cages
like
bacon
seek
enslave
species
sea
minks
gone
extinct
anyone
sought
exterminate
hunted
wider
intelligence
gap
indifferent
superintelligent
beings
may
towards
humans
may
hesitate
wipe
city
ordinary
humans
hesitate
destroy
anthill
building
new
motorway
concerns
safety
limited
emergence
race
artificially
superintelligent
agents
ais
canonical
example
paperclip
factory
illustrates
dangerous
superintelligence
even
built
well-meaning
programmers
unless
well-meaning
friendly
philanthropic
supergoals
built
risks
developing
superintelligence
include
risk
failure
give
supergoal
philanthropy
one
way
could
happen
well-meaning
team
program-
mers
make
big
mistake
designing
goal
system
could
result
return
earlier
example
superintelligence
whose
top
goal
manufacturing
paperclips
con-
sequence
starts
transforming
first
earth
increasing
portions
space
paperclip
manufacturing
facilities
bostrom
2003
subtly
indifference
may
also
refer
risk
exists
even
well-meaning
artifi-
cially
developed
superintelligence
indifference
arises
ignorance
humanity
core
values
well-meaning
artificial
superintelligence
may
dangerous
unless
friendly
humans
yudkowsky
2013
bostrom
2014
particular
towards
core
values
freedom
autonomy
self-expression
otherwise
risk
may
conclude
best
thing
human
race
would
put
hallucinogenic
drugs
bring
false
utopia
namely
state
affairs
might
temporarily
judge
desirable
fact
…things
essential
human
flourishing
irreversibly
lost.
bostrom
2003
conclusion
risk
essentially
like
risk
powerful
technology
advice
offered
authors
discussing
risks
future
life
institute
recently
published
open
letter
robotics
researchers
urging
research
subject
risk
considerations
research
physical
sciences
chemists
biologists
interest
building
chemical
biological
weapons
researchers
interest
building
weapons
future
life
institute
2015
argument
safety
neither
novel
unusual
example
atomic
energy
dangerous
prevent
misuse
international
treaties
limit
production
military-
grade
nuclear
energy
sale
machinery
radioactive
material
capable
producing
sapience
project
atomic
weapons
warning
arms
race
future
life
institute
2015
future
life
institute
open
letter
takes
comparison
nuclear
energy
limits
unlike
nuclear
weapons
autonomous
killer
robots
require
costly
hard-to-obtain
raw
materials
become
ubiquitous
cheap
significant
military
powers
mass-
produced
matter
time
appear
black
market
hands
terrorists
dictators
wishing
better
control
populace
warlords
wishing
perpetrate
ethnic
cleansing
etc
another
analogy
offered
risks
certain
experiments
genetic
engineering
partic-
ular
involving
recombinant
dna
may
create
deadly
pathogens
process
much
less
costly
producing
atomic
energy
well
far
available
easier
misused
results
no-less
catastrophic
stem
misuse
technology
asilomar
con-
ference
called
voluntary
moratorium
certain
experiments
also
offered
safety
guidelines
recommended
research
continues
measures
proved
effective
moratorium
universally
observed
recent
study
found
several
analogies
concerns
expressed
asilomar
conference
concerns
grace
2015
focus
relatively
novel
risks
new
technology
complexity
reasoning
risks7
science
fiction
isaac
asimov
scientist
much
author
took
granted
requirement
safeguards
machine
intelligence
asimov
explained
safeguards
apply
matter
course
every
tool
human
beings
use
asimov
1983
elsewhere
asimov
compares
laws
circuit
interrupters
tool
must
unsafe
use
hammers
handles
screwdrivers
hilts
help
increase
grip
perform
function
efficiently
tool
harm
user
entire
reason
ground-fault
circuit
interrupters
exist
running
tool
power
cut
circuit
senses
current
returning
neutral
wire
hence
might
flowing
user
asimov
1993
wikipedia
robot
visions
conclusion
technology
always
double-edged
sword
even
becomes
ex-
istential
threat
first
one
sorcerer
apprentice
fantasy
help
get
point
offers
compelling
scenario
raging
terminators
tale
dangerous
outcome
apprentice
careless
experiments
demonstrates
simple
fact
potent
forces
fall
inadequately
trained
irresponsible
hands
control
forces
may
easily
lost
deadly
effects
nick
bostrom
put
need
careful
wish
superintelligence
might
get
bostrom
2003
conclusion
clear
artificial
intelligence
means
textbooks
typically
present
artificial
intelligence
subdiscipline
computing
sciences
body
knowledge
solving
difficult
challenges
practice
defines
indirectly
challenges
considered
difficult
solvable
time
writing
textbook
set
challenges
however
consistently
grown
since
infancy
example
study
also
found
one
main
difference
risks
addressed
1975
asilomar
conference
appeared
overwhelmingly
immediate
unlike
human-level
expected
less
fifteen
years
future
singularity
controversy
part
lessons
learned
open
questions
1970s
challenges
playing
chess
jeopardy
game
competently
consid-
ered
difficult
experts
expect
challenges
met
anytime
soon
arguing
could
never
win
games
reason
effectively
ambiguous
do-
mains
natural
language
image
recognition
within
decades
deep
blue
beat
world
chess
champion
ibm
watson
two
world
champions
televised
jeop-
ardy
game
time
writing
clever
app
developers
turn
mobile
devices
grandmaster-level
chess
players
watson
acquire
knowledge
reading
standard
text
encyclopaedias
articles
learned
diagnose
many
medical
con-
ditions
well
doctors
image
recognition
programs
improved
years
captcha
task8
become
increasingly
difficult
humans
read
simulta-
neously
easy
computers
read
captchas
largely
lost
claim
able
tell
computers
humans
apart
bots
disguising
available
lovelorn
women
game
characters
friends
trouble
increasingly
indistinguishable
humans
thus
within
seven
decades
quite
challenges
seemed
possibly
insurmountable
downgraded
child
play
similar
manner
artificial
intelligence
means
practice
changed
accordingly
limited
set
capabilities
one
seeks
approximate
hu-
mans
since
meaning
artificial
intelligence
much
evolved
experts
use
terms
hu-
man-level
artificial
intelligence
hlai
artificial
general
intelligence
agi
reference
point
discipline
progress
humans
challenged
across
sufficiently
wide
range
capabilities
experts
therefore
commonly
take
artificial
intelligence
de-
fined
comparison
human
intelligence
leads
several
problems
first
problem
psychologists
predominantly
reduce
human
intelligence
one
quotient
measured
using
psychometric
tests
hypothesis
single
generic
capability
called
factor
explain
forms
human
intelligence
creative
genius
strategic
talent
social
competence
also
explain
non-human
intelligence
although
abilities
use
tools
plan
ahead
demonstrated
many
animal
species
obviously
factor
theory
help
understand
alien
intelligence9
conclusion
canonical
accounts
human
intelligence
helped
measure
artificial
nonhuman
intelligence
legg
hutter
legg
hutter
2007
define
universal
intelligence
metric10
measuring
level
intelligence
agent
given
environment
human
animal
artificial
metric
uses
kolmogorov
complexity
measures
difficulty
problems
agent
solves
well
measure
proven
hard
impossible
establish
metric
applica-
bility
yet
tested
empirically
importantly
however
theory
universal
intelligence
defines
intelligence
given
agent
function
environment
operates
precisely
intelligence
agent
given
environment
proportional
agent
ability
maximize
utility
solving
problems
environment
likely
pose
thus
agent
exhibits
high
intelligence
environments
happen
maximize
utility
problems
agent
competently
solves
also
exhibit
low
in-
telligence
environments
feature
universal
intelligence
sits
well
un-
derstanding
intelligence
encouraging
result
legg
hutter
theory
far
helped
clarify
artificial
intelligence
mean
practical
terms
termine
whether
attempt
remote
access
made
human
bot
captcha
stands
completely
automated
public
turing
test
tell
computers
humans
apart
test
de-
whatever
search
extra
terrestrial
intelligence
searching
sign
rather
family
metrics
sapience
project
difficulties
pin
means
undermine
ability
assess
risks
know
capabilities
could
could
developed
near
future
long
take
make
available
applications
new
capability
likelihood
losing
con-
trol
technology
emerge
applications
circumstances
unintended
consequences
future
technology
could
pose
existential
risk
explained
sense
technologies
safely
said
become
pow-
erful
trend
may
continue
far
research
superintelligence
bostrom
2014
given
reasons
believe
difference
apes
humans
dwarfed
gap
humans
intelligence
superintelligence
yet
entirely
unclear
much
intelligent
artificial
agents
become
near
future
without
research
left
guess
whether
significant
advances
likely
whether
prepare
our-
selves
capabilities
advanced
could
turn
lights
could
blink
uncertainty
clearly
undesirable
conclusion
debate
ended
barely
begun
2012
compiling
essays
rebuttals
singularity
hypotheses
volume
concluded
rapid
growth
singularity
research
seems
set
continue
perhaps
accelerate
eden
steinhart
2013
since
prominent
scientists
pushed
invest
direction
hawking
2014
future
life
institute
2015
followed
sensational
headlines
controversy
described
promote
debate
merely
caricaturised
muddled
following
questions
remain
open
clear
debate
barely
began
open
questions
investigation
also
led
conclude
issues
centre
around
number
open
questions
following
question
controlled
loss
control
may
indeed
pose
dangerous
outcome
scientists
feel
com-
pelled
ask
risk
eliminated
reduced
research
question
largely
led
conclusion
controlling
behaviour
agents
intelligent
makers
difficult
impossible
bostrom
2014
particular
agent
learns
modify
yampolskiy
2012
one
strategy
maintaining
control
commonly
referred
friendly
research
arising
largely
machine
intelligence
research
institute
yudkowsky
2001
yudkowsky
2013
focuses
methods
designing
ais
whose
behaviour
would
considered
friendly
morally
justified
example
hardwiring
artificial
agents
core
val-
ues
major
difficulty
approach
raises
notoriously
tricky
question
exactly
constitutes
moral
behaviour
people
morality
implies
equal
distribution
singularity
controversy
part
lessons
learned
open
questions
wealth
others
establishing
oppressive
global
caliphate
words
notion
friendly
seems
require
objective
theory
morality
universal
notion
good
notoriously
difficult
goal
computationally
intractable
one
brundage
2014
further-
formal
methods
wing
1990
hinchey
2008
software
engineering
taught
largely
impossible
guarantee
particular
program
implements
desired
behaviour
assuming
behaviour
objective
friendliness
established
formally
defined
another
strategy
controlling
ais
seeks
confine
artificial
agents
boxing
leak-
proof
environment
manner
could
prevent
undesired
outcomes
materializing
example
sealing
ais
special
hardware
confining
simulated
environments
vernor
vinge
vinge
1993
famously
described
difficulty
boxing
approach
us-
ing
example
superintelligent
agent
think
millions
times
faster
cap-
tives
therefore
could
easily
come
helpful
advice
would
incidentally
set
free
promising
strategy
controlling
suggests
restrict
use
artificial
agents
answer-
ing
yes
questions
oracle
strategy
bostrom
2014
minimizes
potential
im-
pact
ais
limiting
one
capability
answer
questions
either
ttrue
false
question
controlling
fits
within
general
question
mechanism
design
body
work
2007
awarded
nobel
prize
economics
studying
optimal
mech-
anisms
reach
certain
goal
far
research
indicates
questions
difficult
information
individual
preferences
available
production
technologies
dis-
tributed
among
many
actors
may
use
privately
held
information
interests11
finally
suggested
outlaw
research
may
lead
artificial
superintelligence
joy
2000
however
even
regulators
conclude
risky
enormous
benefits
must
abandoned
banning
unlikely
stop
rather
ban
likely
push
research
un-
derground
resulting
independent
programmers
private
labs
conducting
unmonitored
experiments
would
even
less
desirable
major
difficulty
control
strategies
even
foolproof
method
controlling
ais
discovered
would
globally
enforced
difficult
military
incentives
nations
monetary
incentives
commercially
motivated
organization
particular
totalitarian
regimes
corporations
may
tempted
loosen
remove
con-
trols
purpose
gaining
advantage
rivals
demonstrated
example
scenario
global
arms
race
istvan
2015
future
life
institute
2015
question
analysing
singularity
hypotheses
conclusion
yielded
distinction
dystopian
utopian
singularity
scenarios
suggesting
mutually
exclusive
circumstances
rather
race
artificial
posthuman
superintelligence
outcome
race
therefore
royal
swedish
academy
sciences
press
release
prize
economic
sciences
2007
sapience
project
open
question
whose
answer
may
mark
difference
best
worst
de-
velopments
human
history
question
win
race
neatly
summa-
rized
successors
descendants
pearce
2013
pithily12
question
prove
ais
becoming
intelligent
critics
artificial
superintelligence
ideas
often
reject
common
perception
ever
made
genuine
breakthroughs
argued
new
capability
merely
insignificant
application
pre-existing
technology
systems
demonstrate
re-
mained
narrow
rigid
brittle
allen
2011
one
questions
central
debate
therefore
whether
one
could
quantify
intellectual
advantage
newer
ais
systems
built
older
principles
metric
measure
intelligence
different
ais
effectively
might
provide
means
find
much
intelligent
become
possibly
improve
understanding
way
going
summary
central
conclusion
controversy
future
replaced
reasoned
well-informed
debate
questions
technology
raises
issues
much
important
remain
confused
misinformed
derailed
fantasy
likes
hollywood
skynet
left
academics
experts
merely
speculated
pundits
amateurs
opinionated
philistines
acknowledgements
author
wishes
thank
tony
willson
encouragement
mary
anna
inspiration
research
self-funded
bibliography
adams
henry
1904
law
acceleration
education
henry
adams
new
york
houghton
mifflin
allen
paul
2011
singularity
near
mit
technology
review
october
12.
http
//www.technolo-
gyreview.com/view/425733/paul-allen-the-singularity-isnt-near/
ars
technica
rupert
goodwins
2015
debunking
biggest
myths
artificial
intelligence
ars
technica
december
25.
http
//arstechnica.com/information-technology/2015/12/demystifying-ar-
tificial-intelligence-no-the-singularity-is-not-just-around-the-corner/
george
dvorsky
humans
amplified
intelligence
could
powerful
io9
may
2013
berlin
413–40
springer
frontiers
collection
2303.2010.00557.x
singularity
controversy
part
lessons
learned
open
questions
blackmore
susan
2001
evolution
memes
human
brain
selective
imitation
device
cyber-
netics
systems
1-2
225–55
doi:10.1080/019697201300001867
bostrom
nick
2003
ethical
issues
advanced
artificial
intelligence
cognitive
emotive
ethical
as-
pects
decision
making
humans
artificial
intelligence
edited
george
eric
lasker
goreti
marreiros
2:12–17
international
institute
advanced
studies
———
2014.
superintelligence
paths
dangers
strategies
oxford
university
press
bringsjord
selmer
alexander
bringsjord
paul
bello
2013
belief
singularity
fideistic
sin-
gularity
hypotheses
edited
amnon
eden
james
moor
johnny
søraker
eric
stein-
hart
395–412
frontiers
collection
springer
berlin
heidelberg
http
//link.springer.com/chap-
ter/10.1007/978-3-642-32560-1_19
brundage
miles
2014
limitations
risks
machine
ethics
journal
experimental
theoretical
arti-
ficial
intelligence
april
http
//www.tandfonline.com/doi/pdf/10.1080/0952813x.2014.895108
chaisson
eric
2013
singular
universe
many
singularities
cultural
evolution
cosmic
context
singularity
hypotheses
edited
amnon
eden
james
moor
johnny
søraker
eric
heidelberg
steinhart
http
//link.springer.com/chapter/10.1007/978-3-642-32560-1_20
chalmers
david
2010
singularity
philosophical
analysis
journal
consciousness
studies
7–65
christian
david
2010
return
universal
history
history
theory
6–27
doi:10.1111/j.1468-
———
2012
humanoid
histories
metanexus
march
19.
dawkins
richard
2000
foreword
meme
machine
susan
blackmore
oxford
university
press
doctorow
cory
charles
stross
2013.
rapture
nerds
london
titan
books
eden
amnon
2009
call
papers
technological
singularity
acceleration
studies
track
7th
european
conference
computing
philosophy—ecap
2009
lifeboat
foundation
january
https
//lifeboat.com/whats.new.linked.to.files/cfptechn.htm
———
2010
call
papers
technological
singularity
acceleration
studies
track
8th
european
conference
computing
philosophy—ecap
2010
lifeboat
foundation
april
http
//life-
boat.com/blog/2010/04/technological-singularity-and-acceleration-studies
eden
amnon
james
moor
johnny
hartz
søraker
eric
steinhart
2010
call
papers
singularity
hypothesis
scientific
philosophical
assessment
october
15.
http
//singularityhypothe-
sis.blogspot.co.uk/p/call-for-papers-dec-2010.html
eden
amnon
james
moor
johnny
søraker
eric
steinhart
eds
2013.
singularity
hypotheses
philosophical
assessment
scientific
springer
http
//www.springer.com/engineering/computational+intelligence+and+complexity/book/978-3-
642-32559-5.
eden
amnon
eric
steinhart
david
pearce
james
moor
2013
singularity
hypotheses
over-
view
singularity
hypotheses
scientific
philosophical
assessment
edited
amnon
eden
james
moor
johnny
søraker
eric
steinhart
1–12
frontiers
collection
springer
berlin
heidelberg
http
//link.springer.com/chapter/10.1007/978-3-642-32560-1_1
future
life
institute
2015
autonomous
weapons
open
letter
robotics
researchers
july
28.
http
//futureoflife.org/ai/open_letter_autonomous_weapons
good
irving
john
1965
speculations
concerning
first
ultraintelligent
machine
advances
com-
puters
edited
franz
alt
morris
rubinoff
6:31–88
academic
press
http
//www.stat.vt.edu/tech_reports/2005/goodtechreport.pdf
grace
katja
2015
asilomar
conference
case
study
risk
mitigation
technical
report
2015–9
berkeley
machine
intelligence
research
institute
https
//intelligence.org/files/theasilomar-
conference.pdf
guardian
carole
cadwalladr
2014
robots
rise
google
new
director
engineer-
ing
thinks
so…
guardian
february
sec
technology
http
//www.theguardian.com/tech-
nology/2014/feb/22/robots-google-ray-kurzweil-terminator-singularity-artificial-intelligence
frontiers
collection
hanson
robin
2008
economics
singularity
ieee
spectrum
37–42
software
engineering
formal
methods
commun
acm
sapience
project
hawking
stephen
max
tegmark
stuart
russell
frank
wilczek
2014
transcending
complacency
superintelligent
machines
huffington
post
april
19.
http
//www.huffingtonpost.com/stephen-
hawking/artificial-intelligence_b_5174265.html
hinchey
mike
michael
jackson
patrick
cousot
byron
cook
jonathan
bowen
tiziana
margaria
2008
54–59
doi:10.1145/1378727.1378742
istvan
zoltan
2015
global
arms
race
create
superintelligent
looming
motherboard
march
http
//motherboard.vice.com/read/a-global-arms-race-to-create-a-superintelligent-ai-is-loom-
ing
joy
bill
2000
future
need
wired
magazine
april
http
//www.wired.com/wired/ar-
chive/8.04/joy.html
kurzweil
ray
2004
law
accelerating
returns
alan
turing
life
legacy
great
thinker
springer
http
//www.kurzweilai.net/the-law-of-accelerating-returns
———
2006.
singularity
near
humans
transcend
biology
penguin
legg
shane
marcus
hutter
2007
universal
intelligence
definition
machine
intelligence
minds
machines
391–444
nazaretyan
akop
2013
nonlinear
futures
mega-history
complexity
theory
anthropology
psychol-
ogy
global
forecasting
omohundro
stephen
2008
basic
drives
proceeding
2008
conference
artificial
gen-
eral
intelligence
2008
proceedings
first
agi
conference
483–92
ios
press
http
//por-
tal.acm.org/citation.cfm
id=1566226
pearce
david
2013
biointelligence
explosion
singularity
hypotheses
scientific
philosophical
assessment
edited
amnon
eden
james
moor
johnny
søraker
eric
steinhart
199–
238.
frontiers
collection
springer
berlin
heidelberg
http
//link.springer.com/chap-
ter/10.1007/978-3-642-32560-1_11
proudfoot
diane
2013
software
immortals
science
faith
singularity
hypotheses
scientific
philosophical
assessment
edited
amnon
eden
james
moor
johnny
søraker
eric
heidelberg
steinhart
http
//link.springer.com/chapter/10.1007/978-3-642-32560-1_18
sagan
carl
1977.
dragons
eden
speculations
evolution
human
intelligence
ballantine
books
sandberg
anders
2014
five
biggest
threats
human
existence
conversation
may
29.
http
//theconversation.com/the-five-biggest-threats-to-human-existence-27053
schneider
susan
2009.
science
fiction
philosophy
time
travel
superintelligence
john
wiley
sons
toffler
alvin
1980.
third
wave
new
york
morrow
vinge
vernor
1993
coming
technological
singularity
survive
post-human
era
proc
vision
interdisciplinary
science
engineering
era
cyberspace
11–22
lewis
research
center
nasa
http
//adsabs.harvard.edu/abs/1993vise.nasa
...
11v
wing
jeannette
1990
specifier
introduction
formal
methods
computer
8–23
yampolskiy
roman
2012
leakproofing
singularity
artificial
intelligence
confinement
problem
journal
consciousness
studies
1–2
194–214
yudkowsky
eliezer
2001
creating
friendly
1.0
analysis
design
benevolent
goal
architec-
tures
san
francisco
machine
intelligence
research
institute
———
2007
three
major
singularity
schools
singularity
institute
blog
september
30.
http
//yudkow-
sky.net/singularity/schools
———
2013
friendly
artificial
intelligence
singularity
hypotheses
scientific
philosophical
assess-
ment
edited
amnon
eden
james
moor
johnny
søraker
eric
steinhart
181–95
frontiers
collection
springer
berlin
heidelberg
http
//link.springer.com/chapter/10.1007/978-3-
642-32560-1_10
collection
frontiers
springer
367–92
berlin
