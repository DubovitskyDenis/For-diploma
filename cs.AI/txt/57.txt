adaptive
imputation
missing
values
incomplete
pattern
classiﬁcation
zhun-ga
liu1
quan
pan1
jean
dezert2
arnaud
martin3
school
automation
northwestern
polytechnical
university
china
email
liuzhunga
nwpu.edu.cn
onera
french
aerospace
lab
f-91761
palaiseau
france
email
jean.dezert
onera.fr
3.irisa
university
rennes
rue
branly
22300
lannion
france
email
arnaud.martin
univ-rennes1.fr
abstract
classiﬁcation
incomplete
pattern
missing
values
either
play
crucial
role
class
determination
little
inﬂuence
eventually
none
classiﬁcation
results
according
context
propose
credal
classiﬁcation
method
incomplete
pattern
adaptive
imputation
missing
values
based
belief
function
theory
ﬁrst
try
classify
object
incomplete
pattern
based
available
attribute
values
underlying
principle
assume
missing
information
crucial
classiﬁcation
speciﬁc
class
object
found
using
available
information
case
object
committed
particular
class
however
object
classiﬁed
without
ambiguity
means
missing
values
play
main
role
achieving
accurate
classiﬁcation
case
missing
values
imputed
based
k-nearest
neighbor
k-nn
self-organizing
map
som
techniques
edited
pattern
imputation
classiﬁed
original
edited
pattern
respectively
classiﬁed
according
training
class
classiﬁcation
results
represented
basic
belief
assignments
fused
proper
combination
rules
making
credal
classiﬁcation
object
allowed
belong
different
masses
belief
speciﬁc
classes
meta-classes
particular
disjunctions
several
single
classes
credal
classiﬁcation
captures
well
uncertainty
imprecision
classiﬁcation
reduces
effectively
rate
misclassiﬁcations
thanks
introduction
meta-classes
effectiveness
proposed
method
respect
classical
methods
demonstrated
based
several
experiments
using
artiﬁcial
real
data
sets
keywords
belief
function
classiﬁcation
missing
values
som
k-nn
introduction
many
practical
classiﬁcation
problems
available
information
making
object
classiﬁcation
partial
incomplete
attribute
values
missing
due
various
reasons
e.g
failure
dysfunctioning
sensors
providing
information
partial
observation
object
interest
occultation
phenomenon
etc
crucial
develop
efﬁcient
techniques
classify
best
possible
objects
missing
attribute
values
incomplete
pattern
search
solution
problem
remains
important
research
topic
pattern
classiﬁcation
ﬁeld
details
pattern
classiﬁcation
found
many
approaches
developed
classifying
incomplete
patterns
broadly
grouped
four
different
types
ﬁrst
simplest
one
remove
directly
patterns
missing
values
classiﬁer
designed
complete
patterns
method
acceptable
incomplete
data
set
small
subset
e.g
less
whole
data
set
effectively
classify
pattern
missing
values
second
type
model-
based
techniques
probability
density
function
pdf
input
data
complete
incomplete
cases
estimated
ﬁrst
means
procedures
object
classiﬁed
using
bayesian
reasoning
instance
expectation-maximization
algorithm
applied
many
problems
involving
missing
data
training
gaussian
mixture
models
model-based
methods
must
make
assumptions
joint
distribution
variables
model
suitable
distributions
sometimes
hard
obtain
third
type
classiﬁers
designed
directly
handle
incomplete
pattern
without
imputing
missing
values
neural
network
ensemble
methods
decision
trees
fuzzy
approaches
support
vector
machine
classiﬁer
last
type
often
used
imputation
estimation
method
missing
values
ﬁlled
proper
estimations
ﬁrst
edited
patterns
classiﬁed
using
normal
classiﬁer
complete
pattern
missing
values
pattern
classiﬁcation
treated
separately
methods
many
works
devoted
imputation
missing
data
imputation
done
either
statistical
methods
e.g
mean
imputation
regress
imputation
etc
machine
learning
methods
e.g
k-nearest
neighbors
imputation
knni
fuzzy
c-means
fcm
imputation
fcmi
self-organizing
map
imputation
somi
etc
knni
missing
values
estimated
using
k-nearest
neighbors
object
training
data
space
fcmi
missing
values
imputed
according
clustering
centers
fcm
taking
account
distances
object
centers
somi
best
match
node
unit
incomplete
pattern
found
ignoring
missing
values
imputation
missing
values
computed
based
weights
activation
group
nodes
including
best
match
node
close
neighbors
existing
methods
usually
attempt
classify
object
particular
class
maximal
probability
likelihood
measure
however
estimation
missing
values
general
quite
uncertain
different
imputations
missing
values
yield
different
classiﬁcation
results
prevent
correctly
commit
object
particular
class
belief
function
theory
bft
also
called
dempster-shafer
theory
dst
extension
offer
mathematical
framework
modeling
uncertainty
imprecise
information
bft
already
applied
successfully
object
classiﬁcation
clustering
multi-source
information
fusion
etc
classiﬁers
complete
pattern
based
dst
developed
denœux
collaborators
come
evidential
k-nearest
neighbors
ek-nn
evidential
neural
network
enn
etc
extra
ignorance
element
represented
disjunction
elements
whole
frame
discernment
introduced
classiﬁers
capture
totally
ignorant
information
however
partial
imprecision
important
classiﬁcation
well
characterized
proposed
credal
classiﬁers
complete
pattern
considering
possible
meta-classes
i.e
particular
disjunctions
several
singleton
classes
model
partial
imprecise
information
credal
classiﬁcation
allows
objects
belong
different
masses
belief
singleton
classes
also
set
classes
corresponding
meta-
classes
belief-based
k-nearest
neighbor
classiﬁer
bk-nn
presented
credal
classiﬁcation
object
done
according
distances
object
nearest
neighbors
well
two
given
acceptance
rejection
distance
thresholds
k-nn
classiﬁer
generally
takes
big
computation
burden
convenient
real
application
thus
simple
credal
classiﬁcation
rule
ccr
developed
belief
value
object
associated
different
classes
i.e
singleton
classes
selected
meta-classes
directly
calculated
distance
center
corresponding
class
distinguishability
degree
w.r.t
object
singleton
classes
involved
meta-class
location
center
meta-class
ccr
considered
similar
distance
involved
singleton
classes
centers
moreover
training
data
available
also
proposed
several
credal
clustering
methods
different
cases
nevertheless
previous
credal
classiﬁcation
methods
mainly
dealing
complete
pattern
without
taking
account
missing
values
recent
work
prototype-based
credal
classiﬁcation
pcc
method
incomplete
patterns
introduced
capture
imprecise
information
caused
missing
values
object
hard
correctly
classify
committed
suitable
meta-class
pcc
well
characterizes
imprecision
classiﬁcation
due
absence
part
attributes
also
reduces
misclassiﬁcation
errors
pcc
missing
values
incomplete
patterns
imputed
using
prototype
class
center
edited
pattern
imputation
respectively
classiﬁed
standard
classiﬁer
complete
pattern
pcc
one
obtains
pieces
classiﬁcation
results
incomplete
pattern
class
problem
global
fusion
results
given
credal
classiﬁcation
unfortunately
pcc
classiﬁer
computationally
greedy
time-consuming
imputation
missing
values
based
class
prototype
precise
order
overcome
limitations
pcc
propose
new
credal
classiﬁcation
method
incomplete
pattern
adaptive
imputation
missing
values
called
credal
classiﬁcation
adaptive
imputation
ccai
short
pattern
classify
usually
consists
multiple
attributes
sometimes
class
pattern
precisely
determined
using
part
subset
available
attributes
implies
attributes
redundant
fact
unnecessary
classiﬁcation
new
method
credal
classiﬁcation
adaptive
imputation
strategy
i.e
ccai
missing
values
proposed
ccai
attempt
classify
object
using
known
attributes
value
ﬁrst
speciﬁc
classiﬁcation
result
obtained
likely
means
missing
values
necessary
classiﬁcation
directly
take
decision
class
object
based
result
however
object
clearly
classiﬁed
available
information
indicates
missing
information
included
missing
attribute
values
probably
crucial
making
classiﬁcation
case
present
sophisticated
classiﬁcation
strategy
edition
pattern
based
proper
imputation
missing
values
k-nearest
neighbors-based
imputation
method
usually
provides
pretty
good
performances
es-
timation
missing
values
main
drawback
big
computational
burden
reduce
computational
burden
self-organizing
map
som
applied
class
optimized
weighting
vectors
used
represent
corresponding
class
nearest
weighting
vectors
object
class
respectively
employed
estimate
missing
values
classiﬁcation
original
incomplete
pattern
without
imputation
missing
values
edited
pattern
imputation
missing
values
adopt
ensemble
classiﬁer
approach
one
respectively
get
simple
classiﬁcation
result
according
training
class
classiﬁcation
result
represented
simple
basic
belief
assignment
bba
including
two
focal
elements
i.e
singleton
class
ignorant
class
belief
object
belonging
class
calculated
based
distance
corresponding
prototype
belief
committed
ignorant
element
fusion
ensemble
multiple
bba
used
determine
class
object
object
directly
classiﬁed
using
known
values
dempster-shafer1
fusion
rule
applied
simplicity
rule
also
bba
fuse
usually
low
conﬂict
case
speciﬁc
result
obtained
rule
otherwise
new
fusion
rule
inspired
dubois
prade
rule
used
classify
edited
pattern
proper
imputation
missing
values
estimation
missing
values
quite
uncertain
naturally
induces
imprecise
classiﬁcation
partial
conﬂicting
beliefs
kept
committed
associated
meta-classes
new
rule
reasonably
reveal
potential
imprecision
classiﬁcation
result
paper
present
credal
classiﬁcation
method
adaptive
imputation
missing
values
based
belief
function
theory
dealing
incomplete
patterns
organized
follows
basics
belief
function
theory
self-organizing
map
brieﬂy
recalled
section
new
credal
classiﬁcation
method
incomplete
patterns
presented
section
iii
proposed
method
tested
evaluated
section
compared
several
classical
methods
paper
concluded
ﬁnal
background
knowledge
belief
function
theory
bft
well
characterize
uncertain
imprecise
information
used
work
classiﬁcation
patterns
som
technique
employed
ﬁnd
optimized
weighting
vectors
used
represent
corresponding
class
reduce
computation
burden
estimation
missing
values
based
k-nn
method
basic
knowledge
bft
som
brieﬂy
recalled
basis
belief
function
theory
belief
function
theory
bft
introduced
glenn
shafer
also
known
dempster-shafer
theory
dst
mathematical
theory
evidence
let
consider
frame
discernment
consisting
exclusive
exhaustive
hypotheses
classes
denoted
power-set
denoted
set
subsets
empty
set
included
example
classiﬁcation
problem
singleton
element
e.g
represents
speciﬁc
class
work
disjunction
union
several
1although
rule
proposed
originally
arthur
dempster
prefer
call
dempster-shafer
rule
widely
promoted
shafer
singleton
elements
called
meta-class
characterizes
partial
ignorance
classiﬁcation
examples
meta-classes
bft
one
object
associated
different
singleton
elements
well
sets
elements
according
basic
belief
assignment
bba
function
satisfying
normalization
condition
cid:80
subsets
called
focal
elements
belief
mass
credal
classiﬁcation
partitioning
deﬁned
n-tuple
···
bba
basic
belief
assignment
object
associated
different
elements
power-set
credal
classiﬁcation
allows
objects
belong
speciﬁc
classes
sets
classes
corresponding
meta-classes
different
belief
mass
assignments
credal
classiﬁcation
well
model
imprecise
uncertain
information
thanks
introduction
meta-
class
combining
multiple
sources
evidence
represented
set
bba
well-known
dempster
rule
still
widely
used
even
justiﬁcation
open
debate
questionable
community
a∈2ω
combination
two
bba
done
rule
combination
deﬁned
mds
cid:54
cid:80
cid:80
b∩c=a
b∩c=∅
mds
rule
commutative
associative
makes
compromise
speciﬁcity
complexity
combination
bba
rule
conﬂicting
beliefs
cid:80
propor-
tionally
redistributed
back
focal
elements
classical
normalization
step
however
b∩c=∅
redistribution
yield
unreasonable
results
high
conﬂicting
cases
well
special
low
conﬂicting
cases
well
different
rules
combination
emerged
overcome
limitations
among
possible
alternatives
rule
ﬁnd
smets
conjunctive
rule
used
transferable
belief
model
tbm
dubois-prade
rule
recently
complex
proportional
conﬂict
redistributions
pcr
rules
unfortunately
pcr
rules
less
appealing
implementation
standpoint
since
associative
become
complex
use
two
bba
combined
altogether
overview
self-organizing
map
self-organizing
map
som
also
called
kohonen
map
introduced
teuvo
kohonen
type
artiﬁcial
neural
network
ann
trained
unsupervised
learning
method
som
deﬁnes
mapping
input
space
low-dimensional
typically
two-dimensional
grid
nodes
allows
approximate
feature
space
dimension
e.g
real
input
vector
projected
space
still
able
preserve
topological
properties
input
space
using
neighborhood
function
thus
som
useful
visualizing
low-dimensional
views
high-dimensional
data
non
linear
projection
node
position
corresponds
weighting
vector
denoted
input
vector
compared
neuron
whose
weighting
vector
close
similar
according
given
metric
called
best
matching
unit
bmu
deﬁned
output
som
respect
real
applications
euclidean
distance
usually
used
compare
input
pattern
mapped
onto
som
location
minimal
distance
considered
som
achieves
non-uniform
quantization
transforms
minimizing
given
metric
e.g
distance
measure
som
competitive
learning
adopted
training
algorithm
iterative
initial
values
weighting
vectors
may
set
randomly
converge
stable
value
end
training
process
input
vector
fed
network
euclidean
distance
weight
vectors
computed
bmu
whose
weight
vector
similar
input
vector
found
weights
bmu
neurons
close
som
grid
adjusted
towards
input
vector
magnitude
change
decreases
time
distance
within
grid
bmu
detailed
information
som
found
work
som
applied
training
class
obtain
optimized
weighting
vectors
used
represent
corresponding
class
number
weighting
vectors
much
smaller
original
samples
associated
training
class
utilize
weighting
vectors
rather
original
samples
estimate
missing
values
object
incomplete
pattern
could
effectively
reduce
computation
burden
iii
credal
classification
incomplete
pattern
new
method
consists
two
main
steps
ﬁrst
step
object
incomplete
pattern
directly
classiﬁed
according
known
attribute
values
missing
values
ignored
one
get
speciﬁc
classiﬁcation
result
classiﬁcation
procedure
done
available
attribute
information
sufﬁcient
making
classiﬁcation
class
object
clearly
identiﬁed
ﬁrst
step
means
unavailable
information
included
missing
values
likely
crucial
classiﬁcation
case
one
enter
second
step
method
classify
object
proper
imputation
missing
values
classiﬁcation
procedure
original
edited
pattern
respectively
classiﬁed
according
class
training
data
global
fusion
classiﬁcation
results
considered
multiple
sources
evidence
represented
bba
used
credal
classiﬁcation
object
new
method
credal
classiﬁcation
incomplete
pattern
adaptive
imputation
missing
values
referred
credal
classiﬁcation
adaptive
imputation
ccai
conciseness
ccai
based
belief
function
theory
well
manage
uncertain
imprecise
information
caused
missing
values
classiﬁcation
first
step
direct
classiﬁcation
incomplete
pattern
using
available
data
let
consider
set
test
patterns
samples
classiﬁed
based
set
labeled
training
patterns
frame
discernment
work
focus
classiﬁcation
incomplete
pattern
attribute
values
absent
consider
test
patterns
e.g
several
missing
values
training
data
set
may
also
incomplete
patterns
applications
however
incomplete
patterns
take
small
amount
say
less
training
data
set
ignored
classiﬁcation
percentage
incomplete
patterns
big
missing
values
must
usually
estimated
ﬁrst
classiﬁer
trained
using
edited
complete
patterns
real
applications
one
also
choose
complete
labeled
patterns
include
training
data
set
training
information
sufﬁcient
simplicity
convenience
consider
labeled
samples
e.g
training
set
complete
patterns
sequel
ﬁrst
step
classiﬁcation
incomplete
pattern
say
respectively
classiﬁed
according
training
class
normal
classiﬁer
dealing
complete
pattern
ﬁrst
missing
values
ignored
work
adopt
simple
classiﬁcation
method2
convenience
computation
directly
classiﬁed
based
distance
prototype
class
prototype
class
corresponding
given
arithmetic
2many
normal
classiﬁers
e.g
k-nn
selected
depending
preference
user
propose
use
simple
classiﬁcation
method
low
computation
complexity
average
vector
training
patterns
class
mathematically
prototype
computed
cid:88
yj∈ωg
number
training
samples
class
c-class
problem
one
get
pieces
simple
classiﬁcation
result
according
class
training
data
result
represented
simple
bba
including
two
focal
elements
i.e
singleton
class
ignorant
class
characterize
full
ignorance
belief
belonging
class
computed
based
distance
corresponding
prototype
normalized
euclidean
distance
adopted
deal
anisotropic
class
missing
values
ignored
calculation
distance
mass
belief
assigned
ignorant
class
therefore
bba
construction
done
mog
cid:118
cid:117
cid:117
cid:116
cid:115
dig
δgj
e−ηdig
e−ηdig
mog
cid:19
cid:18
xij
ogj
cid:88
cid:88
δgj
j=1
yij
ogj
yi∈ωg
xij
value
j-th
dimension
yij
value
j-th
dimension
number
available
attribute
values
object
coefﬁcient
1/p
necessary
normalize
distance
value
test
sample
different
number
missing
values
δgj
average
distance
training
samples
class
prototype
j-th
dimension
number
training
samples
tuning
parameter
bigger
generally
yields
smaller
mass
belief
speciﬁc
class
usually
recommended
take
0.5
0.8
according
various
tests
0.7
considered
default
value
obviously
smaller
distance
measure
bigger
mass
belief
singleton
class
particular
structure
bba
indicates
conﬁrm
degree
object
associated
speciﬁc
class
according
training
data
mass
belief
reﬂects
level
belief
one
full
ignorance
committed
ignorant
class
similarly
one
calculates
independent
bba
mog
based
different
training
classes
bba
done
follows
holds
mo1st
combining
bba
examine
whether
speciﬁc
classiﬁcation
result
derived
object
considered
belong
likely
class
ω1st
obtains
biggest
mass
belief
bba
class
second
biggest
mass
belief
denoted
ω2nd
ω1st
argmaxg
mog
distinguishability
degree
object
associated
different
classes
deﬁned
mo2nd
momax
ω2nd
ωmax
let
chosen
small
positive
distinguishability
threshold
value
condition
satisﬁed
means
classes
involved
computation
clearly
distinguished
case
likely
obtain
speciﬁc
classiﬁcation
result
fusion
bba
condition
also
indicates
available
attribute
information
sufﬁcient
making
classiﬁcation
object
imputation
missing
values
necessary
condition
holds
bba
directly
combined
rule
obtain
ﬁnal
classiﬁcation
results
object
rule
usually
produces
speciﬁc
combination
result
acceptable
computation
burden
low
conﬂicting
case
case
meta-class
included
fusion
result
different
classes
considered
distinguishable
based
condition
distinguishability
moreover
mass
belief
full
ignorance
class
represents
noisy
data
outliers
proportionally
redistributed
singleton
classes
speciﬁc
results
one
knows
priori
noisy
data
involved
distinguishability
condition
satisﬁed
means
classes
ω1st
ω2nd
clearly
distinguished
object
respect
chosen
threshold
value
indicating
missing
attribute
values
play
almost
surely
crucial
role
classiﬁcation
case
missing
values
must
properly
imputed
recover
unavailable
attribute
information
entering
classiﬁcation
procedure
step
method
explained
next
subsection
second
step
classiﬁcation
incomplete
pattern
imputation
missing
values
multiple
estimation
missing
values
estimation
missing
attribute
values
exist
various
methods
particularly
k-nn
imputation
method
generally
provides
good
performance
however
main
drawback
knn
method
big
computational
burden
since
one
needs
calculate
distances
object
training
samples
inspired
propose
use
self
organized
map
som
technique
reduce
computational
complexity
som
applied
class
training
data
weighting
vectors
obtained
optimization
procedure
optimized
weighting
vectors
allow
characterize
well
topological
features
whole
class
used
represent
corresponding
data
class
number
weighting
vectors
usually
small
e.g
nearest
neighbors
test
pattern
associated
weighting
vectors
som
easily
found
low
computational
complexity3
selected
weighting
vector
class
denoted
σωg
class
selected
close
weighting
vectors
provide
different
contributions
weight
vector
deﬁned
based
distance
estimation
missing
values
weight
pωg
object
weighting
vector
σωg
−λd
pωg
cid:80
dωg
euclidean
distance
neighbor
oωg
ignoring
missing
values
average
distance
pair
weighting
vectors
produced
som
classes
number
classes
number
weighting
vectors
obtained
som
class
euclidean
distance
two
weighting
vectors
weighted
mean
value
ˆyωg
selected
weighting
vectors
class
training
class
used
imputation
missing
values
calculated
cid:80
cid:80
k=1
k=1
pωg
σωg
pωg
ˆyωg
missing
values
ﬁlled
values
ˆyωg
according
training
class
get
edited
pattern
xωg
dimensions
xωg
simply
classiﬁed
based
training
data
similarly
done
direct
classiﬁcation
incomplete
pattern
using
step
convenience4
training
som
using
labeled
patterns
becomes
time
consuming
number
labeled
patterns
big
fortunately
done
off-line
experiments
running
time
performance
shown
results
include
computational
time
spent
off-line
procedures
4of
course
sophisticated
classiﬁers
also
applied
according
selection
user
choice
classiﬁer
main
purpose
work
classiﬁcation
estimation
missing
values
also
respectively
done
based
training
classes
according
procedure
c-class
problem
training
classes
therefore
one
get
pieces
classiﬁcation
results
respect
one
object
ensemble
classiﬁer
credal
classiﬁcation
pieces
results
obtained
class
training
data
c-class
problem
considered
different
weights
since
estimations
missing
values
according
different
classes
different
reliabilities
weighting
factor
classiﬁcation
result
associated
class
deﬁned
sum
weights
selected
som
weighting
vectors
contributions
missing
values
imputation
given
ρωg
pωg
result
biggest
weighting
factor
ρωmax
considered
reliable
one
assumes
object
must
belong
one
labeled
classes
i.e
biggest
weighting
factor
normalized
one
relative
weighting
factors
deﬁned
k=1
ˆαωg
ρωg
ρωmax
condition5
ˆαωg
satisﬁed
corresponding
estimation
missing
values
classiﬁcation
result
reliable
likely
object
belong
class
implicitly
assumed
object
belong
one
class
reality
result
whose
relative
weighting
factor
small
w.r.t
still
considered
useful
less
harmful
ﬁnal
classiﬁcation
object
condition
ˆαwg
holds
relative
weighting
factor
set
cid:88
zero
precisely
take
αωg
0
ρωmax
ˆαωg
otherwise
estimation
weighting
discounting
factors
αωg
classically
discounted
mog

ˆmog
αωg
mog
αωg
ˆmog
αωg
mog
classiﬁcation
results
bba
discounted
bba
globally
combined
get
credal
classiﬁcation
result
αωg
threshold
section
iii-a
also
used
measure
distinguishability
degree
one
gets
ˆmog
fully
ignorant
vacuous
bba
plays
neutral
role
global
fusion
process
ﬁnal
classiﬁcation
object
although
done
best
estimate
missing
values
estimation
quite
imprecise
estimations
obtained
different
class
similar
weighting
factors
different
estimations
probably
lead
distinct
classiﬁcation
results
case
prefer
cautiously
keep
rather
ignore
uncertainty
maintain
uncertainty
classiﬁcation
result
uncertainty
well
reﬂected
conﬂict
classiﬁcation
results
represented
bba
rule
suitable
conﬂicting
beliefs
distributed
focal
elements
particular
combination
rule
inspired
rule
introduced
fuse
bba
according
current
context
new
rule
partial
conﬂicting
beliefs
prudently
transferred
proper
meta-class
reveal
imprecision
degree
classiﬁcation
caused
missing
values
new
rule
combination
deﬁned
cid:81

cid:81
cid:83
ˆmog
cid:54
ˆmoj
cid:81
cid:54
ˆmoj
ˆmok
test
pattern
classiﬁed
according
fusion
results
object
considered
belonging
class
singleton
class
meta-class
maximum
mass
belief
called
hard
credal
classiﬁcation
one
object
classiﬁed
particular
class
means
object
correctly
classiﬁed
proper
imputation
missing
values
one
object
committed
meta-class
e.g
means
know
object
belongs
one
speciﬁc
classes
e.g
included
meta-class
specify
one
case
happen
missing
values
essential
accurate
classiﬁcation
object
missing
values
estimated
well
according
context
different
estimations
induce
classiﬁcation
object
distinct
classes
e.g
convenience
fig
shows
functional
ﬂowchart
new
ccai
method
guideline
tuning
parameters
tuning
parameters
important
application
ccai
associated
calculation
mass
belief
speciﬁc
class
bigger
value
lead
smaller
mass
belief
committed
speciﬁc
class
based
various
tests
advise
take
0.5
0.8
value
0.7
taken
default
value
parameter
threshold
tune
changing
classiﬁcation
strategy
also
used
figure
flowchart
proposed
ccai
method
calculation
discounting
factor
bigger
make
fewer
objects
going
sophisticated
classiﬁcation
procedure
imputation
missing
values
also
forces
discounting
factors
zero
according
implies
fewer
simple
classiﬁcation
results
obtained
based
class
useful
global
fusion
step
bigger
makes
fewer
objects
committed
meta-classes
corresponding
low
imprecision
classiﬁcation
increases
risk
misclassiﬁcation
error
tuned
according
compromise
one
accept
misclassiﬁcation
error
imprecision
non
speciﬁcity
classiﬁcation
decision
one
also
apply
cross
validation
e.g
leave-one-out
method
training
data
space
ﬁnd
suitable
threshold
missing
values
test
samples
randomly
distributed
dimensions
experiments
three
experiments
artiﬁcial
real
data
sets
used
test
performance
new
ccai
method
compared
k-nn
imputation
knni
method
fcm
imputation
fcmi
method
som
imputation
somi
method
previous
credal
classiﬁcation
pcc
method
som
technique
also
employed
second
step
ccai
method
ccai
different
previous
somi
method
somi
method
som
applied
whole
training
data
set
missing
values
precisely
estimated
based
activation
group
composed
best
match
node
unit
input
pattern
close
neighbors
edited
pattern
imputation
missing
values
classiﬁed
using
standard
classiﬁer
nevertheless
som
involved
ﬁrst
step
ccai
object
directly
classiﬁed
ignoring
missing
values
second
step
ccai
som
respectively
applied
training
class
multiple
estimations
missing
values
obtained
based
input
pattern
nearest
weighting
vectors
corresponding
nodes
som
class
different
classiﬁcation
results
produced
according
different
estimations
results
globally
fused
ﬁnal
classiﬁcation
conﬂicting
information
committed
meta-class
kept
fusion
characterize
imprecision
classiﬁcation
ccai
done
somi
different
methods
programmed
tested
matlabtm
software
evidential
neural
network
classiﬁer
enn
adopted
sequel
experiments
classify
edited
pattern
estimated
values
pcc
knni
fcmi
since
enn
produce
generally
good
results
classiﬁcation6
evidential
k-nearest
neighbor
ek-nn
method
also
used
classify
edited
pattern
experiment
real
data
comparison
parameters
enn
ek-nn
automatically
optimized
explained
somi
use
m×n
6×8
nodes
mapping
whole
input
data
set
consisting
training
classes
2-dimensional
grid
good
performance
applications
pcc
tuning
parameter
tuned
according
imprecision
rate
one
accept
ccai
small
number
nodes
2-dimensional
grid
som
given
class
take
value
k-nn
imputation
missing
values
seems
provide
good
result
sequel
experiments
order
show
ability
ccai
pcc
deal
meta-classes
hard
credal
classiﬁcation
applied
class
object
decided
according
criterion
maximal
mass
belief
6other
traditional
classiﬁers
complete
pattern
also
selected
according
actual
application
simulations
misclassiﬁcation
declared
counted
one
object
truly
originated
classiﬁed
cid:54
cid:54
considered
imprecise
classiﬁcation
error
rate
denoted
calculated
ne/t
number
misclassiﬁcation
errors
number
objects
test
imprecision
rate
denoted
rij
calculated
rij
ij/t
number
objects
committed
meta-classes
cardinality
value
experiments
classiﬁcation
object
generally
uncertain
imprecise
among
small
number
e.g
classes
take
ri2
since
object
committed
meta-class
including
three
speciﬁc
classes
experiment
artiﬁcial
data
set
ﬁrst
experiment
show
interest
credal
classiﬁcation
based
belief
functions
respect
traditional
classiﬁcation
working
probability
framework
3-class
data
set
obtained
three
2-d
uniform
distributions
shown
fig
considered
class
200
training
samples
200
test
samples
600
training
samples
600
test
samples
total
uniform
distributions
three
classes
characterized
following
interval
bounds
x-label
interval
y-label
interval
155
110
values
second
dimension
corresponding
y-coordinate
test
samples
missing
test
samples
classiﬁed
according
one
available
value
ﬁrst
dimension
corresponding
x-coordinate
several
different
methods
like
fcmi
knni
somi
applied
comparison
ccai
shown
fig
–3-
particularly
classiﬁcation
result
obtained
using
ﬁrst
second
single
step
ccai
denoted
sccai
also
given
fig
–3-
ﬁrst
step
ccai
direct
classiﬁcation
done
without
imputation
missing
value
whereas
object
classiﬁed
imputation
missing
values
incomplete
patterns
second
step
ccai
particular
value
selected
classiﬁer
k-nn
imputation
method7
notation
conciseness
denoted
ωte
cid:44
ωtest
ωtr
cid:44
ωtraining
...
cid:44
error
rate
imprecision
rate
computation
time
sec
speciﬁed
caption
subﬁgure
7in
fact
choice
ranking
affect
seriously
results
figure
training
data
test
data
value
test
sample
missing
class
appears
partially
overlapped
classes
margins
according
value
x-coordinate
shown
fig
missing
value
samples
overlapped
parts
ﬁlled
quite
different
estimations
obtained
different
classes
almost
reliabilities
example
estimation
missing
values
objects
right
margin
left
margin
obtained
according
training
class
edited
pattern
estimation
classiﬁed
class
whereas
committed
class
estimation
drawn
similar
test
samples
left
margin
right
margin
indicates
missing
value
play
crucial
rule
classiﬁcation
objects
unfortunately
estimation
involved
missing
values
020406080100120140160010203040506070
w1trw2trw3trw1tew2tew3te
classiﬁcation
result
fcmi
14.67
time
0.0469s
classiﬁcation
result
knni
14.17
time
7.9531s
classiﬁcation
result
somi
14.33
time
0.9063s
classiﬁcation
result
1st
step
sccai
14.83
time
0.0156s
classiﬁcation
result
2nd
step
sccai
4.83
ri2
19.33
time
0.1719s
classiﬁcation
result
ccai
5.83
ri2
16.83
time
0.0469s
figure
classiﬁcation
results
3-class
artiﬁcial
data
set
different
methods
quite
uncertain
according
context
objects
prudently
classiﬁed
proper
meta-class
e.g
ccai
ccai
results
indicate
objects
belong
one
speciﬁc
classes
included
meta-classes
speciﬁc
classes
clearly
distinguished
020406080100120140160010203040506070
w1w2w3020406080100120140160010203040506070
w1w2w3020406080100120140160010203040506070
w1w2w3020406080100120140160010203040506070
w1w2w3w1,2w1,3w2,3020406080100120140160010203040506070
w1w2w3w1,2w1,3w2,3020406080100120140160010203040506070
w1w2w3w1,2w1,3w2,3
object
based
available
values
one
wants
get
precise
accurate
classiﬁcation
results
one
needs
request
additional
resources
gathering
useful
information
objects
left
margin
right
margin
middle
correctly
classiﬁed
based
known
value
x-coordinate
necessary
estimate
missing
value
classiﬁcation
objects
ccai
however
test
samples
classiﬁed
speciﬁc
classes
traditional
methods
knni
fcmi
causes
many
errors
due
limitation
probability
framework
apply
ﬁrst
step
sccai
without
imputation
missing
value
directly
classify
objects
using
known
value
i.e
value
x-coordinate
produces
bigger
error
rate
methods
indicates
imputation
procedure
important
improve
accuracy
classiﬁcation
second
step
sccai
done
imputation
missing
values
incomplete
patterns
causes
high
imprecision
rate
efﬁcient
solution
takes
much
longer
computation
time
ccai
ccai
adaptive
imputation
strategy
well
balance
error
rate
imprecision
rate
computation
burden
ccai
consisting
two
steps
generally
produces
smaller
error
rate
knni
fcmi
somi
thanks
use
meta-classes
meanwhile
computational
time
ccai
similar
fcmi
much
shorter
knni
introduction
som
technique
estimation
missing
values
shows
computational
complexity
ccai
relatively
low
simple
example
shows
interest
potential
credal
classiﬁcation
obtained
ccai
method
experiment
artiﬁcial
data
set
second
experiment
evaluate
performance
ccai
method
using
data
set
includes
classes
artiﬁcial
data
generated
three
gaussian
distributions
characterized
following
means
vectors
covariance
matrices
denotes
4×4
identity
matrix
100
100
130
used
training
samples
test
samples
500
1000
class
total
training
samples
test
samples
test
sample
missing
values
missing
component
value
randomly
distributed
every
dimension
three
methods
knni
fcmi
somi
pcc
also
applied
performances
comparison
pair
reported
error
rates
imprecision
rates
running
time
sec
averages
trials
performed
independent
random
generation
data
sets
knni
values
ranging
neighbors
tested
mean
error
rate
given
table
pcc
method
parameter
optimized
obtain
acceptable
compromise
error
rate
imprecision
degree
enn
adopted
classify
edited
pattern
imputation
missing
values
fcmi
knni
somi
pcc
classification
results
3-class
data
set
different
methods
table
1500,1
1500,2
1500,3
3000,1
3000,2
3000,3
fcmi
time
6.73
0.9094s
14.38
0.9016s
36.84
0.9391s
6.75
1.3922s
14.73
1.5375s
36.43,1.6500s
knni
time
7.42
3.0005s
15.68
2.7759s
40.11
3.002s
7.54
12.0386s
15.80
11.3857s
40.48
10.2803s
somi
time
7.22
0.9814s
15.43
0.9546s
40.10
1.0322s
7.14
1.7310s
15.20
1.8203s
40.05
1.6094s
pcc
ri2
time
6.20
2.33
0.3484s
13.47
5.93
0.3141s
34.57
7.97
0.3484s
6.17
1.63
0.5453s
14.00
1.60
0.5234s
33.94
8.13
0.5484s
ccai
ri2
time
4.64
3.87
0.2500s
9.76
9.79
0.2344s
29.71
15.6
0.2906s
4.73
3.83
0.3469s
9.90
10.33
0.3063s
29.52
16.83
0.3937s
classiﬁcation
results
applied
methods
i.e
fcmi
knni
somi
pcc
ccai
shown
table
proposed
ccai
method
produces
lowest
error
rate
since
objects
hard
correctly
classify
missing
values
committed
proper
meta-class
meanwhile
ccai
takes
shortest
computation
time
compared
methods
incomplete
patterns
directly
classiﬁed
ignoring
missing
values
considered
unimportant
classiﬁcation
however
missing
values
pattern
imputed
methods
needs
computations
thus
increases
computational
time
moreover
one
see
knni
takes
longest
time
main
drawback
k-nn
based
method
k-nn
strategy
also
adopted
ccai
use
optimized
weighting
vectors
acquired
som
technique
represent
whole
training
data
class
thus
need
calculate
distances
object
obtained
weighting
vectors
rather
training
samples
reduces
lot
computation
burden
experiment
real
data
set
nine
well
known
real
data
sets8
available
uci
machine
learning
repository
used
experiment
evaluate
performance
ccai
respect
knni
fcmi
somi
pcc
enn
ek-nn
employed
standard
classiﬁer
classify
edited
patterns
moreover
8we
select
seven
classes
yeast
data
set
last
three
classes
i.e
vac
pox
erl
contain
quite
samples
single
1st
2nd
step
procedure
ccai
sccai
also
applied
comparison
ﬁrst
step
sccai
object
directly
classiﬁed
using
available
attributes
without
imputation
procedure
whereas
missing
values
imputed
classiﬁcation
second
step
sccai
basic
information
used
real
data
sets
given
table
hepatitis
data
set
many
patterns
already
contained
missing
values
patterns
missing
values
considered
test
samples
others
used
training
samples
missing
values
seven
original
data
sets
assumed
values
missing
completely
random
dimensions
test
sample
cross
validation
performed
seven
data
sets
use
simplest
2-fold
cross
validation9
since
advantage
training
test
sets
large
sample
used
training
testing
fold
2-fold
cross
validation
repeated
times
average
error
rate
imprecision
rate
pcc
ccai
different
methods
given
table
iii
particularly
reported
classiﬁcation
result
knni
average
value
ranging
15.
notation
conciseness
selected
classiﬁer
denoted
a=ek-nn
b=enn
table
iii
method
single
step
ccai
sccai
represents
ﬁrst
step
sccai
represents
second
step
sccai
basic
information
used
data
sets
table
name
breast
hepatitis
statlog
heart
iris
seeds
wine
knowledge
vehicle
yeast
classes
attributes
instances
699
155
270
150
210
178
403
946
1429
one
see
table
iii
credal
classiﬁcation
pcc
ccai
always
produce
lower
error
rate
traditional
fcmi
knni
somi
methods
since
objects
correctly
classiﬁed
using
available
attribute
values
properly
committed
meta-classes
well
reveal
imprecision
classiﬁcation
selected
classiﬁers
i.e
ek-nn
enn
classiﬁcation
edited
patterns
fcmi
knni
somi
pcc
usually
similar
performance
9more
precisely
samples
class
randomly
assigned
two
sets
equal
size
train
test
reciprocally
many
cases10
known
k-nn
based
method
generally
big
computation
burden
choice
ek-nn
enn
made
according
actual
condition
real
applications
ccai
objects
imputation
missing
values
still
classiﬁed
meta-class
indicates
missing
values
play
crucial
role
classiﬁcation
estimation
missing
values
good
words
missing
values
ﬁlled
similar
reliabilities
different
estimated
data
lead
distinct
classiﬁcation
results
cautiously
assign
meta-class
reduce
risk
misclassiﬁcation
compared
previous
method
pcc
new
method
ccai
generally
provide
better
performance
lower
error
rate
imprecision
rate
mainly
accurate
estimation
method
i.e
som+knn
missing
values
adopted
ccai
however
ﬁrst
step
sccai
applied
produces
misclassiﬁcation
errors
methods
due
absence
imputation
missing
data
whereas
imprecision
rate
quite
high
second
step
sccai
adopted
conﬂicting
beliefs
caused
combination
procedure
transferred
meta-classes
ccai
adaptive
imputation
missing
values
provide
good
compromise
error
imprecision
third
experiment
using
real
data
sets
shows
effectiveness
interest
new
ccai
method
respect
methods
conclusion
new
credal
classiﬁcation
method
adaptive
imputation
missing
values
called
ccai
dealing
incomplete
pattern
presented
based
belief
function
theory
ﬁrst
step
ccai
method
objects
incomplete
pattern
directly
classiﬁed
ignoring
missing
values
speciﬁc
classiﬁcation
result
obtained
effectively
reduces
computation
complexity
avoids
imputation
missing
values
however
available
information
sufﬁcient
achieve
speciﬁc
classiﬁcation
object
ﬁrst
step
estimate
recover
missing
values
entering
classiﬁcation
procedure
second
step
som
k-nn
techniques
applied
make
estimation
missing
attributes
good
compromise
estimation
accuracy
computation
burden
credal
classiﬁcation
work
allows
object
belong
different
singleton
classes
meta-class
i.e
disjunction
several
classes
different
masses
belief
object
committed
meta-class
e.g
means
missing
values
accurately
recovered
according
context
estimation
good
different
estimations
lead
object
distinct
classes
e.g
involved
meta-class
sources
information
required
achieve
precise
classiﬁcation
object
necessary
credal
classiﬁcation
10ek-nn
outperforms
enn
sometimes
enn
better
cases
able
well
capture
imprecision
classiﬁcation
thanks
meta-class
effectively
reduces
misclassiﬁcation
errors
effectiveness
interest
proposed
ccai
method
evaluated
three
distinct
experiments
using
artiﬁcial
real
data
sets
references
garcia-laencina
sancho-gomez
figueiras-vidal
pattern
classiﬁcation
missing
data
review
neural
comput
appl
vol
263–282
2010
r.j.
little
d.b
rubin
statistical
analysis
missing
data
john
wiley
sons
new
york
1987
second
edition
published
2002
r.o
duda
p.e
hart
d.g
stork
pattern
classiﬁcation
2nd
edition
wiley-interscience
2000
c.m
bishop
pattern
recognition
machine
learning
springer
2007
ghahramani
m.i
jordan
supervised
learning
incomplete
data
via
approach
cowan
eds
adv
neural
inf
process.
morgan
kaufmann
publishers
inc.
vol
120–127
1994
p.k
sharpe
r.j.
solly
dealing
missing
values
neural
network-based
diagnostic
systems
neural
comput
appl
vol
:73–77
1995
j.r.
quinlan
induction
decision
trees
machine
learning
vol.1
:81–106,1986
r.j.
hathaway
j.c.
bezdek
fuzzy
c-means
clustering
incomplete
data
ieee
trans
syst
man
cybern
cybern
vol.31
735–744
2001
pelckmans
j.d
brabanter
j.a.k
suykens
b.d
moor
handling
missing
values
support
vector
machine
classiﬁers
neural
networks
vol
5–6
684–692
2005
farhangfar
kurgan
impact
imputation
missing
values
classiﬁcation
error
discrete
data
pattern
recognition
vol
3692–3705
2008
d.j
mundfrom
whitcomb
imputing
missing
values
effect
accuracy
classiﬁcation
multiple
linear
regression
viewpoints
vol
no.1
13–19
1998
batista
m.c
monard
study
k-nearest
neighbour
imputation
method
proc
second
international
conference
hybrid
intelligent
systems
ios
press
251–260
2002
luengo
j.a
saez
f.herrera
missing
data
imputation
fuzzy
rule-based
classiﬁcation
systems
soft
computing
vol
863–881
2012
j.deogun
spaulding
shuart
towards
missing
data
imputation
study
fuzzy
k-means
clustering
method
4th
international
conference
rough
sets
current
trends
computing
rsctc04
573–579
2004
fessant
midenet
self-organizing
map
data
imputation
correction
surveys
neural
comput
appl.
vol
300–310
2002
shafer
mathematical
theory
evidence
princeton
univ
press
1976
smarandache
dezert
editors
advances
applications
dsmt
information
fusion
american
research
press
rehoboth
vol
1-4
2004-2015.
http
//fs.gallup.unm.edu/dsmt.htm
smets
combination
evidence
transferable
belief
model
ieee
trans
pattern
anal
mach
intell.
vol
447–458,1990
a.-l.
jousselme
liu
grenier
boss´e
measuring
ambiguity
evidence
theory
ieee
trans
systems
man
cybernetics
part
890–903
sept.
2006
laanaya
martin
aboutajdine
khenchaf
support
vector
regression
membership
functions
belief
functions
application
pattern
recognition
information
fusion
vol
338–350
2010
denœux
k-nearest
neighbor
classiﬁcation
rule
based
dempster-shafer
theory
ieee
trans
systems
man
cybernetics
vol
804–813
1995
l.m
zouhal
denœux
evidence-theoretic
k-nn
rule
parameter
optimization
ieee
trans
systems
man
cybernetics
part
vol
263–271
1998
z.-g.
liu
pan
dezert
new
belief-based
k-nearest
neighbor
classiﬁcation
method
pattern
recognition
vol
834–844
2013
z.-g.
liu
pan
dezert
mercier
credal
classiﬁcation
rule
uncertain
data
based
belief
functions
pattern
recognition
vol
2532–2541
2014
z.-g.
liu
pan
mercier
dezert
new
incomplete
pattern
classiﬁcation
method
based
evidential
reasoning
ieee
trans
cybernetics
vol.45
no.4
635–646
2015
denœux
smets
classiﬁcation
using
belief
functions
relationship
case-based
model-based
approaches
ieee
trans
systems
man
cybernetics
part
vol
1395–1406
2006
denœux
neural
network
classiﬁer
based
dempster-shafer
theory
ieee
trans
systems
man
cybernetics
vol
131–150
2000
deng
felix
chan
mahadevan
deng
parameter
estimation
based
interval-valued
belief
structures
european
journal
operational
research
vol.241
no.2
pp.579–582
2015
m.-h.
masson
denœux
ecm
evidential
version
fuzzy
c-means
algorithm
pattern
recognition
vol
1384–1397
2008
z.-g.
liu
dezert
mercier
pan
belief
c-means
extension
fuzzy
c-means
algorithm
belief
functions
framework
pattern
recognition
letters
vol
291–300
2012
z.-g.
liu
pan
dezert
mercier
credal
c-means
clustering
method
based
belief
functions
knowledge-based
systems
vol
119-132
2015
zhou
martin
pan
z.-g.
liu
median
evidential
c-means
algorithm
application
community
detection
knowledge-
based
systems
vol
69–88
2015
denœux
maximum
likelihood
estimation
uncertain
data
belief
function
framework
ieee
transactions
knowledge
data
engineering
vol
pp.119–130
2013
z.-g.
liu
dezert
pan
mercier
combination
sources
evidence
different
discounting
factors
based
new
dissimilarity
measure
decision
support
systems
vol
133–141
2011
huang
mahadevan
deng
new
decision-making
method
incomplete
preferences
based
evidence
distance
knowledge-based
systems
vol.56
264–272,2014
dezert
smarandache
huang
evidence
supporting
measure
similarity
reducing
complexity
information
fusion
information
science
vol.181
no.10
1818–1835
2011
d.q
han
deng
c.z
han
sequential
weighted
combination
unreliable
evidence
based
evidence
variance
decision
support
systems
vol.56
387–393
2013
kohonen
self-organizing
map
proceedings
ieee
vol.78
no.9
1464–1480
1990
dubois
prade
representation
combination
uncertainty
belief
functions
possibility
measures
computational
intelligence
vol
244–264
1988
l.a.
zadeh
validity
dempster
rule
combination
memo
m79/24
univ
california
berkeley
usa
1979
dezert
tchamova
validity
dempster
fusion
rule
interpretation
generalization
bayesian
fusion
rule
international
journal
intelligent
systems
vol
223–252
march
2014
smarandache
dezert
information
fusion
based
new
proportional
conﬂict
redistribution
rules
proceedings
fusion
2005
int
conf
information
fusion
philadelphia
usa
july
25-29
2005
hammami
dezert
mercier
hamouda
estimation
mass
functions
using
self
organizing
maps
proc
belief
2014
conf
oxford
sept.
26–29
2014
geisser
predictive
inference
introduction
new
york
chapman
hall
1993
lichman
uci
machine
learning
repository
http
//archive.ics.uci.edu/ml
irvine
university
california
school
information
computer
science
2013
classification
results
different
real
data
sets
rates
table
iii
data
set
fcmi
knni
somi
hepatitis
breast
iris
seeds
wine
knowledge
heart
vehicle
yeast
26.40
25.33
3.96
3.81
6.18
7.32
12.02
11.42
6.89
7.33
13.89
14.00
18.22
17.33
15.56
15.24
18.17
17.14
21.75
20.95
29.32
26.97
34.68
33.24
34.76
33.43
30.07
34.50
33.06
39.68
34.32
39.96
37.41
41.18
48.15
46.89
46.00
56.66
57.97
61.82
46.57
44.97
54.29
51.72
27.38
26.67
4.83
3.95
9.07
8.20
14.00
11.54
5.29
4.89
13.02
11.33
18.67
18.44
11.59
11.19
12.70
11.98
26.41
25.71
27.12
26.97
26.22
30.43
29.55
30.90
28.53
33.51
29.66
39.43
32.96
40.69
37.78
41.85
38.27
43.09
41.13
55.67
45.27
57.92
46.04
44.72
54.22
52.81
27.47
25.33
3.85
3.51
6.47
5.93
13.62
12.45
5.14
5.00
13.24
12.67
18.00
17.34
11.63
10.20
12.86
12.59
25.65
24.63
27.53
28.65
31.30
31.46
34.35
32.58
29.78
33.88
31.51
41.69
35.24
42.04
36.67
41.11
41.48
42.96
41.25
54.73
45.68
57.71
45.51
44.86
54.88
53.89
pcc
ri2
22.22
7.56
20.00
6.67
4.39
2.20
3.81
2.34
5.82
1.93
5.42
1.32
10.11
2.86
10.10
2.64
4.80
2.04
5.33
2.67
8.31
6.27
8.67
4.00
13.33
8.67
12.67
9.33
10.51
2.95
9.52
4.76
10.22
3.52
10.48
4.29
17.84
10.32
16.19
14.76
27.38
0.71
26.97
1.69
27.12
0.79
29.78
2.25
29.06
1.61
30.34
2.81
26.72
4.05
28.35
6.31
27.32
5.36
33.32
7.73
29.86
9.97
33.76
11.82
33.41
12.59
36.30
9.63
35.06
25.93
32.96
28.52
35.63
25.75
37.87
27.43
38.63
22.73
43.63
26.95
42.71
11.12
39.86
13.92
51.86
10.87
49.38
13.69
sccai
ri2
23.67
20.00
8.00
4.98
3.22
0.73
6.15
4.72
2.93
12.15
7.03
17.11
6.67
4.00
3.33
12.00
7.33
8.00
17.33
10.67
16.00
9.52
9.52
0.95
10.48
9.52
1.90
22.86
8.10
28.57
6.97
6.18
8.43
7.87
5.62
9.55
14.61
10.67
40.45
27.55
20.10
8.19
30.69
20.35
13.40
34.16
22.08
21.59
17.78
13.70
21.48
23.70
22.59
8.89
50.71
27.66
50.24
52.25
28.61
56.97
46.67
27.08
46.74
56.74
34.38
49.31
ccai
ri2
21.33
5.33
3.66
4.83
1.61
9.00
0.66
4.00
1.33
8.00
4.67
11.33
12.00
9.52
10.00
0.48
16.19
13.81
6.74
1.12
7.30
3.93
12.36
3.93
20.85
6.20
23.57
6.95
30.51
7.69
16.30
0.37
22.96
0.74
34.87
26.48
36.64
22.34
40.28
12.36
49.75
12.64
