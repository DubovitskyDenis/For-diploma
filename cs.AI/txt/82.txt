filter
based
taxonomy
modiﬁcation
improving
hierarchical
classiﬁcation
azad
naik∗
huzefa
rangwala†
abstract
hierarchical
classiﬁcation
supervised
learning
problem
unlabeled
instances
classiﬁed
taxonomy
classes
several
methods
utilize
hierarchical
structure
developed
improve
performance
however
cases
apriori
deﬁned
hierarchical
structure
domain
experts
inconsistent
consequence
performance
improvement
noticeable
comparison
ﬂat
classiﬁcation
methods
propose
scalable
data-driven
ﬁlter
based
rewiring
approach
modify
expert-
deﬁned
hierarchy
experimental
comparisons
top-down
modiﬁed
hierarchy
wide
range
datasets
shows
classiﬁcation
performance
improvement
baseline
hierarchy
i.e.
deﬁned
expert
clustered
hierarchy
ﬂattening
based
hierarchy
modiﬁcation
approaches
comparison
existing
rewiring
approaches
developed
method
rewhier
computationally
eﬃcient
enabling
scale
datasets
large
numbers
classes
instances
features
also
show
modiﬁed
hierarchy
leads
improved
classiﬁcation
performance
classes
training
samples
comparison
ﬂat
state-of-the-art
approaches
source
code
available
reproducibility
www.cs.gmu.edu/∼mlbio/taxmod
keywords—
top-down
hierarchical
classiﬁcation
rewiring
clustering
flattening
introduction
taxonomy
hierarchy
commonly
used
organize
large
volumes
data
successfully
used
diﬀerent
application
domains
bioinformatics1
computer
vision2
web
directories3
application
domains
especially
highlighed
interest
online
prediction
challenges
lshtc4
bioasq5
introduce
unique
computational
statistical
challenges
given
datasets
several
thousand
classes
developed
methods
need
scale
learning
prediction
phases
majority
classes
training
examples
leading
class
imbalance
problem
learned
models
rare
categories
tendency
overﬁt
mispredictions
favor
larger
classes
hierarchies
provide
useful
structural
relationships
parent-child
siblings
among
diﬀerent
classes
exploited
learning
generalized
classiﬁcation
models
past
researchers
demonstrated
usefulness
hierarchies
classiﬁcation
obtained
promising
results
1–5
utilizing
hierarchical
structure
shown
improve
classiﬁcation
performance
rare
categories
well
top-down
methods
leverage
hierarchy
learning
prediction
process
eﬀective
approaches
deal
large-scale
problems
classiﬁcation
decision
top-down
methods
involves
invoking
models
relevant
path
within
hierarchy
though
computationally
eﬃcient
methods
higher
number
misclassiﬁcations
due
error
propagation
several
benchmarks
approaches
outperformed
ﬂat
classiﬁers
ignore
hierarchy
9,10
majority
cases
hierarchy
available
training
classiﬁers
manually
designed
experts
based
domain
knowledge
consistent
classiﬁcation
order
improve
performance
need
restructure
hierarchy
make
favorable
useful
classiﬁcation
motivated
idea
main
focus
paper
generating
improved
representation
expert-deﬁned
hierarchy
summarize
contributions
follows
∗department
computer
science
george
mason
university
email
anaik3
gmu.edu
rangwala
cs.gmu.edu
1http
//geneontology.org/
2http
//www.image-net.org/
3http
//dir.yahoo.com/
4http
//lshtc.iit.demokritos.gr/
5http
//bioasq.org/
expert-deﬁned
original
hierarchy
clustered
hierarchy
flattened
hierarchy
rewired
hierarchy
figure
expert-deﬁned
hierarchy
classes
high
degree
similarities
marked
symbols
cid:70
cid:32
modiﬁed
using
various
methods
agglomerative
clustering
cluster
cohesion
restrict
height
original
height
global-inf
ﬂattening
method
proposed
rewiring
method
modiﬁed
structure
changes
shown
green
color
propose
eﬃcient
data-driven
ﬁlter
based
rewiring
approach
hierarchy
modiﬁcation
unlike
previous
wrapper
based
approaches
11,12
require
multiple
expensive
computations
approach
scalable
applied
problems
high-dimensional
features
large
number
classes
examples
modiﬁcation
method
margin-based
modiﬁcation
level
ﬂattening
inconsistent
node
ﬂattening
learning
based
algorithm
agglomerative
clustering
divisive
clustering
optimal
hierarchy
search
genetic
based
algorithm
proposed
approach
similarity
based
modiﬁcation
type
filter
filter
filter
approach
flattening
flattening
flattening
flattening
wrapper
clustering
wrapper
clustering
wrapper
rewiring
wrapper
rewiring
wrapper
rewiring
filter
scalable
cid:88
cid:88
cid:88
cid:88
table
summary
review
existing
taxonomy
modiﬁcation
methods
characteristics
comparison
hierarchy
modiﬁcation
approaches
clustering
ﬂattening
perform
extensive
empirical
evaluations
case
studies
show
strengths
approach
modiﬁed
hierarchy
used
hierarchical
classiﬁcation
approaches
like
top-down
state-
of-the-art
approaches
incorporating
hierarchical
relationships
modiﬁed
hierarchy
conjunction
scalable
top-down
approach
outperforms
ﬂat
classiﬁers
∼65
rare
categories
i.e.
classes
less
training
examples
across
dmoz
datasets
see
section
4.5
methods
2.1
motivation
manual
process
hierarchy
creation
suﬀers
various
issues
speciﬁcally
hierarchies
generated
grouping
semantically
similar
categories
common
parent
category
however
many
diﬀerent
semantically
sound
hierarchies
may
exist
set
classes
example
categorizing
products
experts
may
generate
hierarchy
ﬁrst
separating
products
based
company
name
e.g.
apple
microsoft
product
type
e.g.
phone
tablet
vice-versa
hierarchies
equally
good
perspective
expert
however
diﬀerent
hierarchies
may
lead
diﬀerent
classiﬁcation
results
apriori
clear
domain
experts
generate
new
nodes
hierarchy
expansion
merge
two
nodes
link
creation
creating
hierarchies
resulting
certain
degree
arbitrariness
iii
large
number
categories
pose
challenge
manual
design
consistent
hierarchy
dynamic
changes
may
require
hierarchical
restructuring
remove
inconsistencies
various
approaches
hierarchy
modiﬁcation
proposed
approaches
broadly
categorized
two
categories
flattening
approaches
14–16
identiﬁed
inconsistent
nodes
based
error
rate
classiﬁcation
margins
ﬂattened
removed
rewiring
approaches
11,12,19
parent-child
relationships
within
hierarchy
modiﬁed
improve
classiﬁcation
performance
clustering
based
methods
also
adapted
studies
8,17
consistent
hierarchy
generated
scratch
using
agglomerative
divisive
clustering
algorithms
summary
various
existing
methods
characteristics
shown
table
understand
qualitative
diﬀerence
hierarchy
generated
using
various
approaches
performed
experiments
smaller
newsgroup6
dataset
containing
classes
figure
shows
hierarchy
structure
obtained
using
clustering
ﬂattening
rewiring
based
approaches
respectively
hierarchy
generated
using
clustering
completely
ignores
expert-deﬁned
hierarchy
information
contains
valuable
prior
knowledge
classiﬁcation
flattening
approaches
group
together
classes
diﬀerent
hierarchical
branches
e.g
soc.religion.christian
religion.misc
contrary
rewiring
approaches
provide
ﬂexibility
grouping
classes
diﬀerent
sub-branches
details
figure
discussed
later
case
study
section
4.1
2.2
proposed
rewiring
approach
wrapper
based
approaches
modify
hierarchy
making
one
changes
evaluated
classiﬁcation
performance
improvement
using
learning
6http
//qwone.com/sim
jason/20newsgroups/
symbol
ˆyi
ˆyn
description
prediction
otherwise
expert-deﬁned
original
hierarchy
set
leaf
categories
classes
input
vector
i-th
training
example
true
label
i-th
training
example
predicted
label
i-th
test
example
binary
label
used
i-th
training
example
learn
weight
vector
n-th
node
hierarchy
otherwise
predicted
label
i-th
test
example
n-th
node
hierarchy
ˆyn
total
number
training
examples
weight
vector
model
n-th
node
optimal
objective
function
value
n-th
node
obtained
using
validation
dataset
dropped
subscript
places
ease
description
misclassiﬁcation
penalty
parameter
modiﬁed
hierarchy
obtained
rewiring
set
nodes
hierarchy
except
root
threshold
grouping
similar
classes
rewiring
method
parent
n-th
node
children
n-th
node
siblings
n-th
node
table
notation
algorithm
modiﬁed
changes
retained
performance
results
improve
otherwise
changes
discarded
process
repeated
repeated
procedure
hierarchy
modiﬁcation
continues
optimal
hierarchy
satisﬁes
certain
criteria
reached
wrapper
approaches
scalable
large
datasets
propose
eﬃcient
data-driven
ﬁlter
based
rewiring
approach
hierarchy
modiﬁed
based
certain
relevance
criterion
pairwise
sibling
similarity
diﬀerent
classes
within
hierarchy
approach
single
step
require
experimental
evaluation
multiple
iterations
refer
proposed
rewiring
approach
rewhier
table
captures
common
notations
used
paper
algorithm
illustrates
approach
hierarchy
modiﬁcation
speciﬁcally
consists
two
steps
grouping
similar
classes
pairs
ensure
classes
high
degree
similarity
grouped
together
parent
node
modiﬁed
taxonomy
step
identiﬁes
similar
classes
pairs
exist
within
expert-deﬁned
hierarchy
pairwise
cosine
similarity
used
similarity
measure
experiments
less
prone
curse
dimensionality
similarity
scores
computed
determine
set
similar
pairs
classes
using
empirically
deﬁned
cut-oﬀ
threshold
dataset
detailed
analysis
regarding
selection
discussed
section
4.4
example
figure
step
group
together
class
pairs
high
similarity
scores
cid:2
religion.misc
soc.religion.christian
electronics
windows.x
electronics
graphics
···
cid:3
pairwise
similarity
computation
diﬀerent
classes
one
major
bottlenecks
step
make
scalable
distribute
similarity
computation
across
multiple
compute
nodes
inconsistency
identiﬁcation
correction
obtain
consistent
hierarchy
group
together
similar
class
pairs
common
parent
node
iteratively
starting
similar
class
pairs
check
potential
inconsistencies
i.e.
pairs
classes
diﬀerent
branches
sub-trees
order
resolve
identiﬁed
inconsistencies
take
corrective
measures
using
three
basic
elementary
operations
node
creation
parent-child
rewiring
iii
node
deletion
figure
illustrates
various
hierarchical
structures
obtained
execution
elementary
operations
expert-deﬁned
hierarchy
figure
node
creation
operation
groups
together
identiﬁed
similar
class
pairs
diﬀerent
branches
algorithm
rewhier
algorithm
data
original
hierarchy
input-output
result
modiﬁed
hierarchy
initialization
ist
step
grouping
similar
classes
pair
compute
cosine
similarity
possible
class
pairs
similar
class
grouping
identify
similar
class
pairs
similarity
scores
value
greater
empirically
deﬁned
threshold
parameter
let
|c|
denotes
number
pairs
represented
set
s|c|
i-th
pair
represented
using
iind
step
inconsistency
identiﬁcation
correction
|c|
rewire
check
rewiring
needed
rewire
check
rewiring
needed
inconsistent
pair
check
cid:54
check
similarity
siblings
foreach
cid:1
cid:0
rewire
break
end
end
foreach
cid:0
rewire
break
cid:1
end
end
rewire
rewire
perform
node
creation
nnew
create
new
node
nnew→lca
si→nnew
lca
denotes
lowest
common
ancestor
else
rewire
else
pcrewire
pcrewire
end
end
end
end
perform
node
deletion
return
sub-trees
hierarchy
using
new
node
parent
lowest
common
ancestors
similar
classes
expert-deﬁned
hierarchy
node
creation
parent-child
rewiring
node
deletion
figure
modiﬁed
hierarchical
structures
obtained
applying
elementary
operations
expert-deﬁned
hierarchy
leaf
nodes
marked
rectangle
structural
changes
shown
red
color
figure
illustrates
operation
similar
class
pairs
grouped
together
newly
created
node
operation
used
proper
subset
leaf
nodes
diﬀerent
branches
similar
i.e.
similar
leaf
nodes
branch
otherwise
parent-child
rewiring
operation
used
parent-child
rewiring
pcrewire
shown
figure
operation
simply
assigns
rewires
leaf
node
one
parent
another
parent
node
hierarchy
useful
leaf
node
identiﬁed
similar
sibling
leaf
nodes
within
given
hierarchy
branch
example
figure
computed
similarity
score
determines
leaf
node
similar
nodes
comparison
current
siblings
desirable
classiﬁcation
perspective
assign
node
child
rather
node
deletion
refers
deletion
nodes
hierarchy
deemed
useless
classiﬁcation
figure
node
deleted
leaf
nodes
classiﬁed
node
operation
used
post-processing
step
algorithm
reﬁne
hierarchy
rewhier
algorithm
determines
outer
loop
best
corrective
measures
node
creation
parent-
child
rewiring
need
taken
inconsistencies
addressed
rewhier
calls
node
deletion
procedure
ﬁnal
modiﬁcation
step
unnecessary
nodes
deleted
noted
new
modiﬁed
hierarchy
obtained
inconsistencies
removal
used
train
classiﬁer
state-of-the-art
classiﬁcation
approaches
embed
parent-child
relationships
hierarchy
either
within
regularization
term
referred
hr-lr
hr-svm
loss
term
referred
hiercost
intuition
behind
hierarchy
regularized
logistic
regression
hr-lr
approach
data-sparse
child
nodes
beneﬁt
training
data-rich
parent
nodes
shown
achieve
best
performance
standard
benchmarks
however
training
models
computationally
expensive
due
coupling
diﬀerent
classes
within
formulation
make
method
scalable
distributed
computation
using
hadoop
map-reduce
proposed
conjunction
parallel
training
odd
even
levels
method
requires
special
hardware
software
conﬁgurations
large
datasets
hence
use
method
experiments
case
hiercost
cost-sensitive
learning
approach
adapted
method
intuitively
captures
hierarchical
information
treating
misclassiﬁcations
diﬀerently
based
commonalities
ancestors
true
predicted
labels
intrinsically
method
scales
large
datasets
due
trivial
decomposition
learned
models
diﬀerent
leaf
categories
method
outperforms
hr-lr
method
without
additional
parameter
conﬁgurations
hence
paper
use
hiercost
approach
evaluation
rewired
hierarchies
2.3
top-down
hierarchical
classiﬁcation
propose
use
top-down
approach
modiﬁed
hierarchies
scales
well
training
prediction
speciﬁcally
train
binary
one-
vs-rest
classiﬁers
nodes
discriminate
positive
examples
examples
nodes
i.e.
negative
examples
hierarchy
paper
use
logistic
regression
underlying
base
model
training
objective
uses
logistic
loss
minimize
empirical
risk
squared
l2-norm
term
denoted
||2
control
model
complexity
prevent
overﬁtting
objective
function
training
model
corresponding
node
provided
2.1
2.1
min
log
exp
cid:34
cid:88
i=1
cid:16
cid:16
−yn
cid:17
cid:17
cid:35
cid:107
cid:107
dataset
total
leaf
nodes
nodes
clef
diatoms
ipc
dmoz-small
dmoz-2010
dmoz-2012
399
553
2388
17222
13963
311
451
1139
12294
11947
levels
train
test
features
10000
1940
46324
6323
128710
383408
1006
993
28926
1858
34880
103435
371
1123497
51033
381580
348548
table
dataset
statistics
node
hierarchy
solve
2.1
obtain
optimal
weight
vector
denoted
complete
set
parameters
nodes
n∈n
constitutes
learned
model
top-down
classiﬁer
given
feature
vector
weight
vector
given
models
conditional
probability
ˆyn
2.2
decision
function
2.3
cid:40
cid:1
cid:14
cid:0
exp
cid:0
−yn
cid:0
ˆyn
cid:1
cid:1
cid:41
2.2
2.3
ˆyn
otherwise
test
example
feature
vector
top-down
classiﬁer
predicts
class
label
ˆyi
shown
2.4
essentially
algorithm
starts
root
recursively
selects
best
child
node
reaches
terminal
node
predicted
label
2.4
ˆyi

initialize
root
argmaxq∈c
return

experimental
protocol
3.1
datasets
used
extensive
set
datasets
evaluating
performance
proposed
rewiring
approach
various
statistics
datasets
used
listed
table
clef
diatoms
image
datasets
rest
text
datasets
ipc7
collection
patent
documents
dmoz
datasets
archive
web-pages
available
lshtc8
challenge
website
evaluating
dmoz-2010
dmoz-2012
datasets
use
provided
test
split
results
reported
two
benchmarks
blind
prediction
i.e.
know
ground
truth
labels
test
set
obtained
web-portal
interface9,10
text
datasets
apply
tf-idf
transformation
l2-norm
normalization
word-frequency
feature
vector
3.2
evaluation
metrics
flat
measures
micro-f1
µf1
macro-f1
mf1
used
evaluating
performance
compute
µf1
sum
category
speciﬁc
true
positives
false
positives
false
negatives
diﬀerent
classes
compute
score
cid:80
cid:80
cid:80
cid:80
c∈l
c∈l
c∈l
c∈l
µf1
7http
//www.wipo.int/classiﬁcations/ipc/en/
8http
//lshtc.iit.demokritos.gr/
9http
//lshtc.iit.demokritos.gr/node/81
http
//lshtc.iit.demokritos.gr/lshtc3
oracleupload
unlike
µf1
gives
equal
weight
instance
mf1
gives
equal
weight
classes
score
skewed
favor
larger
classes
computed
|l|
cid:88
c∈l
2pcrc
hierarchical
measures
hierarchy
used
evaluating
classiﬁer
performance
hierarchy
based
measure
include
hierarchical
hf1
deﬁned
cid:80
cid:80
i=1
ˆyi
i=1
ˆyi
hf1
cid:80
cid:80
i=1
ˆyi
i=1
ˆyi
sets
ancestors
predicted
true
labels
include
label
include
root
node
respectively
note
consistent
evaluation
used
original
hierarchy
methods
unless
noted
3.3
methods
comparison
3.3.1
hierarchical
methods
based
hierarchy
used
training
process
use
following
methods
comparison
top-down
logistic
regression
td-lr
expert-deﬁned
hierarchy
provided
domain
experts
used
training
classiﬁers
clustering
approach
hierarchy
generated
using
agglomerative
clustering
used
evaluation
restricted
height
clustered
hierarchy
original
height
ﬂattening
using
cluster
cohesion
global
inconsistent
node
flattening
global-inf
hierarchy
modiﬁed
ﬂattening
removing
2.1
inconsistent
nodes
based
optimal
optimization
objective
value
obtained
node
empirically
deﬁned
global
cut-oﬀ
threshold
optimal
hierarchy
search
optimal
hierarchy
identiﬁed
hierarchical
space
gradually
modifying
expert-deﬁned
hierarchy
using
elementary
operations
promote
demote
merge
reducing
number
operations
hence
hierarchy
evaluations
restricted
modiﬁcation
hierarchy
branches
encountered
maximum
classiﬁcation
errors
modiﬁed
approach
referred
t-easy
original
paper
largest
evaluated
dataset
244
classes
15795
instances
3.3.2
flat
method
hierarchy
ignored
binary
one-versus-rest
l2-regularized
classiﬁers
trained
leaf
categories
prediction
decision
unlabeled
test
instances
based
maximum
prediction
score
achieved
across
several
leaf
categories
classiﬁers
3.3.3
state-of-the-art
cost-sensitive
learning
similar
ﬂat
method
cost
value
associated
instance
loss
function
shown
3.5
approach
referred
hiercost
evaluations
used
best
cost
function
exponential
tree
distance
extrd
proposed
paper
3.5
min
log
exp
cid:34
cid:88
i=1
cid:16
cid:16
−yn
cid:17
cid:17
cid:35
cid:107
cid:107
cost
value
assigned
example
3.4
experimental
settings
make
experimental
results
comparable
previously
published
results
use
train-test
split
provided
public
benchmarks
experiments
divide
training
dataset
train
small
validation
dataset
ratio
90:10.
ﬁnal
reported
testing
performance
metric
µf1
td-lr
figure
77.04
0.18
77.94
0.04
clustering
flattening
agglomerative
global-inf
figure
79.42
0.12
79.82
0.07
figure
78.00
0.09
78.20
0.01
proposed
rewhier
figure
81.24
0.08
81.94
0.04
table
µf1
performance
comparison
using
diﬀerent
hierarchy
modiﬁcation
approaches
newsgroup
dataset
table
shows
mean
standard
deviation
bracket
across
ﬁve
runs
dataset
clef
diatoms
ipc
dmoz-small
dmoz-2010
dmoz-2012
evaluation
metrics
µf1
µf1
µf1
µf1
µf1
µf1
td-lr
72.74
35.92
53.27
44.46
49.32
42.51
45.10
30.65
40.22
28.37
50.13
29.89
agglomerative
clustering
global-inf
t-easy
flattening
rewiring
methods
73.24
38.27
56.08
44.78
49.83
44.50
45.94
30.75
77.14
46.54
61.31
51.85
52.30
45.65
46.61
31.86
42.37
30.41
50.64
30.58
78.12
48.83
cid:78
62.34
cid:78
53.81
cid:78
53.94
cid:77
46.10
cid:77
rewhier
78.00
47.10
cid:78
62.05
cid:78
52.14
cid:78
54.28
cid:77
46.04
cid:77
48.25
cid:77
32.92
cid:78
43.10
31.21
51.82
31.24
table
µf1
performance
comparison
using
diﬀerent
hierarchy
modiﬁcation
approaches
cid:78
cid:77
indicates
improvements
statistically
signiﬁcant
0.01
0.05
signiﬁcance
level
used
sign-test
non-
parameteric
wilcoxon
rank
test
statistical
evaluation
µf1
scores
respectively
test
performed
rewiring
methods
best
baseline
global-inf
statistical
tests
performed
dmoz-2010
dmoz-2012
datasets
access
true
labels
online
evaluation
system
denotes
scalable
misclassiﬁcation
penalty
parameter
set
cid:2
0.001
0.01
0.1
100
1000
cid:3
best
parameter
done
independent
held-out
dataset
provided
benchmarks
model
trained
choosing
selected
using
validation
set
used
retrain
models
entire
training
set
proposed
rewiring
approach
compute
pairwise
similarities
classes
using
entire
training
dataset
additionally
use
liblinear
solver11
optimization
experiments
source
code
made
available
website
http
//cs.gmu.edu/∼mlbio/taxmod
discussion
results
4.1
case
study
understand
quality
diﬀerent
hierarchical
structures
expert-deﬁned
clustered
ﬂattened
rewired
newsgroup
dataset
shown
figure
perform
top-down
using
hierarchy
separately
dataset
11269
training
instances
7505
test
instances
classes
evaluate
hierarchy
randomly
selecting
ﬁve
diﬀerent
sets
training
test
split
ratio
original
dataset
results
classiﬁcation
performance
shown
table
see
using
modiﬁed
hierarchies
substantially
improves
classiﬁcation
performance
comparison
baseline
expert-deﬁned
hierarchy
comparing
clustered
ﬂattened
proposed
rewired
hierarchies
classiﬁcation
performance
obtained
using
rewired
hierarchy
found
signiﬁcantly
better
ﬂattened
clustered
hierarchy
rewired
hierarchy
resolve
inconsistencies
grouping
together
classes
diﬀerent
hierarchical
branches
4.2
evaluating
rewiring
approaches
4.2.1
performance
based
flat
metrics
table
shows
µf1
performance
comparison
rewiring
approaches
expert-deﬁned
clustered
ﬂattened
hierarchy
baselines
rewiring
approaches
consistently
outperform
baselines
datasets
across
metrics
image
datasets
relative
performance
improvement
larger
performance
improvement
∼11
using
mf1
scores
comparison
11http
//www.csie.ntu.edu.tw/∼cjlin/liblinear/
table
hf1
performance
comparison
expert-deﬁned
new
modiﬁed
hierarchy
dmoz-2010
dataset
hf1
score
available
online
evaluation
system
dmoz-2012
dataset
modiﬁed
hierarchy
supported
flattening
global-inf
t-easy
rewiring
methods
dataset
clef
diatoms
ipc
dmoz-small
dmoz-2012
hierarchy
used
original
modiﬁed
original
modiﬁed
original
modiﬁed
original
modiﬁed
original
79.06
80.87
62.80
63.88
64.73
66.29
63.37
64.97
73.19
81.43
81.82
64.28
66.35
67.23
68.10
baseline
td-lr
global-inf
t-easy
flattening
rewiring
methods
dataset
clef
diatoms
ipc
dmoz-small
dmoz-2010
dmoz-2012
2.5
8.5
607
20190
50040
3.5
830
25600
63000
268
26432
table
total
training
time
mins
rewhier
80.14
81.28
63.24
64.27
68.34
68.36
66.18
66.30
74.21
rewhier
7.5
1284
168
42000
94800
executed
elementary
operation
dataset
hierarchy
modiﬁcation
clef
diatoms
ipc
t-easy
promote
demote
merge
proposed
rewhier
method
pcrewire
156
412
table
number
elementary
operation
executed
rewiring
approaches
baseline
td-lr
method
table
results
p-values
0.01
0.05
denoted
cid:78
cid:77
respectively
compute
sign-test
µf1
non-parametric
wilcoxon
rank
test
comparing
scores
obtained
per
class
rewiring
methods
best
baseline
i.e.
global-inf
rewiring
approaches
signiﬁcantly
outperform
global-inf
method
across
diﬀerent
datasets
proposed
rewhier
approach
shows
competitive
classiﬁcation
performance
comparison
t-easy
approach
smaller
datasets
t-easy
approach
better
performance
searches
optimal
hierarchy
hierarchical
space
however
main
drawback
t-easy
approach
requires
computationally
expensive
learning-based
evaluations
reaching
optimal
hierarchy
making
intractable
large
real-world
classiﬁcation
benchmarks
dmoz
see
detailed
discussion
runtime
comparison
4.2.2
performance
based
hierarchical
metrics
hierarchical
evaluation
metrics
hf1
computes
errors
misclassiﬁed
examples
based
deﬁnition
deﬁned
hierarchy
table
shows
hf1
score
best
baseline
method
global-inf
rewiring
methods
evaluated
original
modiﬁed
hierarchy
rewiring
methods
shows
best
performance
datasets
able
restructure
hierarchy
based
dataset
better
suited
classiﬁcation
figure
performance
comparison
rewiring
approaches
best
method
global-inf
varying
training
size
t-easy
approach
scalable
dmoz-small
dataset
figure
sorted
cosine
similarity
scores
dmoz-small
dataset
4.2.3
runtime
comparison
table
compare
training
times
diﬀerent
models
training
learn
models
parallel
diﬀerent
classes
using
multiple
compute
nodes
combined
obtain
ﬁnal
runtime
proposed
rewiring
approach
also
compute
similarity
diﬀerent
classes
parallel
see
table
td-lr
takes
least
time
overhead
associated
modifying
hierarchy
followed
global-inf
model
requires
retraining
models
hierarchy
ﬂattening
rewiring
approaches
expensive
compute
intensive
task
either
performing
similarity
computation
proposed
approach
multiple
hierarchy
evaluations
using
t-easy
approach
t-easy
method
takes
longest
time
due
large
number
expensive
hierarchy
evaluations
elementary
operations
optimal
hierarchy
reached
table
shows
number
elementary
operations
executed
using
t-easy
rewhier
approach
see
t-easy
approach
performs
large
number
operations
even
smaller
datasets
e.g.
412
operations
ipc
datasets
comparison
rewhier
4.3
eﬀect
varying
training
size
figure
shows
comparison
rewiring
approaches
global-inf
approach
clef
dmoz-small
datasets
varying
percentage
training
size
datasets
see
rewiring
approaches
outperform
ﬂattening
approaches
clef
dataset
smaller
training
percentage
rewhier
approach
better
performance
reason
behavior
might
over-ﬁtting
optimal
hierarchy
training
data
case
t-easy
approach
results
poor
performance
unseen
examples
training
dataset
enough
examples
expected
t-easy
method
gives
best
performance
cost
expensive
runtime
run
t-easy
larger
dmoz
datasets
4.4
threshold
selection
group
similar
classes
pairs
figure
shows
sorted
descending
order
class
pairs
cosine
similarity
scores
dmoz-small
dataset
see
similarity
scores
become
nearly
constant
1000
pairs
drops
6000
shown
figure
provide
interesting
similar
classes
grouping
information
taxonomy
modiﬁcation
dataset
choosing
threshold
similarity
score
1000-th
class
pair
reasonable
choice
similar
approach
determine
threshold
applied
datasets
well
figure
percentage
rare
categories
≤10
examples
per
class
classes
improved
ﬂat
method
dataset
flat
method
clef
diatoms
ipc
dmoz-small
dmoz-2010
dmoz-2012
51.31
54.17
45.74
30.80
27.06
27.04
hf1
80.58
63.50
64.00
60.87
53.94
66.45
td-lr
hiercost
expert-deﬁned
35.92
44.46
42.51
30.65
28.37
28.54
hf1
74.52
56.15
62.57
63.14
54.82
68.12
rewhier
expert-deﬁned
rewhier
47.10
52.14
46.04
32.92
29.48
29.94
hf1
52.30
80.14
54.16
63.24
50.10
62.57
32.98
66.18
56.43
29.81
29.78
69.00
hf1
82.18
64.13
68.45
65.58
58.24
69.74
54.20
55.78
51.04
33.43
30.35
30.27
hf1
84.42
66.31
69.43
66.30
58.93
70.21
table
comparative
performance
results
4.5
improvement
flat
state-of-the-art
approaches
figure
presents
percentage
classes
improved
td-lr
hiercost
approaches
comparison
ﬂat
approach
dmoz
datasets
containing
rare
categories
i.e.
less
training
examples
dmoz-2010
dmoz-2012
benchmarks
use
separate
held
test
dataset
since
true
labels
provided
test
set
used
online
competition
figure
observe
approaches
outperforms
ﬂat
approach
irrespective
hierarchy
used
rare
categories
beneﬁt
utilization
hierarchical
relationships
using
hierarchy
improves
accuracy
moreover
use
rewhier
train
td-lr
hiercost
approaches
improves
classiﬁcation
performance
comparison
using
expert-deﬁned
hierarchy
hiercost
approach
consistently
outperforms
td-lr
approach
hiercost
penalizes
misclassiﬁed
instances
based
assignment
within
hierarchy
table
gives
comprehensive
results
classes
figure
gives
hf1
improvements
rare
categories
classes
terms
prediction
runtime
approaches
outperform
ﬂat
hiercost
approaches
ﬂat
hiercost
models
invoke
classiﬁers
trained
leaf
nodes
make
prediction
decision
dmoz-
2012
dataset
ﬂat
hiercost
approaches
take
∼220
minutes
predicting
labels
test
instances
whereas
td-lr
model
3.5
times
faster
hardware
conﬁguration
related
work
work
closely
related
rewiring
approach
developed
tang
expert-deﬁned
hierarchy
gradually
modiﬁed
iteratively
subset
hierarchy
modiﬁed
evaluated
classiﬁcation
performance
improvement
using
learning
algorithm
modiﬁed
changes
retained
performance
results
improve
otherwise
changes
discarded
process
repeated
repeated
procedure
hierarchy
modiﬁcation
continues
optimal
hierarchy
reached
expensive
evaluation
step
makes
approach
intractable
large-scale
datasets
another
drawback
approach
deciding
branch
hierarchy
explore
ﬁrst
modiﬁcation
elementary
operation
promote
demote
merge
apply
step
work
similar
direction
found
earlier
studies
focused
ﬂattening
based
approaches
level
nodes
selectively
ﬂattened
removed
based
certain
criterion
work
learning
based
approach
proposed
hf1
figure
percentage
improvement
hf1
scores
various
approaches
ﬂat
approach
classes
rare
categories
nodes
ﬂatten
decided
based
classiﬁcation
performance
improvement
validation
set
approach
although
useful
smaller
datasets
scalable
due
expensive
evaluation
process
node
removal
recently
naik
proposed
taxonomy
adaptation
nodes
intelligently
ﬂattened
based
empirically
deﬁned
cut-oﬀ
threshold
objective
function
values
computed
node
hierarchy
modiﬁcation
using
approach
scalable
beneﬁcial
classiﬁcation
theoretically
justiﬁed
approaches
towards
hierarchy
modiﬁcation
involves
generating
hierarchy
scratch
ignoring
expert-deﬁned
hierarchy
approaches
exploit
hierarchical
clustering
algorithms
generating
hierarchy
8,17,26,27
constructing
hierarchy
using
clustering
approaches
popular
due
sensitivity
predeﬁned
parameters
number
levels
conclusion
future
work
propose
data-driven
ﬁlter
based
rewired
approach
hierarchy
modiﬁcation
suited
method
robust
adapted
work
conjunction
state-of-the-art
approaches
literature
utilize
hierarchical
relationships
irrespective
classiﬁers
trained
modiﬁed
hierarchy
consistently
gives
better
performance
use
clustering
ﬂattening
modify
original
hierarchy
comparison
previous
rewiring
approaches
method
gives
competitive
results
much
better
runtime
performance
allow
approaches
scale
signiﬁcantly
large
datasets
e.g.
dmoz
experiments
datasets
skewed
distribution
shows
eﬀectiveness
proposed
method
comparison
ﬂat
state-of-the-art
methods
especially
classes
rare
categories
future
plan
study
eﬀect
method
conjunction
feature
selection
non-linear
classiﬁcation
methods
acknowledgement
nsf
grant
203337
202882
huzefa
rangwala
references
cai
hofmann
hierarchical
document
categorization
support
vector
machines
cikm
pages
78–87
2004
koller
sahami
hierarchically
classifying
documents
using
words
icml
pages
170–178
1997
mccallum
rosenfeld
mitchell
improving
text
classiﬁcation
shrinkage
hierarchy
classes
icml
pages
359–367
1998
dumais
chen
hierarchical
classiﬁcation
web
content
acm
sigir
pages
256–263
2000
sun
lim
hierarchical
text
classiﬁcation
evaluation
icdm
pages
521–528
2001
liu
wan
qin
chen
ren
site
abstraction
rare
category
classiﬁcation
large-scale
web
directory
www
special
interest
tracks
posters
pages
1108–1109
2005
naik
rangwala
inconsistent
node
ﬂattening
improving
top-down
hierarchical
classiﬁcation
ieee
dsaa
2016
zhu
ogihara
hierarchical
document
classiﬁcation
using
automatically
generated
hierarchy
jiis
:211–230
2007
xiao
zhou
hierarchical
classiﬁcation
via
orthogonal
transfer
icml
pages
801–808
2011
zimek
buchwald
frank
kramer
study
hierarchical
ﬂat
classiﬁcation
proteins
ieee/acm
tcbb
:563–571
2010
tang
zhang
liu
acclimatizing
taxonomic
semantics
hierarchical
content
classiﬁcation
acm
sigkdd
pages
384–393
2006
davison
hierarchy
evolution
improved
classiﬁcation
cikm
pages
2193–2196
2011
charuvaka
rangwala
hiercost
improving
large
scale
hierarchical
classiﬁcation
cost
sensitive
learning
ecml
pkdd
2015
babbar
partalas
gaussier
mr.
amini
maximum-margin
framework
training
data
synchronization
large-scale
hierarchical
classiﬁcation
neural
information
processing
pages
336–343
2013
wang
flatten
hierarchies
large-scale
hierarchical
text
categorization
icdim
pages
139–144
2010
babbar
partalas
gaussier
amini
ﬂat
versus
hierarchical
classiﬁcation
large-scale
taxonomies
nips
pages
1824–1832
2013
punera
rajan
ghosh
automatically
learning
document
taxonomies
hierarchical
classiﬁcation
www
special
interest
tracks
posters
2005
malik
improving
hierarchical
svms
hierarchy
ﬂattening
lazy
classiﬁcation
large-scale
workshop
ecir
2010
nitta
improving
taxonomies
large-scale
hierarchical
classiﬁers
web
docs
cikm
pages
1649–1652
2010
steinbach
ert¨oz
kumar
challenges
clustering
high
dimensional
data
new
directions
statistical
physics
pages
273–309
2004
gopal
yang
recursive
regularization
large-scale
classiﬁcation
hierarchical
graphical
dependencies
acm
sigkdd
pages
257–265
2013
dimitrovski
kocev
loskovska
dˇzeroski
hierarchical
annotation
medical
images
pattern
recognition
:2436–2449
2011
dimitrovski
kocev
loskovska
dˇzeroski
hierarchical
classiﬁcation
diatom
images
using
predictive
clustering
trees
ecological
informatics
7:19–29
2012
yang
liu
re-examination
text
categorization
methods
acm
sigir
pages
42–49
1999
gao
koller
discriminative
learning
relaxed
hierarchy
large-scale
visual
recognition
iccv
pages
2072–2079
2011
aggarwal
gates
merits
building
categorization
systems
supervised
clustering
sigkdd
pages
352–356
1999
chuang
chien
practical
web-based
approach
generating
topic
hierarchy
text
segments
cikm
pages
127–136
2004
