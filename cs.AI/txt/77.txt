pomdp-lite
robust
robot
planning
uncertainty
min
chen
emilio
frazzoli
david
hsu
wee
sun
lee
abstract—
partially
observable
markov
decision
process
pomdp
provides
principled
general
model
planning
uncertainty
however
solving
general
pomdp
com-
putationally
intractable
worst
case
paper
introduces
pomdp-lite
subclass
pomdps
hidden
state
variables
constant
change
deterministically
show
pomdp-lite
equivalent
set
fully
observable
markov
decision
processes
indexed
hidden
parameter
useful
modeling
variety
interesting
robotic
tasks
develop
simple
model-based
bayesian
reinforcement
learning
algorithm
solve
pomdp-lite
models
algorithm
performs
well
large-scale
pomdp-lite
models
1020
states
outperforms
state-of-the-art
general-purpose
pomdp
algorithms
show
algorithm
near-bayesian-optimal
suitable
conditions
introduction
time
must
exploit
imperfect
robot
control
sensor
noise
unexpected
environment
changes
contribute
uncertainties
pose
signiﬁcant
challenges
robust
robot
planning
robots
must
explore
order
gain
information
reduce
uncertainty
information
achieve
task
objectives
partially
observable
markov
decision
process
pomdp
provides
principled
general
framework
balance
exploration
exploitation
optimally
found
application
many
robotic
tasks
ranging
navigation
manipulation
human-robot
interaction
however
solving
pomdps
exactly
computationally
intractable
worst
case
rapid
progress
efﬁcient
approximate
pomdp
algorithms
recent
years
e.g.
remains
challenge
scale
large
pomdps
complex
dynamics
complexity
pomdp
lies
system
dynam-
ics
partial
observability
particularly
conﬂuence
two
introduce
pomdp-lite
factored
model
restricts
partial
observability
state
variables
constant
change
deterministically
may
appear
restrictive
pomdp-lite
powerful
enough
model
variety
interesting
robotic
tasks
chen
hsu
lee
department
computer
science
national
university
singapore
singapore
117417
singapore
frazzoli
laboratory
information
decision
systems
massachusetts
institute
technology
cambridge
02139
usa
chen
supported
singapore-mit
alliance
research
technol-
ogy
smart
graduate
fellowship
frazzoli
supported
singapore
nrf
smart
future
urban
mobility
irg
hsu
supported
a*star
industrial
robotics
program
grant
r-252-506-001-305
lee
supported
aoard
grant
fa2386-12-1-4031
views
conclusions
contained
herein
authors
interpreted
necessarily
representing
ofﬁcial
policies
endorsements
either
expressed
implied
air
force
research
laboratory
u.s.
government
unknown
system
parameters
double-pendulum
ac-
robot
link
mass
unknown
priori
swings
standup
conﬁguration
unknown
types
autonomous
driving
robot
vehicle
encounters
human
driver
unknown
behavior
uncontrolled
trafﬁc
intersection
human-robot
collaborative
task
human
according
unknown
human
preference
unknown
goals
assistive
agent
helps
human
cooking
one
several
dishes
without
knowing
human
intention
advance
robot
poses
object
tasks
require
robot
gather
information
unknown
quantities
noisy
observations
achieving
task
objective
time
fact
belong
special
case
hidden
variables
remain
constant
throughout
mainly
focus
special
case
interestingly
famous
tiger
problem
appeared
seminal
paper
pomdps
also
belongs
special
case
small
modiﬁcation
tiger
agent
stands
front
two
closed
doors
tiger
behind
one
doors
agent
objective
open
door
without
tiger
pomdp
model
state
unknown
tiger
position
agent
three
actions
open
left
door
open
right
door
listen
produce
observation
produces
noisy
observation
tiger
left
tiger
right
correct
probability
0.85.
listening
cost
agent
opens
door
tiger
gets
reward
otherwise
incurs
penalty
−100
perform
well
agent
must
decide
optimal
number
listening
actions
taking
open
action
tiger
toy
problem
captures
essence
robust
planning
uncertainty
trade
gathering
information
exploiting
information
achieve
task
objective
original
tiger
repeated
game
agent
opens
door
game
resets
tiger
going
behind
two
doors
equal
probability
change
one-shot
game
game
terminates
agent
opens
door
one-shot
game
single
state
variable
tiger
position
remains
unchanged
game
thus
admits
pomdp-lite
model
repeated
game
pomdp
pomdp-lite
pomdp-lite
equivalent
set
markov
decision
processes
mdps
indexed
hidden
parameter
key
idea
equivalence
transformation
combine
pomdp
state
observation
form
expanded
mdp
state
capture
pomdp
state-transition
uncertainty
observation
uncertainty
mdp
transition
dynamics
one-shot
tiger
example
form
two
mdps
indexed
ls,0.85
ls,0.15
ls,0.15
ls,0.85
ls,0.15
ls,0.85
ls,0.85
ls,0.15
ls,0.85
ls,0.15
ls,0.15
ls,0.85
ol,1
-100
or,1,10
null
ol,1
-100
or,1,10
end
ol,1
-100
or,1,10
ol,1,10
or,1
-100
null
ol,1,10
or,1
-100
ol,1,10
or,1
-100
end
tiger
left
tiger
right
fig
pomdp
model
one-shot
tiger
transformed
set
two
mdps
node
labeled
pair
representing
pomdp
state
observation
start
state
null
observation
labeled
accordingly
special
terminal
state
labeled
end
edge
labeled
triple
representing
action
probability
reaching
next
state
action
reward
tiger
position
left
right
fig
mdp
state
pair
consisting
pomdp
state
observation
example
mdp
tiger
left
represents
true
tiger
position
agent
receives
observation
agent
takes
action
probability
0.15
transit
new
state
receives
observation
see
section
iii
details
general
construction
equivalence
enables
develop
online
algorithm
pomdp-lite
model-based
bayesian
reinforce-
ment
learning
hidden
parameter
value
known
problem
would
simply
become
mdp
well-established
algorithms
gather
information
unknown
hidden
parameter
robot
must
explore
maintains
belief
i.e.
probability
distribution
hidden
parameter
follows
internal
reward
approach
model-based
bayesian
modiﬁes
mdp
reward
function
order
encourage
exploration
time
step
online
algorithm
solves
internal
reward
mdp
choose
action
updates
belief
incor-
porate
new
observation
received
algorithm
simple
implement
performs
well
large-scale
pomdp-lite
tasks
1020
states
outperforms
state-of-
the-art
general-purpose
pomdp
algorithms
furthermore
near-bayesian-optimal
suitable
conditions
related
work
pomdp
planning
huge
literature
see
e.g.
brief
review
focuses
online
search
algorithms
time
step
online
algorithm
performs
look-ahead
search
com-
putes
best
action
current
belief
robot
executes
action
algorithm
updates
belief
based
observation
received
process
repeats
new
belief
next
time
step
online
search
algorithms
scale
focusing
current
belief
rather
possible
beliefs
robot
may
encounter
since
online
algorithms
recompute
best
action
scratch
step
naturally
handle
unexpected
environment
changes
without
additional
over-
head
pomcp
despot
fastest
online
pomdp
algorithms
available
today
employ
idea
sampling
future
contingencies
pomcp
performs
monte
carlo
tree
search
mcts
low
overhead
scales
large
pomdps
extremely
poor
worst-case
performance
mcts
sometimes
overly
greedy
despot
samples
ﬁxed
number
future
contingencies
deterministically
advance
performs
heuristic
search
resulting
search
tree
substantially
improves
worst-case
performance
bound
also
ﬂexible
easily
incorporates
domain
knowledge
despot
successfully
implemented
real-time
autonomous
driving
crowd
also
crucial
component
system
humanitarian
robotics
automation
technology
challenge
hratc
2015
demining
task
instead
solving
general
pomdp
take
different
approach
identify
structural
property
enables
simpler
efﬁcient
algorithms
model-based
mixed
observability
bayesian
like
pomdp-lite
markov
decision
process
momdp
also
factored
model
however
place
restriction
partially
observable
state
variables
fact
equivalent
general
pomdp
every
pomdp
represented
momdp
vice
versa
hidden
goal
markov
decision
process
hgmdp
hidden
parameter
markov
decision
process
hip-mdp
related
pomdp-
lite
restrict
partially
observability
static
hidden
variables
work
hgmdp
relies
myopic
heuristic
planning
unlikely
perform
well
tasks
need
exploration
work
hip-mdp
focuses
mainly
learning
hidden
structure
data
several
approaches
bayesian
internal
reward
approach
among
successful
simple
performs
well
practice
internal
reward
methods
divided
two
main
categories
pac-mdp
bayesian
optimal
pac-mdp
al-
gorithms
optimal
respect
true
mdp
provide
strong
theoretical
guarantee
may
suffer
exploration
empirically
bayesian
optimal
algorithms
optimal
respect
optimal
bayesian
policy
simply
try
achieve
high
expected
total
reward
particular
bayesian
exploration
bonus
beb
algorithm
achieves
lower
sample
complexity
pac-
mdp
algorithms
however
beb
requires
dirichlet
prior
hidden
parameters
algorithm
inspired
beb
constructs
exploration
bonus
differently
allows
arbitrary
discrete
prior
useful
feature
practice
iii
pomdp-lite
deﬁnition
pomdp-lite
special
class
pomdp
de-
terministic
assumption
partially
observable
variable
speciﬁcally
partially
observable
variable
pomdp-
lite
static
deterministic
dynamic
formally
introduce
pomdp-lite
tuple
set
fully
observable
states
hidden
parameter
ﬁnite
number
possible
values
...
state
space
cross
product
fully
observable
states
hidden
parameter
|xi
set
actions
set
observations
transition
function
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
speciﬁes
probability
reach-
ing
state
cid:48
cid:48
agent
takes
action
state
cid:48
cid:48
according
deterministic
assumption
observation
function
cid:48
cid:48
o|θ
cid:48
cid:48
speciﬁes
probability
receiving
observation
taking
action
reaching
state
cid:48
cid:48
reward
function
speciﬁes
reward
received
agent
takes
action
state
discount
factor
pomdp-lite
state
unknown
agent
main-
tains
belief
probability
distribution
states
step
agent
takes
action
receives
new
observation
belief
updated
according
bayes
rule
cid:48
solution
pomdp-lite
policy
maps
belief
states
actions
i.e.
value
policy
expected
reward
respect
initial
belief
cid:2
cid:80
γtr
cid:3
t=0
denote
state
action
time
optimal
policy
highest
value
belief
states
i.e.
corresponding
optimal
value
function
satisﬁes
bellman
equation
cid:26
cid:88
x∈x
θ∈θ
max
o|b
cid:0
cid:1
cid:27
cid:88
o∈o
equivalent
transformation
set
mdps
cid:83
section
show
important
property
pomdp-
lite
model
equivalent
collection
mdps
indexed
mdp
model
parameter
tuple
set
states
set
actions
cid:48
transition
function
reward
function
discount
factor
theorem
let
pomdp-lite
model
...
equals
collection
mdps
indexed
θi∈θ
proof
proof
theorem
show
equivalence
ﬁrst
reduce
direction
easy
simply
treat
part
state
pomdp-
lite
model
remaining
part
become
part
pomdp-
lite
model
without
change
interesting
direction
reduce
let
ﬁrst
consider
case
value
re-
mains
constant
given
pomdp-lite
model
becomes
mdp
model
parameter
consists
cid:48
cid:48
cid:48
cid:48
pomdp-lite
dpθi
fig
graphic
model
pomdp-lite
left
mdp
model
parameter
right
following
elements
state
space
null
null
simply
means
observation
received
set
actions
identical
actions
pomdp-lite
model
transition
function
cid:48
cid:48
cid:48
|θi
cid:48
|θi
cid:48
cid:48
|θi
speciﬁes
probability
reaching
state
cid:48
taking
action
state
cid:48
|θi
cid:48
|θi
cid:48
tran-
sition
observation
probability
function
pomdp-
lite
model
reward
function
speciﬁes
reward
received
agent
takes
action
state
discount
factor
graphic
model
fig
shows
relationship
pomdp-lite
model
corresponding
mdp
model
parameter
since
hidden
parameter
ﬁnite
number
values
pomdp-
lite
reduced
collection
mdps
indexed
show
simple
extension
allows
handle
case
value
hidden
variable
changes
deter-
ministically
key
intuition
deterministic
dynamic
hidden
variable
introduce
additional
uncertainties
model
i.e.
given
initial
value
hidden
variable
history
time
step
...
xt−1
at−1
value
hidden
variable
predicted
using
deterministic
function
thus
given
initial
value
hidden
variable
deterministic
function
pomdp-lite
model
reduced
mdp
model
state
compared
static
case
augmented
history
i.e.
˜ot
value
fully
captured
rest
mdp
model
similar
static
case
particular
set
actions
identical
pomdp-
lite
model
transition
function
st+1
˜ot+1|θt+1
xt+1
xt+1|θt
reward
func-
tion
discount
factor
since
ﬁnite
number
values
pomdp-lite
reduced
collection
mdps
indexed
algorithm
part
present
efﬁcient
model
based
brl
algorithm
pomdp-lite
solution
brl
problem
policy
maps
tuple
belief
state
actions
i.e.
value
policy
belief
cid:88
cid:88
cid:48
cid:48
state
given
bellman
equation
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
cid:80
mean
reward
function
cid:48
cid:80
cid:48
mean
transition
function
second
line
follows
fact
belief
update
deterministic
i.e.
cid:48
optimal
bayesian
value
function
max
cid:48
cid:48
cid:48
cid:88
cid:26
cid:27
cid:48
optimal
action
maximizes
right
hand
size
like
optimal
policy
original
pomdp-
lite
problem
optimal
bayesian
policy
chooses
actions
based
affect
next
state
also
based
affect
next
belief
however
optimal
bayesian
policy
computationally
intractable
instead
exploring
updating
belief
step
algorithm
explores
explicitly
modify
reward
function
words
state
action
pair
reward
bonus
based
much
information
reveal
reward
bonus
used
algorithm
motivated
observation
belief
gets
updated
whenever
information
hidden
parameter
revealed
thus
use
divergence
two
beliefs
measure
amount
information
gain
reward
bonus
deﬁned
formally
follows
deﬁnition
belief
updated
measure
information
gain
divergence
cid:12
cid:12
cid:12
cid:12
based
i.e.
cid:107
cid:107
cid:80
cid:88
cid:3
cid:2
cid:107
cid:48
cid:107
reward
bonus
deﬁned
expected
divergence
current
belief
next
belief
cid:48
cid:48
cid:107
cid:48
cid:107
θi∈θ
cid:48
cid:48
constant
tuning
factor
cid:48
updated
belief
observing
cid:48
time
step
algorithm
solves
internal
reward
mdp
chooses
action
greedily
respect
following
value
function
max
cid:48
cid:48
cid:88
cid:26
cid:27
cid:48
reward
bonus
term
deﬁned
deﬁnition
parts
identical
equation
except
belief
updated
equation
solve
using
standard
value
iteration
algorithms
time
complexity
|a||x|2|θ|2
work
interested
problems
large
state
space
thus
using
uct
online
mdp
solver
achieve
online
performance
details
algorithm
described
˜q∗
algorithm
st+1|bt
st+1
cid:80
st+1
algorithm
maximum
steps
end
arg
maxa
st+1
executeaction
bt+1
pdatebelief
st+1
end
˜q∗
analysis
initialize
values
greedily
choose
action
update
belief
although
algorithm
greedy
algorithm
actually
performs
sub-optimally
polynomial
number
time
steps
section
present
theoretical
results
bound
sample
complexity
algorithm
unless
stated
otherwise
proof
lemmas
section
deferred
appendix
clean
analysis
assume
reward
function
bounded
sample
complexity
sample
complexity
measures
number
samples
needed
algorithm
perform
optimally
start
deﬁnition
sample
complexity
state
action
pair
deﬁnition
given
initial
belief
target
accuracy
reward
bonus
tuning
factor
deﬁne
sample
complexity
function
visited
times
starting
belief
corresponding
reward
bonus
visiting
new
belief
cid:48
less
i.e.
cid:48
declare
known
sampled
times
cease
update
belief
sampling
known
state
action
pairs
following
assumption
theorem
hold
true
general
assumption
essentially
says
earlier
try
state-action
pair
information
gain
give
concrete
example
illustrate
assumption
lemma
assumption
reward
bonus
monotonically
decreases
i.e.
state
action
pairs
timesteps
bi+1
theorem
let
present
central
theoretical
result
bounds
sample
complexity
algorithm
respect
optimal
bayesian
policy
cid:0
|s|2|a|
sample
complexity
let
denote
policy
followed
algorithm
time
let
corresponding
state
belief
probability
least
i.e.
algorithm
4-close
optimal
bayesian
policy
cid:1
1−γ
cid:18
cid:80
1−γ
1−γ
cid:19
time
steps
words
algorithm
acts
sub-optimally
polynomial
number
time
steps
although
algorithm
primary
designed
discrete
prior
theorem
applied
many
prior
distributions
apply
two
simple
special
classes
provide
concrete
sample
complexity
bound
first
show
case
independent
dirichlet
prior
reward
bonus
monotonically
decreases
sample
complexity
pair
bounded
polynomial
function
case
also
satisﬁes
assumption
lemma
independent
dirichlet
prior
let
number
times
visited
known
reward
function
independent
dirichlet
prior
transition
dynamics
pair
monotonically
decreases
rate
cid:0
1/n
cid:1
cid:1
sample
complexity
function
cid:0
|s|2|a|
strength
algorithm
lies
ability
handle
discrete
prior
use
simple
example
discrete
prior
unknown
deterministic
mdps
show
advantage
state
following
lemma
intuition
behind
lemma
quite
simple
sampling
state
action
pair
agent
know
effect
without
noise
1−γ
lemma
discrete
prior
deterministic
mdps
let
discrete
prior
deterministic
mdps
sample
complexity
function
proof
theorem
key
intuition
prove
algorithm
quickly
achieves
near-optimality
time
step
algo-
rithm
-optimistic
respect
bayesian
policy
value
optimism
decays
zero
given
enough
samples
proof
theorem
follows
standard
argu-
ments
previous
pac-mdp
results
ﬁrst
show
close
value
acting
according
optimal
bayesian
policy
assuming
probability
escaping
known
state-action
set
small
use
hoeffding
bound
show
escaping
probability
large
polynomial
number
time
steps
begin
proof
following
lemmas
ﬁrst
lemma
essentially
says
solve
internal
reward
mdp
using
current
mean
belief
state
additional
exploration
bonus
deﬁnition
lead
value
function
-optimistic
bayesian
policy
lemma
optimistic
let
value
function
algorithm
value
function
bayesian
cid:1
policy
cid:0
|s|2|a|
following
deﬁnition
generalization
known
state-action
mdp
bayesian
settings
mdp
whose
dynamics
transition
function
reward
function
equal
mean
mdp
pairs
known
set
pairs
value
taking
pairs
equal
current
value
estimate
deﬁnition
given
current
belief
set
value
estimate
pair
i.e.
1−γ
cid:80
cid:48
cid:48
set
known
pairs
cid:48
i.e.
deﬁne
known
state-
action
mdp
follows
additional
state
actions
agent
returned
probability
received
reward
cid:48
cid:48
ﬁnal
lemma
shows
internal
reward
mdp
known
state-action
mdp
low
error
set
known
pairs
lemma
accuracy
fix
history
time
step
let
belief
state
set
known
pairs
mkt
known
state-action
mdp
greedy
policy
respect
current
belief
i.e.
arg
maxa
mkt
ready
prove
theorem
proof
proof
theorem
let
mkt
described
lemma
let
1/
see
lemma
mkt
let
denote
event
pair
generated
executing
starting
time
steps
mkt
mkt
mkt
ﬁrst
inequality
follows
fact
equals
unless
occurs
bounded
since
limit
reward
bonus
still
maintain
optimism
second
inequality
follows
deﬁnition
third
inequality
follows
lemma
last
inequality
follows
lemma
fact
precisely
optimal
policy
in-
ternal
reward
mdp
time
suppose
−4
otherwise
hoeffding
inequality
happen
time
steps
probability
notation
suppresses
logarithmic
factors
cid:18
cid:80
cid:19
experiments
evaluate
algorithm
experimentally
compare
several
state
art
algorithms
pomdp
literature
pomcp
despot
two
successful
online
pomdp
planners
scale
large
pomdps
qmdp
myopic
ofﬂine
solver
widely
used
efﬁciency
sarsop
state
art
ofﬂine
pomdp
solver
helps
calibrate
best
performance
achievable
pomdps
moderate
size
mean
mdp
common
myopic
approximation
bayesian
planning
exploration
sarsop
pomcp
despot
used
software
provided
authors
slight
modiﬁcation
pomcp
make
strictly
follow
1-second
time
limit
planning
algorithm
mean
mdp
mdp
needs
solved
step
use
online
mdp
solver
uct
similar
parameter
settings
used
pomcp
reward
bonus
scalar
used
algorithm
typically
much
smaller
one
required
theorem
common
trend
internal
reward
algorithms
tuned
ofﬂine
using
planning
ﬁrst
apply
algorithms
two
benchmarks
prob-
lems
pomdp
literature
demonstrate
scaling
ability
algorithm
larger
pomdps
rocksample
robot
moving
grid
contains
rocks
may
good
bad
probability
0.5
initially
step
robot
move
adjacent
cell
sense
rock
robot
sample
rock
grid
contains
rock
sample
rock
gives
reward
+10
rock
good
−10
otherwise
move
sample
produce
observation
sensing
produces
observation
set
good
bad
accuracy
decreasing
exponentially
robot
distance
rock
increases
robot
reaches
terminal
state
passes
east
edge
map
discount
factor
0.95.
hidden
parameter
property
rock
remains
constant
thus
problem
modeled
pomdp-lite
battleship
ships
placed
random
grid
subject
constraint
ship
may
placed
adjacent
diagonally
adjacent
another
ship
ship
different
size
...
goal
ﬁnd
sink
ships
initially
agent
know
conﬁguration
ships
step
agent
ﬁre
upon
one
cell
grid
receives
observation
ship
hit
otherwise
receive
observation
reward
per
step
terminal
reward
n×n
hitting
every
cell
every
ship
illegal
ﬁre
twice
cell
discount
factor
hidden
parameter
conﬁguration
ships
remains
constant
thus
problem
also
modeled
pomdp-lite
results
rocksample
battleship
shown
table
algorithms
except
qmdp
sarsop
ofﬂine
algorithms
run
real
time
second
per
step
result
sarsop
replicated
results
test
averaged
1000
runs
means
problem
size
large
algorithm
short
rocksample
short
battleship
see
table
algorithm
achieves
similar
performance
state
art
ofﬂine
solvers
problem
size
small
however
size
problem
increases
ofﬂine
solvers
start
fail
algorithm
outperforms
online
algorithms
finally
show
robot
arm
grasping
task
originated
amazon
picking
challenge
v-rep
simulation
view
shown
fig
goal
robot
arm
grasp
cup
shelf
quickly
robustly
robot
knows
conﬁguration
exactly
movement
deterministic
however
due
sensor
limitations
initial
position
cup
uncertain
gripper
table
performance
comparison
grasping
cup
v-rep
simulation
model
evaluation
states
|x|
actions
|a|
observations
|o|
pomcp
despot
mean
mdp
pomdp-lite
continuous
continuous
return
success
rate
return
success
rate
12.91
2.10
14.17
0.36
100
9.63
2.52
11.35
0.46
99.8
11.28
1.81
14.16
0.46
98.6
100
15.46
1.20
15.56
0.31
tactile
sensor
inside
ﬁnger
gives
positive
readings
inner
part
ﬁnger
gets
touch
cup
robot
needs
move
around
localize
cup
grasp
soon
possible
usually
modeled
pomdp
problem
however
model
pomdp-lite
algorithm
achieve
much
better
performance
compared
solving
pomdp
introduce
planning
model
task
restrict
movement
gripper
plane
shown
fig
divide
plane
regions
relative
gripper
cup
region
gripper
get
touch
cup
moving
along
x-axis
y-axis
cup
region
gripper
get
touch
cup
moving
along
y-axis
cup
region
gripper
sense
cup
moving
single
direction
gripper
move
along
axis
step
size
0.01.
reward
movement
region
region
region
gripper
close
open
ﬁngers
reward
−10
picking
cup
gives
reward
100
pick
successful
−100
otherwise
compare
algorithm
pomcp
despot
mean
mdp
since
qmdp
sarsop
support
continuous
state
space
algorithm
tested
via
model
evaluation
v-rep
simulation
model
evaluation
means
use
planning
model
examine
policy
v-rep
simulation
means
compute
best
action
using
planning
model
execute
v-rep
simulation
next
state
observation
obtained
v-rep.
results
model
evaluation
v-rep
simulation
re-
ported
table
time
used
online
planning
algorithms
second
per
step
run
1000
trials
model
evaluation
100
trials
v-rep
simulation
see
algorithm
achieves
higher
return
success
rate
settings
compared
algorithms
conclusion
introduced
pomdp-lite
subclass
pomdp
hidden
variables
either
static
change
de-
terministically
pomdp-lite
equivalent
set
mdps
indexed
hidden
parameter
exploiting
equivalence
developed
simple
online
algorithm
pomdp-lite
model-based
bayesian
reinforcement
learning
preliminary
experiments
suggest
algorithm
outperforms
state-of-the-art
general-purpose
pomdp
solvers
large
pomdp-lite
models
makes
promising
tool
large-scale
robot
planning
uncertainty
currently
implementing
experimenting
algorithm
kinova
mico
robot
object
manipula-
table
performance
comparison
rocksample
battleship
544
372
800
247
808
states
|x|
actions
|a|
observations
|o|
qmdp
sarsop
pomcp
despot
mean
mdp
pomdp-lite
17.55
0.44
21.47
0.04
19.95
0.23
20.80
0.22
15.11
0.17
21.03
0.21
16.10
0.40
21.56
0.11
20.11
0.23
21.12
0.21
11.64
0.17
21.52
0.20
15.51
0.23
18.59
0.43
7.53
0.16
18.63
0.20
108
1013
100
12.11
0.23
12.56
0.38
6.96
0.11
16.81
0.20
57.40
0.19
56.34
0.22
57.46
0.17
58.16
0.17
1020
225
119.33
0.58
117.39
0.88
122.60
0.59
127.12
0.61
kearns
singh
near-optimal
reinforcement
learning
polynomial
time
machine
learning
vol
2-3
209–232
2002
kocsis
szepesv´ari
bandit
based
monte-carlo
planning
machine
learning
ecml
2006.
springer
2006
282–293
kolter
near-bayesian
exploration
polynomial
time
proceedings
26th
annual
international
conference
machine
learning
2009
koval
pollard
srinivasa
pre-
post-contact
policy
decomposition
planar
contact
manipulation
uncertainty
int
robotics
research
2015
kurniawati
hsu
lee
sarsop
efﬁcient
point-
based
pomdp
planning
approximating
optimally
reachable
belief
spaces
proc
robotics
science
systems
2008
nikolaidis
ramakrishnan
shah
efﬁcient
model
learning
joint-action
demonstrations
human-robot
collaborative
tasks
proc
acm/ieee
int
conf
human-robot
interaction
2015
ong
png
hsu
lee
planning
uncer-
tainty
robotic
tasks
mixed
observability
international
journal
robotics
research
vol
1053–1068
2010
papadimitriou
tsitsiklis
complexity
markov
decision
processes
mathematics
operations
research
vol
441–450
1987
poupart
vlassis
hoey
regan
analytic
solution
discrete
bayesian
reinforcement
learning
proceedings
23rd
international
conference
machine
learning
acm
2006
rohmer
singh
freese
v-rep
versatile
scal-
able
robot
simulation
framework
intelligent
robots
systems
iros
2013
ieee/rsj
international
conference
ieee
2013
1321–1326
ross
pineau
paquet
chaib-draa
online
planning
algorithms
pomdps
artiﬁcial
intelligence
research
vol
663–704
2008
roy
thrun
coastal
navigation
mobile
robots
advances
neural
information
processing
systems
mit
press
1999
vol
1043–1049
seiler
kurniawati
singh
online
approximate
solver
pomdps
continuous
action
space
proc
ieee
int
conf
robotics
automation
2015
silver
veness
monte-carlo
planning
large
pomdps
advances
neural
information
processing
systems
2010
smith
simmons
point-based
pomdp
algorithms
im-
proved
analysis
implementation
proc
conf
uncertainty
artiﬁcial
intelligence
2005
somani
hsu
lee
despot
online
pomdp
planning
regularization
advances
neural
information
processing
systems
2013
sondik
optimal
control
partially
observable
markov
pro-
cesses
ph.d.
dissertation
stanford
university
stanford
california
usa
1971
sorg
singh
lewis
variance-based
rewards
approximate
bayesian
reinforcement
learning
uai
proceedings
twenty-sixth
conference
uncertainty
artiﬁcial
intelligence
2010
strehl
littman
incremental
model-based
learners
formal
learning-time
guarantees
uai
proceedings
22nd
conference
uncertainty
artiﬁcial
intelligence
2006
thrun
burgard
fox
probabilistic
robotics
mit
press
2005
simulation
view
simpliﬁed
model
fig
robot
arm
grasping
task
tion
interesting
important
investigate
extensions
handle
large
observation
action
spaces
references
asmuth
littman
nouri
wingate
bayesian
sampling
approach
exploration
reinforcement
learning
proceedings
twenty-fifth
conference
uncertainty
artiﬁcial
intelligence
2009
bai
cai
hsu
lee
intention-aware
online
pomdp
planning
autonomous
driving
crowd
proc
ieee
int
conf
robotics
automation
2015
bai
hsu
lee
planning
learn
proc
ieee
int
conf
robotics
automation
2013
bandyopadhyay
frazzoli
hsu
lee
rus
intention-aware
motion
planning
algorithmic
foundations
robotics
x—proc
int
workshop
algorithmic
foundations
robotics
wafr
2012
doshi-velez
konidaris
hidden
parameter
markov
decision
processes
semiparametric
regression
approach
discovering
latent
task
parametrizations
arxiv
preprint
arxiv:1308.3513
2013
fern
natarajan
judah
tadepalli
decision-theoretic
model
assistance
proc
aaai
conf
artiﬁcial
intelligence
2007
hsiao
kaelbling
lozano-p´erez
grasping
pomdps
proc
ieee
int
conf
robotics
automation
2007
kaelbling
littman
cassandra
planning
acting
partially
observable
stochastic
domains
artiﬁcial
intelligence
vol
101
1–2
99–134
1998.
techinical
proofs
proof
lemma
proof
let
denote
let
denote
according
deﬁnition
dirichlet
distri-
bution
sj|b
reward
bonus
term
described
cid:88
cid:20
cid:88
cid:0
cid:54
=sk
sk|b
sj∈s
cid:18
cid:88
sj|b
cid:48
sj|b
cid:1
cid:21
cid:1
cid:0
cid:19
cid:54
=sk
cid:88
sample
complexity
cid:0
cid:1
cid:0
|s|2|a|
cid:1
proof
lemma
ﬁrst
introduce
notations
used
proof
denote
step
history
...
belief
state
time
step
following
deﬁnition
divergence
reward
function
transition
function
deﬁnition
denote
set
mean
reward
func-
tion
set
mean
transition
function
i.e.
given
belief
cid:48
cid:48
suppose
belief
changes
divergence
denoted
cid:107
cid:107
cid:107
cid:107
cid:88
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:48
cid:48
cid:12
cid:12
cid:88
cid:48
based
deﬁnition
introduce
regret
step
history
belief
updated
step
deﬁnition
given
step
history
belief
updated
regret
ith
action
deﬁned
deﬁne
1−γ
γie
total
regret
history
following
deﬁnition
measures
extra
value
reward
bonus
term
using
internal
reward
deﬁnition
given
policy
step
history
reward
bonus
ith
action
deﬁne
γib
total
extra
value
reward
bonus
cid:80
i=0
next
lemma
going
bound
regret
using
extra
value
reward
bonus
cid:80
i=0
lemma
given
step
history
let
regret
updating
belief
deﬁned
deﬁnition
let
extra
value
reward
bonus
term
deﬁned
deﬁnition
constant
tunning
factor
reward
bonus
cid:0
|s|2|a|t
cid:1
1−γ
proof
begin
proof
showing
divergence
reward
function
bounded
reward
bonus
chosen
properly
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:88
cid:88
cid:88
|s||a|
cid:107
cid:107
|s||a|i−1
cid:88
|bi
θi∈θ
θi∈θ
cid:107
bl+1
cid:107
l=0
i−1
cid:88
i−1
cid:88
i−1
cid:88
l=0
|s||a|
|s||a|
|s||a|
l=0
l=0
ﬁrst
inequality
follows
fact
bounded
triangle
inequality
second
inequality
also
follows
triangle
inequality
i.e.
cid:107
bsi
cid:107
cid:107
bsi
bsi−1
cid:107
...
cid:107
bs1
cid:107
third
inequality
follows
monotonicity
assumption
reward
bonus
similarly
show
divergence
transition
function
bounded
reward
bonus
bsi
|s|2|a|
i−1
cid:88
l=0
finally
going
show
regret
bounded
extra
value
reward
bonus
total
cid:88
i=0
cid:18
cid:88
cid:18
|s|2|a|
cid:18
|s|2|a|t
i=0
γie
cid:18
i−1
cid:88
l=0
cid:19
cid:88
cid:19
cid:88
i=0
i=0
cid:19
cid:19
γib
second
inequality
transformation
equation
fol-
cid:88
lows
cid:88
cid:88
cid:88
third
inequality
transformation
equation
follows
deﬁnition
since
arbitrary
equation
history
t−t
t−t
min
t−t−1
t−t−1
bt+1
cid:9
cid:8
apply
equation
repeatedly
steps
get
cid:27
min
t−t
t−t
cid:9
cid:8
second
inequality
follows
lemma
proves
lemma
proof
lemma
proof
consider
sequence
states
actions
state
following
ﬁrst
state
...
action
pair
generated
suppose
belief
γir
k−1
cid:80
γi
i=0
mkt
ﬁrst
inequality
follows
fact
divergence
reward
function
transition
bounded
reward
bonus
value
second
inequality
follows
t−1
cid:80
cid:18
i−1
cid:80
cid:80
cid:19
cid:80
i=0
l=0
i=0
γib
i=0
ready
prove
lemma
proof
proof
lemma
let
1/
see
lemma
initial
belief
state
consider
state
let
new
belief
formed
updating
steps
t−t
t−t
cid:26
cid:88
max
cid:27
t−t−1
cid:48
cid:88
cid:48
cid:48
|b0
cid:26
cid:26
cid:0
cid:1
max
cid:48
|bt
min
cid:48
t−t−1
bt+1
cid:48
cid:88
cid:88
cid:48
cid:48
|b0
cid:48
|bt
t−t−1
cid:48
cid:27
t−t−1
bt+1
cid:48
cid:48
min
s|bt
cid:48
|b0
cid:48
cid:26
cid:0
cid:1
cid:88
t−t−1
bt+1
cid:48
cid:1
cid:27
cid:88
cid:48
|b0
cid:0
cid:26
cid:88
cid:8
min
min
cid:48
|b0
cid:0
cid:48
t−t−1
bt+1
cid:9
t−t−1
cid:48
t−t−1
cid:48
t−t−1
cid:48
t−t−1
bt+1
cid:48
ﬁrst
inequality
transformation
equation
follows
max
min
max
cid:0
cid:1
k−1
cid:80
k−1
cid:80
i=0
i=0
mkt
cid:27
