ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
latent
variable
interpretation
sum-product
networks
robert
peharz
robert
gens
franz
pernkopf
senior
member
ieee
pedro
domingos
abstract—one
central
themes
sum-product
networks
spns
interpretation
sum
nodes
marginalized
latent
variables
lvs
interpretation
yields
increased
syntactic
semantic
structure
allows
application
algorithm
efﬁciently
perform
mpe
inference
literature
interpretation
justiﬁed
explicitly
introducing
indicator
variables
corresponding
lvs
states
however
pointed
paper
approach
conﬂict
completeness
condition
spns
fully
specify
probabilistic
model
propose
remedy
problem
modifying
original
approach
introducing
lvs
call
spn
augmentation
discuss
conditional
independencies
augmented
spns
formally
establish
probabilistic
interpretation
sum-weights
give
interpretation
augmented
spns
bayesian
networks
based
results
ﬁnd
sound
derivation
algorithm
spns
furthermore
viterbi-style
algorithm
mpe
proposed
literature
never
proven
correct
show
indeed
correct
algorithm
applied
selective
spns
particular
applied
augmented
spns
theoretical
results
conﬁrmed
experiments
synthetic
data
103
real-world
datasets
index
terms—sum-product
networks
latent
variables
mixture
models
expectation-maximization
mpe
inference
introduction
um-product
networks
promising
type
prob-
abilistic
model
combining
domains
deep
learning
graphical
models
one
main
advantages
many
interesting
inference
scenarios
expressed
single
forward
and/or
backward
passes
i.e
inference
scenarios
computational
cost
linear
spn
rep-
resentation
size
spns
shown
convincing
performance
applications
image
completion
com-
puter
vision
classiﬁcation
speech
language
modeling
since
proposition
one
central
themes
spns
interpretation
hierarchically
structured
latent
variable
models
essentially
approach
interpretation
mixture
models
consider
example
gaussian
mixture
model
components
set
random
variables
rvs
ppxq
σkq
gaussian
pdf
means
covariances
kth
component
mixture
weights
gmm
interpreted
two
ways
convex
combination
pdfs
thus
pdf
marginal
distribution
peharz
institute
physiology
idn
medical
university
graz
biotechmed–graz
e-mail
robert.peharz
gmail.com
gens
domingos
department
computer
science
engineering
university
washington
e-mail
rcg
cs.washington.edu
pedrod
cs.washington.edu
pernkopf
signal
processing
speech
communication
lab
graz
university
technology
e-mail
pernkopf
tugraz.at
manuscript
received
november
2015
revised
june
2016
accepted
september
2016.
distribution
ppx
latent
marginal-
ized
variable
ppx
σkq
ppz
second
interpretation
interpretation
yields
syntactically
well-structured
model
example
following
interpretation
clear
draw
samples
ppxq
using
ancestral
sampling
structure
also
semantic
nature
instance
represents
clustering
class
variable
furthermore
interpretation
allows
application
algorithm
essentially
maximum-likelihood
learning
missing
data
enables
advanced
bayesian
techniques
mixture
models
seen
special
case
spns
single
sum
node
corresponds
single
generally
spns
arbitrarily
many
sum
nodes
corresponding
leading
hierarchically
structured
model
interpretation
spns
justiﬁed
explicitly
introducing
lvs
spn
model
using
so-called
indicator
variables
corresponding
lvs
states
however
shown
paper
justiﬁcation
actually
simplistic
since
potentially
conﬂict
completeness
condition
leading
incompletely
speciﬁed
model
remedy
propose
augmentation
spn
additionally
ivs
also
introduces
so-called
twin
sum
nodes
order
completely
specify
model
investigate
independency
structure
model
resulting
augmentation
ﬁnd
parallel
local
independence
assertions
bayesian
networks
bns
allows
deﬁne
representation
augmented
spn
using
interpretation
differential
approach
augmented
spns
give
sound
derivation
soft
algorithm
spns
closely
related
interpretation
inference
scenario
ﬁnding
most-probable-explanation
mpe
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
i.e
ﬁnding
probability
maximizing
assignment
rvs
using
results
form
ﬁrst
point
problem
generally
np-hard
spns
proposed
mpe
solution
found
efﬁciently
maximizing
model
rvs
i.e
non-latent
rvs
lvs
proposed
algorithm
replaces
sum
nodes
max
nodes
recovers
solution
using
viterbi-
style
backtracking
however
shown
algorithm
delivers
correct
mpe
solution
paper
show
algorithm
indeed
correct
applied
selective
spns
particular
since
augmented
spns
selective
algorithm
obtains
mpe
solution
augmented
spns
however
applied
non-augmented
spns
algorithm
still
returns
mpe
solution
augmented
spn
implicitly
assumes
weights
twin
sums
deterministic
i.e
except
single
leads
phenomenon
mpe
inference
call
low-depth
bias
i.e
shallow
parts
spn
preferred
backtracking
main
contribution
paper
provide
sound
theoretical
foundation
interpretation
spns
related
concepts
i.e
algorithm
mpe
inference
theoretical
ﬁndings
conﬁrmed
experiments
synthetic
data
103
real-world
datasets
paper
organized
follows
remainder
section
introduce
notation
review
spns
discuss
related
work
section
propose
augmentation
spns
show
soundness
hierarchical
model
give
interpretation
furthermore
discuss
indepen-
dency
properties
augmented
spns
interpretation
sum-weights
conditional
probabilities
algo-
rithm
spns
derived
section
section
discuss
mpe
inference
spns
experiments
presented
section
section
concludes
paper
proofs
theoretical
ﬁndings
deferred
appendix
1.1
background
notation
rvs
denoted
upper-case
letters
set
values
denoted
valpxq
corresponding
lower-case
letters
denote
elements
valpxq
e.g
element
valpxq
sets
rvs
denoted
boldface
letters
set
valpxnq
use
corresponding
lower-case
boldface
letters
ele-
ments
valpxq
e.g
element
valpxq
sub-
set
xrys
denotes
projection
onto
tx1
deﬁne
valpxq
elements
valpxq
interpreted
complete
evidence
assigning
ﬁxed
value
partial
evidence
represented
subset
valpxq
element
sigma-algebra
induced
rvs
use
valpxqu
be-
ing
borel-sets
discrete
rvs
choice
yields
power-set
2valpxq
example
partial
evidence
discrete
valpxq
represents
evidence
takes
one
states
r´8
real-valued
represents
evidence
takes
value
smaller
formally
speaking
partial
evidence
used
express
domain
marginalization
maximization
particular
sets
rvs
tx1
use
product
axn
represent
partial
sets
tśn
evidence
elements
denoted
using
boldface
notation
e.g
deﬁne
rys
txrys
furthermore
use
symbolize
combination
complete
partial
evidence
i.e
rvs
complete
evidence
partial
evidence
hx2
xzx1
given
node
directed
graph
let
chpnq
papnq
set
children
parents
respectively
furthermore
let
descpnq
set
descendants
recursively
deﬁned
set
containing
child
descendant
similarly
deﬁne
ancpnq
ancestors
recursively
deﬁned
set
containing
parent
ancestor
spns
deﬁned
follows
deﬁnition
sum-product
network
sum-product
net-
work
spn
set
rvs
tuple
connected
rooted
acyclic
directed
graph
set
non-negative
parameters
graph
contains
three
types
nodes
distributions
sums
products
leaves
distributions
internal
nodes
either
sums
products
distribution
node
also
called
input
distribution
simply
distribution
valpyq
distribution
function
subset
rvs
i.e
either
pmf
discrete
rvs
pdf
continuous
rvs
mixed
distribution
function
discrete
continuous
rvs
mixed
sum
node
computes
weighted
sum
children
i.e
řcpchpsq
computes
product
children
i.e
ścpchppq
non-negative
weight
associated
edge
contains
weights
outgoing
sum-edges
product
node
sets
spsq
ppsq
contain
sum
nodes
product
nodes
respectively
size
|s|
spn
deﬁned
number
nodes
edges
node
scope
deﬁned
scpnq
ťcpchpnq
scpcq
distribution
otherwise
function
computed
function
computed
root
denoted
spxq
without
loss
generality
assume
scope
root
use
symbols
nodes
spns
denotes
distribution
denotes
sum
denotes
product
symbols
denote
generic
nodes
indicate
child
parent
relationship
another
node
respectively
distribution
spn
deﬁned
normalized
output
i.e
pspxq9spxq
node
deﬁne
sub-spn
rooted
spn
deﬁned
graph
induced
descendants
corresponding
parameters
inference
unconstrained
spns
generally
intractable
however
efﬁcient
inference
spns
enabled
two
structural
constraints
completeness
decomposability
spn
complete
sums
holds
chpsq
scpc1q
scpc2q
spn
decomposable
products
holds
chppq
scpc1q
scpc2q
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
furthermore
sum
node
called
selective
choices
sum-weights
possible
inputs
holds
one
child
non-zero
spn
called
selective
sum
nodes
selective
shown
integrating
spxq
arbitrary
sets
i.e
marginalization
reduces
corresponding
integrals
input
distributions
eval-
uating
sums
products
usual
way
property
known
validity
spns
key
efﬁcient
inference
paper
consider
complete
decomposable
spns
without
loss
generality
assume
locally
normalized
sum-weights
i.e
sum
node
haveřcpchpsq
thus
i.e
spn
normalization
constant
rvs
ﬁnitely
many
states
use
so-called
indicator
variables
ivs
input
distributions
ﬁnite-
state
state
valpxq
introduce
xpx1q
1px
x1q
assigning
probability
mass
complete
decomposable
spn
represents
extended
network
polynomial
used
differential
approach
inference
assume
evidence
evaluated
spn
derivatives
spn
function
respect
ivs
interpreting
ivs
real-valued
variables
see
details
yield
bspeq
bλx
spx
ezxq
representing
inference
scenario
modiﬁed
evidence
i.e
evidence
modiﬁed
set
com-
putationally
attractive
feature
differential
approach
evaluated
valpxq
simultaneously
using
single
back-propagation
pass
spn
evidence
evaluated
similarly
second
higher
derivatives
get
b2speq
bλx
xλy
spx
eztx
otherwise
furthermore
differential
approach
generalized
spns
arbitrary
input
distributions
i.e
spns
rvs
countably
inﬁnite
uncountably
many
states
details
1.2
related
work
spns
related
negation
normal
forms
nnfs
poten-
tial
deep
network
representation
propositional
theories
like
spns
structural
constraints
nnfs
enable
certain
polynomial-time
queries
represented
theory
particular
notions
smoothness
decompos-
ability
determinism
nnfs
translate
notions
completeness
decomposability
selectivity
spns
re-
spectively
work
nnfs
led
concept
network
polynomials
multilinear
representation
bns
ﬁnitely
many
states
bns
cast
inter-
mediate
d-dnnf
deterministic
decomposable
nnf
repre-
sentation
order
generate
arithmetic
circuit
acs
representing
bns
network
polynomial
acs
re-
stricted
sums
products
equivalent
spns
slightly
different
syntax
acs
learned
optimizing
objective
trading
log-likelihood
training
set
inference
cost
measured
worst-case
number
arithmetic
operations
required
inference
i.e
number
edges
learned
models
still
represent
bns
context-speciﬁc
indepen-
dencies
similar
approach
learning
markov
networks
represented
acs
followed
spns
ﬁrst
time
proposed
represented
distribution
deﬁned
via
background
graphical
model
directly
normalized
output
network
work
spns
applied
image
data
generic
architecture
reminiscent
convolutional
neural
networks
proposed
structure
learning
algorithms
restricted
image
domain
proposed
discriminative
learning
spns
optimizing
conditional
likelihood
proposed
furthermore
growing
body
literature
theoretical
aspects
spns
relationship
types
probabilistic
models
two
families
functions
identiﬁed
efﬁciently
representable
deep
shallow
spns
spn
considered
shallow
three
layers
shown
spns
w.l.o.g
assumed
locally
normalized
notion
consistency
allow
exponentially
compact
models
decomposability
results
independently
found
furthermore
sound
derivation
inference
mechanisms
generalized
spns
given
i.e
spns
rvs
uncountably
inﬁnitely
many
states
representation
spns
found
lvs
associated
sum
nodes
model
rvs
organized
two
layer
bipartite
structure
actual
spn
structure
captured
structured
condi-
tional
probability
tables
cpts
using
algebraic
decision
dia-
grams
recently
notion
spns
generalized
sum-
product
functions
arbitrary
semirings
yields
general
unifying
framework
learning
inference
subsuming
among
others
spns
probabilistic
modeling
nnfs
logical
propositions
function
representations
integration
optimization
latent
variable
interpretation
pointed
sum
node
spn
interpreted
marginalized
similar
gmm
example
section
sum
node
one
postulates
discrete
whose
states
correspond
children
state
product
introduced
children
switched
on/off
corresponding
ivs
illustrated
fig
1.1
ivs
fig
set
still
computes
value
fig
since
setting
ivs
corresponds
marginalizing
sum
interpreted
latent
marginalized
however
regard
larger
structural
context
fig
recognize
justiﬁcation
actually
simplistic
explicitly
introducing
ivs
renders
ancestor
incomplete
descendant
thus
scope
note
setting
ivs
incomplete
spn
generally
correspond
graphical
representations
spns
ivs
depicted
nodes
containing
small
circle
general
distributions
nodes
containing
gaussian-like
pdf
sum
products
nodes
symbols
empty
nodes
arbitrary
type
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
procedure
augmentspn
sps1q
ksu
fig
problems
occurring
ivs
lvs
introduced
excerpt
spn
containing
sum
corresponding
introducing
ivs
renders
incomplete
assuming
descpnq
remedy
extending
spn
introducing
twin
sum
node
marginalization
furthermore
note
also
corresponds
say
know
probability
distribution
state
corresponding
namely
weights
know
distribution
state
corresponding
intuitively
recognize
state
irrelevant
case
since
inﬂuence
resulting
distribution
model
rvs
nevertheless
probabilistic
model
completely
speciﬁed
unsatisfying
remedy
problems
shown
fig
introduce
twin
sum
node
whose
children
ivs
corresponding
twin
connected
child
additional
product
node
interconnected
since
new
product
node
scope
scpnq
tzu
rendered
complete
furthermore
takes
state
corresponding
actually
state
corresponding
new
product
node
speciﬁed
conditional
distribution
namely
weights
twin
sum
node
clearly
given
ivs
set
network
depicted
fig
still
computes
function
network
fig
fig
since
constantly
outputs
long
use
normalized
weights
weights
used
twin
sum
node
basically
assume
arbitrary
normalized
weights
cause
constantly
output
however
natural
choice
would
use
uniform
weights
maximizing
entropy
resulting
model
although
choice
weights
crucial
evaluating
evidence
spn
plays
role
mpe
inference
see
section
let
formalize
explicit
introduction
lvs
denoted
augmentation
cks
2.1
augmentation
spns
let
spn
spsq
assume
arbitrary
ﬁxed
ordering
children
chpsq
tc1
|chpsq|
let
probability
space
valpzsq
ksu
state
corresponds
child
call
associated
sets
sum
nodes
deﬁne
tzs
distinguish
lvs
refer
former
model
rvs
node
deﬁne
sum
ancestors/descendants
ancspnq
ancpnq
spsq
descspnq
descpnq
spsq
let
¯ws
¯ws
sps1q
introduce
new
product
node
disconnect
connect
connect
child
child
weight
sps1q
end
end
sps1q
ksu
connect
new
λzs
child
end
scpsq
introduce
twin
sum
node
ksu
connect
λzs
child
let
w¯s
λzs
¯ws
scpsq
descppk
connect
child
end
procedure
end
return
end
end
end
fig
pseudo-code
augmentation
spn
sum
node
deﬁne
conditioning
sums
scpsq
tsc
ancspsqztsu
chpscq
descpcqu
furthermore
assume
set
locally
normalized
twin-
weights
containing
twin-weight
¯ws
weight
spn
ready
deﬁne
augmenta-
tion
spn
deﬁnition
augmentation
spn
let
spn
set
twin-weights
result
algorithm
augmentspn
shown
fig
called
augmented
spn
denoted
augpsq
within
context
called
kth
former
child
introduced
product
node
λzs
respectively
sum
node
introduced
called
twin
sum
node
respect
denote
original
spn
called
link
steps
4–11
augmentspn
introduce
links
interconnected
sum
node
kth
child
link
single
parent
namely
simply
copies
former
child
steps
13–15
introduce
ivs
corresponding
associated
proposed
saw
fig
discussion
render
sum
nodes
incomplete
sums
clearly
conditioning
sums
scpsq
thus
necessary
introduce
twin
sum
node
steps
17–23
treat
problem
following
proposition
states
soundness
augmentation
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
proposition
let
spn
augpsq
zspsq
complete
decomposable
spn
s1pxq
spxq
proposition
states
marginal
distribution
augmented
spn
distribution
rep-
resented
original
spn
completely
spec-
iﬁed
probabilistic
model
thus
augmentation
provides
sound
way
generalize
interpretation
mixture
models
general
spns
example
augmentation
shown
fig
note
understand
augmentation
mainly
theoretical
tool
establish
work
interpreta-
tion
spns
cases
neither
necessary
advisable
explicitly
construct
augmented
spn
interesting
question
sizes
original
spn
augmented
spn
relate
lower
bound
|s1|
ωp|s|q
holding
e.g
spns
single
sum
node
asymptotic
upper
bound
|s1|
op|s|2q
see
note
introduction
links
ivs
twin
sums
cause
linear
increase
spn
size
number
edges
introduced
connecting
twins
links
conditioning
sums
bounded
|s|2
since
number
twins
links
bounded
|s|
there-
fore
|s1|
op|s|2q
asymptotic
upper
bound
indeed
achieved
certain
types
spns
consider
e.g
chain
consisting
sum
nodes
distribution
nodes
kth
sum
parent
1qth
sum
kth
distribution
sum
parent
last
two
distributions
kth
sum
preceding
sums
conditioning
sums
yielding
introduced
2´k
edges
i.e
case
|s1|
indeed
grows
quadratically
|s|
edges
total
gives
2pk
pk´1q
2.2
conditional
independencies
augmented
spns
probabilistic
interpretation
sum-weights
helpful
introduce
notion
conﬁgured
spns
takes
similar
role
conditioning
literature
dnnfs
deﬁnition
conﬁgured
spn
let
spn
zspsq
valpyq
conﬁgured
spn
obtained
deleting
ivs
corresponding
link
yry
augpsq
deleting
nodes
rendered
unreachable
root
intuitively
conﬁgured
spn
isolates
computa-
tional
structure
selected
sum
edges
sur-
vive
conﬁgured
spn
equipped
weights
augmented
spn
therefore
conﬁgured
spn
general
locally
normalized
note
following
properties
conﬁgured
spns
proposition
let
spn
zspsq
zspsqzy
let
valpyq
let
augpsq
holds
node
scope
corresponding
node
complete
decomposable
spn
xyyyz
node
scpnq
fig
augmentation
spn
example
spn
tx1
x3u
containing
sum
nodes
augmented
spn
containing
ivs
corresponding
zs1
zs2
zs3
zs4
links
twin
sum
nodes
¯s2
¯s3
¯s4
nodes
introduced
augmentation
smaller
circles
used
valpyq
holds
ypx
y1q
s1px
y1q
otherwise
next
theorem
shows
certain
conditional
independen-
cies
augmented
spn
ease
discussion
make
following
deﬁnitions
deﬁnition
let
sum
node
spn
associated
rvs
model
rvs
lvs
divided
three
sets
parents
lvs
i.e
zancspsqzzs
children
model
rvs
lvs
i.e
scpsq
zdescspsqzzs
non-descendants
remaining
rvs
i.e
zspsqqzpzp
zsq
show
parents
children
non-
descendants
play
likewise
role
independencies
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
contained
scope
corresponding
sum
node
model
rvs
lvs
unconnected
among
respectively
constrain
twin-weights
equal
sum-weights
see
becomes
independent
special
choice
twin
weights
effectively
removes
edges
lvs
recov-
ering
structure
next
section
use
augmented
spn
interpretation
derive
algorithm
spns
algorithm
algorithm
general
scheme
maximum
like-
lihood
learning
rvs
complete
evidence
missing
thus
augmented
spns
amenable
due
lvs
associated
sum
nodes
moreover
twin-weights
kept
ﬁxed
applied
augmented
spns
actually
optimizes
weights
original
spn
approach
already
pointed
suggested
evidence
marginal
posteriors
given
bspeq
ppzs
eq9ws
bspeq
used
updates
updates
however
correct
ones
actually
leave
weights
unchanged
using
augmented
spns
formally
derive
standard
updates
sum-weights
input
distributions
chosen
exponential
family
3.1
updates
weights
assume
dataset
tep1q
eplqu
i.i.d
samples
eplq
combination
complete
partial
evidence
model
rvs
section
1.1.
let
zspsq
set
lvs
consider
arbitrary
sum
node
shows
weights
interpreted
conditional
probabilities
interpretation
s1pzs
¯wk
mentioned
twin-weights
¯wk
kept
ﬁxed
using
well-known
em-updates
bns
discrete
rvs
updates
sum-weight
given
summing
expected
statistics
s1pzs
eplqq
followed
renormalization
make
event
explicit
introducing
switching
parent
twin
sum
exists
assumes
two
states
valpysq
tys
y¯su
y¯s
twin
sum
exist
takes
single
value
valpysq
tysu
clearly
observed
renders
independent
switch-
ing
parent
explicitly
introduced
augmented
spn
depicted
fig
simply
introduce
two
new
ivs
λys
λys
y¯s
switch
on/off
output
respectively
easy
see
constantly
set
i.e
marginalized
augmented
spn
performs
exactly
computations
furthermore
easy
see
completeness
decomposability
augmented
spn
maintained
fig
dependency
structure
augmented
spn
fig
repre-
sented
bns
i.e
independent
given
show
sum-weights
conditional
distribution
conditioned
event
select
path
one
problem
original
interpretation
conditional
distribution
speciﬁed
complementary
event
show
twin-weights
precisely
conditional
distribution
requires
event
select
path
twin
indeed
complementary
event
select
path
shown
following
lemma
lemma
let
spn
let
sum
node
parents
valpzpq
conﬁgured
spn
contains
either
twin
ready
state
theorem
concerning
conditional
independencies
augmented
spns
theorem
let
spn
augpsq
let
arbitrary
sum
¯wk
¯ws
respect
let
parents
children
non-descendants
respectively
exists
two-partition
valpzpq
i.e
valpzpq
s1pzs
wks1pyn
s1pzs
¯wks1pyn
theorem
follows
weights
twin-
weights
sum
node
interpreted
conditional
probability
tables
cpts
conditioned
conditionally
independent
given
i.e
s1pzs
s1pzs
¯wk
using
result
deﬁne
representing
augmented
spn
follows
sum
node
connect
parents
rvs
scpsq
children
obtain
representation
augmented
spn
serving
useful
tool
understand
spns
context
probabilistic
graphical
models
example
interpretation
shown
fig
note
representation
zhao
recovered
representation
augmented
spns
proposed
representation
spns
using
bipartite
structure
parent
model
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
λzs
λzs
λzs
looooomooooon
λys
y¯s
λys
λzs
λzs
λzs
looooomooooon
fig
explicitly
introducing
switching
parent
augmented
spn
part
augmented
spn
containing
sum
node
three
children
twin
explicitly
introduced
switching
parent
using
ivs
λys
λys
y¯s
switching
parent
introduced
using
switch-
ing
parent
required
expected
statistics
translate
s1pzs
eplqq
compute
use
differential
approach
also
section
1.1.
first
note
s1pzs
eplqq
b2s1peplqq
bλys
bλzs
ﬁrst
derivative
given
bs1peplqq
bλys
bs1peplqq
bs1peplqq
speplqq
λzs
speplqq
common
product
parent
λys
augmented
spn
see
fig
differentiating
λzs
yields
second
derivative
b2s1peplqq
bλys
ysbλzs
bs1peplqq
speplqq
delivering
required
posteriors
s1pzs
eplqq
bs1peplqq
s1peplqq
speplqq
want
construct
augmented
spn
explicitly
express
terms
original
spn
since
lvs
marginalized
holds
s1peplqq
speplqq
1peplqq
bspeplqq
yielding
s1pzs
eplqq
bspeplqq
speplqq
speplqq
delivering
required
statistics
updating
sum-
weights
turn
updates
input
distri-
butions
3.2
updates
input
distributions
simplicity
derive
updates
univariate
input
distri-
butions
i.e
distributions
|scpdyq|
similar
updates
rather
easily
derived
also
multi-
variate
input
distributions
so-called
distribution
selectors
dss
introduced
derive
differential
approach
generalized
spns
similar
switching
parents
twin
sum
nodes
dss
rvs
render
respective
model
rvs
independent
remaining
rvs
formally
let
set
input
distributions
scope
txu
assume
arbitrary
ﬁxed
ordering
let
rdxs
index
ordering
let
discrete
|dx
states
so-called
gated
spn
obtained
replacing
distribution
product
node
λwx
rdx
introduced
product
denoted
gate
shown
rendered
independent
rvs
spn
conditioned
moreover
conditional
distribution
given
rdx
therefore
incorporated
two
family
interpretation
input
distribution
chosen
exponential
family
natural
parameters
θdx
m-step
given
expected
sufﬁcient
statistics
θdx
gpwx
eplqqş
eplqqθdx
pxqdx
integral
eplqqθdx
pxqdx
reduces
rdx
eplq
contains
complete
evidence
θdx
px1q
eplq
contains
partial
evidence
gpwx
eplqq
eplqqθdx
pxqdx
pxqθdx
pxqdx
pxqdx
depending
type
evaluating
less
demanding
simple
practical
case
gaussian
interval
permitting
closed
form
solution
integrating
gaussian
statistics
θpxq
x2q
using
truncated
gaussians
obtain
posteriors
gpwx
eplqq
required
use
differential
approach
note
gpwx
eplqq
gpeplqq
bλwx
gpeplqq
peplqq
rdx
gate
want
construct
gated
spn
explicitly
use
identity
gpeplqq
thus
required
posteriors
given
bspeplqq
bdx
gpwx
eplqq
bspeplqq
speplqq
bdx
peplqq
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
initialize
input
distributions
converged
spsq
chpsq
θdx
ndx
input
eplq
evaluate
upward-pass
backprop
backward-pass
spsq
chpsq
procedure
expectation-maximization
complete
evidence
θpxq
end
eplq
complete
w.r.t
partial
evidence
else
pxqθpxqdx
pxqdx
bdx
end
θdx
θdx
ndx
ndx
end
end
spsq
chpsq
set
parameters
řc1
pchpsq
θdx
ndx
end
return
end
procedure
fig
pseudo-code
algorithm
spns
algorithm
spns
sum-weights
input
distributions
summarized
fig
section
5.1
empirically
verify
derivation
show
standard
successfully
trains
spns
suitable
structure
hand
note
recently
zhao
poupart
derived
concave-convex
procedure
cccp
yield
sum-weight
updates
algorithm
presented
result
surprising
cccp
rather
different
approaches
general
probable
explanation
spns
applied
reconstructing
data
using
mpe
inference
given
distribution
evidence
mpe
formalized
ﬁnding
ppxq
assume
actually
arg
max
xpe
maximum
mpe
special
case
map
deﬁned
ﬁnding
arg
max
yperys
şerzs
ppy
two-partition
i.e
mpe
map
generally
np-hard
bns
map
inher-
ently
harder
mpe
using
result
follows
map
inference
np-hard
also
spns
particular
theorem
shows
decision
version
map
np-complete
naive
bayes
model
class
variable
marginalized
naive
bayes
represented
augmentation
spn
single
sum
node
representing
class
variable
therefore
map
spns
generally
np-hard
since
map
augmented
spn
representing
naive
bayes
model
corresponds
mpe
inference
original
spn
i.e
mixture
model
follows
also
mpe
inference
generally
np-hard
spns
proof
tailored
spns
found
however
considering
sub-class
selective
spns
section
1.1
mpe
solution
obtained
using
viterbi-style
backtracking
algorithm
max-product
networks
deﬁnition
max-product
network
let
spn
deﬁne
max-product
network
mpn
replacing
distribution
node
maximizing
distribution
node
hscpdq
ˆdpyq
max
ypy
dpyq
sum
node
max
node
max
ˆcpchpˆsq
wˆs
product
node
corresponds
product
node
theorem
let
selective
spn
let
corresponding
mpn
let
node
corresponding
node
every
hscpnq
ˆnpx
max
xpx
npxq
theorem
shows
mpn
maximizes
prob-
ability
corresponding
selective
spn
proof
see
appendix
also
shows
actually
ﬁnd
maximizing
assignment
product
maximizing
assignment
given
combining
maximizing
assignments
children
sum
maximizing
assignment
given
maximizing
assignment
single
child
whose
weighted
maximum
maximal
among
children
children
maxima
readily
given
upwards
pass
mpn
thus
ﬁnding
maximizing
assignment
node
selective
spn
recursively
reduces
ﬁnding
maximizing
assignments
children
node
accomplished
viterbi-like
backtracking
proce-
dure
algorithm
denoted
mpeselective
shown
fig
denotes
queue
nodes
denote
en-queue
de-queue
operations
respectively
note
theorem
already
derived
special
case
namely
arithmetic
circuits
representing
network
polynomials
bns
discrete
rvs
direct
corollary
theorem
mpe
inference
tractable
augmented
spns
since
augmented
spns
selective
spns
easily
seen
augmentspn
sum
exactly
one
set
causing
one
child
non-zero
therefore
use
mpeselective
augmented
spns
order
ﬁnd
mpe
solution
model
rvs
lvs
note
mpe
solution
augmented
spn
general
correspond
mpe
solution
original
spn
discarding
states
lvs
however
procedure
frequently
used
approximation
models
mpe
tractable
model
rvs
lvs
model
rvs
alone
mpeselective
applied
original
spns
augmented
spns
also
goal
recover
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
procedure
mpeselective
initialize
zero-vector
length
|x|
evaluate
corresponding
mpn
upwards
pass
root
node
mpn
empty
max
node
arg
max
ˆcpchpˆnq
wˆn
else
product
node
chp
ˆnq
else
maximizing
distribution
node
corresponding
distribution
node
x˚rscpnqs
arg
max
xperscpnqs
npxq
end
end
return
end
procedure
fig
pseudo-code
mpe
inference
selective
spns
mpe
solution
model
rvs
lvs
states
lvs
assigned
max-backtracking
sum-
children
states
one-to-one
correspondence
states
lvs
whose
sums
visited
backtracking
assigned
causes
confusion
since
lvs
appear
undeﬁned
contexts
illustrations
section
however
since
algorithm
used
approximation
mpe
model
rvs
discarding
states
lvs
situation
paid
attention
nevertheless
show
applying
mpeselec-
tive
original
non-selective
spns
effectively
simulates
mpeselective
corresponding
augmented
spn
thereby
however
deterministic
twin-weights
implicitly
assumed
i.e
twin-weights
except
single
see
let
modify
mpeselective
applied
original
spn
returning
mpe
solution
corresponding
augmented
spn
first
note
augmented
mpn
every
twin
node
simply
outputs
maximal
twin-weight
among
children
whose
states
contained
evidence
twin
node
let
maximal
weight
denoted
ˆw¯s
effect
twin
nodes
simulated
original
spn
replacing
weight
original
spn
˜ws
˜ws
correction
factor
given
˜ws
ś¯s
ˆw¯s
product
runs
twins
sums
conditioning
sum
using
corrected
weights
max
node
corresponding
mpn
gets
input
mpn
augmented
spn
i.e
twin
nodes
simulated
identify
maximizing
states
lvs
whose
sums
visited
backtracking
states
sums
visited
given
child
correspond
maximal
twin-weight
ˆw¯s
pseudo-code
somewhat
technical
modiﬁcation
mpeselective
found
see
algorithm
used
essentially
equiv-
alent
mpeselective
augmented
spns
˜ws
sum
nodes
implies
twin-weights
illustration
fig
low-depth
bias
using
spn
rvs
tx1
x3u
structure
introduced
augmentation
depicted
small
nodes
edges
deterministic
twin-weights
used
state
zs1
corresponding
preferred
since
probabilities
dampened
weights
respectively
deterministic
therefore
although
model
completely
speciﬁed
shown
viterbi-
like
algorithm
recovers
mpe
solution
nevertheless
corresponds
mpe
inference
augmented
spn
special
twin-weights
i.e
deterministic
weights
however
using
deterministic
twin-weights
rather
unnatural
choice
since
prefers
one
arbitrary
state
others
cases
actually
rendered
irrelevant
case
mpe
inference
also
bias
towards
less
structured
sub-models
call
low-
depth
bias
illustrated
fig
shows
spn
three
rvs
augmented
spn
two
twin
sum
nodes
¯s2
¯s3
corresponding
respectively
twin-weights
deterministic
selection
state
zs1
biased
towards
state
corresponding
distribution
assuming
inde-
pendence
among
comes
fact
values
dampened
weights
respectively
generally
smaller
therefore
using
deterministic
weights
twin
sum
nodes
introduce
bias
towards
selection
sub-
spns
less
deep
less
structured
using
uniform
weights
twin
sum
nodes
somewhat
fairer
since
case
gets
dampened
¯s2
¯s3
¯s3
¯s2
uniform
weights
extend
opposite
choice
deterministic
twin-weights
former
represent
strongest
possible
dampening
via
twin-weights
therefore
actually
penalize
less
structured
distributions
investigating
effects
subject
future
work
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
experiments
5.1
experiments
algorithm
-1000
spns
applied
image
data
generic
architecture
reminiscent
convolutional
neural
networks
proposed
refer
architecture
archi-
tecture
standard
used
experiments
two
reasons
first
explicitly
constructing
proposed
structure
train
standard
hardly
possible
current
hardware
since
number
nodes
grows
opl3q
square-length
modeled
image
domain
pixels
instead
sparse
hard
algorithm
used
virtualizes
structure
i.e
sum
products
generated
see
details
second
using
standard
seemed
unsuited
train
large
dense
spns
either
trapped
local
optima
due
gradient
vanishing
phenomenon
experiments,2
investigated
three
questions
derivation
correct
complete
missing
data
result
hard
improved
standard
given
suited
sparse
structure
yield
good
solution
parameters
question
important
since
original
derivation
con-
tained
error
questions
concerned
general
applicability
training
spn
used
datasets
spn
structures
obtainable
datasets
comprise
caltech-101
inclusive
background
class
orl
face
images
i.e
total
103
datasets
input
distributions
spns
single-dimensional
gaussians
pixel
means
set
averages
4-quantiles
variances
constantly
ran
fig
iterations
various
settings
update
combination
three
different
types
parameters
i.e
sum-weights
gaussian
means
gaussian
variances
set
parameters
types
encoded
string
letters
weights
means
variances
combinations
use
original
parameters
initialization
obtained
use
random
initialization
sum-
weights
drawn
dirichlet
distribution
uniform
hyper-parameter
i.e
uniform
dis-
tribution
standard
simplex
gaussian
means
uniformly
drawn
r´1
gaussian
variances
r0.01
parameters
actually
updated
initialized
randomly
otherwise
original
parameters
used
kept
ﬁxed
combinations
use
complete
data
missing
training
data
ran-
domly
discarding
observations
independently
sample
combinations
thus
total
ran
103
8652
times
yielding
259560
em-iterations
avoid
pathological
solu-
tions
used
lower
bound
0.01
gaussian
vari-
ances
iteration
observed
decreasing
likelihood
code
available
wmv
wmv
-2000
-3000
-4000
-5000
-6000
-7000
iteration
-5000
-5500
-6000
-6500
iteration
fig
normalized
log-likelihood
em-iterations
averaged
103
datasets
random
initializations
training
set
test
set
curves
outside
displayed
region
better
readability
curves
start
approximately
´8000
nats
decreased
approximately
´11000
nats
training
set,3
i.e
derived
algorithm
showed
monotonicity
experiments
moreover
seen
fig
training
log-likelihood
actually
increased
iterations
curves
missing
data
scenarios
similar
gives
afﬁrmative
evidence
question
fig
shows
log-likelihood
test
set
note
optimizing
parameter
sets
led
severe
overﬁtting
achieving
extremely
high
likelihoods
training
set
achieved
extremely
poor
likelihoods
test
set
also
parameter
sets
wmv
tend
overﬁt
although
strong
regarding
question
closer
inspected
test
log-
likelihood
original
parameters
used
ini-
tialization
i.e
parameters
obtained
post-trained
using
table
summarizes
results
parameter
sets
including
gaussian
variances
optimized
i.e
test
log-likelihood
increased
time
i.e
83.5
92.23
datasets
furthermore
oracle
knowledge
ideal
number
iterations
i.e
column
best
average
log-likelihood
increased
0.58
1.39
relative
original
parameters
improvement
happens
ﬁrst
iteration
yielding
0.52
1.05
improvement
results
indicate
parameters
obtained
slightly
underﬁt
given
datasets
similar
fig
see
parameter
sets
including
gaussian
variances
except
tiny
occasional
decreases
always
10´8
converged
attributed
numerical
artifacts
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
table
changes
test
log-likelihoods
original
parameters
post-trained
using
inc.
percentage
datasets
log-likelihood
increased
ﬁrst
iteration
pos.
neg
relative
change
log-likelihood
averaged
datasets
datasets
positive
change
datasets
negative
change
respectively
table
differences
log-likelihood
ground-truth
mpe
solution
found
exhaustive
enumeration
averaged
100
independent
draws
sum-weights
numbers
parentheses
number
times
mpe
solution
found
results
augmented
spns
using
uniform
twin-weights
1st
iteration
best
inc.
pos
neg
pos
neg
91.26
-0.03
-0.21
83.50
-0.30
92.23
-31.93
39.81
-32.06
39.81
38.83
-37.25
-37.37
wmv
38.83
0.87
0.58
1.39
-13.45
-13.33
-17.21
-17.12
0.55
0.52
1.06
-13.47
-13.41
-17.24
-17.18
-0.03
-0.21
-0.30
-31.93
-32.06
-37.25
-37.37
0.61
0.67
1.18
14.44
14.79
14.27
14.63
0.96
0.73
1.53
14.51
14.98
14.35
14.78
rvs
rvs
rvs
0.5
1.0
2.0
0.5
1.0
2.0
0.5
1.0
2.0
mpedet
mpeuni
0.00
100
0.00
100
0.00
100
0.00
100
0.00
100
0.00
100
0.00
100
-0.10
0.00
100
-0.10
0.00
100
-0.11
-0.63
0.00
100
0.00
100
-0.85
-0.82
0.00
100
table
log-likelihoods
sum-weights
trained
using
random
initialization
percentage
data
sets
log-likelihood
larger
original
parameters
pos.
neg
relative
log-likelihood
w.r.t
original
parameters
data
sets
data
sets
relative
log-likelihood
positive/negative
respectively
similar
table
results
augmented
spns
using
deterministic
table
twin-weights
1st
iteration
best
pos
neg
pos
neg
train
70.87
test
41.75
0.68
-0.11
1.38
0.40
-1.00
-0.48
100.00
67.96
3.97
0.46
3.97
0.76
-0.18
wmv
prone
overﬁtting
datasets
decreased
test
log-likelihood
however
remaining
datasets
test
log-
likelihood
could
improved
substantially
least
average
turn
question
pointed
hard
variant
used
time
ﬁnds
effective
spn
structure
optimizing
using
random
initialization
amounts
using
oracle
structure
obtained
discarding
learned
parameters
dataset
selected
random
initialization
yielded
highest
likelihood
training
set
iteration
30.
run
compared
log-likelihoods
log-likelihoods
obtained
original
parameters
results
summarized
table
see
data
sets
log-likelihood
training
set
larger
original
parameters
also
case
individual
random
start
best
one
every
random
restart
always
yielded
higher
training
log-likelihood
original
parameters
thus
considering
actual
optimization
objective
likelihood
training
set
successfully
trains
spns
given
suited
oracle
structure
furthermore
seen
table
also
prone
overﬁtting
algorithm
67.96
datasets
delivered
higher
test
log-likelihood
original
parameters
using
oracle
knowledge
ideal
number
iterations
column
best
5.2
experiments
mpe
inference
illustrate
correctness
mpeselective
fig
applied
augmented
spns
generated
spns
using
architecture
arranging
binary
rvs
2ˆ2
3ˆ3
4ˆ4
grid
respectively
inputs
used
two
indicator
variables
representing
two
states
sum-weights
drawn
dirichlet
distribution
rvs
rvs
rvs
0.5
1.0
2.0
0.5
1.0
2.0
0.5
1.0
2.0
mpedet
mpeuni
0.00
100
0.00
100
0.00
100
0.00
100
0.00
100
0.00
100
-0.10
0.00
100
-0.12
0.00
100
-0.15
0.00
100
0.00
100
-0.89
-1.11
0.00
100
0.00
100
-1.01
uniform
α-parameters
t0.5
networks
drew
100
independent
parameters
sets
ran
mpeselective
augmented
spn
equipped
uniform
twin-weights
deterministic
twin-weights
uniform
twin-weights
denote
result
obtained
mpeselective
mpeuni
deter-
ministic
twin-weights
denote
result
mpedet
described
section
mpedet
corresponds
essentially
result
mpeselective
applied
original
spn
assignment
log-likelihoods
eval-
uated
augmented
spn
deterministic
weights
augmented
spn
uniform
weights
original
spn
discarding
states
lvs
additionally
found
ground
truth
mpe
assignments
two
augmented
spns
original
spn
using
exhaustive
enumeration
results
relative
ground
truth
mpe
solutions
shown
tables
seen
mpeuni
always
ﬁnds
mpe
solution
augmented
spn
uniform
twin-weights
mpedet
always
ﬁnds
mpe
solution
augmented
spns
deterministic
twin-weights
gives
empirical
evidence
correctness
mpeselec-
tive
mpe
inference
augmented
spns
furthermore
wanted
investigate
quality
algorithms
serving
approximation
mpe
inference
original
spns
spns
considered
mpedet
delivered
average
slightly
better
approx-
imations
mpeuni
however
results
interpreted
caution
due
rather
similar
nature
distributions
considered
closer
investigating
approximate
mpe
original
spns
interesting
di-
rection
subject
future
research
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
similar
table
results
original
spns
table
links
clearly
remain
decomposable
moreover
rendered
complete
rvs
rvs
rvs
0.5
1.0
2.0
0.5
1.0
2.0
0.5
1.0
2.0
mpedet
mpeuni
-0.06
-0.06
-0.09
-0.09
-0.10
-0.10
-0.38
-0.31
-0.48
-0.47
-0.40
-0.37
-1.04
-0.76
-1.18
-0.76
-0.67
-0.92
conclusion
paper
revisited
interpretation
spns
hierarchically
structured
model
pointed
original
approach
explicitly
incorporate
lvs
produce
sound
probabilistic
model
remedy
proposed
augmentation
spns
proved
soundness
model
within
augmented
spns
in-
vestigated
independency
structure
represented
showed
sum-weights
interpreted
structured
cpts
within
using
augmented
spns
derived
algorithm
sum-weights
single-
dimensional
input
distributions
exponential
families
mpe-inference
generally
np-hard
spns
showed
viterbi-style
backtracking
algorithm
recovers
mpe
solution
selective
spns
particular
aug-
mented
spns
experiments
give
empirical
evidence
supporting
theoretical
results
furthermore
showed
standard
successfully
train
generative
spns
given
suitable
network
structure
hand
appendix
proofs
a.1
proof
proposition
complete
decomposable
spn
s1pxq
spxq
immediate
computing
s1pxq
valpxq
done
marginalizing
i.e
setting
λzs
case
easy
see
none
structural
changes
modiﬁes
output
spn
i.e
outputs
agree
i.e
s1pxq
spxq
remains
show
complete
decompos-
able
root
scope
steps
4–11
augmentspn
introduce
links
representing
private
copies
sum
children
clearly
leave
spn
complete
decomposable
steps
13–15
introduced
scope
thus
scope
root
since
done
sum
nodes
introduced
root
scope
steps
13–15
render
products
non-
decomposable
since
would
imply
reachable
two
distinct
children
product
contradiction
fact
spn
decomposable
however
shown
fig
steps
13–15
render
ancestor
sums
incomplete
treated
steps
17–23
twin
sum
introduced
clearly
complete
scope
tzu
furthermore
incompleteness
conditioning
sum
caused
links
scope
scope
links
augmented
step
21.
a.2
proof
proposition
deleting
ivs
links
scopes
twin
sum
remains
since
complete
left
one
child
thus
also
scope
ancestor
remains
graph
rooted
acyclic
since
root
link
deleting
nodes
edges
introduce
cycles
deleted
also
link
deleted
internal
nodes
left
leaves
roots
point
xyyyz
scope
root
also
complete
decomposable
whenever
link
deleted
corresponding
sum
node
twin
sum
node
remain
trivially
complete
since
left
single
child
furthermore
com-
pleteness
decomposability
ancestor
¯sy
left
intact
since
neither
¯sy
changes
scope
according
point
scope
since
scpnq
disconnected
ivs
deleted
links
descendants
i.e
descendants
disconnected
conﬁguration
since
present
must
still
reachable
root
therefore
also
descendants
reachable
i.e
input
ﬁxed
ivs
links
deleted
conﬁgured
spn
evaluate
zero
augmented
spn
outputs
sums
twin
sums
therefore
therefore
also
output
nodes
remains
includes
root
therefore
ypx
s1px
must
y1ry
deleted
i.e
y1ry
descpnq
root
using
lemma
follows
ypx
y1q
a.3
proof
lemma
must
contain
either
since
scope
root
proposition
show
let
denote
set
paths
length
root
node
scpnq
paths
constructed
extending
path
πk´1
child
path
last
node
scope
let
smallest
number
path
containing
show
induction
|πk|
note
contains
single
path
pnq
root
therefore
induction
basis
holds
induction
step
show
given
|πk´1|
also
|πk|
let
pn1
nk´1q
single
path
πk´1
nk´1
product
node
single
child
scpcq
due
decomposability
nk´1
sum
node
must
ancspsqztsu
therefore
single
child
conﬁgured
spn
therefore
single
way
extend
path
therefore
|πk|
single
path
either
lead
since
descp¯sq
descpsq
contains
single
path
one
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
a.4
proof
theorem
lemma
valpzpq
conﬁgured
spn
contains
either
let
subset
valpzpq
subset
valpzpq
fix
want
compute
s1pzs
i.e
marginalize
according
proposition
equals
zpzs
according
proposi-
tion
sub-spn
rooted
former
child
since
locally
normalized
sub-
spn
also
locally
normalized
since
scope
former
child
sub-set
marginalized
λzs
link
outputs
since
λzs
sum
outputs
consider
set
nodes
clearly
since
scope
including
λzs
set
must
ancpsq
let
topologically
ordered
list
ancpsq
root
let
scpnlq
scpnlq
show
induction
nlpzs
zrzlsq
nlpyn
zrzlsq
since
yn,1
n1pzs
induction
basis
holds
assume
holds
nl´1
sum
due
completeness
nlpzs
zrzlsq
ÿcpchpnlq
wnl
cpyn
zrzlsq
nlpyn
zrzlsq
i.e
induction
step
holds
sums
product
due
decomposability
must
single
child
scope
hence
child
must
node
ancpsq
nlpzs
zrzlsq
nmpyn
zrzmsq
źcpchpnlqznm
nlpyn
zrzlsq
cpyn
scpcqq
i.e
induction
step
holds
products
therefore
induction
also
holds
root
follows
show
twin
sum
exist
empty
holds
trivially
otherwise
input
clearly
outputs
¯wk
shown
similar
way
a.5
proof
theorem
prove
theorem
using
inductive
argument
theorem
clearly
holds
deﬁnition
consider
product
assume
theorem
holds
chpˆpq
theorem
also
holds
since
ˆppx
źcpchpˆpq
max
xpx
cpxq
max
xpx
źcpchpˆpq
cpxq
max
xpx
ppxq
max
product
switched
due
decomposability
consider
max
node
corresponding
sum
node
let
support
spn-node
set
supn
npxq
since
selective
support
partitioned
supports
children
i.e
sups
ťcpchpsq
supc
supc1ş
supc2
assuming
theorem
holds
chpˆsq
ˆspx
max
cpchpsq
max
cpchpsq
max
cpchpsq
max
max
xpx
cpxq
max
cpxq
xpsupc
max
xpsupc
cpxq
xpsups
spxq
max
xpx
spxq
slight
abuse
notation
actually
use
suprema
sets
supc
deﬁne
supremum
empty
set
used
fact
support
sum
node
partitioned
supports
children
selective
sums
whenever
single
child
see
induction
step
also
holds
therefore
theorem
holds
nodes
acknowledgments
would
like
thank
anonymous
reviewers
constructive
comments
work
supported
austrian
science
fund
fwf
p25244-n15
austrian
science
fund
fwf
p27803-n15
research
partly
funded
onr
grant
n00014-16-1-2697
afrl
contract
fa8750-13-2-0019
references
poon
domingos
sum-product
networks
new
deep
architecture
proceedings
uai
2011
337–346
gens
domingos
learning
structure
sum-product
networks
proceedings
icml
2013
873–880
dennis
ventura
learning
architecture
sum-
product
networks
using
clustering
variables
proceedings
nips
2012
2042–2050
peharz
geiger
pernkopf
greedy
part-wise
learning
sum-product
networks
proceedings
ecml/pkdd
vol
8189.
springer
berlin
2013
612–627
amer
todorovic
sum-product
networks
modeling
activities
stochastic
structure
proceedings
cvpr
2012
1314–1321
gens
domingos
discriminative
learning
sum-
product
networks
proceedings
nips
2012
3248–3256
peharz
kapeller
mowlaee
pernkopf
modeling
speech
sum-product
networks
application
bandwidth
extension
proceedings
icassp
2014
3699–3703
cheng
kok
pham
chieu
chai
language
modeling
sum-product
networks
proceedings
interspeech
2014
2098–2102
z¨ohrer
peharz
pernkopf
representation
learning
single-channel
source
separation
bandwidth
extension
ieee/acm
transactions
audio
speech
language
processing
vol
2398–2409
2015
dempster
laird
rubin
maximum
likelihood
incomplete
data
via
algorithm
journal
royal
statistical
society
series
vol
1–38
1977
ghahramani
jordan
supervised
learning
incom-
plete
data
via
approach
proceedings
nips
1994
120–127
s.-w.
lee
watkins
zhang
non-parametric
bayesian
sum-product
networks
icml
workshop
learning
tractable
probabilistic
models
2014
trapp
peharz
skowron
madl
pernkopf
trappl
structure
inference
sum-product
networks
using
inﬁnite
sum-product
trees
nips
workshop
practical
bayesian
nonparametrics
2016.
ieee
transactions
pattern
analysis
machine
intelligence
accepted
pre-print
version
october
2016
pearl
probabilistic
reasoning
intelligent
systems
networks
san
francisco
usa
morgan
kaufmann
plausible
inference
publishers
inc.
1988
koller
friedman
probabilistic
graphical
models
principles
techniques
mit
press
2009
darwiche
differential
approach
inference
bayesian
networks
journal
acm
vol
280–305
2003
peharz
tschiatschek
pernkopf
domingos
theoretical
properties
sum-product
networks
proceedings
aistats
2015
744–752
campos
new
complexity
results
map
bayesian
networks
proceedings
ijcai
2011
2100–2106
peharz
foundations
sum-product
networks
probabilis-
tic
modeling
ph.d.
dissertation
graz
university
technology
2015
peharz
gens
domingos
learning
selective
sum-
product
networks
icml
workshop
learning
tractable
proba-
bilistic
models
2014
zhao
melibari
poupart
relationship
be-
tween
sum-product
networks
bayesian
networks
proceed-
ings
icml
2015
116–124
darwiche
compiling
knowledge
decomposable
negation
normal
form
proceedings
ijcai
1999
284–289
decomposable
negation
normal
form
journal
acm
vol
608–647
2001
darwiche
marquis
knowledge
compilation
map
journal
artiﬁcial
intelligence
research
vol
229–264
2002
darwiche
logical
approach
factoring
belief
networks
proceedings
2002
409–420
lowd
domingos
learning
arithmetic
circuits
proceedings
uai
2008
383–392
boutilier
friedman
goldszmidt
koller
context-
speciﬁc
independence
bayesian
networks
proceedings
uai
1996
115–123
lowd
rooshenas
learning
markov
networks
arithmetic
circuits
proceedings
aistats
2013
406–414
rooshenas
lowd
learning
sum-product
networks
direct
indirect
variable
interactions
proceedings
icml
2014
710–718
adel
balduzzi
ghodsi
learning
structure
sum-product
networks
via
svd-based
algorithm
proceed-
ings
uai
2015
32–41
vergari
mauro
esposito
simplifying
regulariz-
ing
strengthening
sum-product
network
structure
learning
proceedings
ecml/pkdd
2015
343–358
delalleau
bengio
shallow
vs.
deep
sum-product
net-
works
proceedings
nips
2011
666–674
friesen
domingos
sum-product
theorem
foun-
dation
learning
tractable
models
proceedings
icml
2016
1909–1918
johnson
kotz
balakrishnan
continuous
univariate
distributions
wiley
1994
zhao
poupart
uniﬁed
learning
http
//arxiv.org/abs/1601.00318
parameters
sum-product
approach
networks
robert
peharz
received
msc
degree
computer
engineering
ph.d
degree
electrical
engineering
graz
university
technology
main
research
interest
lies
machine
learning
particular
probabilistic
modeling
applications
signal
process-
ing
speech
audio
processing
com-
puter
vision
currently
research
idn
interdisciplinary
developmental
neu-
unit
roscience
medical
university
graz
applying
machine
learning
techniques
detect
early
markers
neurological
conditions
infants
funded
biotechmed-graz
cooperation
interdisciplinary
network
major
universities
graz
focus
basic
bio-medical
research
technological
development
medical
applications
robert
gens
received
s.b
degree
elec-
trical
engineering
computer
science
massachusetts
institute
technology
cam-
bridge
usa
2009
m.sc
degree
computer
science
engineering
university
washington
seattle
usa
2012.
summer
2014
re-
search
intern
microsoft
research
redmond
usa
currently
ph.d.
student
computer
science
engineering
uni-
versity
washington
seattle
usa
supported
2014
google
ph.d.
fellowship
deep
learning
mr.
gens
recipient
outstanding
student
paper
award
neural
information
processing
systems
conference
2012.
franz
pernkopf
received
msc
dipl
ing
degree
electrical
engineering
graz
uni-
versity
technology
austria
summer
1999.
earned
ph.d
degree
university
leoben
austria
2002.
2002
awarded
erwin
schr¨odinger
fellowship
research
associate
department
electrical
engineering
university
wash-
ington
seattle
2004
2006.
currently
associate
professor
laboratory
signal
processing
speech
communication
graz
university
technology
austria
research
interests
include
machine
learning
discriminative
learning
graphical
models
feature
selection
ﬁnite
mixture
models
image-
speech
processing
applications
bodlaender
van
den
eijkhof
van
der
gaag
complexity
mpa
problem
probabilistic
networks
proceedings
ecai
2002
675–679
park
darwiche
complexity
results
approxima-
tion
strategies
map
explanations
journal
artiﬁcial
intelli-
gence
research
vol
101–133
2004
kwisthout
probable
explanations
bayesian
networks
complexity
tractability
international
journal
approximate
reasoning
vol
1452–1469
2011
darwiche
modeling
reasoning
bayesian
networks
cambridge
university
press
2014
http
//alchemy.cs.washington.edu/spn
online
http
//spn.cs.washington.edu/pubs.shtml
https
//www.spsc.tugraz.at/tools
online
fei-fei
fergus
perona
learning
generative
visual
models
training
examples
incremental
bayesian
approach
tested
101
object
categories
computer
vision
image
understanding
vol
106
59–70
2007
samaria
harter
parameterisation
stochastic
model
human
face
identiﬁcation
proceedings
2nd
ieee
workshop
applications
computer
vision
1994
138–142
pedro
domingos
professor
computer
science
university
washington
author
master
algorithm
winner
sigkdd
innovation
award
highest
honor
data
science
fellow
as-
sociation
advancement
artiﬁcial
intelli-
gence
received
fulbright
scholarship
sloan
fellowship
national
science
foun-
dations
career
award
numerous
best
paper
awards
received
ph.d.
university
california
irvine
author
co-author
200
technical
publications
held
visiting
positions
stanford
carnegie
mellon
mit
co-founded
international
machine
learning
society
2001.
research
spans
wide
variety
topics
machine
learning
artiﬁcial
intelligence
data
science
including
scaling
learning
algorithms
big
data
maximizing
word
mouth
social
networks
unifying
logic
probability
deep
learning
