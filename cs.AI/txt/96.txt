near-optimal
active
learning
halfspaces
via
query
synthesis
noisy
setting
lin
chen1
hamed
hassani3
amin
karbasi1
1department
electrical
engineering
2yale
institute
network
science
yale
university
3computer
science
department
eth
zürich
lin.chen
amin.karbasi
yale.edu
hamed
inf.ethz.ch
abstract
paper
consider
problem
actively
learning
linear
classiﬁer
query
synthesis
learner
construct
artiﬁcial
queries
order
estimate
true
deci-
sion
boundaries
problem
recently
gained
lot
in-
terest
automated
science
adversarial
reverse
engineer-
ing
heuristic
algorithms
known
applications
queries
constructed
novo
elicit
in-
formation
e.g.
automated
science
evade
detection
minimal
cost
e.g.
adversarial
reverse
engineering
de-
velop
general
framework
called
dimension
coupling
reduces
d-dimensional
learning
problem
d−1
low-
dimensional
sub-problems
solves
sub-problem
efﬁ-
ciently
appropriately
aggregates
results
outputs
linear
classiﬁer
provides
theoretical
guarantee
possible
schemes
aggregation
proposed
method
proved
resilient
noise
show
frame-
work
avoids
curse
dimensionality
computational
complexity
scales
linearly
dimension
moreover
show
query
complexity
near
optimal
within
constant
factor
optimum
algorithm
sup-
port
theoretical
analysis
compare
performance
existing
work
observe
consistently
outperforms
prior
arts
terms
query
complexity
often
running
orders
magnitude
faster
introduction
passive
model
supervised
learn-
contrast
ing
labels
provided
without
in-
teractions
learning
mechanism
key
insight
learning
algorithm
active
learning
perform
signiﬁcantly
better
allowed
choose
data
points
label
approach
found
far-
reaching
applications
including
classical
problems
e.g.
classiﬁcation
tong
koller
2002
infor-
mation
retrieval
tong
chang
2001
speech
recogni-
tion
hakkani-tur
riccardi
gorin
2002
well
modern
ones
e.g.
interactive
recommender
systems
karbasi
ioannidis
massoulie
2012
optimal
deci-
sion
making
javdani
2014
applica-
tions
unlabeled
data
usually
abundant
easy
obtain
training
labels
either
time-consuming
ex-
pensive
acquire
require
asking
expert
throughout
paper
objective
actively
learn
unknown
halfspace
⟨h∗
via
query
synthesis
a.k.a
membership
queries
denotes
standard
inner
product
euclidean
space
unit
normal
vector
halfspace
want
learn
would
like
note
learning
halfspace
mathematically
equivalent
learning
unit
normal
vector
therefore
focus
learning
hereinafter
addition
noted
using
kernel
trick
easily
extend
halfspace
learn-
ing
complex
e.g.
non-linear
decision
boundaries
shawe-taylor
cristianini
2004
hypothesis
space
consists
possibilities
unit
normal
vectors
unit
sphere
sd−1
x
denotes
standard
euclidean
norm
active
learning
halfspaces
via
query
synthesis
algorithm
allowed
query
whether
point
resides
true
halfspace
algorithm
queries
true
outcome
sign
⟨h∗
sign
⟨h∗
means
otherwise
note
information
obtain
query
sign
inner
product
rather
value
example
queries
form
sign
⟨h∗
ei⟩
ith
standard
basis
vector
reveal
sign
ith
component
nothing
value
noiseless
setting
observe
true
outcome
query
i.e
sign
⟨h∗
noisy
setting
outcome
ﬂipped
version
true
sign
indepen-
dent
ﬂip
probability
denoting
outcome
sign⟨h∗
1~2
unit
sphere
sd−1
x
hence
term
sd−1
query
space
given
would
like
seek
active
learning
al-
gorithm
adaptively
selects
vectors
observes
noisy
responses
query
sign⟨h∗
xi⟩
mate
thatˆh
h∗
probability
least
since
length
selected
vector
affect
outcome
query
query
points
iii
outputs
using
queries
possible
esti-
main
contribution
paper
develop
noise
resilient
active
learning
algorithm
access
noisy
membership
queries
best
knowl-
edge
ﬁrst
show
near-optimal
algorithm
outperforms
theory
practice
naive
repe-
tition
mechanism
recent
spectral
heuristic
meth-
ods
alabdulmohsin
gao
zhang
2015
speciﬁcally
develop
framework
called
dimension
coupling
following
guarantees
query
complexity
log
log
log
log
plexity
log
computational
complexity
particular
noiseless
setting
note
settings
com-
computational
complexity
query
com-
putational
complexity
scales
linearly
dimension
moreover
query
complexity
settings
near-
optimal
empirical
experiments
demonstrate
runs
orders
magnitude
faster
existing
methods
rest
paper
structured
follows
section
start
investigating
problem
2-dimensional
case
present
algorithm
called
dc2
section
generalize
d-dimensional
case
present
gen-
eral
framework
called
empirical
results
shown
section
extensively
review
related
literature
sec-
tion
dc2
solving
2-dimensional
problem
gain
intuition
studying
general
dimensional
problem
might
beneﬁcial
study
spe-
cial
case
dimension
two
words
study
section
learn
normalized
pro-
jection
true
unit
normal
vector
onto
span
two
orthonormal
vec-
tors
span
linear
subspace
spanned
note
underlying
space
still
d-dimensional
i.e.
goal
learn
per
normalized
projection
onto
2-dimensional
subspace
formally
given
two
orthonormal
vectors
denote
normalized
projection
onto
span
i.e.
⟨h∗
e1⟩
+⟨h∗
e2⟩
⟨h∗
e1⟩
+⟨h∗
e2⟩
e22
objective
ﬁnd
unit
vector
span
ˆe
h⊥
fact
require
latter
hold
probability
least
emphasize
noise
characterized
inde-
pendent
ﬂip
probability
generally
present
dimensional
problem
one
may
propose
use
sim-
ple
binary
search
detailed
discussion
examples
presented
appendix
ﬁnd
unit
vector
re-
sides
ǫ-close
make
noise-tolerant
binary
search
algorithm
queries
point
say
query
times
obtain
noisy
versions
sign⟨h∗
xi⟩
view
majority
vote
noisy
versions
true
outcome
kääriäinen
2006
karp
kleinberg
2007
nowak
2011
call
method
repetitive
querying
however
query
complexity
log
1~ǫ
log
log
1~ǫ
log
1~δ
suboptimal
theoretically
prove
bound
appendix
empirically
referred
repetitive-dc
section
result
instead
present
bayesian
algorithm
termed
dc2
solves
2-dimensional
problem
query
complexity
log
1~ǫ
log
1~δ
recall
unit
vector
inside
span
e.g.
equivalently
algorithm
dc2
input
orthonormal
vectors
estimation
error
success
probability
least
end
output
unit
vector
estimate
nor-
find
vector
solution
fol-
malized
orthogonal
projection
onto
span
set
uniform
i.e.
1~2π
lowing
equation
sign⟨x
pm−1
ask
oracle
value
sign⟨xm
h∗⟩
date
distribution
pm−1
based
response
obtained
oracle
up-
multiple
solutions
choose
one
arbitrarily
return
arg
maxh∈s
ptǫ
represented
pair
two-dimensional
unit
circle
e.g.
c1e1
c2e2
notation
use
point
corresponding
unit
vector
c1e1
c2e2
interchangeably
setting
easy
see
span
sign⟨x
h∗⟩
sign⟨x
h⊥⟩
simplify
take
bayesian
approach
beginning
queries
performed
dc2
assumes
prior
infor-
mation
vector
therefore
takes
uniform
distribution
pdf
prior
belief
performing
query
posterior
belief
updated
according
observation
let
denote
pdf
posterior
perform-
ing
ﬁrst
queries
manner
dc2
runs
total
rounds
round
speciﬁc
query
selected
posed
oracle
number
speciﬁed
later
see
theorem
upon
completion
round
algorithm
returns
ﬁnal
output
vector
maximises
posterior
pdf
ptǫ
multiple
maximisers
picks
one
arbitrarily
proceed
detailed
description
dc2
formal
description
provided
algorithm
shown
algorithm
round
say
round
algorithm
maintains
updates
distribution
encodes
current
belief
true
location
note
distributions
stored
ef-
ﬁciently
result
vector
xm+1
computed
efﬁciently
indeed
pdf
piecewise
constant
unit
circle
see
figure
precisely
round
points
u2m
or-
dered
clock-wise
unit-circle
constant
restricted
sectors
ui+1
piecewise
constant
property
pdf
established
induction
recall
initial
distribution
uni-
form
thus
piecewise
constant
bayesian
update
step
line
algorithm
preserves
property
al-
gorithm
updates
distribution
pm+1
show
true
discuss
bayesian
update
step
detail
schematic
illustration
round
order
ﬁnd
xm+1
see
line
algo-
rithm
dc2
ﬁrst
ﬁnds
line
passes
centre
cuts
two
halves
measure
respect
note
ﬁnding
line
key
step
algorithm
bayesian
update
line
done
steps
piecewise
constant
property
line
found
easy
see
xm+1
two
points
orthogonal
line
result
dc2
round
ﬁnd
xm+1
operations
denote
half-circle
containing
xm+1
half
refer
figure
noisy
response
query
sign⟨xm+1
h∗⟩
obtained
line
probability
distribution
updated
pm+1
following
way
first
consider
event
outcome
sign⟨xm+1
h∗⟩
sign⟨xm+1
h∗⟩
1~2
similarly
sign⟨xm+1
h∗⟩
1~2
there-
pm+1
observe
sign⟨xm+1
h∗⟩
fore
bayes
theorem
obtain
following
update
rules
also
observe
sign⟨xm+1
h∗⟩
pm+1
pm+1
pm+1
pm+1
note
factor
due
normalization
easy
verify
pm+1
also
piecewise
constant
distribution
sectors
see
figure
theorem
shows
log
log
rounds
probability
least
dc2
outputs
unit
vector
span
ˆe
h⊥
also
discussed
computational
complexity
dc2
i.e.
log
theorem
proof
appendix
independent
ﬂip
probability
log
log
log
log
8πǫ
max
log
sufﬁcient
guarantee
dc2
outputs
probabil-
ity
least
vector
within
distance
log
1−ρ
log
1−ρ
log
1−ρ
log
1−ρ
log
log
log2
1−ρ
log
log
would
like
remark
independent
ﬂip
probability
i.e.
noiseless
case
algorithm
dc2
reduces
binary
search
let
⌈log2
dc2
outputs
vector
within
distance
log
1−ρ
24ρ
log
log
1−ρ
figure
upon
completion
round
left
ﬁgure
distri-
bution
pdf
constant
sectors
ui+1
next
round
right
ﬁgure
order
ﬁnd
xm+1
dc2
ﬁrst
ﬁnds
diagonal
line
red
line
separates
two
half-circles
measure
1~2
w.r.t
vector
xm+1
one
two
points
unit
circle
orthogonal
line
updating
pm+1
note
points
in-
side
get
factor
either
depending
outcome
query
true
thus
pm+1
piecewise
constant
pdf
sectors
takes
input
present
detailed
discussion
examples
ap-
pendix
comments
order
guarantee
dc2
holds
probability
one
thus
parameter
irrelevant
noiseless
setting
furthermore
round
dc2
distribution
represented
two
numbers
starting
ending
points
sector
vector
computed
efﬁciently
orthogonal
vector
midpoint
there-
fore
assuming
one
unit
complexity
performing
tor
following
three
properties
section
devise
algorithm
called
dc2
two
orthonormal
vec-
tors
uses
noisy
responses
queries
form
queries
dc2
implemented
complexity
i.e.
log
1~ǫ
dimension
coupling
based
framework
sign⟨x
h∗⟩
outputs
probability
least
vec-
span
ˆe
ˆe
h∗
e1e1+h∗
e2e2
⟨h∗
e1⟩e1+⟨h∗
e2⟩e2
words
unit
vector
within
distance
normalized
projection
onto
subspace
span
current
section
explain
framework
estimates
using
calls
dc2
for-
mal
description
given
algorithm
later
let
begin
discussion
motivating
example
let
orthonormal
basis
suppose
i=1
ciei
form
i=1
arbi-
trarily
chosen
orthonormal
basis
assume
w.l.o.g
normalized
i.e.
objective
i=1
learn
coefﬁcients
i=1
within
given
precision
using
noisy
responses
selected
sign
queries
key
insight
task
partitioned
dc2
ˆe12
ˆe34
ˆe12
dc2
ˆe34
dc2
scheme
balanced
full
binary
tree
dc2
ˆe123
ˆe123
dc2
ˆe12
ˆe12
dc2
scheme
unbalanced
full
binary
tree
figure
two
possible
divide-and-conquer
schemes
dimensional
problem
scheme
represented
full
binary
tree
leaf
nodes
divide-and-conquer
fashion
many
smaller
tasks
involving
dimensions
ﬁnal
answer
values
i=1
obtained
aggregating
answers
subproblems
example
assume
c1e1
c2e2
c3e3
c4e4
standard
basis
vectors
deﬁne
c3e3
c4e4
c2
c1e1
c2e2
c2
e34
e12
note
e12
normalized
orthogonal
projection
onto
span
e34
normalized
orthogo-
nal
projection
onto
span
consider
follow-
use
relation
c2
ing
procedure
learn
ﬁrst
ﬁnd
e12
e34
4e34
ﬁnd
based
orthonormal
vectors
e12
e34
procedure
original
four-dimensional
problem
broken
three
two-dimensional
problems
2e12
+c2
procedure
illustrated
figure
ﬁrst
call
dc2
obtain
estimate
ˆe12
e12
call
dc2
obtain
estimate
ˆe34
e34
ﬁnally
call
dc2
ˆe12
ˆe34
obtain
estimate
scheme
illustrated
figure
call
dc2
obtain
ˆe12
estimate
e12
call
dc2
ˆe12
example
another
example
4-dimensional
problem
discussed
example
let
consider
another
obtain
ˆe123
estimates
normalized
orthogonal
projec-
tion
onto
span
ﬁnally
call
dc2
ˆe123
obtain
estimate
denote
examples
show
two
possibilities
divide-and-
conquer
schemes
4-dimensional
problem
fact
scheme
corresponds
full
binary
tree
leaf
nodes
general
idea
similar
break
problem
two-dimensional
problems
solved
efﬁciently
divide-and-conquer
scheme
corresponds
full
binary
tree
leaf
nodes
consider
decomposition
generality
suppose
ﬁrst
two
leaf
nodes
com-
bined
write
i=1
ciei
without
loss
i=1
c1e1
c2e2
c2
i=3
ciei
ˆc12
ciei
c2
normalized
orthogonal
projection
last
step
taken
ˆc12
c2
note
c1e1+c2e2
1+c2
onto
span
hence
using
dc2
obtain
probability
least
1−δ
good
approxi-
mation
ˆe12
within
distance
projection
therefore
small
enough
ˆc12ˆe12
i=3
ciei
since
expressed
approximately
terms
or-
thonormal
vectors
ˆe12
effectively
reduced
dimensionality
problem
idea
repeat
procedure
newly
obtained
representation
hence
repeating
procedure
times
total
reach
vector
ﬁnal
approximation
present
general
method
algorithm
algorithm
dimension
coupling
input
orthonormal
basis
output
unit
vector
estimate
replace
two
vectors
e′′
vec-
tor
dc2
e′′
end
let
remaining
vector
return
theorem
proof
appendix
outlined
algorithm
divide-and-conquer
scheme
rep-
resented
full
binary
tree
call
two-dimensional
subroutine
dc2
times
provided
output
dc2
probability
within
distance
true
value
5~18
en-
sures
estimation
error
prob-
ability
least
result
theorem
desire
framework
estimate
within
distance
probability
least
enough
corresponding
parameters
dc2
d−1
d−1
theorem
indicates
requires
log
+log
queries
since
call
dc2
needs
log
log
queries
recall
computational
complexity
dc2
hence
computational
com-
log
log
special
case
absence
plexity
log
+log
log
empirical
results
noise
query
complexity
time
complexity
section
extensively
evaluate
performance
following
baselines
random-sampling
queries
generated
sam-
pling
uniformly
random
unit
sphere
sd−1
uncertainty-sampling
queries
sampled
uni-
formly
random
orthogonal
complement
vector
learned
linear
svm
spectral
query-by-bagging
bag
size
set
1000
queries
generated
iteration
query
largest
disagreement
picked
abe
mamitsuka
1998
approximated
ellipsoid
consistent
previ-
ous
query
selected
approximately
halve
ellipsoid
alabdulmohsin
gao
zhang
2015
query-label
pairs
iteration
version
largest
space
repetitive-dc
noisy
setting
one
easy
way
apply
query
point
times
use
majority
rule
determine
label
i.e.
combination
repetitive
querying
section
framework
section
metrics
compare
different
algorithms
esti-
mation
error
query
complexity
execution
time
particular
increase
number
queries
measure
average
estimation
errors
execution
times
baselines
conﬁdence
intervals
nature
ac-
tive
learning
via
query
synthesis
data
points
queries
generated
synthetically
baselines
used
fastest
available
implementations
matlab
noiseless
setting
figures
dimension
respectively
show
terms
estima-
tion
error
outperforms
baselines
signiﬁ-
cantly
outperforms
random-sampling
uncertainty-
sampling
query-by-bagging
note
esti-
mation
errors
plotted
log-scales
terms
execu-
tion
times
see
fig
runs
three
orders
magnitude
faster
baselines
training
svm
iteration
random-sampling
uncertainty-
sampling
query-by-bagging
comes
huge
computational
cost
similarly
spectral
requires
solving
convex
optimization
problem
iteration
thus
per-
formance
drastically
deteriorates
dimension
increases
makes
infeasible
many
practical
problems
uncertainty-sampling
noisy
setting
set
noise
level
0.1
compare
performance
random-
sampling
query-by-
bagging
repetitive-dc
mentioned
alabdulmohsin
gao
zhang
2015
also
observed
experiments
spectral
work
even
small
amounts
noise
incorrectly
shrinks
version
space
misses
true
linear
separator
therefore
excluded
see
figures
signiﬁcantly
outperforms
methods
terms
estimation
error
precisely
using
number
queries
estimation
error
around
two
orders
magnitude
smaller
baselines
also
observe
two
ﬁgures
still
runs
around
100
times
faster
random-sampling
uncertainty-sampling
query-by-bagging
clearly
higher
computational
cost
repetitive-dc
per-
forms
bayesian
update
query
finally
increase
dimension
1000
random-sampling
uncertainty-sampling
query-by-bagging
be-
come
signiﬁcantly
slower
hence
figure
show
estimation
error
noise
levels
0.01
0.1
0.2
decreases
repetitive-dc
queries
observed
figure
consuming
number
queries
achieve
estimation
error
one
order
noise
intensity
small
three
orders
magnitude
noise
intensity
0.2
smaller
repetitive-dc
related
work
sample
complexity
learning
hypothesis
tra-
ditionally
studied
context
probably
approximately
correct
pac
learning
valiant
1984
pac
learning
the-
ory
one
assumes
set
hypotheses
along
set
unlabeled
data
points
given
data
point
drawn
i.i.d
distribution
clas-
sical
pac
bounds
yield
sample
complexity
i.e.
number
required
i.i.d
examples
output
hy-
pothesis
estimation
error
probability
least
ﬁxed
estimation
error
deﬁned
prx∼d
unknown
true
hypothesis
realizable
case
learning
halfspace
i.e.
perfectly
separates
data
points
positive
negative
labels
known
d~ǫ
i.i.d
samples
one
ﬁnd
lin-
ear
separator
estimation
error
main
advantage
using
active
learning
methods
i.e.
sequentially
querying
data
points
reduce
sample
complexity
exponential
gument
based
sphere
packing
shows
algorithm
fast
ideally
log
1~ǫ
fact
simple
counting
ar-
needs
log
1~ǫ
examples
achieve
estimation
er-
ror
dasgupta
kalai
monteleoni
2009
distribution
uniform
unit
sphere
easy
see
halving
bi-
section
leads
log
1~ǫ
using
halving
method
one
principle
extend
result
dimen-
sion
need
carefully
construct
version
space
i.e.
set
hypotheses
consistent
queries
outcomes
iteration
ﬁnd
query
halves
volume
uniform
case
density
general
case
distribution
known
dasgupta
2004
finding
query
high
dimension
challenging
use
notation
ignore
terms
logarithmic
dependent
100
10-1
10-2
10-3
random
uncertainty
bagging
spectral
100
150
number
queries
100
10-1
10-2
10-3
200
random
uncertainty
bagging
spectral
100
150
200
250
300
number
queries
104
103
102
101
100
10-1
10-2
10-3
350
400
450
noiseless
noiseless
103
102
101
100
10-1
uncertainty
bagging
random
104
103
102
101
100
10-1
uncertainty
bagging
random
101
100
10-1
10-2
10-3
random
uncertainty
bagging
spectral
dimension
execution
time
noiseless
noise
0.01
noise
0.1
noise
0.2
rep.
noise
0.01
rep.
noise
0.1
rep.
noise
0.2
10-2
10-4
repetitive
10-3
10-2
estimation
error
10-1
100
10-2
10-4
10-3
repetitive
10-2
estimation
error
10-1
100
10-4
number
queries
104
noisy
noisy
noisy
1000
figure
figures
show
estimation
error
noiseless
setting
increase
number
queries
100
respectively
figure
shows
corresponding
execution
times
figure
show
scatter
plots
execution
time
estimation
error
different
methods
noise
level
0.1.
allow
algorithm
use
budget
800
1800
queries
figure
respectively
figure
presents
estimation
error
repetitive-dc
increase
number
queries
1000
noise
levels
0.01
0.1
0.2.
particular
one
successful
approach
suffer
aforementioned
computational
challenge
pool-based
active
learning
settles
2010
instead
ideally
halving
space
effective
approaximations
performed
notable
algorithms
uncertainty
sampling
lewis
gale
1994
query-by-committee
qbc
freund
1997
fact
problem
closely
re-
lated
learning
homogeneous
linear
separators
uniform
distribution
pool-based
setting
problem
well
understood
exist
efﬁcient
pool-based
algorithms
balcan
broder
zhang
2007
dasgupta
kalai
monteleoni
2005
dasgupta
dasgupta
hsu
2008
pre-
sented
efﬁcient
perceptron-based
algorithm
achieve
near-optimal
query
complexity
similar
re-
sults
obtained
log-concave
distributions
balcan
long
2013
pool-based
meth-
dasgupta
kalai
monteleoni
2009
ods
require
access
1~ǫ
number
un-
labeled
samples
iteration
otherwise
perform
poorly
balcan
broder
zhang
2007
means
dasgupta
kalai
monteleoni
2009
terms
sample
complexity
need
grow
pool
points
moreover
need
store
awasthi
balcan
long
2014
balcan
beygelzimer
langford
2006
pool-based
learning
linear
separators
noisy
setting
much
less
studied
dependency
sample
complexity
size
exponentially
fast
guarantee
exceptions
exponential
note
order
noise
well
understood
attractive
alternative
pool-based
framework
query
synthesis
access
membership
queries
angluin
1988
learner
request
unla-
beled
data
instance
input
space
including
queries
learner
synthesizes
scratch
way
pool
size
limitation
entirely
eliminated
many
recent
ap-
plications
ranging
automated
science
king
2009
robotics
cohn
ghahramani
jordan
1996
ad-
versarial
reverse
engineering
lowd
meek
2005
query
synthesis
appropriate
model
instance
security-
sensitive
applications
e.g.
spam
ﬁlters
intrusion
de-
tection
systems
routinely
use
machine
learning
tools
growing
concern
ability
adversarial
attacks
identify
blind
spots
learning
algorithms
con-
cretely
classiﬁers
commonly
deployed
detect
mis-
creant
activities
however
attacked
adversaries
generate
exploratory
queries
elicit
information
return
allows
evade
detection
nelson
2012
work
show
adversary
use
active
learning
methods
making
synthetically
novo
queries
thus
identify
linear
separator
used
classiﬁcation
emphasize
active
learning
via
synthesized
queries
learning
algorithm
query
label
points
order
explore
hypothesis
space
noise-
less
setting
ignore
dependency
pool
size
log
1~ǫ
one
potentially
use
pool-based
algo-
rithms
uniform
distribution
main
contribu-
tion
paper
develop
noise
resilient
active
learn-
ing
algorithm
access
noisy
membership
queries
best
knowledge
ﬁrst
show
near
optimal
algorithm
outperforms
theory
prac-
tice
naive
repetition
mechanism
recent
spectral
heuristic
methods
alabdulmohsin
gao
zhang
2015
kääriäinen
2006
kääriäinen
2006.
active
learning
non-realizable
case
international
conference
al-
gorithmic
learning
theory
63–77
springer
karbasi
ioannidis
massoulie
2012
karbasi
ioan-
nidis
massoulie
2012.
comparison-based
learn-
ing
rank
nets
icml
karp
kleinberg
2007
karp
kleinberg
2007.
noisy
binary
search
applications
pro-
ceedings
eighteenth
annual
acm-siam
symposium
discrete
algorithms
881–890
society
industrial
applied
mathematics
king
2009
king
2009.
automation
sci-
ence
science
lewis
gale
1994
lewis
gale
1994.
sequential
algorithm
training
text
classiﬁers
pro-
ceedings
17th
annual
international
acm
sigir
con-
ference
research
development
information
re-
trieval
lowd
meek
2005
lowd
meek
2005.
ad-
versarial
learning
kdd
641–647
acm
nelson
2012
nelson
rubinstein
huang
joseph
lee
rao
tygar
2012.
query
strategies
evading
convex-inducing
classiﬁers
jmlr
nowak
2011
nowak
2011.
geometry
gen-
eralized
binary
search
ieee
transactions
information
theory
:7893–7906
settles
2010
settles
2010.
active
learning
literature
survey
university
wisconsin
madison
55-66
:11
shawe-taylor
cristianini
2004
shawe-taylor
cristianini
2004.
kernel
methods
pattern
analysis
cambridge
univ
cambridge
tong
chang
2001
tong
chang
2001.
sup-
port
vector
machine
active
learning
image
retrieval
proceedings
9th
acm
international
conference
multimedia
107–118
acm
tong
koller
2002
tong
koller
2002.
sup-
port
vector
machine
active
learning
applications
text
classiﬁcation
jmlr
2:45–66
valiant
1984
valiant
1984.
theory
learnable
communications
acm
:1134–1142
references
1988.
queries
concept
abe
mamitsuka
1998
abe
mamitsuka
1998.
query
learning
strategies
using
boosting
bagging
icml
morgan
kaufmann
pub
alabdulmohsin
gao
zhang
2015
alabdulmohsin
gao
zhang
2015.
efﬁcient
active
learning
halfspaces
via
query
synthesis
aaai
2015
angluin
1988
angluin
learning
machine
learning
balcan
awasthi
balcan
long
2014
awasthi
long
2014.
power
localization
efﬁciently
learning
linear
separators
noise
proceedings
46th
annual
acm
symposium
theory
computing
449–458
acm
balcan
long
2013
balcan
m.-f.
long
2013.
active
passive
learning
linear
separators
un-
der
log-concave
distributions
colt
288–316
balcan
beygelzimer
langford
2006
balcan
m.-f.
beygelzimer
langford
2006.
agnostic
ac-
tive
learning
proceedings
23rd
international
conference
machine
learning
65–72
acm
balcan
broder
zhang
2007
balcan
m.-f.
broder
zhang
2007.
margin
based
active
learning
learning
theory
springer
35–50
cohn
ghahramani
jordan
1996
cohn
ghahra-
mani
jordan
1996.
active
learning
sta-
tistical
models
jair
dasgupta
hsu
2008
dasgupta
hsu
2008.
hierarchical
sampling
active
learning
proceedings
25th
international
conference
machine
learning
208–215
acm
dasgupta
kalai
monteleoni
2005
dasgupta
2005.
analysis
kalai
monteleoni
perceptron-based
active
learning
international
con-
ference
computational
learning
theory
249–263
springer
dasgupta
kalai
monteleoni
2009
dasgupta
kalai
monteleoni
perceptron-based
active
learning
learning
research
feb
:281–299
dasgupta
2004
dasgupta
2004.
analysis
greedy
active
learning
strategy
advances
neural
information
processing
systems
337–344
freund
1997
freund
seung
shamir
tishby
1997.
selective
sampling
using
query
committee
algorithm
machine
learning
hakkani-tur
riccardi
gorin
2002
hakkani-tur
riccardi
gorin
2002.
active
learning
automatic
speech
recognition
icassp
volume
iv–3904
ieee
javdani
2014
javdani
chen
karbasi
krause
bagnell
srinivasa
2014.
near
optimal
bayesian
active
learning
decision
making
ais-
tat
2009.
analysis
journal
machine
appendix
proof
theorem
distributed
iid
bernoulli
random
variables
denote
m-th
round
dc2
takes
place
independent
probability
observe
ﬂipped
let
sequence
independent
identically
probability
space
generated
sequence
version
sign⟨xm
h∗⟩
also
observe
correct
version
sign⟨xm
h∗⟩
consider
query
form
sign⟨x
h∗⟩
query
di-
sign
h∗⟩
see
figure
two
parts
preferred
part
sign⟨x
sign⟨x
h⊥⟩
unpreferred
part
sign⟨x
−sign⟨x
h⊥⟩
two
parts
separated
line
vides
unit
circle
two
parts
half-circles
depending
passes
origin
refer
figure
schematic
explanation
therefore
region
line
general
preferred
figure
point
line
have⟨z
h⊥⟩
h⊥⟩
perform
query⟨x
h⊥⟩
likely
noisy
response
indeed
true
value
h⊥⟩
query
ﬁgure
sector
cut
line
sector
also
lies
preferred
part
query⟨x
h⊥⟩
setting
say
query
sign⟨x
h∗⟩
prefers
frequently
use
line
rather
query
sign⟨x
h∗⟩
unit
circle
say
query
sign⟨x
h∗⟩
cuts
region
point
belongs
preferred
part
query
oth-
erwise
say
query
prefer
also
causes
ambiguity
finally
region
line
passes
region
other-
wise
say
query
cut
cut
prefers
preferred
part
prefer
otherwise
see
figure
finally
two
smaller
sector
see
figure
clearly
round
dc2
vector
chosen
points
deﬁne
distance
length
≥x
y2
noisy
outcome
sign⟨xm
h∗⟩
observed
explained
section
chosen
way
preferred
unpreferred
parts
equal
measures
pm−1
i.e.
pm−1
fxm
pm−1
uxm
let
see
happens
posterior
belief
round
noisy
two
different
update
rules
depending
following
cases
i.e.
observe
conduct
query
sign⟨xm
h∗⟩
result
query
correct
value
sign⟨xm
h∗⟩
case
measure
updated
follows
pm+1
=2
fxm
uxm
i.e.
observe
ﬂipped
value
−sign⟨xm
h∗⟩
case
measure
updated
follows
pm+1
fxm
uxm
consider
number
given
goal
show
clearly
result
theorem
follows
bet-
pr∃y
ptǫ
ptǫ
cid:6
ter
illustration
assume
w.l.o.g
con-
sider
point
right-hand
side
unit
circle
also
consider
points
ǫ~4
ǫ~2
divide
sector
starting
ending
pints
denote
point
see
figure
also
δ+1
figure
different
regions
proof
theorem
8π⋅
let
sector
starting
zi−1
ending
de-
noted
note
beginning
algorithm
uniform
measure
unit
circle
regions
dc2
total
rounds
round
con-
ducts
query
associated
line
ℓxm
let
as ai 
log
1−ρ
consider
following
events
least
lines
separate
equivalently
least
lines
cut
region
region
cut
log
lines
ℓtǫ
ptǫ
ptǫ
easy
see
pr⋃k
j=1
cid:6
queries
hence
pigeon-hole
principle
al-
ways
region
cut
lines
write
j=1
using
lemma
stated
let
bound
using
fact
 e2
lemma
thus
obtain
j=1
given
lemma
show
expression
upper
bounded
δ~2
hence
using
relations
get
proof
main
theorem
value
chosen
way
using
update
rules
explained
proof
deﬁne
random
variable
log
easy
see
zi−1
2ζi
log
1−ρ
also
uniform
thus
hence
i=1
2ζi
log
1−ρ
i=1
2ζi
prlog
pr
i=1
last
step
follows
directly
called
cher-
noff
bound
i=1fxi
note
vector
always
member
pre-
ferred
part
test
result
round
dc2
lemma
consider
region
unit
circle
contain
assume
round
dc2
sequence
queries
associated
lines
ℓx1
ℓx2
ℓxm
conducted
deﬁne
events
none
lines
ℓxi
cuts
lines
prefer
integer
log
log
exp
ensures
log
log
ensures
finally
ensures
log
log
exp
proof
log
m−k
log
log
1−ρ
 a 
log
m−k
log
log
1−ρ
⎧⎪⎪⎪⎨⎪⎪⎪⎩
⎧⎪⎪⎪⎨⎪⎪⎪⎩
2⎫⎪⎪⎪⎬⎪⎪⎪⎭
2⎫⎪⎪⎪⎬⎪⎪⎪⎭
⎧⎪⎪⎪⎨⎪⎪⎪⎩
2⎫⎪⎪⎪⎬⎪⎪⎪⎭
log
log
1−ρ
exp
plugging
values
conclude
right
side
bounded
lemma
let
vectors
chosen
dc2
round
fxi
uxi
associated
pre-
ferred
unpreferred
parts
i.e
pi−1
fxi
pi−1
uxi
1~2
consider
two
points
i=1fxi
i=1uxi
deﬁne
j=1
exactly
lines
prefer
calculate
beginning
puts
uniform
measure
hence
 a 2π
let
ﬁrst
investigate
dynamics
pi−1
conduct
variables
log
time
assuming
line
i-th
query
condition
event
i.e
given
none
lines
cut
setting
deﬁne
random
ℓxi
cut
different
update
rules
depending
two
cases
whether
line
ℓxi
prefers
prefer
ﬁrst
case
line
ℓxi
prefers
know
either
probability
pi−1
probability
pi−1
thus
write
zi−1
log
log
second
obtain
zi−1
log
log
order
ﬁnd
upper
bound
assume
without
loss
generality
case
ℓxi
prefer
using
similar
argument
ﬁrst
rounds
lines
ﬁrst
case
last
rounds
lines
second
case
note
given
order
lines
statistically
equivalent
simple
order
consider
m−j
i=1
log2
 a 2π
i=m−j+1
i=m−j+1
m−j
i=1
obtain
noting
hence
log
0⎤⎥⎥⎥⎦
⎤⎥⎥⎥⎦
pr⎡⎢⎢⎢⎣log2
pr⎡⎢⎢⎢⎣
m−j
i=1
let
deﬁne
m−j
i=1
i=m−j+1
log2
log
m−j
i=1
pr⎡⎢⎢⎢⎢⎣
⎡⎢⎢⎢⎢⎢⎣
using
union
bound
m−j
m−j
i=m−j+1
i=1+
bound
obtain
simpliﬁcations
⎤⎥⎥⎥⎥⎥⎦
log
m−j
log
 a 
log
1−ρ
pr⎡⎢⎢⎢⎢⎣
m−j
i=1
using
chernoff
bound
get
⎧⎪⎪⎪⎨⎪⎪⎪⎩
exp
log
m−j
log
log
1−ρ
 a 
bound
similarly
write
simple
steps
2⎫⎪⎪⎪⎬⎪⎪⎪⎭
⎤⎥⎥⎥⎥⎦
log
m−j
log
log
1−ρ
⎡⎢⎢⎢⎢⎢⎣
m−j
i=1+
m−j
i=m−j+1
 a 
⎤⎥⎥⎥⎥⎦
figure
example
illustrate
dc2
noiseless
setting
ﬁrst
round
arbitrarily
chosen
choice
ﬁgure
sign
⟨x1
h∗⟩
sign
⟨x1
h⊥⟩
point
red
line
sign
points
outside
half-circle
result
therefore
distribution
pdf
uniform
region
red
line
zero
round
easy
see
direction
along
red
line
chosen
ﬁgure
sign
⟨x2
h∗⟩
hence
end
second
round
dc2
concludes
vector
could
uniformly
point
inside
generic
round
vector
orthogonal
mid-point
sector
rm−1
considered
candidate
choice
ﬁgure
sign
⟨xm
h⊥⟩
thus
end
round
dc2
concludes
uniformly
point
inside
using
chernoff
bound
get
⎧⎪⎪⎪⎨⎪⎪⎪⎩
exp
log
m−j
log
log
1−ρ
note
upper
bounds
decrease
increase
hence
proof
theorem
follows
letting
also
plugging
bounds
2⎫⎪⎪⎪⎬⎪⎪⎪⎭
dc2
noiseless
case
dc2
outlined
algorithm
noiseless
case
reduces
binary
search
section
explain
dc2
noiseless
case
binary
search
help
running
example
given
figure
see
round
dc2
possible
region
belong
halved
ﬁrst
note
initial
distribution
assumed
uniform
distribution
vector
see
step
algorithm
indeed
point
unit
circle
thus
dc2
chooses
arbitrarily
using
query
sign⟨x1
h∗⟩
also
give
value
sign⟨x1
h⊥⟩
depending
value
easy
verify
half
possibly
contain
see
figure
let
denote
region
hence
probability
dis-
tribution
current
belief
up-
dated
follows
1~π
words
time
vector
could
anywhere
points
inside
half-circle
equiprobable
⎤⎥⎥⎥⎥⎥⎦
unit
circle
round
belong
half-circle
thus
ﬁrst
round
dc2
halves
admissible
region
continuing
theme
hard
verify
see
figure
round
value
non-zero
uniform
region
quarter-circle
inductive
manner
letting
rm−1
denote
admissible
region
sector
round
see
figure
assuming
pm−1
non-zero
uniform
sector
rm−1
round
precisely
vector
orthogonal
midpoint
sector
rm−1
therefore
observing
value
sign⟨xm
h∗⟩
admissible
re-
gion
better
half
rm−1
compatible
observation
i.e.
contains
also
sector
uniform
zero
outside
also
easy
see
circular
angle
sector
following
statement
immediate
theorem
consider
absence
noise
let
=⌈log2
distance
outputs
vector
within
comments
order
guarantee
dc2
holds
probability
one
thus
parameter
irrelevant
noiseless
setting
furthermore
round
dc2
distribution
represented
two
numbers
starting
ending
points
sector
vector
computed
efﬁ-
ciently
orthogonal
vector
midpoint
therefore
assuming
one
unit
complexity
perform-
ing
queries
dc2
implemented
complexity
finally
using
theorem
conclude
requires
log
queries
computational
complexity
log
analysis
repetitive
querying
firstly
would
like
compute
probability
majority
vote
gives
correct
outcome
let
indicator
random
variable
event
i-th
query
gives
right
outcome
know
i.i.d
bernoulli
random
variables
success
proba-
i=1
hoeffding
inequality
bility
let
r~2
r~2
1~2
e−2
1~2−ρ
suppose
binary
search
queries
distinct
points
total
throughout
entire
procedure
union
bound
probability
majority
votes
give
right
outcome
greater
equal
n0e−2
1~2−ρ
order
ensure
probability
least
need
therefore
total
number
queries
least
log
n0~δ
1~2
log
n0~δ
1~2
n0r
recall
plugging
expression
nr0
ob-
tain
query
complexity
repetitive
querying
log
1~ǫ
see
theorem
log
1~ǫ
log
log
1~ǫ
log
1~δ
proof
theorem
round
replace
two
vectors
say
output
dc2
cardinality
decreases
therefore
call
dc2
result
cardinality
decreasing
initially
elements
algorithm
terminates
one
element
i.e.
ﬁnal
output
algorithm
thus
throughout
entire
process
algorithm
car-
dinality
decreases
therefore
calls
dc2
probability
success
dc2
least
union
bound
probability
success
second
part
theorem
prove
gen-
eral
statement
assume
run
input
least
orthonormal
set
note
underlying
space
remains
d-dimensional
euclidean
space
prove
outputs
vector
close
normalized
orthogonal
projection
onto
span
precisely
deﬁne
i=1⟨ei
h∗⟩ei
∑t
i=1⟨ei
h∗⟩2
runs
d−1
rounds
calls
dc2
d−1
times
out-
conclude
runs
time
uses
dc2
times
also
union
bound
probability
puts
probability
least
d−1
vector
ˆh
h⊥
5ǫd
exactly
similarly
way
discussed
least
outputs
dc2
close
estimate
ful
happens
probability
least
use
inductive
argument
prove
thatˆh−h⊥
d−1
within
distance
corresponding
objective
thus
assuming
calls
dc2
success-
use
induction
result
clear
prove
result
assuming
holds
without
loss
generality
assume
algorithm
calls
dc2
vectors
willl
replaced
output
dc2
de-
note
ˆe1
write
i=1
i=3
ciei
ˆc1h⊥1
ciei
=⟨h⊥
ei⟩
ˆc1
=c2
h⊥1
c1e1+c2e2
projection
also
onto
span
using
precisely
normalized
orthogonal
h⊥1
c1e1
+c2e2
notation
note
c2
c2
+c2
+c2
ˆe1
h⊥1
obtaining
ˆe1
algorithm
recall
dc2
output
recursively
call
thirdly
similarly
ciei
ciei
h⊥
−⟨h⊥
ˆe1⟩ˆe1
i=3
ciei
−⟨h⊥
ˆe1⟩ˆe1
i=1
i=3
c1e1
c2e2
−⟨h⊥
ˆe1⟩ˆe1
⟨h⊥
h⊥1⟩h⊥1
−⟨h⊥
ˆe1⟩ˆe1
⟨h⊥
h⊥1⟩h⊥1
−⟨h⊥
h⊥1⟩ˆe1
+⟨h⊥
h⊥1⟩ˆe1
−⟨h⊥
ˆe1⟩ˆe1
⟨h⊥
h⊥1⟩
h⊥1
ˆe1
+⟨h⊥
h⊥1
ˆe1⟩ˆe1
h⊥1
ˆe1
+h⊥1
ˆe1
therefore
⟨h⊥
ˆe1⟩ˆe1
i=3
ciei
2ǫ√1
plugging
get
last
step
follows
5~18
h′
h⊥
hence
h⊥−ˆh⊥
⩽ˆh⊥−h′+h′−h⊥
+5ǫ
ˆe1
suppose
output
call
denoted
ˆh⊥
assumption
induc-
tion
output
ˆh⊥
within
distance
normalized
orthogonal
projection
onto
span
ˆe1
denote
know
show
ˆh⊥
h′
⟨h⊥
ˆe1⟩ˆe1
⟨h⊥
ˆe1⟩2
i=3
ciei
i=3
h′
h⊥
true
completes
proof
therefore
sufﬁces
show
firstly
h⊥
ˆh⊥
⩽ˆh⊥
h′
+h′
h⊥
thath′
h⊥
deﬁne
=⟨h⊥
ˆe1⟩2
i=3
h′
h⊥
xxxxxxxxxxxh⊥
⟨h⊥
ˆe1⟩ˆe1
xxxxxxxxxxx
⟨h⊥
ˆe1⟩2
i=3
ciei
i=3
βh⊥
⟨h⊥
ˆe1⟩ˆe1
i=3
ciei
⟨h⊥
ˆe1⟩ˆe1
⟨h⊥
ˆe1⟩ˆe1
i=3
ciei
i=3
ciei
secondly
i
i=3
 β2
⟨h⊥
ˆe1⟩2
i=3
−ˆc2
⟨h⊥
ˆe1⟩2
i=3
 ⟨h⊥
ˆe1⟩2
ˆc2
 ⟨h⊥
ˆe1⟩2
−⟨h⊥
h⊥1⟩2 
 ⟨h⊥
ˆe1⟩
−⟨h⊥
h⊥1⟩ 
⋅ ⟨h⊥
ˆe1⟩
+⟨h⊥
h⊥1⟩ 
 ⟨h⊥
ˆe1
h⊥1⟩ 
⋅ ⟨h⊥
ˆe1⟩
+⟨h⊥
h⊥1⟩ 
h⊥
⋅ˆe1
h⊥1
h⊥
⋅ˆe1
+h⊥
⋅h⊥1
last
step
follows
fromh⊥
=ˆe1
=h⊥1
ˆe1
h⊥1
since
5~18
1~2
obtain
∈√1
2ǫ
max
1√1
1√1
2ǫ
