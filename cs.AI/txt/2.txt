scalable
models
computing
hierarchies
information
networks
baoxu
shi∗
tim
weninger†
∗†department
computer
science
engineering
university
notre
dame
notre
dame
indiana
usa
dated
march
2018
information
hierarchies
organizational
structures
often
used
organize
present
large
com-
plex
information
well
provide
mechanism
effective
human
navigation
fortunately
many
statistical
computational
models
exist
automatically
generate
hierarchies
however
existing
approaches
consider
linkages
information
networks
increasingly
common
real-world
scenarios
current
approaches
also
tend
present
topics
abstract
probably
distribution
words
etc
rather
tangible
nodes
original
network
furthermore
statistical
techniques
present
many
previous
works
yet
capable
processing
data
web-scale
paper
present
hierarchical
document
topic
model
hdtm
uses
distributed
vertex-programming
process
calculate
nonparametric
bayesian
genera-
tive
model
experiments
three
medium
size
data
sets
entire
wikipedia
dataset
show
hdtm
infer
accurate
hierarchies
even
large
information
networks
introduction
number
online
resources
web
documents
continues
increase
need
better
organiza-
tional
structures
guide
readers
towards
information
seek
increases
hierarchies
taxonomies
invaluable
tools
purpose
taxonomies
widely
used
libraries
via
library
congress
system
dewey
decimal
system
hierarchies
ﬁxture
early
world
wide
web
perhaps
famous
example
yahoo
search
engine
originally
taxonomic-collection
hyperlinks
organized
topic
systems
developed
effectiveness
topical
organi-
zation
logarithmic
depth
allowed
users
quickly
ﬁnd
relevant
documents
searching
unfortunately
taxonomy
curation
documents
articles
books
etc
mostly
manual
process
possible
number
curated
documents
relatively
small
process
becomes
increas-
ingly
impractical
number
documents
grows
web-scale
motivated
research
towards
automatic
inference
taxonomies
1–6
document
repositories
contain
linkages
documents
creating
document-graph
links
provide
among
things
proper
context
terms
topics
document
document-graphs
especially
common
nonﬁction
scientiﬁc
literature
citations
viewed
inter-document
links
similarly
world
wide
web
www
considered
single
large
document-graph
web
pages
represent
documents
hyperlinks
link
documents
web
sites
www
could
also
considered
document
graphs
web
site
simply
subgraph
www
web
site
subgraphs
particular
collection
documents
speciﬁc
purposeful
organizational
structure
often
carefully
designed
guide
user
entry
page
i.e.
homepage
progressively
speciﬁc
web
pages
similarly
scientiﬁc
literature
categorized
hierarchy
increasingly
speciﬁc
scientiﬁc
topics
citation
links
encyclopedia
articles
categorized
hierarchy
increasingly
speciﬁc
articles
cross
references
thus
assert
document-graphs
generally
information
networks
contain
hidden
node
hierarchies
taxonomies
versus
hierarchies
paper
draw
speciﬁc
distinctions
hierarchy
taxonomy
taxonomy
deﬁned
classiﬁcation
objects
increasingly
ﬁner
granularities
non-leaf
node
concep-
tual
combination
children
biological
taxonomy
canonical
example
deﬁnition
bshi
nd.edu
tweninge
nd.edu
classiﬁed
species
say
homo
sapiens
i.e.
humans
placed
leaf
taxonomy
inner
nodes
e.g.
primate
mammal
animal
declare
new
species
rather
conceptual
agglomer-
ations
species
furthermore
species
described
path
taxonomy
example
homo
sapiens
generally
described
primates
mammals
animals
among
others
hierarchy
hand
arrangement
objects
objects
considered
level
others
necessarily
means
objects
hierarchy
live
internal
nodes
example
business
government
military
chain
command
hierarchy
president
speciﬁc
object
generals
captains
case
high-level
nodes
like
president
agglomerations
captains
ceo
aggregation
managers
clerks
hierarchies
documents
similar
studies
document
hierarchies
actually
hierarchies
documents
literal
sense
example
hierarchical
lda
hlda
topicblock
tree
structured
stick
breaking
tssb
model
learn
conceptual
taxonomy
non-leaf
topics
combination
words
represent
real
document
corpus
hierarchical
pachinko
allocation
model
hpam
constructs
tree-like
conceptual
taxonomy
like
hlda
topic
multiple
parents
related
models
leaves
contained
actual
literal
documents
contrary
per-
spective
internal
nodes
existing
models
contain
ephemeral
word-topic
distributions
rather
actual
documents
see
figure
brief
comparison
model
outputs
hdtm
model
introduced
paper
requires
inner
nodes
previous
work
made
ephemeral
distributions
literal
documents
requirement
asserts
documents
general
others
explore
assertion
examples
review
similar
assertions
made
previous
research
web
sites
document
hierarchies
web
site
viewed
directed
graph
web
pages
vertices
hyperlinks
directed
edges
web
pages
excluding
inter-site
hyperlinks
cases
designating
web
site
entry
page
root
allows
web
site
viewed
rooted
directed
graph
web
site
creators
curators
purposefully
organize
hyperlinks
documents
topically
meaningful
manner
result
web
documents
away
root
document
typically
contain
speciﬁc
topics
web
documents
graphically
close
root
document
example
web
site
university
notre
dame
shown
figure
contains
root
web
docu-
ment
entry
page
dozens
children
web
documents
even
small
subset
documents
edges
corresponding
web
graph
quite
complicated
messy
breadth
ﬁrst
traversal
web
graph
starting
root
node
simple
way
distill
document
hierarchy
web
graph
unfortunately
ﬁxed
breadth-ﬁrst
hierarchy
account
many
intricacies
real
world
web
graphs
explanation
purposes
let
assume
four
types
hyperlink
edges
web
site
parent-
to-child
links
upward
links
shortcuts
cross-topic
links
parent-to-child
links
direct
user
one
web
page
topically
speciﬁc
web
page
e.g.
hyperlink
engineering
nd.edu
cse.nd.edu
parent-to-child
hyperlink
computer
science
topically
speciﬁc
engineering
upward
links
hyperlinks
reference
general
document
e.g.
may
exist
hyperlink
engineering.nd.edu
/the-arts
engineering
college
would
like
reference
artistic
happenings
university
shortcut
links
hyperlinks
skip
general
web
documents
speciﬁc
web
documents
way
featuring
speciﬁc
topic
e.g.
computer
science
professor
wins
prestigious
award
grant
professor
web
page
may
linked
news
section
root
web
page
cross
topic
links
hyperlinks
move
across
topical
subtrees
e.g.
college
science
may
reference
working
relationship
college
engineering
creating
hyperlink
two
web
pages
goal
infer
document
hierarchy
hdtm
model
sense
trying
ﬁnd
parent-
to-child
links
event
one
parent-to-child
link
particular
web
page
goal
ﬁnd
best
topically-relevant
parent
web
document
inferred
hierarchy
except
root
fig
truncated
web
site
graph
university
notre
dame
node
color
size
represent
more-general
less-
general
nodes
determined
hierarchy
inference
algorithm
grey
edges
hyperlinks
selected
present
inferred
hierarchy
web
researchers
practitioners
used
hyperlink
structures
organize
web
documents
many
years
pagerank
hits
algorithms
two
famous
examples
information
propagation
links
pagerank
instance
uses
model
random
web
surfer
i.e
random
walker
randomly
follows
hyperlinks
web
current
measure
web
page
authority
corresponds
probability
random
surfer
lands
upon
web
page
pagerank
score
hdtm
model
pagerank
notion
authority
loosely
corresponds
topical
generality
web
pages
high
random
surfer
probability
likely
topically
general
others
term
propagation
document
graphs
document-graph
structure
also
used
enrich
document
description
adding
features
im-
prove
retrieval
performance
intuition
behind
previous
works
helpful
framing
generative
model
limitation
random
walker
model
looks
graphical
structure
network
word
distributions
found
document
clearly
important
factor
consider
generating
document
hierarchies
previous
work
song
qin
show
given
web
page
enriched
propagating
information
children
relevance
propagation
model
modiﬁes
language
distribution
web
page
mixture
children
according
formula
cid:48
|child
cid:88
c∈child
frequency
term
document
propagation
cid:48
frequency
term
document
propagation
child
page
sitemap
parameter
control
mixing
factor
children
propagation
algorithm
assumes
sitemap
constructed
ahead
time
purposes
web
information
retrieval
language
models
often
used
normalize
smooth
word
distributions
illustration
purposes
apply
dirichlet
prior
smoothing
function
smooth
term
distribution
cid:48
used
place
usual
original
dirichlet
prior
smoothing
function
yielding
cid:48
w|c
|d|
cid:48
distribution
terms
smoothing
parameter
length
modiﬁed
propagation
algorithm
|d|
cid:48
|d|
http
//www.nd.edu//academics/the-arts/aboutresearch.nd.eduengineering.nd.eduscience.nd.edual.nd.edulaw.nd.educse.nd.eduame.nd.eduee.nd.eduacms.nd.eduphysics.nd.edu
hlda
hdtm
0.95
notre
dame
engineering
university
science
computer
computer
engineering
computer
engineering
science
notre
dame
science
notre
dame
department
department
table
comparison
probable
words
top
document
root
topic
hlda
hdtm
result
upward
propagation
function
root
document
web
site
entry
page
contain
words
web
pages
web
site
different
non-zero
probabilities
probable
words
occur
frequently
generally
across
documents
thus
propagated
small
preliminary
example
table
shows
top
six
probable
words
top
document
via
text
propagation
root
topics
hlda
hdtm
computer
science
department
web
site
university
notre
dame
small
example
reinforces
intuition
certain
web
sites
hidden
hierarchical
topical
structure
previous
term
propagation
work
web
site
sitemaps
constructed
ahead
time
using
url
heuristics
manually
goal
hdtm
learn
document
hierarchy
automatically
conjunc-
tion
topical
hierarchy
hierarchies
documents
many
different
collections
exist
hidden
hierarchies
technically
web
site
wikipedia
documents
categories
form
unique
document
graph
wikipedia
categories
especially
interesting
provide
type
ontology
wherein
categories
speciﬁc
sub-categories
general
parent-categories
wikipedia
articles
represented
least
one
category
description
allows
users
drill
relevant
articles
number
clicks
browsing
category
graph
partial
example
wikipedia
category
graph
shown
figure
ﬁgure
illustrates
document
graph
construed
document
hierarchy
speciﬁcally
wikipedia
community
hand-crafted
category
hierarchy
represented
colored
circles
edges
top
article-graph
represented
grey
squares
grey
edges
although
category
graph
imperfect
incomplete
wikipedia
browsers
get
sense
granularity
topic
simply
viewing
article
placement
category
hierarchy
wikipedia
construction
mind
goal
loosely
interpreted
automatically
inferring
wikipedia
category
hierarchy
wikipedia
article
graph
inference
tried
evaluated
section
bibliographical
networks
may
also
hierarchically
structured
bibliographic
network
papers
authors
wherein
author
could
collection
documents
represented
nodes
citation
represented
edge
graph
apart
web
citation
graphs
bioinformatics
networks
example
protein
networks
also
hierarchically
organized
protein
fold
prediction
nodes
network
proteins
two
proteins
connected
structural
evolutionary
relationships
challenges
contributions
goal
work
construct
node
hierarchies
information
network
using
node
features
e.g.
text
inter-node
edges
purposes
node
appears
level
another
node
refers
conceptual
granularity
nodes
words
given
information
network
explicitly
identiﬁed
root
web
site
homepage
aim
learn
node-hierarchy
best
captures
conceptual
hierarchy
document-graph
problem
poses
three
technical
challenges
fig
truncated
portion
wikipedia
category
subgraph
rooted
node
computing
circles
squares
represent
wikipedia
categories
articles
respectively
solid
color
lines
represent
category-to-category
links
dashed
black
lines
represent
category-to-article
links
solid
grey
lines
represent
article-to-article
links
document-topic
inference
document
hierarchies
parent
documents
consist
topics
general
children
requires
parent
documents
viewed
mixture
topics
contained
within
children
children
documents
topically
underneath
selected
parent
paper
introduce
hierarchical
document-topic
model
hdtm
generates
course-to-ﬁne
representation
text
information
wherein
high-level
documents
live
near
top
hierarchy
low-level
speciﬁc
documents
live
near
leaves
selecting
document
placement
placement
document
within
hierarchy
drives
topic
mix-
ing
inference
links
edges
hint
context
relationship
documents
document
placement
inferred
hierarchy
constrained
edges
within
original
document-graph
words
edge
exists
ﬁnal
inferred
hierarchy
must
also
exist
original
document-graph
vice
versa
unlike
existing
models
hlda
select
topic
paths
using
nested
chinese
restaurant
process
ncrp
hdtm
performs
docu-
ment
placement
based
stochastic
process
resembling
random
walks
restart
rwr
original
document-graph
use
stochastic
process
document-graph
frees
algorithm
rigid
parameters
perhaps
importantly
adoption
rwr
stochastic
process
instead
ncrp
allows
documents
live
non-leaf
nodes
frees
algorithm
depth
parameter
hlda
analysis
web
site-scale
many
document-graph
collections
number
edges
grows
quadrat-
ically
number
nodes
limits
scalability
many
topic
diffusion
algorithms
important
side-effect
rwr
process
ability
adapt
hdtm
model
inference
algo-
rithm
distributed
graph
processing
system
capable
processing
billion-node
graphs
remainder
paper
organized
follows
reviewing
related
work
introduce
hdtm
model
show
inference
performed
next
show
inference
adapted
large
scale
graph
processing
system
order
run
web-scale
data
sets
experiments
section
describes
myriad
tests
run
various
data
sets
quantitative
qualitative
evaluations
performed
conclude
examples
real-world
use
cases
discuss
avenues
future
research
provide
link
hdtm
source
code
related
work
initial
efforts
hierarchical
clustering
used
greedy
heuristics
single-link
complete-link
agglo-
moration
rules
infer
dendrograms
root
node
split
series
branches
terminate
computingcomputer
programmingdata
interchangecomputer
standardscomputer
errorsinformation
assurancevideo
game
glitchessoftware
anomaliessoftware
engineering
hlda
topicblock
hpam
tssb
hdtm
fslda
fig
generative
structures
related
work
circles
squares
represent
topics
documents
respectively
topic
multinomial
words
grey
boxes
separate
distribution
levels
path
white
triangles
hierarchical
lda
topicblock
learn
conceptual
taxonomy
non-leaf
topics
combination
words
represent
real
document
corpus
hpam
constructs
tree-like
conceptual
taxonomy
abstract
topic
multiple
parents
topics
tssb
mapped
document
multinomial
hdtm
fslda
hand
convert
corpus
document
hierarchy
mapping
topic
one
one
document
single
document
leaf
al.
point
manually-curated
web
hierarchies
like
open
directory
project
typically
ﬂatter
contain
fewer
inner
nodes
agglomerative
clustering
tech-
niques
produce
hierarchical
clustering
algorithms
include
top-down
processes
iteratively
partition
data
incremental
methods
like
cobweb
classit
algorithms
optimized
hierarchical
text
clustering
processes
typically
deﬁnes
hierarchical
clustering
algorithms
made
prob-
abilistic
setting
build
bottom-up
hierarchies
based
bayesian
hypothesis
testing
hand
lot
recent
work
uses
bayesian
generative
models
ﬁnd
likely
explanation
observed
text
links
ﬁrst
hierarchical
generative
models
hierarchical
latent
dirichlet
allocation
hlda
hlda
document
sits
leaf
tree
ﬁxed
depth
illustrated
figure
note
non-leave
nodes
figure
conceptual
topics
containing
word
distribution
instead
docu-
ment
document
represented
mixture
multinomials
along
path
taxonomy
document
root
documents
placed
respective
leaf
nodes
stochasically
using
nested
chinese
restaurant
process
ncrp
along
side
lda-style
word
sampling
process
ncrp
recursive
version
standard
chinese
restaurant
process
crp
progresses
according
following
analogy
empty
chinese
restaurant
inﬁnite
number
tables
table
inﬁnite
number
chairs
ﬁrst
customer
arrives
sits
ﬁrst
chair
ﬁrst
table
γ+n−1
probability
second
customer
chose
sit
occupied
table
probability
γ+n−1
current
customer
number
sit
new
unoccupied
table
probability
...
...
...
...
customers
currently
sitting
table
parameter
deﬁnes
afﬁnity
sit
previously
occupied
table
nested
version
crp
extends
original
analogy
follows
table
chinese
restaurant
cards
name
another
chinese
restaurant
customer
sits
given
table
reads
card
gets
goes
restaurant
reseated
according
crp
customer
visits
restaurants
ﬁnally
seated
able
eat
process
creates
tree
depth
width
determined
parameter
process
also
called
chinese
restaurant
franchise
analogy
adams
proposed
hierarchical
topic
model
called
tree
structured
stick
breaking
tssb
illustrated
figure
wherein
documents
live
internal
nodes
rather
exclusively
leaf
nodes
how-
ever
process
involves
chaining
together
conjugate
priors
makes
inference
complicated
also
make
use
link
data
work
along
line
include
hierarchical
labeled
lda
hllda
petinot
hllda
well
ﬁxed
structure
lda
fslda
reisinger
pasca
modify
hlda
ﬁxing
hierarchical
structure
learning
hierarchical
topic
distributions
hierarchical
pachinko
allocation
model
hpam
shown
figure
produces
directed
acyclic
graph
dag
ﬁxed
depth
allowing
internal
non-document
node
represented
mixture
abstract
i.e.
higher
level
topics
network-only
data
community
discovery
process
ﬁnding
self-similar
group
clusters
shrink
algorithm
creates
hierarchical
clusters
identifying
tightly-knit
communities
ﬁnding
dis-
parate
clusters
looking
hubs
heuristics
clauset
discover
dendrograms
monte
carlo
sampling
however
dendrograms
poorly
represent
manually
curated
hierarchies
taxonomies
pursuing
stochastic
block
models
sbm
alternative
line
network
clustering
research
partitions
nodes
communities
order
generatively
infer
link
probabilities
several
extensions
original
sbm
since
proposed
survey
see
one
downside
block-model
processes
assign
probabilities
every
possible
edge
requiring
complexity
every
sampling
iteration
furthermore
sbm
methods
typically
concerned
topical/conceptual
properties
nodes
hdtm
merges
document
text
inter-document
links
single
model
assume
words
latent
topics
within
link
structure
graph
graph
structure
explains
topical
relationships
interlinked
documents
topic
modeling
network
structure
tmn
similar
regard
regularizes
statistical
topic
model
harmonic
regularizer
based
graph
structure
data
result
topic
proportions
linked
documents
similar
however
hierarchical
information
discovered
easily
inferred
model
topic-sensitive
pagerank
combines
document
topics
pagerank
algorithm
arguing
pagerank
score
document
ought
inﬂuenced
topical
connection
referring
document
like
tmn
model
topic-senstive
pagerank
construct
type
information
network
hierarchy
work
generative
models
combine
text
links
include
probabilistic
model
document
connectivity
link-plsa-lda
pairwise-link-lda
methods
latent
topic
model
hypertext
lthm
method
role
discovery
social
networks
author-topic-model
others
models
operate
encoding
link
probability
discrete
random
variable
bernoulli
trial
parameterized
topics
documents
relational
topic
model
rtm
builds
links
topics
observed
links
given
high
likelihood
topicblock
model
combines
non-parametric
hlda
stochastic
block
models
generate
document
taxonomies
text
links
however
topicblock
like
hlda
permit
documents
reside
non-leaf
nodes
resulting
tree
apply
topic
modeling
algorithms
web-scale
data
several
parallel
algorithms
introduced
newman
proposed
exact
distributed
gibbs
sampling
algorithm
well
approximate
distributed
gibbs
sampling
algorithm
uses
local
gibbs
sampling
global
synchronization
smyth
intro-
duced
asynchronous
distributed
algorithm
capable
learning
lda-style
topics
ahmed
recently
released
yahoo
lda
scalable
approximate
inference
framework
large-scale
streaming
data
parallel
approaches
use
conventional
procedural
programming
paradigms
result
guarantee
statistically
sound
samples
gibbs
iterations
although
conven-
tional
parallel
distributed
algorithms
indeed
divide
document
set
smaller
groups
maximum
number
subgroups
subject
number
processors
contrast
vertex-programming
paradigm
e.g.
pregel
graphlab
graphx
distribute
sampling
operations
much
ﬁner
granularity
treating
graph-node
independent
computing
unit
although
distributed
variants
topic
inference
made
signiﬁcant
contributions
large
scale
topic
models
one
hand
large
scale
graph
processing
hand
unaware
par-
allel
algorithm
capable
joining
two
subjects
infer
topical
hierarchies
large
scale
information
networks
contrast
previous
work
hdtm
builds
hierarchy
documents
text
inter-document
links
model
node
hierarchy
contains
single
document
hierarchy
width
depth
ﬁxed
distributed
version
proposed
algorithm
ability
handle
graph
millions
nodes
billions
tokens
iii
hierarchical
document
topic
model
problem
inferring
document
hierarchy
learning
problem
akin
ﬁnding
single
best
parent
document-node
unlike
previous
algorithms
discover
latent
topic
taxonomies
hierarchical
document-topic
model
hdtm
ﬁnds
hidden
hierarchies
selecting
edges
document
graph
section
presents
detailed
description
model
beginning
document
graph
documents
edges
document
collec-
tion
words
word
item
vocabulary
basic
assumption
hdtm
similar
models
document
generated
probabilistically
mixing
words
among
topics
distributions
topics
represented
multinomial
variable
associated
set
distributions
words
w|z
dirichlet
hyper-parameter
document-speciﬁc
mixing
proportions
denoted
vector
parametric-bayes
topic
models
also
include
parameter
denotes
number
topics
wherein
one
possible
values
k-d
vector
hdtm
non-parametric
bayesian
models
require
parameter
input
instead
hdtm
exist
topics
one
graph
node
document
mixture
topics
path
root
document
i=1
θip
w|z
process
generating
document
choose
topic
proportions
distribution
θ|α
θ|α
dirichlet
distribution
sample
words
mixture
distribution
w|θ
chosen
step
original
lda
model
single
document
mixture
distribution
w|θ
cid:80
hlda
extension
lda
topics
situated
taxonomy
ﬁxed
depth
hier-
archy
generated
nested
chinese
restaurant
process
ncrp
represents
l-dimensional
vector
deﬁning
l-level
path
root
document
ncrp
process
every
doc-
ument
lives
leaf
words
document
mixture
topic-words
path
root
random
walks
restart
ncrp
stochastic
process
could
used
infer
document
hierarchies
ncrp
process
forces
documents
leaves
tree
hdtm
replaces
ncrp
random
walk
restart
rwr
also
known
personalized
pagerank
ppr
contrast
random
walk
teleportation
aka
pagerank
random
walks
selecting
random
starting
point
probability
walker
randomly
walks
new
connected
location
chooses
jump
random
location
probability
called
jumping
probability
hdtm
root
node
ﬁxed
either
entry
page
web
site
heuristic
manually
therefore
purposes
hierarchy
inference
random
walker
forced
start
restart
root
node
say
wish
ﬁnd
rwr-probability
node
target
node
model
random
walker
visiting
document
time
next
time
step
walker
chooses
document
among
outgoing
neighbors
v|u
hierarchy
uniformly
random
words
time
walker
lands
node
v|u
probability
1/deg
deg
outdegree
document
time
exists
edge
v|u
i.e
edge
current
node
target
node
original
graph
record
probability
new
path
possibility
later
sampling
alg
describes
process
algorithmically
procedure
allows
new
paths
root
cid:32
probabilistically
generated
based
current
hierarchy
effectively
allowing
documents
migrate
hierarchy
sampling
algorithm
random
walk
restart
path
probs
current
node
target
weight
input
globals
graph
hierarchy
restart
prob
output
foreach
t.ch
cid:54
log
rwr
p.put
len
t.ch
cid:16
cid:17
1−γ
child
recur
edge
exists
generating
document
paths
document
hierarchy
tree
document-node
one
parent
selecting
path
document
graph
akin
selecting
parent
grandparents
etc
d|u
document
graph
hdtm
creates
samples
probability
distribution
documents
parent
probability
document
parent
deﬁned
dept
cid:89
t=0
degt
walkers
current
position
time
dept
depth
degt
outdegree
hierarchy
words
probability
landing
product
emission
probabilities
document
path
random
walker
function
assigns
higher
probabilities
parents
shallower
depth
deeper
positions
line
intuition
ﬂatter
hierarchies
easier
human
understanding
deep
hierarchies
simply
put
restart
probability
controls
much
resistance
placing
document
successive
depths
algorithmically
hdtm
infers
document
hierarchies
drawing
paths
document
thus
documents
drawn
following
generative
process
document
assigned
topic
dir
document
draw
path
rwr
draw
l-dim
topic
proportion
vector
dir
=len
word
choose
topic
n|θ
mult
choose
word
mult
βcd
βcd
topic
zth
position
generative
process
hierarchical
nodes
represent
documents
topics
internal
nodes
contain
shared
terminology
descendants
illustration
figure
shows
two
potential
outputs
three-node
graph
wikipedia
articles
computer
science
data
mining
machine
learning
clearly
data
mining
machine
learning
could
regarded
children
computer
science
indeed
wiki-link
data
mining
machine
learning
graph
two
key
ideas
conveyed
illustration
ﬁrst
hierarchy
left
picks
edges
ignore
computer
science
machine
learning
wiki-link
denoted
thin-grey
line
whereas
hierarchy
right
picks
edges
ignore
data
mining
machine
learning
wiki-link
thus
toy
example
two
possible
hierarchy
outcomes
presented
one
left
one
right
figure
fig
illustration
two
hdtm
samples
data
node
hierarchy
contains
document
associated
topic
generative
process
general
terms
likely
found
topics
near
root
vice
versa
second
idea
conveyed
illustration
word-topic
distributions
colored
circles
stored
along
documents
words
black
squares
position
hierarchy
word
distribution
within
topic
constrained
document
may
propagate
terms
upwards
currently
selected/sampled
hierarchical
topology
thus
impossible
hierarchy
left
topic
distribution
machine
learning
contain
word
unique
computer
science
data
mining
similarly
impossible
hierarchy
right
topic
distribution
data
mining
contain
word
unique
machine
learning
even
though
indeed
possible
hierarchical
topology
left
like
earlier
models
statistical
pressure
general
terms
topics
towards
root
hierarchy
every
path
hierarchy
includes
root
node
paths
nodes
higher
levels
nodes
lower
levels
moving
tree
topics
therefore
documents
become
speciﬁc
hyperparameters
also
play
important
role
shape
character
hierarchy
parameter
affects
smoothing
topic
distributions
parameter
affects
smoothing
word
distributions
parameter
perhaps
important
parameter
affects
depth
hierarchy
specif-
ically
set
large
e.g.
0.95
resulting
hierarchy
shallow
low
values
e.g.
0.05
may
result
deep
hierarchies
smaller
probabilistic
penalty
step
random
walker
takes
inference
exact
inference
model
intractable
approximation
technique
posterior
inference
used
gibbs
sampling
algorithm
ideal
situation
simultaneously
allows
exploration
topic
distributions
potential
graphical
hierarchies
variables
needed
gibbs
sampler
nth
word
document
assignment
nth
word
document
topic
corresponding
document
zth
level
nw1,1computer
z1w1,2science
z1w1,3is
z1w1,4the
z1z1w1,1computerw1,2science
w1,3is
w2,3study
w1,4the
w2,4of
w3,1machinew3,3is
w3,4awd
nw3,1machine
z1w3,2learning=z3w3,3is
=z1w3,4a
=z1w3,5scientific
z3w3,6discipline
z3w3,2learningw3,5scientificw3,6discipline
z3wd
nw2,1data
z2w2,2mining
z2w2,3study=z1w2,4of=z1w2,5algorithm
z1w2,6for
z2z2w2,1dataw2,2miningw2,6forcomputer
sciencedata
miningmachine
learningwd
nw1,1computer
z1w1,2science
z1w1,3is
z1w1,4the
z1z1w1,1computerw1,2science
w1,3is
w2,3study
w1,4the
w2,4of
w3,3is
w3,4acomputer
sciencewd
nw2,1data
z2w2,2mining
z2w2,3study=z1w2,4of=z1w2,5algorithm
z1w2,6for
z2z2w2,1dataw2,2miningw2,6forw3,6discipline
w3,1machinedata
miningwd
nw3,1machine
z2w3,2learning=z3w3,3is
=z1w3,4a
=z1w3,5scientific
z2w3,6discipline
z3w3,2learningw3,5scientificz3machine
learning
variables
integrated
forming
collapsed
gibbs
sampler
sampling
performed
two
parts
given
current
level
allocations
word
sample
path
given
current
state
hierarchy
sample
words
use
topic
distributions
inform
path
selections
make
hierarchy
hierarchy
topology
inform
topic
distributions
sampling
document
paths
ﬁrst
gibbs
sampling
step
draw
path
document
root
graph
sampling
distribution
path
cd|c−d
wd|c−d
w−d
wd|c
w−d
cd|c−d
count
terms
document
w−d
words
without
document
equation
expression
bayes
theorem
ﬁrst
term
represents
probability
data
given
choice
path
root
second
term
represents
probability
selecting
path
speciﬁcally
second
term
represents
probability
drawing
path
document
depth
rwr
process
recall
node
emission
probability
1/degt
restart
probability
probability
deﬁned
recursively
k|c−d
cd,1
k−1
degt
cid:89
k=0
words
probability
reaching
equal
probability
random
walker
restart
probability
document
time
ﬁrst
term
probability
given
word
based
current
path
topic
assignment
wd|c
w−d
max
cid:89
k=1
cid:80
cid:81
cid:81
cid:80
c−d
w−d
c−d
w−d
counts
elements
array
satisfy
given
condition
max
maximum
depth
current
hierarchy
state
expression
c−d
w−d
counts
w−d
i.e.
number
words
appear
c−d
i.e.
number
paths
current
document
except
path
length
expression
counts
iii
i.e.
number
words
i.e.
words
appear
document
situated
end
path
length
i.e.
one
topics
size
vocabulary
adapted
standard
ratio
normalizing
constants
dirichlet
distribution
sampling
word
levels
given
current
state
variables
word
sampler
must
ﬁrst
pick
assignment
word
document
sampling
distribution
n|c
n|c
n|c
n|zd
\zd
\wd
ﬁrst
term
distribution
word
assignments
n|c
czd
η-smoothed
frequency
seeing
word
topic
level
path
second
term
distribution
levels
k|zd
k−1
cid:89
degt
dj−1
j=1
degt
dk−1
×
number
elements
vector
satisfy
given
condition
abuses
notation
product
combines
terms
representing
nodes
jth
level
path
parent
second
set
terms
represents
document
level
symbol
refers
terms
representing
ancestors
particular
node
refers
ancestors
node
including
distributed
hdtm
common
complaint
among
data
science
practitioners
graphical
models
especially
non-parametric
bayesian
graphical
models
perform
well
scale
mind
also
implemented
hdtm
inference
algorithm
scalable
distributed
vertex-programming
paradigm
mechanism
behind
gibbs
sampling
markov
chain
monte
carlo
methods
requires
sequen-
tial
sampling
steps
execution
step
depends
results
previous
step
making
gibbs
samplers
mcmc
method
general
difﬁcult
parallelize
approximate
distributed
lda
ad-lda
one
attempt
ﬁnd
approximate
distributed
solutions
serial
inference
problem
dividing
docu-
ments
parts
number
processors
initializes
topic
distribution
globally
dataset
using
last
gibbs
sampling
every
gibbs
iteration
processor
samples
iteration
processors
ﬁnished
global
synchronization
performed
updated
following
distribution
lessons
ad-lda
sought
also
implement
scalable
distributed
version
hdtm
however
major
difference
lda/hlda
hdtm
hlda
uses
ncrp
stochastic
process
assign
terms
topics
hdtm
samples
paths
graph
documents
using
graph-based
random
walk
restart
method
process
random
walks
network
topology
combined
term
sampling
process
described
good
candidate
vertex-programming
paradigm
using
frameworks
like
pregel
graphlab
vertex
programming
although
mapreduce
widely
used
general
purpose
parallel
scheme
easily
deal
scalable
data
optimized
iterative
computational
tasks
statistical
inference
logistic
regres-
sion
mapreduce
materializes
intermediate
results
disk
order
tolerate
task
failures
mapreduce
therefore
relatively
high
i/o
costs
compared
designs
keep
data
memory
across
iterations
apart
mapreduce
another
scalable
solution
build
custom
distributed
system
using
message
passing
interface
mpi
custom
approaches
usually
closer
optimal
developers
tune
code
based
needs
minimize
unnecessary
overhead
however
drawbacks
also
signiﬁcant
mpi
barebone
communication
speciﬁcation
developers
need
write
code
job
dispatching
load
balancing
dealing
node
failure
evaluating
aforementioned
approaches
decided
use
emerging
computational
paradigm
called
vertex
programming
vertex
programming
aims
improve
performance
graph/network
com-
puting
automatically
distributing
in-memory
computation
vertex-centric
scheduling
unlike
mapreduce
paradigm
writes
every
intermediate
result
disk
vertex
programming
keeps
data
up-to-
date
in-memory
reduces
i/o
overhead
materializing
data
disk
periodic
checkpoints
vertex-centric
scheduling
views
every
graph-vertex
elementary
computing
unit
uses
mpi
message
passing
system
transfer
data
graph-edges
typically
vertex-programs
rather
easy
algorithm
distributed
random
walk
restart
messages
received
containing
path
probabilities
globals
vertices
hierarchy
restart
prob
input
output
messages
sent
adjacent
edges
vertex
parallel
getmsg
sum
foreach
child
t.ch
cid:16
cid:17
sendmsg
child
log
1−γ
len
t.ch
document
parallel
add
path-probs
incoming
message
send
prob
message
children
implement
distributed
easily
much
computationally
efﬁcient
conventional
pro-
cedural
programming
working
iterative
computational
tasks
several
vertex
programming
frameworks
including
spark-graphx
hama
graphlab
graphlab
create
graphlab
2.0
powergraph
evaluated
powergraph
framework
ultimately
chosen
decision
based
part
insurmountable
troubles
experienced
several
months
im-
plementation
attempts
spark
graphx
framework
ultimately
concluded
c++
mpi
based
powergraph
faster
scalable
much
smaller
memory
footprint
frameworks
distributed
inference
algorithm
algorithm
path-global
update
messages
received
containing
path
probabilities
globals
vertices
hierarchy
restart
prob
input
output
messages
sent
adjacent
edges
vertex
parallel
foreach
sendmsg
dk.n
dk.z
getmsg
foreach
dk.n
u.n
dk.z
u.z
document
parallel
send
local
node
distributed
hdtm
inference
algorithm
similar
procedural
hdtm
detail
entire
distributed
hdtm
inference
algorithm
paper
however
source
code
referenced
section
hdtm
vertex-programming
model
changes
sampling
sequence
attention
global
syn-
chronization
required
firstly
random
walk
restart
must
executed
gibbs
iteration
every
visited
node
random
walk
probability
next
every
node
gathers
w−d
separately
decides
new
path
according
shown
alg
path
sampled
node
pick
sample
assignments
word
across
documents/nodes
parallel
according
global
synchronization
step
required
document/node
update
number
words
topic
assignments
globally
fortunately
topics
words
sampled
accord-
ing
information
provided
path
root
document
document
level
sufﬁces
update
nodes
path
instead
actual
global
update
nodes
vertex-programming
based
path-global
update
function
shown
alg
furthermore
update
executed
synchronization
barrier
built-in
vertex-programming
frameworks
highly
optimized
lock
global
system
synchronization
barrier
already
wikipedia
cat
wikipedia
article
compsci
web
site
bib
network
documents
tokens
links
vocabulary
609
5,570,868
2,014
146,624
1,957,268
1,316,879,537
44,673,134
4,225,765
1,078
771,309
63,052
15,101
4,713
43,345
8,485
3,908
table
comparison
probable
words
top
document
root
topic
hlda
experimental
results
section
describes
method
results
evaluating
hdtm
model
quantitative
qualita-
tive
analysis
hierarchical
document-topic
model
ability
learn
accurate
interpretable
hierarchies
document
graphs
shown
main
evaluations
explore
empirical
likelihood
data
large
case
study
wherein
human
judges
asked
evaluate
constructed
hierarchies
data
hdtm
evaluated
four
corpora
wikipedia
category
graph
wikipedia
document
graph
computer
science
web
site
university
illinois
bibliographic
network
cikm
sigir
conferences
wikipedia
data
set
used
several
times
past
topic
modeling
purpose
however
computational
resources
needed
infer
topic
models
prior
studies
severely
constricted
dataset
size
gruber
al.
crawled
105
pages
starting
article
nips
conference
ﬁnding
799
links
performed
larger
evaluation
topicblock
model
using
14,675
document
152,674
links
however
truncated
article
ﬁrst
100
terms
limited
vocabulary
10,000
popular
words
wikipedia
category
data
set
crawl
category
graph
wikipedia
beginning
category
computing
shown
figure
wikipedia
category
collection
articles
set
links
categories
however
categories
typically
text
associated
text
article
associated
particular
category
associated
category
text
example
category
internet
includes
articles
internet
hyperlink
world
wide
web
etc
total
crawled
category
graph
consisted
609
categories
text
6,745
articles
category
graph
rather
sparse
2,014
edges
categories
vocabulary
size
146,624
5,570,868
total
tokens
perform
text
preprocessing
procedure
including
stop
word
removal
stemming
experiments
due
empirical
ﬁndings
models
robust
presents
stop
words
etc
settings
also
explore
hdtm
robustness
presence
large
noisy
corpus
computer
science
department
web
site
university
illinois
urbana-champaign
chosen
second
data
set
represents
rooted
web
graph
familiar
topics
inferring
document
hierarchy
goal
ﬁnd
organizational
structure
computer
science
department
intuition
web
sites
reﬂect
business
organization
underlying
entity
thus
expected
subtrees
consisting
courses
faculty
news
research
areas
etc
found
high
levels
speciﬁc
web
pages
found
lower
levels
hierarchy
web
site
crawled
starting
entry
page
captured
1,078
web
pages
63,052
hyperlinks
total
15,101
unique
terms
771,309
tokens
bibliographic
network
consists
documents
titles
4,713
articles
sigir
cikm
conferences
exist
3,908
terms
across
43,345
tokens
document
collection
collection
links
include
citations
papers
within
cikm
sigir
conferences
due
different
citation
styles
vendor
abbreviations
ambiguous
names
without
human
interference
bibliographical
meta-
data
extraction
algorithms
extract
portion
correct
data
49–51
choose
use
complete
citation
data
set
available
authors
arnetminer
project
however
citation
graph
guaranteed
complete
sigir
1998
paper
ponte
croft
chosen
root
document
data
set
in-collection
citations
construct
larger
data
set
ﬁner
granularity
english
wikipedia
article
graph
used
wikipedia
every
wiki-article
two
different
link
types
category
links
point
article
categories
wiki-links
point
article
wiki-articles
category
links
important
hint
encoded
hierarchical
structure
codiﬁed
top
wikipedia
article-graph
largely
developed
example
wiki-article
barack
obama
category
link
harvard
law
school
alumni
wiki-links
harvard
law
school
dreams
fa-
ther
among
others
later
experiments
infer
document
hierarchy
wikipedia
compare
crowd-encoded
wikipedia
category
hierarchy
wikipedia
snapshot
4,606,884
documents
1,878,158,318
tokens
44,739,242
links
vocabulary
6,064,216
words
document
graph
single
connected
component
picked
largest
component
graph
data
set
giant
component
contains
1,957,268
documents
44,673,134
document
document
links
42.49
nodes
edges
component
additionally
1,316,879,537
terms
4,225,765
unique
tokens
appear
component
approximately
total
number
terms
graph-preprocessing
step
replaced
edges
pointing
redirection
pages
edges
point
directly
actual
wiki-article
summary
sampling
algorithm
using
either
traditional
inference
model
sec
iii
high-throughput
distributed
sampling
algorithm
sec
iii
basically
sample
full
hierarchy
sampling
paths
node
except
root
words
document
given
state
sampler
time
i.e.
iteratively
sample
variable
conditioned
others
illustrated
iii
conditional
distribution
latent
variables
hdtm
model
given
document-network
running
markov
chain
sufﬁcient
number
times
approach
stationary
distribution
process
approaching
stationary
distribution
called
burn-in
period
burn-in
collect
samples
selected
interval
i.e.
sampling
lag
collected
samples
markov
chain
full
hierarchies
constructed
selection
path
node
word
document
therefore
sampled
hierarchy
contains
one
estimation
position
document
hierarchy
position
word
document
given
sampled
hierarchy
assess
goodness
hierarchy
measuring
log
probability
hierarchy
observed
words
conditioned
hyperparameters
cid:16
cid:17
log
d|γ
using
log
likelihood
function
possible
pick
sampled
hierarchy
maximizes
log
likelihood
ﬁnal
result
later
relax
assumption
use
robust
measurement
computes
mode
sampled
hierarchies
determines
certainty
result
quantitative
analysis
hdtm
distinct
qualities
make
apples
apples
comparison
difﬁcult
hdtm
ﬁrst
model
generate
document
hierarchies
based
graphs
nothing
directly
compare
however
models
related
work
perform
similar
tasks
comparisons
performed
applicable
related
models
typically
perform
quantitative
evaluation
measuring
log
likelihood
held
data
performing
task
like
link
prediction
log
likelihood
analysis
looks
goodness
held
data
unfortunately
creation
hold
data
set
possible
document
especially
documents
ﬁrst
second
level
document
important
resulting
hierarchy
removing
certain
documents
might
even
cause
graph
separate
would
make
hierarchy
inference
impossible
instead
ﬁrst
quantitative
evaluation
compare
highest
log
likelihood
generated
model
sampler
quantitative
experiments
performed
many
aforementioned
algorithms
including
hlda
topicblock
tssb
fslda
ﬁxed
structure
fslda
determined
breadth
ﬁrst
iteration
document
graph
url
heuristics
used
original
paper
exist
data
sets
depth
hlda
topicblock
cases
gibbs
sampler
run
5,000
iterations
burn-in
2,000
sampling
lag
20.
figure
shows
log
likelihood
sample
computer
science
web
site
data
sets
complete
log
likelihood
log
likelihood
corresponding
average
node
depth
high
likelihood
corresponds
hierarchies
high
average
depth
fig
analysis
log
likelihood
scores
5,000
iterations
hdtm
gibbs
sampler
run
web
site
collection
fig
best
cumulative
log
complete
likelihood
tested
value
lower
values
result
deeper
hierarchies
showed
similarly-shaped
results
interestingly
figure
shows
higher
likelihood
values
strongly
correlated
hierarchies
deeper
average
depth
gibbs
sampling
algorithm
run
hdtm
various
values
figure
shows
best
cumulative
log
likelihood
tested
values
observe
hdtm
0.05
achieved
best
likelihood
score
likelihood
scores
decreased
steadily
increasing
values
hdtm
0.95
achieved
worst
likelihood
score
table
iii
shows
results
different
algorithms
three
smaller
data
sets
topicblock
tssb
clearly
infer
models
best
likelihood
remaining
algorithms
including
hdtm
mixed
results
although
log
likelihood
results
initially
seem
hdtm
performs
poorly
demonstrate
section
likelihood
results
expected
preferred
large-scale
analysis
recall
goal
hdtm
generate
document
hierarchies
original
document-graph
thus
positive
result
somehow
demonstrate
hdtm
demonstrates
hierarchies
similar
010002000300040005000−8−7−6−5−4−3−2−10x
108iterationlog
complete
probability−14−12−10−8−6−4−2x
107456789101112log
complete
likelihoodaverage
node
depth0500100015002000250030003500400045005000−12−10−8−6−4−20x
107iterationbest
cumulative
log
liklihoodγ.05.15.25.35.45.55.65.75.85.95
compsci
web
site
wikipedia
cat
bib
network
-1.8570
-9.2412
-8.5306
-0.2404
-0.5689
-48.9149
-148.071
-148.166
-50.6732
-2.9827
-0.0336
-149.622
-0.4758
-0.5183
-8.5448
-0.4192
-0.4655
-0.6602
hdtm
0.05
hdtm
0.95
hlda
1.0
topicblock
1.0
tssb
fslda
table
iii
log
likelihood
results
best
sample
among
5,000
gibbs
iterations
values
×106
higher
values
better
best
results
bold
hierarchy
human
many
humans
would
create
given
document
graph
mind
performed
large
scale
experiment
full
english
wikipedia
article
graph
also
recall
wikipedia
category
graph
establishes
hierarchy
human-built
top
wikipedia
maintains
pseudo-hierarchical
classiﬁcation
almost
every
document
structure
categories
described
tree
rooted
main
topic
classiﬁcations
fundamental
categories
however
ﬁnd
87.01
categories
in-degree
larger
indicating
actual
category
complex
ﬁnd
al-
though
wikipedia
category
graph
mostly
resembles
top-down
coarse
ﬁne
hierarchy
technically
undirected
loopy
graph
wiki-articles
almost
always
belong
one
category
be-
cause
idea
object
described
article
rightfully
multiple
classiﬁcation
perspectives
example
wiki-article
computer
science
belongs
category
name
also
belongs
computer
engineering
electrical
engineering
among
others
large
scale
evaluation
task
infer
hierarchy
full
wikipedia
article
graph
see
well
matches
category
graph
unfortunately
difﬁcult
measure
similarity
category
graph
hdtm-hierarchies
using
conventional
evaluation
methods
like
log
likelihood
consider
example
category
university
notre
dame
faculty
contains
wiki-articles
faculty
members
vastly
different
topical
ﬁelds
cases
bond
described
primarily
links
rather
content
nevertheless
log
likelihood
score
judges
models
based
goodness
words
thus
unreasonable
measure
hierarchy
quality
likelihood
scores
alone
indeed
hierarchy
best
possible
likelihood
score
may
different
actual
human
created
hierarchy
order
perform
thorough
evaluation
also
compare
graph
structure
graph
topologies
compared
many
different
ways
among
many
options
chose
deltacon
measure
similarity
hdtm
hierarchy
wikipedia
category
hierarchy
three
reasons
deltacon
calculate
similarity
two
graphs
nodes
partially
overlap
observed
messy
wikipedia
category
graph
underlying
metric
deltacon
afﬁnity
score
calculated
fast
belief
propagation
fabp
models
connectivity
nodes
deltacon
process
millions
nodes
hours
deltacon
metric
returns
similarity
score
two
compared
graphs
ranging
inclusive
completely
dissimilar
completely
similar
although
comparing
hlda
variants
important
existing
hierarchical
topic
models
incapable
processing
data
sets
scale
attempted
use
hlda
implementation
mallet
package
130
hours
needed
order
compute
single
gibbs
sampling
iteration
fortunately
able
use
graphlab
implementation
lda
based
models
developed
ahmed
smyth
although
lda
directly
generate
graph
able
compare
category
hierarchy
connecting
documents
within
single
lda-cluster
made-up
root
node
simulate
hierarchy
large
scale
models
inferred
using
400
gibbs
sampling
iterations
burn-in
100
lag
10.
addition
hdtm
lda
also
use
deltacon
compare
original
wikipedia
article
graph
randomly
generated
article-hierarchy
baselines
case
wikipedia
article
graph
expect
wikipedia
article
graph
full
graph
hierarchy
perform
reasonably
well
category
hierarchy
largely
built
top
article
graph
topology
currently
coexist
coevolve
case
randomly
generated
article
hierarchy
randomly
select
single
parent
node
original
article
graph
table
shows
hierarchy
inferred
hdtm
indeed
similar
wikipedia
category
graph
followed
random
article
hierarchy
recall
deltacon
looks
graph
topology
similarity
thus
hdtm
hierarchy
expected
topologically
similar
another
hierarchy
even
random
article
graph
results
demonstrate
hdtm
identify
preserve
critical
topological
features
deltacon
hdtm
r=science
0.05
0.046852
hdtm
r=science
0.95
0.046851
hdtm
r=barack
obama
0.05
0.046857
hdtm
r=barack
obama
0.95
0.046856
random
article
hierarchy
0.046700
article
graph
0.044208
lda
0.026770
lda
0.037949
table
comparison
wikipedia
category
graph
generated
hierarchies
deltacon
scoring
means
higher
better
random
article
hierarchy
generated
randomly
picking
one
parent
among
possible
parents
i.e.
references
article
differences
deltacon
scores
show
hdtm
preserve
crucial
connectivity
information
constructing
new
hierarchical
structure
original
graph
inferring
hierarchy
graph
usual
increase
number
lda
topics
increases
goodness
score
case
measured
deltacon
instead
likelihood
interestingly
parameter
selection
different
roots
signiﬁcantly
inﬂuence
deltacon
score
understand
recall
different
root-nodes
values
result
different
classiﬁcation
perspectives
different
perspectives
still
subsets
category
graph
similar
topological
properties
hence
similar
scores
using
deltacon
metric
recall
hdtm
basic
form
generates
hierarchy
picking
best
parent
node/document
except
root
iterative
gibbs
sampling
process
almost
certainty
given
node
pick
different
parents
different
iterations
example
say
node
two
parents
possible
iterations
1–5
node
samples
node
parent
iterations
6–20
node
samples
node
parent
two
questions
come
mind
parent
ultimately
picked
node
ﬁnal
output
graph
distribution
samples
say
certainty
inferred
graph
classic
gibbs
sampling
maximum
posteriori
estimation
determined
mode
samples
node
translates
choosing
parent
appeared
samples
ﬁnal
parent
case
node
would
chose
node
parent
node
sampled
times
compared
samples
node
answer
second
question
therefore
model
certainty
inferred
hierarchy
calculate
number
times
node
picks
ﬁnal
parent
total
number
samples
normalized
number
possible
choices
i.e.
parents
node
results
certainty
score
node
certaintyd
deg−
.33.
represents
total
number
samples
number
times
ﬁnal
parent
sampled
deg−
indegree
node
representing
total
number
parents
node
could
pick
certainty
score
outside
fraction
used
measure
normalized
difference
raw
probability
probability
random
guess
applying
function
example
would
calculate
certainty
score
node
would
figure
shows
probability
density
functions
certainties
hdtms
parent
sampling
process
ﬁgures
left
right
ﬁnd
probability
densities
appear
polynomial
distributions
interesting
plateaus
end
certainty
scores
corresponding
near-certain
perfectly-certain
scores
nodes
indegree
values
respectively
results
others
displayed
paper
ﬁnd
root
general
document
like
science
changing
affect
certainty
distribution
given
relatively
speciﬁc
root
like
barack
obama
larger
values
increase
certainty
overall
discussed
earlier
measuring
topical
similarity
hierarchies
precarious
many
cases
documents
correctly
situated
topically
unrelated
parent
may
still
strong
contextual
association
parent
document
outweighs
topical/language-oriented
similarity
determined
parameter
example
consider
wiki-articles
honest
leadership
open
government
act
alexi
giannoulias
even
though
two
articles
topically
dissimilar
hdtm
parameterized
fig
probability
density
function
parent
sample
certainty
fig
jaccard
coefﬁcient
versus
certainty
bins
error
bars
drawn
bin
many
small
visible
higher
score
better
high
value
likely
place
articles
children
hierarchy
root
barack
obama
high
values
weigh
topological
link
important
documents
inferred
topicality
hand
hdtm
parameterized
low
value
alexi
giannoulias
likely
situated
state
senators
honest
leadership
open
government
act
likely
situated
legislation
quantitatively
evaluate
topical
topological
ﬁtness
hierarchy
together
compare
simi-
larily
among
sets
parents
children
inferred
hierarchy
sets
wiki-articles
wikipedia
categories
comparing
sets
items
use
jaccard
coefﬁcient
task
speciﬁcally
jaccard
coefﬁcient
calculated
jcoef
icient
|cd
cpa|
|cd|
|cpa|
|cd
cpa|
document
belongs
set
categories
cpa
union
corresponding
categories
nodes
ancestors
jaccard
coefﬁcient
useful
gives
quan-
titative
measure
describing
well
inferred
hierarchical
structure
matches
human-annotated
categorical
structure
wikipedia
figure
shows
relationship
jaccard
coefﬁcients
hierarchies
illustration
clarity
0.00.51.01.52.00.250.500.751.00certaintydensityscience
γ=0.05
science
γ=0.95
0.00.51.01.52.00.250.500.751.00certaintydensityobama
γ=0.05
obama
γ=0.95
0.050.060.070.000.250.500.751.00certaintyjaccard
coefficientscience
γ=0.05
science
γ=0.95
0.050.060.070.000.250.500.751.00certaintyjaccard
coefficientobama
γ=0.05
obama
γ=0.95
ﬁgure
jaccard
coefﬁcients
binned
equal
intervals
observe
clear
correlation
node
certainty
function
jaccard
coefﬁcient
means
decisions
hdtm
certain
tend
similar
human-created
hierarchies
recall
hdtm
uses
parameter
balance
weight
topological
links
topical
similarities
hence
speciﬁc
nodes
like
barack
obama
chosen
root
higher
reduce
jaccard
coefﬁ-
cient
high
certainty
nodes
higher
values
favor
topology
rather
content
path
parent
selection
process
results
also
underscore
importance
selection
root
node
provided
root
node
node
document
graph
inferred
hierarchy
created
perspective
root
hierarchy
rooted
barack
obama
would
therefore
organize
say
congressional
legislation
differently
hierarchy
generated
wiki-article
science
yet
seen
graph
similarity
metrics
views
drastically
different
document-topic
hierarchies
topologically
similar
discussion
order
properly
understand
results
captured
table
iii
recall
log
likelihood
metric
observations
conﬁguration
model
original
work
lda
found
likelihood
increases
number
topics
increases
along
lines
chang
demon-
strated
ﬁne
grained
topics
appear
models
larger
number
topics
lower
interpretability
despite
higher
likelihood
scores
simply
put
exists
negative
correlation
likelihood
scores
human
interpretability
lda
similar
topic
models
applying
lessons
experiments
recall
hdtm
many
topics
documents
non-root
document
topics
mixtures
topics
path
root
also
recall
hlda
topicblock
tssb
generate
large
number
latent
topics
hlda
topicblock
inﬁnitely
many
topics/tables
ncrp
practically
speaking
number
topics
ﬁnal
model
much
larger
number
documents
conditioned
parameter
tssb
topic
generation
said
interleaving
two
stick
breaking
processes
practically
generates
even
larger
topic
hierarchies
fslda
algorithm
many
topics
hlda
however
fslda
hierarchy
redrawn
gibbs
iterations
word
distributions
resulting
lower
likelihood
simply
put
number
topics
hdtm
fslda
cid:28
hpam
hlda
topicblock
cid:28
tssb
similarly
figure
shows
deeper
hierarchies
higher
likelihood
scores
long
document-to-root
paths
found
deep
hierarchies
able
provide
ﬁne
grained
words
document
resulting
higher
likelihood
therefore
better
likelihood
values
hlda
topicblock
tssb
due
larger
number
topics
models
infer
necessarily
due
better
model
outputs
case
better
way
evaluate
model
accuracy
external
task
manually
judging
coherence
topics
qualitative
analysis
measure
coherence
inferred
groupings
word
intrusion
task
developed
chang
slightly
modiﬁed
create
document
intrusion
task
task
human
subject
presented
randomly
ordered
set
eight
document
titles
task
human
judge
ﬁnd
intruder
judge
asked
ﬁnd
document
place
belong
set
documents
without
intruder
document
make
sense
together
human
judge
easily
able
ﬁnd
intruder
example
given
set
computer
science
documents
titles
systems
networking
databases
graphics
alan
turing
people
even
non-computer
scientists
would
pick
alan
turing
intruder
remaining
words
make
sense
together
computer
science
disciplines
set
systems
networking
ram
minesweeper
alan
turing
identifying
single
intruder
difﬁcult
human
judges
forced
make
choice
choose
intruder
random
indicating
grouping
poor
coherence
construct
set
document
titles
present
human
judge
grouping
hierarchy
selected
random
select
seven
documents
selected
random
grouping
fewer
documents
available
selected
grouping
select
documents
available
groupings
size
less
thrown
addition
documents
intruder
document
selected
fig
illustration
intruder
detection
task
wikipedia
collection
wherein
human
judges
presented
set
document
titles
asked
select
document
belong
random
among
entire
collection
documents
minus
documents
test
group
titles
shufﬂed
presented
human
judges
comparison
models
extracting
document
groupings
evaluation
slightly
different
model
hdtm
fslda
store
document
node
hierarchy
grouping
selected
ﬁrst
picking
document
random
choosing
siblings
topicblock
hlda
store
documents
leaves
taxonomy
often
include
several
documents
grouping
selected
models
ﬁrst
picking
document
random
choosing
documents
leaf-topic
hierarchies
tssb
model
constructed
allowed
multiple
documents
live
inner
nodes
at-
tempts
evaluate
groupings
inner
nodes
documents
unsuccessful
nodes
siblings
also
difﬁcult
ﬁnd
hierarchies
generated
sparse
ﬁnd
practical
groupings
thus
human
judges
tssb
groupings
could
found
document-graph
collection
different
types
labels
presented
judges
compsci
web
site
collection
labeled
web
page
title
url
wikipedia
collection
labeled
category
title
shown
figure
bibliography
network
labeled
title
paper
analyzing
human
judgments
intruder
detection
tasks
described
offered
amazon
mechanical
turk
specialized
training
expected
judges
tasks
created
data
set
model
combination
user
presented
tasks
time
cost
0.07
per
task
task
evaluated
separate
judges
order
measure
trustworthiness
judge
easy
tasks
selected
i.e.
groupings
clear
intruders
gold-standard
answers
created
judges
answer
gold-standard
answers
correctly
thrown
paid
total
solicitation
attracted
31,494
judgments
across
models
tasks
13,165
judgments
found
trustworthy
judges
model
precision
measured
based
well
intruders
detected
judges
speciﬁcally
intruder
selected
human
judge
model
task
intruder
word
task
model
indicator
function
number
judges
model
precision
basically
fraction
judges
agreeing
model
sumj
mpm
figure
shows
boxplots
precision
four
models
three
corpora
cases
hdtm
performs
best
likelihood
scores
necessarily
correspond
human
judgments
paired
two-tailed
t-tests
statistical
signiﬁcants
0.05
performed
hdtm
0.95
0.05
models
represented
figure
respectively
languagerevivalbilingualeducationlinguisticpurismclassicalphilologistsandroidsoftwaremobilebusinesssoftwarevideogamejournalismfreemobilesoftwarelibraryautomationpointing-devicetextinputmulti-touchauditorydisplaysnetworkaccelerationcomputerhardwaretuningcopyprotectioncomputeroptimizationspreadsheetsoftwareindiccomputingonlinespreadsheetsspreadsheetformats1/52/53/54/55/5
compsci
web
site
wikipedia
fig
10.
model
precision
ﬁve
models
three
document-graph
collections
higher
better
represents
statistical
signiﬁcance
hdtm
0.95
0.05
respectively
bib
network
00.10.20.30.40.50.60.70.80.91model
typemodel
precisiontopicblockhldaγ=1fslda∗∗°hdtmγ=0.95hdtmγ=0.0500.10.20.30.40.50.60.70.80.91model
typemodel
precisiontopicblockhldaγ=1fslda∗∗∗°hdtmγ=0.95hdtmγ=0.050.10.20.30.40.50.60.70.80.9112345model
precision°hldaγ=1topicblockfslda∗°hdtmγ=0.95hdtmγ=0.05
fig
11.
constructed
hierarchy
bibliographic
network
hdtm
.95.
words
root
document
represent
probable
words
root
topic
probable
words
documents
shown
due
space
constraints
bibliography
network
data
relatively
low
precision
scores
almost
certainly
difﬁcult
judges
probably
computer
scientists
differentiate
topics
research
paper
titles
figure
shows
small
portion
document
hierarchy
bibliographic
network
data
set
constructed
hdtm
.95.
root
document
children
hierarchy
despite
145
in-collection
links
remaining
120
documents
live
deeper
hierarchy
hdtm
determined
speciﬁc
warrant
ﬁrst
level
position
better
one
subtrees
recall
document
associated
topics
root
root
single
general
topic
seven
probable
terms
root
level
also
shown
adjacent
root
title
figure
11.
terms
like
hlda
topicblock
terms
general
entire
collection
similar
sets
words
exist
node
hierarchy
shown
illustration
maintain
clarity
reproducibility
hdtm
source
code
analysis
code
scripts
generated
results
found
paper
downloaded
https
//github.com/nddsg/hdtm/releases/tag/kais
wikipedia
web
site
bibliographical
data
publicly
available
free
download
replicated
data
repository
alanguagemodelingapproachtoinformationretrieval
retrieval
information
language
combiningmultipleclassifiersfortextcategorizationprobabilisticcombinationoftextclassifiersusingreliabilityindicators
modelsandresultsparameterizedgenerationoflabeleddatasetsfortextcategorizationbasedonahierarchicaldirectoryusingbayesianpriorstocombineclassifiersforadaptivefiltering.on-linespamfilterfusion.spamfilteringforshortmessages.relaxedonlinesvmsforspamfiltering.robustnessofadaptivefilteringmethodsinacross-benchmarkevaluation.generalizingfromrelevancefeedbackusingnamedentitywildcards.predictingthecost-qualitytrade-offforinformationretrievalqueriesorganizingstructuredwebsourcesbyqueryschemas
aclusteringapproach.informationretrievalasstatisticaltranslation.cross-lingualrelevancemodels.asearchengineforhistoricalmanuscriptimages.amethodfortransferringretrievalscoresbetweencollectionswithnon-overlappingvocabularies.evaluatingaprobabilisticmodelforcross-lingualinformationretrieval.stemminginthelanguagemodelingframework.translatingunknownquerieswithwebcorporaforcross-languageinformationretrieval.miningtranslationsofoovtermsfromthewebthroughcross-lingualqueryexpansion.probabilisticstructuredquerymethods.addressingthelackofdirecttranslationresourcesforcross-languageretrieval.triangulationwithouttranslation.ambiguousqueries
testcollectionsneedmoresense.bayesianextensiontothelanguagemodelforadhocinformationretrieval.comparingcross-languagequeryexpansiontechniquesbydegradingtranslationresources.measuringpseudorelevancefeedback
clir.cross-lingualquerysuggestionusingquerylogsofdifferentlanguages.statisticalcross-languageinformationretrievalusingn-bestquerytranslations.studyofcrosslingualinformationretrievalusingon-linetranslationsystems.usingthewebforautomatedtranslationextractionincross-languageinformationretrieval.bootstrappingdictionariesforcross-languageinformationretrieval.detectionandtranslationofoovtermspriortoquerytime.and16others
conclusions
hierarchical
document-topic
model
hdtm
bayesian
generative
model
creates
document
topic
hierarchies
rooted
document
graphs
initial
hypothesis
document
graphs
web
sites
wikipedia
bibliographic
networks
contain
hidden
hierarchy
unlike
previous
work
hdtm
allows
documents
live
non-leaf
nodes
hierarchy
requires
random
walk
restart
path
sampling
technique
interesting
side-effect
random
walker
adaptation
path
sampling
step
much
faster
easier
scale
ncrp
rwr
creates
sampling
distribution
parents
document
whereas
ncrp
process
creates
sampling
distribution
possible
paths
taxonomy
several
quantitative
experiments
comparing
hdtm
related
models
performed
however
results
show
likelihood
scores
poor
indicator
hierarchy
interpretability
especially
number
topics
different
comparison
models
large
qualitative
case
study
also
performed
showed
cohesiveness
document
groupings
generated
hdtm
statistically
better
many
comparison
models
despite
poor
likelihood
scores
acknowledgments
work
sponsored
afosr
grant
fa9550-15-1-0003
john
templeton
foundation
grant
fp053369-m.
adams
ghahramani
jordan
nips
2010
19–27
blei
jordan
journal
machine
learning
research
993
2003
eisenstein
xing
www
new
york
new
york
usa
2012
739
chambers
smyth
steyvers
nips
2010
334–342
chang
blei
annals
applied
statistics
121
2010
mimno
mccallum
icml
new
york
new
york
usa
2007
633–640
blei
grifﬁths
jordan
tenenbaum
nips
2004
17–24
blei
grifﬁths
jordan
journal
acm
2010
song
j.-r.
wen
shi
xin
t.-y
liu
qin
zheng
zhang
g.-r.
xue
w.-y
trec
2004
qin
t.-y
liu
x.-d.
zhang
chen
w.-y
sigir
new
york
new
york
usa
2005
408
zhai
lafferty
acm
tois
179
2004
http
//cse.nd.edu
lin
zou
qin
liu
jiang
zou
plos
one
e56499
2013
murzin
brenner
hubbard
chothia
journal
molecular
biology
247
536
1995
nallapati
mcfarland
manning
aistats
vol
2011
543–551
furukawa
matsuo
ohmukai
uchiyama
ishizuka
icwsm
2008
willett
information
processing
management
577
1988
http
//www.dmoz.org
zhao
karypis
fayyad
data
mining
knowledge
discovery
141
2005
fisher
machine
learning
139
1987
gennari
langley
fisher
artiﬁcial
intelligence
1989
heller
ghahramani
icml
new
york
new
york
usa
2005
297–304
petinot
mckeown
thadani
acl
2011
670–675
reisinger
paca
acl
620
2009
huang
sun
han
deng
sun
liu
cikm
new
york
new
york
usa
2010
219
clauset
moore
newman
nature
453
2008
holland
laskey
leinhardt
social
networks
109
1983
goldenberg
zheng
fienberg
airoldi
foundations
trends
machine
learning
foundations
trends
machine
learning
129
2009
mei
cai
zhang
zhai
www
new
york
new
york
usa
2008
101–110
haveliwala
www
new
york
new
york
usa
2002
517–526
cohn
hofmann
nips
2000
430–436
542–550
2007
1081–1088
81–88
nallapati
ahmed
xing
cohen
sigkdd
new
york
new
york
usa
2008
gruber
rosen-zvi
weiss
uai
2008
230–239
mccallum
corrada-emmanuel
wang
ijcai
2005
786–791
rosen-zvi
grifﬁths
steyvers
smyth
uai
2004
487–494
newman
smyth
welling
asuncion
advances
neural
information
processing
systems
smyth
welling
asuncion
advances
neural
information
processing
systems
2009
ahmed
aly
gonzalez
narayanamurthy
smola
proceedings
ﬁfth
acm
international
conference
web
search
data
mining
acm
2012
123–132
low
bickson
gonzalez
guestrin
kyrola
hellerstein
proceedings
vldb
xin
gonzalez
franklin
stoica
grades
first
international
workshop
graph
data
endowment
vldb
endowment
2012
management
experiences
systems
2013
bahmani
chowdhury
goel
pvldb
173
2010
related
works
denote
jumping
probability
however
would
ambiguous
dirichlet
hyper-parameter
malewicz
austern
bik
dehnert
horn
leiser
czajkowski
proceedings
2010
acm
sigmod
international
conference
management
data
acm
2010
135–146
zaharia
chowdhury
franklin
shenker
stoica
proceedings
2nd
usenix
conference
hot
topics
cloud
computing
vol
2010
k.-h.
lee
y.-j
lee
choi
chung
moon
sigmod
record
2012
zou
x.-b
w.-r.
jiang
z.-y
lin
g.-l.
chen
brieﬁngs
bioinformatics
bbs088
2013
mccune
weninger
madey
acm
computing
surveys
2015
mccallum
mimno
wallach
advances
neural
information
processing
systems
2009
giles
bollacker
lawrence
proceedings
third
acm
conference
digital
libraries
1973–1981
acm
1998
89–98
ley
string
processing
information
retrieval
springer
2002
1–10
tang
zhang
yao
zhang
sigkdd
new
york
new
york
usa
2008
990
ponte
croft
sigir
new
york
new
york
usa
1998
275–281
faloutsos
koutra
vogelstein
sdm
162
2013
mccallum
2002
2002
chang
gerrish
wang
boyd-graber
blei
advances
neural
information
processing
systems
2009
288–296
