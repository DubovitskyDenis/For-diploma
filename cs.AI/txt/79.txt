oﬀ-policy
corrections
anna
harutyunyan1
cid:63
marc
bellemare2
tom
stepleton2
r´emi
munos2
brussel
google
deepmind
aharutyu
vub.ac.be
bellemare
stepleton
munos
google.com
abstract
propose
analyze
alternate
approach
oﬀ-policy
multi-step
temporal
diﬀerence
learning
oﬀ-policy
returns
corrected
current
q-function
terms
rewards
rather
target
policy
terms
transition
probabilities
prove
approximate
corrections
suﬃcient
oﬀ-policy
convergence
policy
evaluation
control
provided
certain
conditions
conditions
relate
distance
target
behavior
policies
eligibility
trace
parameter
discount
factor
formalize
underlying
tradeoﬀ
oﬀ-policy
illustrate
theoretical
relationship
empirically
continuous-state
control
task
introduction
reinforcement
learning
learning
oﬀ-policy
samples
generated
behavior
policy
used
learn
distinct
target
policy
usual
approach
oﬀ-policy
learning
disregard
altogether
discard
transitions
whose
target
policy
probabilities
low
example
watkins
cuts
trajectory
backup
soon
non-greedy
action
encountered
similarly
policy
evaluation
importance
sampling
methods
weight
returns
according
mismatch
target
behavior
probabilities
corresponding
actions
approach
treats
transitions
conservatively
hence
may
unnec-
essarily
terminate
backups
introduce
large
amount
variance
many
oﬀ-policy
methods
particular
monte
carlo
kind
option
judge
oﬀ-policy
actions
probability
sense
however
temporal
diﬀerence
methods
maintain
approximation
value
function
along
way
eligiblity
traces
providing
continuous
link
one-step
monte
carlo
approaches
value
function
assesses
ac-
tions
terms
following
expected
cumulative
reward
thus
provides
way
directly
correct
immediate
rewards
rather
transitions
show
paper
approximate
corrections
suﬃcient
oﬀ-policy
con-
vergence
subject
tradeoﬀ
condition
eligibility
trace
parameter
distance
target
behavior
policies
two
extremes
tradeoﬀ
one-step
q-learning
on-policy
learning
formalizing
continuum
tradeoﬀ
one
main
insights
paper
cid:63
work
carried
internship
google
deepmind
particular
propose
oﬀ-policy
return
operator
augments
return
correction
term
based
current
approximation
function
formalize
three
algorithms
stemming
operator
oﬀ-policy
special
case
on-policy
policy
evaluation
oﬀ-policy
control
policy
evaluation
on-
oﬀ-policy
novel
closely
related
several
existing
algorithms
family
section
discusses
detail
prove
convergence
subject
tradeoﬀ
def=
maxx
cid:107
·|x
·|x
cid:107
measure
dissimilarity
behavior
target
policies
precisely
prove
amount
oﬀ-policy-ness
inherent
maximum
allowed
backup
length
value
1−γ
taking
value
guarantees
convergence
with-
involving
policy
probabilities
desirable
due
instabilities
variance
introduced
likelihood
ratio
products
importance
sampling
approach
control
fact
identical
watkins
except
cut
eligiblity
trace
oﬀ-policy
actions
sutton
barto
mention
variation
call
naive
analyze
algorithm
ﬁrst
time
prove
convergence
small
values
although
able
prove
tradeoﬀ
similar
policy
evaluation
case
provide
empirical
evidence
existence
tradeoﬀ
conﬁrming
intuition
naive
naive
one
might
ﬁrst
suppose
ﬁrst
give
technical
background
deﬁne
operators
specify
incremental
versions
algorithms
based
operators
state
convergence
follow
proving
convergence
subject
tradeoﬀ
policy
evaluation
conservatively
small
values
control
illustrate
tradeoﬀ
emerge
empirically
bicycle
domain
control
setting
finally
conclude
placing
algorithms
context
within
existing
work
preliminaries
consider
environment
modelled
usual
discrete-time
markov
deci-
sion
process
composed
ﬁnite
state
action
spaces
discount
factor
transition
function
mapping
distribution
reward
function
−rmax
rmax
policy
maps
state
distribution
q-function
mapping
given
policy
deﬁne
operator
q-functions
def=
cid:48
cid:48
cid:48
cid:48
cid:48
policy
corresponds
unique
q-function
describes
ex-
pected
discounted
sum
rewards
achieved
following
def=
cid:88
cid:88
cid:48
cid:48
cid:88
t≥0
operator
denotes
successive
applications
commonly
treat
one
particular
q-function
write
bellman
operator
bellman
equation
def=
πqπ
−1r
bellman
optimality
operator
deﬁned
def=
maxπ
well
known
e.g
optimal
q-function
def=
supπ
unique
solution
bellman
optimality
equation
write
greedy
def=
π|π
a|x
maxa
cid:48
cid:48
denote
set
greedy
policies
w.r.t
thus
greedy
temporal
diﬀerence
learning
rests
fact
iterates
operators
guaranteed
converge
respective
ﬁxed
points
given
sample
experience
cid:48
cid:48
sarsa
updates
q-function
estimate
kth
iteration
follows
qk+1
αkδ
cid:48
γqk
cid:48
td-error
k∈n
sequence
nonnegative
stepsizes
one
need
consider
short
experiences
may
sample
trajectories
accordingly
apply
repeatedly
particu-
larly
ﬂexible
way
via
weighted
sum
n-step
operators
def=
n+1q
cid:88
λγp
def=
λnf
n≥0
naturally
remains
ﬁxed
point
taking
yields
usual
bellman
operator
removes
recursion
approximate
function
restores
monte
carlo
sense
well-known
trades
bias
bootstrapping
approximate
q-function
variance
using
sampled
multi-step
return
intermediate
values
usually
performing
best
practice
λ-operator
eﬃciently
implemented
online
setting
via
mechanism
called
eligibility
traces
see
section
fact
corresponds
number
online
algorithms
subtly
diﬀerent
sarsa
canonical
instance
finally
make
important
distinction
target
policy
wish
estimate
behavior
policy
actions
generated
learning
said
on-policy
otherwise
oﬀ-policy
write
denote
expectations
sequences
·|xi
xi+1
·|xi
assume
conditioning
wherever
appropriate
throughout
write
cid:107
cid:107
supremum
norm
cid:124
oﬀ-policy
return
operators
describe
monte
carlo
oﬀ-policy
corrected
return
operator
heart
contribution
given
target
return
generated
behavior
operator
attempts
approximate
return
would
generated
utilizing
correction
built
current
approximation
application
state-action
pair
deﬁned
follows
def=
cid:2
cid:88
use
shorthand
eπq
cid:80
cid:0
eπq
cid:125
cid:123
cid:122
a∈a
a|x
oﬀ-policy
correction
gives
usual
expected
discounted
sum
future
rewards
reward
trajectory
augmented
oﬀ-policy
correction
deﬁne
diﬀerence
expected
respect
target
policy
q-value
q-value
taken
action
thus
much
reward
corrected
determined
approximation
target
policy
probabilities
notice
actions
similarly
valued
correction
little
eﬀect
learning
roughly
on-policy
q-function
converged
correct
estimates
correction
takes
immediate
reward
expected
reward
respect
exactly
indeed
see
later
ﬁxed
point
behavior
policy
deﬁne
n-step
λ-versions
usual
way
cid:1
cid:3
t≥1
def=
def=
cid:2
cid:88
+γn+1eπq
xn+1
cid:3
t=1
cid:0
eπq
cid:1
note
parameter
takes
monte
carlo
version
operator
rather
traditional
monte
carlo
form
algorithm
consider
problems
oﬀ-policy
policy
evaluation
oﬀ-policy
control
problems
given
data
generated
sequence
behavior
policies
k∈n
policy
evaluation
wish
estimate
ﬁxed
target
policy
control
wish
estimate
algorithm
constructs
sequence
k∈n
estimates
qπk
trajectories
sampled
applying
rπk
-operator
kth
interim
target
policy
distinguish
three
algo-
rithms
qk+1
rπk
algorithm
oﬀ-policy
corrections
given
initial
q-function
stepsizes
k∈n
sample
trajectory
xtk
qk+1
γeπk
qk+1
xt+1
qk+1
δπk
λγe
qk+1
qk+1
αkδπk
end
end
end
on-policy
oﬀ-policy
cid:54
greedy
oﬀ-policy
policy
evaluation
ﬁxed
target
policy
write
corresponding
operator
on-policy
policy
evaluation
special
case
oﬀ-policy
control
k∈n
sequence
greedy
policies
respect
write
corresponding
operator
wish
write
update
terms
simulated
trajectory
xtk
drawn
according
first
notice
rewritten
tδπ
def=
γeπq
xt+1
expected
td-error
oﬄine
forward
view1
qk+1
tδπk
resembles
many
existing
algorithms
subtly
diﬀers
due
basis
section
discusses
distinctions
detail
practical
every-visit
form
written
rather
qk+1
δπk
t−si
true
online
version
derived
given
van
seijen
sutton
t=0
s=0
cid:88
cid:88
cid:3
cid:2
cid:88
t≥0
cid:88
t=0
corresponding
online
backward
view
three
algorithms
summa-
rized
algorithm
following
theorem
states
suﬃciently
close
oﬀ-policy
algorithm
converges
ﬁxed
point
theorem
consider
sequence
q-functions
computed
according
algo-
rithm
ﬁxed
policies
let
maxx
cid:107
·|x
·|x
cid:107
1−γ
conditions
required
convergence
1–3
section
5.3
almost
surely
k→∞
lim
state
similar
albeit
weaker
result
theorem
consider
sequence
q-functions
computed
according
al-
gorithm
greedy
policy
respect
1−γ
conditions
required
convergence
1–3
section
5.3
almost
surely
k→∞
lim
proofs
theorems
rely
showing
contrac-
tions
stated
conditions
invoking
classical
stochastic
approxima-
tion
convergence
ﬁxed
point
proposition
4.5
focus
contraction
lemmas
crux
proofs
outline
sketch
online
convergence
argument
discussion
theorem
states
exists
degree
converges
oﬀ-policy-ness
1−γ
tradeoﬀ
oﬀ-policy
learning
algorithm
policy
evaluation
control
case
result
theorem
weaker
holds
values
smaller
1−γ
notice
threshold
corresponds
policy
evaluation
case
arbitrary
oﬀ-policy-ness
able
prove
convergence
left
open
problem
main
technical
diﬃculty
lies
fact
control
greedy
pol-
icy
respect
current
may
change
drastically
one
step
next
changes
incrementally
small
learning
steps
current
may
oﬀer
good
oﬀ-policy
correction
evaluate
new
greedy
policy
order
circumvent
problem
may
want
use
slowly
changing
target
policies
example
could
keep
ﬁxed
slowly
in-
creasing
periods
time
seen
form
optimistic
policy
iteration
policy
improvement
steps
alternate
approximate
policy
evalua-
tion
steps
policy
ﬁxed
theorem
guarantees
convergence
value
function
policy
another
option
would
deﬁne
conjec-
empirical
average
ture
deﬁning
changes
slowly
becomes
increasingly
greedy
could
extend
tradeoﬀ
theorem
control
case
left
future
work
previous
greedy
policies
cid:48
cid:80
i=1
cid:48
def=
analysis
begin
verifying
ﬁxed
points
policy
evaluation
control
settings
respectively
prove
contractive
properties
operators
always
contraction
converge
ﬁxed
point
contraction
particular
choices
given
terms
contraction
coeﬃcients
depend
distance
policies
finally
give
proof
sketch
online
convergence
algorithm
begin
convenient
rewrite
state-action
pairs
cid:88
t−1
t≥1
write
def=
λγp
def=
λγp
follows
surprising
along
bellman
equations
directly
yields
ﬁxed
points
λqπ
remains
analyze
behavior
gets
iterated
5.1
λ-return
policy
evaluation
ﬁrst
consider
case
ﬁxed
arbitrary
policy
simplicity
take
ﬁxed
well
hold
sequence
k∈n
long
satisﬁes
condition
imposed
lemma
consider
policy
evaluation
algorithm
assume
behavior
policy
ε-away
target
policy
sense
maxx
cid:107
·|x
·|x
cid:107
1−γ
sequence
k≥1
converges
exponentially
fast
cid:107
cid:107
1−λγ
proof
first
notice
cid:107
cid:107
sup
cid:107
cid:107
cid:107
cid:107
cid:88
cid:12
cid:12
cid:12
cid:88
max
cid:88
sup
cid:107
cid:107
max
y|x
cid:88
y|x
b|y
b|y
b|y
b|y
cid:12
cid:12
cid:12
let
λγp
resolvent
matrix
cid:2
λγp
cid:3
cid:2
λγp
cid:3
cid:2
λγp
cid:3
cid:2
cid:3
taking
sup
norm
since
ε-away
cid:107
cid:107
cid:107
cid:107
1−λγ
thus
cid:107
cid:107
5.2
λ-return
control
next
consider
case
kth
target
policy
greedy
respect
value
estimate
following
lemma
states
possible
select
small
nonzero
still
guarantee
convergence
lemma
consider
oﬀ-policy
control
algorithm
cid:107
λqk
cid:107
cid:107
cid:107
sequence
k≥1
converges
exponentially
fast
1−γ
proof
fix
let
λγp
using
write
λγp
λγp
taking
sup-norm
since
cid:107
cid:107
cid:107
cid:107
deduce
result
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
5.3
online
convergence
minimum
visit
frequency
cid:80
bounded
stepsizes
cid:80
ready
prove
online
convergence
algorithm
let
following
hold
every
sample
trajectory
length
k≥0
cid:80
finite
trajectories
eµk
k≥0
t≥0
assumption
requires
trajectories
ﬁnite
w.p
satisﬁed
proper
behavior
policies
equivalently
may
require
mdp
trajecto-
ries
eventually
reach
zero-value
absorbing
state
proof
closely
follows
proposition
5.2
requires
rewriting
update
suitable
form
verifying
assumptions
proposition
4.5.
proof
sketch
let
def=
cid:80
s=0
t−si
denote
accumulating
trace
follows
assumptions
total
update
phase
bounded
allows
write
online
version
k+1
dkαk
def=
cid:0
rπk
cid:2
cid:88
k+1
qk+1
cid:1
dkαk
tδπk
eµk
tδπk
t≥0
cid:104
cid:88
cid:0
t≥0
cid:3
cid:105
cid:1
t≥0
use
shorthand
combining
assumptions
combined
turn
assumption
assures
new
stepsize
sequence
˜αk
dkαk
satisﬁes
assumption
prop
4.5.
assumptions
require
variance
noise
term
bounded
residual
converge
zero
shown
identically
corresponding
results
assumption
assumption
satisﬁed
finally
assumption
satisﬁed
lemmas
policy
evaluation
control
cases
respectively.2
conclude
k∈n
converges
respective
settings
w.p
sequence
def=
cid:80
def=
dkαk
experimental
results
although
proof
tradeoﬀ
see
section
control
case
wished
investigate
whether
tradeoﬀ
observed
experimentally
end
applied
bicycle
domain
agent
must
simultaneously
balance
bicycle
drive
goal
position
six
real-valued
variables
describe
state
angle
velocity
etc
bicycle
reward
function
proportional
angle
goal
gives
falling
reaching
goal
discount
factor
0.99.
q-function
approximated
using
multilinear
interpolation
uniform
grid
size
···
stepsize
tuned
0.1.
chieﬂy
interested
interplay
parameter
ε-greedy
exploration
policy
main
performance
indicator
frequency
goal
reached
greedy
policy
500,000
episodes
training
report
three
ﬁndings
higher
values
lead
improved
learning
low
values
exhibit
lower
performance
q-function
diverges
high
relative
together
ﬁndings
suggest
indeed
λ−ε
tradeoﬀ
control
case
well
lead
conclude
proper
care
beneﬁcial
oﬀ-policy
control
note
control
case
goes
without
modiﬁcations
values
prescribed
lemma
fig
left
performance
bicycle
domain
conﬁguration
average
ﬁve
trials
marks
lowest
value
0.03
causes
divergence
right
maximum
non-diverging
function
left-hand
shaded
region
corresponds
hypothesized
bound
parameter
settings
right-hand
shaded
region
produce
meaningful
policies
learning
speed
performance
figure
left
depicts
performance
terms
goal-reaching
frequency
three
values
agent
performs
best
0.05
0.003
0.03
high
w.r.t
values
λ.3
divergence
value
determined
highest
safe
choice
result
divergence
figure
right
illustrates
marked
decrease
safe
value
increases
note
left-hand
shaded
region
corresponding
policy
evaluation
bound
1−γ
supporting
hypothesis
true
bound
section
appears
clear
maximum
safe
value
depends
particular
notice
stops
diverging
exactly
predicted
bound
related
work
section
place
presented
algorithms
context
existing
work
focusing
particular
action-value
methods
usual
let
t≥0
trajectory
generated
following
behavior
policy
i.e
·|xt
time
sarsa
updates
q-function
follows
qs+1
aλr
cid:124
cid:123
cid:122
cid:125
γt−srt
γn+1q
xs+n+1
as+n+1
t=s
denotes
update
made
time
rewritten
terms
one-step
td-errors
recall
randløv
alstrøm
agent
trained
using
sarsa
0.95
s+n
cid:88
averageendperformance✏=0✏=0.003✏=0.03maximumnon-diverging 
cid:88
t−sδt
xt+1
at+1
t≥s
sarsa
on-policy
algorithm
converges
value
function
behavior
policy
diﬀerent
algorithms
arise
instantiating
diﬀerently
table
provides
full
details
text
specify
revealing
components
update
7.1
policy
evaluation
one
imagine
considering
expectations
action-values
corresponding
states
eπq
place
value
sampled
action
i.e
γeπq
xt+1
eπq
one-step
update
general
q-learning
generalization
expected
sarsa
arbitrary
policies
refer
direct
eligibility
trace
extensions
algorithms
formed
via
equations
general
expected
sarsa
ﬁrst
mentioned
sutton
unfor-
tunately
oﬀ-policy
setting
general
converge
value
function
target
policy
stated
following
proposition
proposition
stable
point
general
i−λγ
µ−p
−1r
ﬁxed
point
operator
proof
writing
algorithm
operator
form
get
γn+1
cid:105
cid:88
t≥0
t=0
n≥0
cid:104
cid:88
cid:88
cid:104
cid:105
cid:104
πqµ
cid:105
λγp
thus
ﬁxed
point
satisﬁes
following
λγp
πqµ
µqµ
cid:104
cid:105
solving
yields
result
alternatively
replacing
terms
expectation
one
may
replace
value
next
state
xt+1
eπq
xt+1
obtaining
γeπq
xt+1
exactly
policy
evaluation
algorithm
speciﬁcally
get
on-policy
induced
on-policy
correction
may
serve
variance
reduction
term
expected
sarsa
may
helpful
refer
n-step
return
table
observe
leave
variance
analysis
algorithm
future
work
cid:54
recover
oﬀ-policy
stated
conditions
converges
target
policy
probability
methods
algorithms
directly
descend
basic
sarsa
often
learning
oﬀ-policy
requires
special
treatment
example
typical
oﬀ-policy
technique
importance
sampling
classical
monte
carlo
method
allows
one
sample
available
distribution
obtain
unbiased
consistent
samples
desired
one
reweighing
samples
likelihood
ratio
according
two
distribu-
tions
updates
ordinary
per-decision
algorithm
policy
evaluation
made
follows
t−sδt
cid:88
cid:89
t≥s
i=s+1
at+1|st+1
at+1|st+1
ai|xi
ai|xi
xt+1
at+1
family
algorithms
converges
probability
soft
stationary
behavior
several
recent
oﬀ-policy
algorithms
reduce
variance
methods
cost
added
bias
however
oﬀ-policy
perhaps
related
closest
tree-backup
algorithm
also
discussed
precup
one-step
td-error
algorithms
back
tree
neither
requires
knowledge
behavior
policy
important
diﬀerence
weighting
updates
oﬀ-policy
precaution
weighs
updates
along
trajectory
cumulative
target
probability
trajectory
point
cid:88
t≥s
cid:89
i=s+1
t−sδπ
ai|xi
weighting
simpliﬁes
convergence
argument
allowing
con-
verge
without
restrictions
distance
drawback
case
near
on-policy-ness
close
product
probabilities
cuts
traces
unnecessarily
es-
pecially
policies
stochastic
show
paper
plain
td-learning
converge
oﬀ-policy
special
treatment
subject
tradeoﬀ
condition
condition
applies
on-
oﬀ-policy
without
modiﬁcations
ideal
algorithm
able
au-
tomatically
cut
traces
like
case
extreme
oﬀ-policy-ness
reverting
near
on-policy
7.2
control
perhaps
popular
version
due
watkins
dayan
oﬀ-policy
truncates
return
bootstraps
soon
behavior
policy
takes
non-greedy
action
described
following
update
t−sδt
s+τ
cid:88
t=s
min
as+u
arg
maxa
xs+u
note
update
arg
maxa
replacing
probability
product
policies
similar
small
truncation
may
greatly
reduce
beneﬁt
complex
backups
special
case
deterministic
greedy
policies
cid:81
brid
sarsa
watkins
n-step
return
cid:80
s+n
peng
williams
meant
remedy
hy-
t=s
γt−srt
i=s+1
γn+1
maxa
xs+n+1
requires
following
form
td-error
max
xt+1
max
fact
update
rule
general
deﬁned
greedy
policy
following
steps
proof
proposition
limit
algorithm
converges
ﬁxed
point
operator
diﬀerent
unless
behavior
always
greedy
sutton
barto
mention
another
naive
version
watkins
cut
trace
non-greedy
actions
exactly
algo-
rithm
described
paper
notice
despite
similarity
watkins
equivalence
representation
diﬀerent
one
would
derived
setting
since
n-step
return
uses
corrected
immediate
reward
maxa
instead
im-
mediate
reward
alone
correction
invisible
watkins
since
behavior
policy
assumed
greedy
return
cut
conclusion
formulated
new
algorithms
family
oﬀ-policy
policy
evalu-
ation
control
unlike
traditional
oﬀ-policy
learning
algorithms
meth-
ods
involve
weighting
returns
policy
probabilities
yet
right
conditions
converge
correct
ﬁxed
points
policy
evaluation
convergence
subject
tradeoﬀ
degree
bootstrapping
dis-
tance
policies
discount
factor
control
determining
existence
non-trivial
ε-dependent
bound
remains
open
problem
supported
telling
empirical
results
bicycle
domain
hypothesize
bound
exists
closely
resembles
1−γ
bound
policy
evaluation
case
acknowledgements
authors
thank
hado
van
hasselt
others
google
deepmind
well
anonymous
reviewers
thoughtful
feedback
paper
references
richard
bellman
dynamic
programming
princeton
university
press
1957
dimitry
bertsekas
john
tsitsiklis
neuro-dynamic
programming
athena
scientiﬁc
1996
assaf
hallak
aviv
tamar
r´emi
munos
shie
mannor
generalized
emphatic
temporal
diﬀerence
learning
bias-variance
analysis
arxiv:1509.05172
2015
michael
kearns
satinder
singh
bias-variance
error
bounds
temporal
diﬀerence
updates
conference
computational
learning
theory
pages
142–
147
2000
ashique
mahmood
richard
sutton
oﬀ-policy
learning
based
weighted
importance
sampling
linear
computational
complexity
conference
un-
certainty
artiﬁcial
intelligence
2015
ashique
mahmood
huizhen
martha
white
richard
sutton
em-
phatic
temporal-diﬀerence
learning
arxiv
preprint
arxiv:1507.01569
2015
jing
peng
ronald
williams
incremental
multi-step
q-learning
machine
learning
1-3
:283–290
1996
doina
precup
richard
sutton
satinder
singh
eligibility
traces
oﬀ-
policy
policy
evaluation
international
conference
machine
learning
2000
doina
precup
richard
sutton
sanjoy
dasgupta
oﬀ-policy
temporal-
diﬀerence
learning
function
approximation
international
conference
machine
learning
2001
10.
martin
puterman
markov
decision
processes
discrete
stochastic
dynamic
programming
john
wiley
sons
inc.
new
york
usa
1st
edition
1994
11.
jette
randløv
preben
alstrøm
learning
drive
bicycle
using
reinforcement
learning
shaping
international
conference
machine
learning
1998
12.
gavin
rummery
mahesan
niranjan
on-line
q-learning
using
connectionist
systems
technical
report
cambridge
university
engineering
department.
1994
13.
satinder
singh
peter
dayan
analytical
mean
squared
error
curves
tem-
poral
diﬀerence
learning
machine
learning
:5–40
1998
14.
richard
sutton
learning
predict
methods
temporal
diﬀerences
machine
learning
:9–44
1988
15.
richard
sutton
generalization
reinforcement
learning
successful
examples
using
sparse
coarse
coding
advances
neural
information
processing
systems
1996
16.
richard
sutton
andrew
barto
reinforcement
learning
introduction
cambridge
univ
press
1998
17.
richard
sutton
ashique
mahmood
doina
precup
hado
van
hasselt
new
interim
forward
view
monte
carlo
equivalence
international
conference
machine
learning
pages
568–576
2014
18.
hado
philip
van
hasselt
insights
reinforcement
learning
formal
analysis
empirical
evaluation
temporal-diﬀerence
learning
algorithms
phd
thesis
universiteit
utrecht
january
2011
19.
harm
van
seijen
richard
sutton
true
online
international
conference
machine
learning
pages
692–700
2014
20.
harm
van
seijen
hado
van
hasselt
shimon
whiteson
marco
wiering
theoretical
empirical
analysis
expected
sarsa
adaptive
dynamic
pro-
gramming
reinforcement
learning
pages
177–184
ieee
2009
21.
christopher
watkins
peter
dayan
q-learning
machine
learning
:272–292
1992
22.
christopher
john
cornish
hellaby
watkins
learning
delayed
rewards
phd
thesis
king
college
cambridge
1989
cid:80
cid:54
cid:81
cid:80
cid:80
cid:80
cid:80
cid:80
cid:80
cid:80
cid:80
cid:81
cid:81
cid:80
cid:80
cid:3
cid:81
cid:81
cid:2
cid:81
cid:3
cid:54
cid:81
cid:2
cid:3
cid:2
cid:80
cid:80
cid:80
cid:80
cid:80
cid:80
cid:80
cid:80
cid:80
cid:80
cid:54
cid:54
cid:54
