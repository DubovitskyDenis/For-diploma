towards
resolving
unidentiﬁability
inverse
reinforcement
learning
kareem
amin
university
michigan
amkareem
umich.edu
satinder
singh
university
michigan
baveja
umich.edu
abstract
consider
setting
inverse
reinforcement
learning
irl
learner
extended
ability
ac-
tively
select
multiple
environments
observing
agent
be-
havior
environment
ﬁrst
demonstrate
learner
experiment
transition
dynamic
ﬁxed
set
states
actions
exists
algorithm
reconstructs
agent
reward
function
fullest
extent
theoretically
possible
requires
small
logarithmic
number
experiments
con-
trast
result
known
irl
single
ﬁxed
environments
namely
true
reward
function
fun-
damentally
unidentiﬁable
extend
setting
realistic
case
learner
may
select
transition
dynamic
rather
restricted
ﬁxed
set
environments
may
try
connect
problem
maximizing
information
derived
experiments
active
submodular
function
maximization
demonstrate
greedy
algorithm
near
optimal
logarithmic
factors
finally
empirically
validate
algorithm
environment
inspired
behavioral
psychology
introduction
inverse
reinforcement
learning
irl
ﬁrst
introduced
russell
concerned
problem
in-
ferring
unknown
reward
function
agent
behaving
optimally
markov
decision
process
basic
for-
mulation
problem
asks
given
known
environment1
optimal
agent
policy
deduce
reward
function
makes
optimal
mdp
irl
seen
number
applications
develop-
ment
autonomous
systems
autonomous
vehicle
operation
even
cooperative
human
agent
might
great
diﬃcultly
describing
incentives
however
problem
fundamental
almost
study
involves
behavioral
modeling
consider
experi-
mental
psychologist
attempting
understand
internal
motivations
subject
say
mouse
consider
marketer
observing
user
behavior
website
hoping
understand
potential
consumer
value
various
oﬀers
noted
russell
fundamental
complication
goals
irl
impossibility
identifying
ex-
act
reward
function
agent
behavior
gen-
eral
may
inﬁnitely
many
reward
functions
consis-
tent
observed
policy
ﬁxed
environment
1we
use
terminology
environment
refer
mdp
without
reward
function
since
true
reward
function
fundamentally
unidentiﬁ-
able
much
previous
work
irl
concerned
development
heuristics
prefer
certain
re-
wards
better
explanations
behavior
others
contrast
make
several
major
contributions
to-
wards
directly
resolving
issue
unidentiﬁability
irl
paper
ﬁrst
contribution
separate
causes
unidentiﬁability
three
classes
trivial
reward
func-
tion
assigning
constant
reward
state-action
pairs
makes
behaviors
optimal
agent
constant
reward
execute
policy
including
observed
reward
function
behaviorally
invariant
certain
arithmetic
operations
re-scaling
finally
behavior
ex-
pressed
observed
policy
may
suﬃcient
distinguish
two
possible
reward
functions
rationalize
observed
behavior
i.e.
observed
behavior
could
optimal
reward
functions
refer
ﬁrst
two
cases
unidentiﬁability
repre-
sentational
unidentiﬁability
third
experimental
unidentiﬁability
second
contribution
demonstrate
representational
unidentiﬁability
unavoidable
experimen-
tal
unidentiﬁability
contrast
previous
methods
demonstrate
latter
eliminated
com-
pletely
cases
moreover
manner
make
precise
section
argue
ways
representational
unidentiﬁability
superﬁcial
eliminating
experimental
unidentiﬁability
one
arrives
fullest
possible
characterization
agent
reward
func-
tion
one
hope
third
contribution
develop
slightly
richer
model
irl
suppose
learner
observe
agent
behaving
optimally
number
environments
learner
choosing
notice
many
moti-
vating
examples
reasonable
assume
learner
indeed
power
one
ask
operator
vehicle
drive
multiple
terrains
exper-
imental
psychologist
might
observe
mouse
across
num-
ber
environments
experimenter
orga-
nize
dynamics
maze
one
key
results
right
choice
environments
learner
eliminate
experimental
unidentiﬁability
study
repeated
experimentation
irl
two
settings
one
learner
omnipotent
restrictions
environments
presented
agent
another
restrictions
type
environments
learner
present
show
former
case
experimental
unidentiﬁability
eliminated
small
number
environments
latter
case
cast
problem
budgeted
exploration
show
number
environments
simple
greedy
algorithm
approximately
maximizes
information
revealed
environments
closely
related
work
prior
work
irl
mostly
focused
inferring
agent
reward
function
data
acquired
ﬁxed
en-
vironment
consider
setting
learner
actively
select
multiple
environments
explore
using
observations
obtained
environments
infer
agent
reward
studying
model
agent
make
active
selections
environments
irl
setting
novel
best
knowledge
previous
applications
active
learning
irl
con-
sidered
settings
single
environment
learner
query
agent
action
state
information
reward
prior
work
using
data
collected
multiple
exogenously
ﬁxed
environments
predict
agent
behavior
also
applications
methods
single-environment
mdps
adapted
multi-
ple
environments
nevertheless
works
attempt
resolve
ambiguity
inherent
recovering
true
reward
irl
describe
irl
ill-
posed
problem
result
works
ultimately
consider
objective
mimicking
predicting
agent
optimal
behavior
perfectly
reasonable
objective
interested
settings
identiﬁcation
goal
among
many
reasons
may
learner
explicitly
desires
interpretable
model
agent
behavior
learner
desires
transfer
learned
reward
function
new
settings
economics
literature
problem
inferring
agent
utility
behavior
long
studied
heading
utility
preference
elicitation
models
analyze
markovian
environments
assume
ﬁxed
environment
learner
ask
certain
types
queries
bound
queries
elic-
iting
whether
state-action
reward
instead
interested
cases
learner
make
inferences
agent
behavior
external
source
information
manipulate
environments
agent
acts
setting
preliminaries
denote
environment
tuple
...
ﬁnite
set
states
agent
ﬁnd
ﬁnite
set
actions
available
agent
collection
transition
dynamics
a∈a
represent
row-stochastic
matrix
rd×d
cid:48
denoting
agent
probability
transitioning
state
cid:48
state
selecting
action
agent
discount
factor
represent
agent
reward
function
vector
indicating
undiscounted
payout
ar-
riving
state
note
joint
choice
markovian
environment
reward
function
ﬁxes
mdp
policy
mapping
slight
abuse
notation
represent
matrix
take
s-row
s-row
action
chosen
state
let
opt
denote
set
policies
optimal
maximizing
agent
expected
time-discounted
rewards
mdp
consider
repeated
experimen-
tation
setting
suppose
learner
able
select
sequence
environments2
...
sequentially
observing
...
satisfying
opt
unknown
agent
reward
function
call
experiment
goal
experimenter
output
re-
ward
estimate
approximating
true
reward
function
many
settings
assumption
learner
di-
rectly
observe
agent
full
policy
strong
realistic
assumption
learner
observes
trajec-
tories
denotes
sequence
state-action
pairs
drawn
according
distribution
induced
agent
playing
policy
environment
refer
former
feedback
model
policy
observation
setting
latter
trajectory
observation
setting
fundamental
theorem
irl
follows
rewriting
bellman
equations
associated
optimal
policy
single
mdp
noting
components
vector
γpπ
−1r
correspond
q-value
action
policy
reward
states
theorem
russell
let
arbitrary
environment
opt
γpπ
−1r
0.3
key
take-away
theorem
pol-
icy
observation
setting
set
reward
functions
con-
sistent
observed
optimal
policy
precisely
satisfying
set
linear
constraints
furthermore
constraints
computed
environment
policy
thus
object
make
recurring
reference
set
reward
functions
consistent
experiment
denoted
|∀a
γpπ
−1r
rmin
rmax
since
intersection
linear
constraints
de-
ﬁnes
convex
polytope
fact
later
algorith-
mic
importance
immediate
corollary
theorem
given
sequence
experiments
...
set
rewards
consistent
precisely
cid:44
also
think
trajectory
inducing
partial
policy
states
visited
trajectory
particu-
lar
let
denote
domain
say
two
policies
cid:48
consistent
denoted
cid:48
cid:48
thus
given
set
rewards
consistent
observa-
tion
precisely
γpπ
−1r
rmin
rmax
given
sequence
...
deﬁne
trajectory
setting
2deﬁned
state
action
spaces
3the
inequality
read
component-wise
rela-
tion
holds
standard
holds
component
identification
section
give
nuanced
characteriza-
tion
means
identify
reward
function
argue
multiple
types
uncertainty
in-
volved
identifying
categorize
represen-
tational
unidentiﬁability
experimental
unidentiﬁability
furthermore
argue
ﬁrst
type
ways
super-
ﬁcial
ought
ignored
second
type
eliminated
begin
deﬁnition
let
cid:48
reward
func-
tions
deﬁned
state
space
say
cid:48
behaviorally
equivalent
environment
also
deﬁned
agent
whose
reward
function
behaves
identically
agent
whose
reward
function
cid:48
definition
two
reward
vectors
cid:48
deﬁned
behaviorally
equivalent
denoted
cid:48
set
actions
transition
dynamics
discount
deﬁning
environment
opt
opt
cid:48
behavioral
equivalence
deﬁnes
equivalence
relation
vectors
let
cid:48
cid:48
denote
equivalence
classes
deﬁned
manner
intuitively
cid:48
behaviorally
equivalent
induce
identical
optimal
policies
every
single
environment
therefore
really
diﬀerent
reward
functions
simply
diﬀerent
representations
incentives
observe
behavioral
equivalence
classes
in-
variant
multiplicative
scaling
positive
scalars
component-wise
translation
constant
intuitively
easy
see
adding
reward
every
state
reward
function
aﬀect
agent
decision-making
simply
background
reward
agent
gets
free
similarly
scaling
positive
constant
simply
changes
units
used
represent
rewards
agent
care
whether
reward
represented
dollars
cents
prove
formally
following
theorem
theorem
let
cid:126
denote
vector
components
equal
cid:126
proof
first
consider
cid:126
deﬁned
statement
theorem
fix
environment
action
arbitrary
policy
begin
claiming
γpπ
cid:126
cid:126
woodbury
formula
matrix
inversion
tells
γpπ
γpπ
−1γpπ
furthermore
row-stochastic
matrix
cid:126
cid:126
therefore
γpπ
cid:126
γpπ
−1γpπ
cid:126
cid:126
γpπ
cid:126
γpπ
−1γ
cid:126
−1γpπ
cid:126
since
must
cid:126
reward
function
arbitrary
envi-
ronment
consider
opt
theorem
know
opt
γpπ
−1r
occurs
γpπ
since
positive
scalar
finally
conclude
opt
γpπ
cid:126
last
condition
implying
opt
cid:126
theorem
since
choice
arbitrary
deﬁnition
cid:126
concluding
proof
thus
argue
one
reason
reward
functions
can-
identiﬁed
trivial
one
classic
irl
problem
consistent
representation
reward
functions
uncountable
number
functions
namely
cid:126
cid:126
behaviorally
identical
however
distinguishing
be-
tween
functions
irrelevant
whether
agent
true
reward
function
1/3
2/3
simply
matter
units
used
represent
rewards
light
observation
convenient
canon-
ical
element
equivalence
class
con-
stant
reward
function
take
canonicalized
rep-
resentation
cid:126
otherwise
note
way
theo-
rem
translated
re-scaled
maxs
mins
carefully
non-constant
take
canonicalized
representation
mins
maxs
mins
canonicalization
consistent
behavioral
equivalence
state
following
theorem
whose
proof
found
appendix
consequence
theorem
use
notation
interchangeably
refer
equivalence
class
unique
canonical
element
theorem
cid:48
cid:48
canonicalized
representation
next
consider
issue
trivial/constant
rewards
cid:126
since
irl
problem
ﬁrst
formulated
ob-
served
single
experiment
ever
determine
agent
reward
function
constant
reward
function
algebraic
reason
fact
cid:126
always
solution
linear
system
intuitive
reason
fact
optimal
policy
agent
whose
reward
cid:126
therefore
consider
agent
whose
true
reward
cid:54
cid:126
even
policy
observation
setting
cid:126
furthermore
dis-
appear
multiple
experimentation
sequence
experiments
also
remains
cid:126
consider
agent
whose
true
reward
function
cid:126
crucial
consequence
irl
algorithm
guarantees
identify
cid:126
necessarily
misiden-
tiﬁes
non-trivial
reward
functions
agent
trivial
reward
function
allowed
behave
arbitrar-
ily
therefore
may
choose
behave
consistently
non-trivial
reward
irl
algorithm
guaran-
tees
identiﬁcation
trivial
rewards
therefore
misidentify
agent
whose
true
reward
leads
following
revised
deﬁnition
iden-
tiﬁcation
accounts
call
representational
unidentiﬁability
definition
say
irl
algorithm
succeeds
identiﬁcation
observing
behavior
4we
get
1/3
2/3
subtracting
every
state
dividing
figure
observing
agent
behavior
envi-
ronment
set
rewards
consistent
observed
behavior
depicted
shaded
region
previous
work
concerned
designing
selection
rules
pick
point
region
depicted
red
circle
amount
experimentation
remove
representational
unidentiﬁability
setting
depicted
darker
shaded
region
nevertheless
adding
constraints
cid:48
cid:48
induced
second
experiment
disproves
original
selection.
removing
experimental
unidentiﬁability
agent
true
reward
algorithm
outputs
whenever
cid:54
cid:126
notice
deﬁnition
accomplishes
two
things
first
excuses
algorithm
decisions
repre-
sented
words
asserts
salient
task
irl
computing
member
literal
secondly
true
reward
function
constant
i.e
cid:54
cid:126
demands
algorithm
identify
represen-
tational
decisions
however
agent
really
reward
function
cid:126
algorithm
allowed
output
anything
words
algorithm
allowed
behave
arbitrarily
agent
behaves
arbitrarily.5
also
note
deﬁnition
relaxed
give
notion
approximate
identiﬁcation
state
definition
say
irl
algorithm
-identiﬁes
reward
function
observing
behavior
agent
true
reward
algorithm
outputs
||∞
whenever
cid:54
cid:126
even
deﬁnition
may
attainable
single
experiment
may
contain
multiple
behavioral
classes
call
phenonmenon
experimental
uniden-
tiﬁability
due
fact
experiment
may
simply
insuﬃcient
distinguish
cid:48
next
section
observe
source
uncertainty
reward
function
decreased
multiple
experimentation
depicted
figure
see
cap-
tion
details
words
distinguishing
represen-
tational
unidentiﬁability
experimental
unidentiﬁability
formally
resolve
latter
concrete
example
given
figure
de-
picts
grid-world
square
representing
state
ﬁgures
thick
lines
represent
impenetrable
walls
agent
policy
depicted
arrows
circle
indicating
agent
deciding
stay
grid
location
goal
learner
infer
reward
state
fig-
ures
depict
agent
policy
takes
shortest
path
location
labeled
start-
ing
location
one
explanation
behavior
depicted
5we
comment
practical
matter
one
usually
interested
rationalizing
behavior
agent
believed
non-trivial
figure
agent
policy
ﬁxed
environment
agent
move
one
four
directions
stay
location
represented
black
circle
thick
purple
lines
represent
impassable
walls
experiment
revealing
getting
requires
steps
getting
requires
fewer
agent
prefers
figure
agent
large
reward
state
zero
reward
every
state
however
equally
possible
explanation
state
also
gives
positive
reward
smaller
exists
shortest
path
also
passes
agent
take
depicted
figure
without
additional
in-
formation
two
explanations
distinguished
example
experimental
unidentiﬁability
nevertheless
resolved
additional
experimenta-
tion
observing
agent
environment
de-
picted
figure
learner
infers
indeed
rewarding
state
finally
observing
agent
behavior
environment
figure
reveals
agent
prefer
traveling
state
getting
requires
steps
getting
requires
steps
fewer
sub-
sequent
observations
allow
learner
relate
agent
reward
state
agent
reward
state
omnipotent
experimenter
setting
consider
repeated
experimentation
setting
environments
available
selection
exper-
imenter
completely
unrestricted
formally
envi-
ronment
selected
experimenter
belongs
class
containing
environment
every
feasible
set
transition
dynamics
call
om-
nipotent
experimenter
setting
describe
algorithm
omnipotent
experi-
menter
setting
-identiﬁes
using
log
d/
ex-
periments
omnipotent
experimenter
extremely
powerful
result
demonstrates
guarantee
ob-
tained
repeated
irl
setting
far
stronger
available
standard
single-environment
irl
setting
fur-
thermore
clariﬁes
distinction
experimental
unidentiﬁability
representational
unidentiﬁability
figure
typical
environment
second
phase
algo-
rithm
dotted
lines
represent
transitions
action
solid
lines
represent
transitions
action
4.1
omnipotent
identiﬁcation
algorithm
algorithm
proceeds
two
stages
in-
volve
simple
binary
searches
ﬁrst
stage
identify
states
smin
smax
smin
rmin
smax
rmax
second
stage
identiﬁes
αsrmin
rmax
throughout
al-
gorithm
makes
use
two
agent
actions
denote
therefore
describing
algorithm
assume
|a|
environment
selected
algorithm
fully
determined
choices
pa1
pa2
fact
|a|
omnipotent
experimenter
set-
ting
one
reduce
two-action
setting
making
remaining
actions
equivalent
either
a2.6
ﬁrst
address
task
identifying
smax
suppose
two
candidates
cid:48
smax
key
idea
ﬁrst
stage
algorithm
give
agent
abso-
lute
choice
two
states
setting
pa1
pa1
cid:48
cid:48
setting
pa2
cid:48
pa2
cid:48
agent
selecting
reveals
cid:48
agent
selecting
reveals
cid:48
test
conducted
d/2
distinct
pairs
states
single
experiment
thus
given
candidates
smax
single
experiment
narrow
set
candidates
k/2
guaranteed
one
remaining
states
satisﬁes
rmax
log
experiments
identify
single
state
smax
satisﬁes
smax
conducting
analogous
procedure
identiﬁes
state
smin
smin
smax
identiﬁed
take
...
sd−2
remaining
states
consider
environment
transition
dynamics
parameterized
αs1
...
αsd−2
typical
environment
phase
depicted
figure
environment
sets
smin
smax
sinks
pa1
smin
smin
pa1
smax
smax
pa2
smin
smin
pa2
smax
smax
remaining
pa1
smin
αsi
pa1
smax
αsi
taking
action
state
represents
probability
gamble
best
worst
state
fi-
nally
also
sets
pa2
taking
action
state
represents
receiving
sure
select-
ing
agent
reveals
αsrmin
rmax
choice
reveals
αsrmin
rmax
thus
binary
search
conducted
independently
order
determine
srmin
approximation
rmax
algorithm
succeeds
-identiﬁcation
summarized
following
theorem
proof
theorem
straightforward
analysis
binary
search
6doing
possible
setting
transition
dy-
namics
set
arbitrarily
theorem
let
deﬁned
letting
smin
smax
smin
smax
identiﬁed
described
true
reward
function
cid:54
cid:126
canonical
form
ˆr||∞
takeaway
setting
problems
regard-
ing
identiﬁcation
irl
circumvented
repeated
experimentation
thought
even
policy
obser-
vations
irl
question
fundamentally
ill-posed
how-
ever
see
repeated
experimentation
fact
possible
identify
arbitrary
precision
well-
deﬁned
sense
results
informative
believe
unrealistic
imagine
learner
arbi-
trarily
inﬂuence
environment
agent
next
section
develop
theory
repeated
experimentation
learner
restricted
select
environments
restricted
subset
possible
transition
dynamics
restricted
experimenter
setting
consider
setting
experimenter
restricted
universe
environments
choose
need
contain
every
possible
transition
dynamic
as-
sumption
required
execute
binary
search
algorithm
previous
section
best
experimenter
could
ever
hope
try
every
environment
gives
experimenter
available
information
agent
reward
function
thus
interested
max-
imizing
information
gained
experimenter
minimizing
number
experiments
conducted
prac-
tice
observing
agent
may
expensive
hard
come
even
small
budget
experiments
learner
would
like
select
environments
maximally
reduce
experimental
unidentiﬁability
sequence
experiments
observed
know
consistent
observed
sequence
thus
value
repeated
experimen-
tation
allowing
learner
select
environments
informative
possible
contrast
note
previous
work
irl
largely
focused
designing
heuristics
selection
problem
picking
ﬁxed
set
equally
possible
reward
functions
thus
interested
making
small
irl
traditionally
focused
selecting
exogenously
ﬁxed
deﬁning
mean
small
review
preexisting
methods
selecting
5.1
generalized
selection
heuristics
standard
single-environment
setting
given
en-
vironment
observed
policy
learner
must
make
selection
among
one
rewards
heuris-
tic
suggested
motivated
idea
given
state
reward
function
maximizes
dif-
ference
q-value
observed
action
state
action
cid:54
gives
strongest
ex-
planation
behavior
observed
agent
thus
reasonable
linear
selection
criterion
maximize
sum
diﬀerences
across
states
adding
regularization
term
encourages
selection
reward
functions
also
sparse
putting
together
standard
selection
heuristic
single-environment
irl
select
maximizes
cid:18
cid:88
s∈s
γpπ
min
cid:54
cid:19
−1r
λ|r
two
natural
candidates
generalizing
se-
lection
rule
repeated
experimentation
setting
instead
single
experiment
experimenter
en-
countered
sequence
observations
ﬁrst
sum
environment
state
pairs
minimum
diﬀerence
q-value
action
selected
agent
action
second
sum
states
taking
minimum
environment
action
pairs
one
could
make
arguments
motivating
ultimately
objective
heuristic
however
argue
strong
algorithmic
reason
preferring
lat-
ter
objective
particular
former
objective
grows
dimensionality
environments
added
quickly
resulting
intractable
dimension
objective
latter
equation
however
remains
constant.7
cid:88
s∈s
maximize
r∈k

min
cid:54
=πi
−1r
−λ|r
selection
rules
single-environment
setting
generalizable
repeated
experimen-
tation
setting
including
heuristics
inﬁnite
state
set-
ting
trajectory
heuristics
well
approaches
already
adapted
multiple
environments
due
space
con-
straints
discuss
foundational
approach
goal
simply
emphasize
dichotomy
adapting
pre-existing
irl
methods
data
gathered
multiple
environments
however
data
generated
problem
best
select
environments
begin
latter
problem
focus
next
section
5.2
adaptive
experimentation
given
universe
candidate
environments
ask
select
small
number
environments
environments
maximally
informative
must
ﬁrst
decide
mean
informative.
propose
set
experiments
either
policy
tra-
jectory
setting
natural
objective
minimize
mass
resulting
space
possible
rewards
respect
measure
distribution
lebesgue
measure
uniform
distribution
corresponds
natural
goal
reducing
volume
much
possible
thus
deﬁne
volµ
cid:90
pr∼µ
ﬁnd
convenient
cast
maximization
problem
therefore
also
deﬁne
volµ
7writing
equation
standard
form
requires
translating
min
constraints
thus
number
constraints
grows
number
experiments
demonstrate
experimental
results
tractable
solvers
upper
bound
volume
−rmax
rmax
goal
maximize
objective
several
desirable
properties
first
foremost
reducing
volume
eliminate
space
possible
reward
functions
i.e
experimental
uniden-
tiﬁability
secondly
repeated
experimentation
setting
fundamentally
active
learning
setting
think
true
unknown
function
labels
environ-
ments
either
corresponding
policy
trajectory
thus
volume
operator
corresponds
reducing
version
space
possible
rewards
furthermore
see
later
section
objective
monotone
sub-
modular
function
assumption
well-studied
active
learning
literature
allowing
prove
guarantees
greedy
algorithm
finally
normally
think
lebesgue
measure
vol
volume
d-dimensional
euclidean
space
uniform
distribution
−rmax
rmin
how-
ever
choice
makes
objective
quite
general
example
making
uniform
-net
vol
corre-
sponds
counting
number
rewards
-apart
respect
metric
many
settings
naturally
comes
discrete
space
corners
hypercube
readily
modeled
correct
choice
fact
thought
simply
prior
−rmax
rmax
ready
describe
simple
algorithm
adaptively
selects
environments
attempting
greed-
ily
maximize
depicted
algorithm
algorithm
greedy
environment
selection
input
arg
max
min
min
r∈k
π∈opt
observe
policy
end
return
order
state
performance
guarantee
algo-
rithm
use
fact
submodular
non-
decreasing
function
subsets
environment
observation
pairs
2u×o
set
possible
observations
lemma
submodular
non-decreasing
function
proof
given
set
component
use
denote
union
singleton
set
let
set
possible
observations
trajectory
trajectory
setting
policy
policy
setting
let
space
possible
environments
fix
2u×o
cid:54
deﬁnition
vol
cid:54
establishes
submodularity
since
arbi-
trary
right-hand-side
second
equality
non-
zero
also
monotone
vol
cid:82
cid:82
cid:54
performance
algorithm
function
many
experiments
attempted
thus
analysis
must
policy
observations
trajectory
observations
figure
plot
displays
ˆr||∞
error
predicted
vector
policy
observation
setting
bars
indicating
standard
error
plot
displays
trajectory
setting
take
account
let
deterministic
algorithm
deploys
experiments
worst-case
performance
depends
true
reward
policies
observed
say
sequence
experiments
...
consistent
chooses
environment
ej+1
observing
subse-
quence
experiments
...
ei-
ther
trajectory
policy
consistent
denoting
set
consistent
experiments
best
perfor-
mance
algorithm
guarantee
experiments
optn
maxan
minr
mine∈c
submodularity
allows
prove
greedy
environment
selection
algorithm8
needs
slightly
experiments
logarithmic
factor
attain
optn
theorem
returned
greedy
environment
se-
lection
algorithm
satisﬁes
optn
optn/
proof
theorem
uses
many
techniques
used
guillory
work
interactive
set
cover
technical
reasons
state
theorem
directly
corollary
results
assume
ﬁnite
hypothesis
class
whereas
inﬁnite
space
pos-
sible
rewards
nevertheless
proofs
easily
adapted
setting
full
proofs
given
appendix
finally
note
line
computable
exactly
without
parametric
assumptions
class
environ-
ments
space
rewards
practice
de-
scribe
next
section
approximate
exact
maxi-
mization
sampling
environments
rewards
optimizing
sampled
sets
experimental
analysis
deploy
techniques
discussed
setting
demon-
strating
maximizing
indeed
eﬀective
identi-
fying
imagine
agent
8n.b
trajectory
setting
one
would
replace
mini-
mization
opt
line
algorithm
minimization
consistent
opt
dropped
grid
world
experimenter
would
like
infer
agent
reward
space
grid
imag-
ine
experimenter
power
construct
walls
agent
environment
alternatively
re-
fer
environment
maze
motivate
value
repeated
experimentation
recall
figure
restricted
environment
learner
learner
example
make
action
causes
agent
travel
bottom
corner
maze
top
corner
however
learner
modify
dynamics
environment
far
construct
maze
walls
evaluate
algorithm
grids
size
10.
agent
reward
given
vector
r100
||r||∞
rmax
rmax
taken
follows
simulation
randomly
assign
state
reward
rmax
assign
states
reward
1.9
remaining
states
give
reward
agent
discount
rate
taken
0.8.
goal
learner
deter-
mine
states
rewarding
determine
latter
states
yield
1/10
reward
former
figure
display
main
experimental
results
four
diﬀerent
algorithms
policy
observation
setting
figure
trajectory
setting
error
represents
||r−
ˆr||∞
algorithm
prediction
error
bars
representing
standard
error
simulations
figure
horizontal
line
displays
best
re-
sults
achieved
without
repeated
experimentation
learner
selects
single
environment
observing
pol-
icy
stuck
whatever
experimental
unidentiﬁabil-
ity
exists
scenario
select
according
classic
irl
heuristic
given
section
5.1
choice
since
performance
method
depends
environ-
ment
used
choice
randomly
generated
100
diﬀerent
environments
environ-
ments
selected
0.05
evaluated
single-environment
approaches
9for
motivation
one
might
think
agent
mouse
rewards
corresponding
food
pellets
various
shiny
objects
mouse
cage
simulations
best
error
among
1300
diﬀerent
single-environment
algorithms
displayed
horizontal
line
immediately
see
experimental
unidentiﬁa-
bility
using
single
environment
makes
diﬃcult
distinguish
actual
reward
function
err
best
choice
greater
remaining
algorithms
describe
greater
detail
conduct
repeated
experimentation
algorithms
uses
diﬀerent
rule
select
new
environment
round
given
sequence
environ-
ment
policy
pairs
generated
algorithms
solve
end
round
done
choice
0.5
algorithms
besides
greedy
algorithm
previous
section
implement
two
algorithms
conduct
repeated
experiments
non-adaptively
randuniform
round
selects
maze
uniformly
random
space
possible
mazes
wall
present
probability
0.5
note
randuniform
tend
select
mazes
roughly
half
walls
present
thus
also
consider
randvaried
round
selects
maze
diﬀerent
distribution
mazes
drawn
generated
two-step
process
first
row
column
select
numbers
i.i.d
uniform
distribution
wall
along
row
column
respectively
created
probability
respectively
although
probability
particular
wall
present
still
0.5
correlations
creates
variable
mazes
e.g
allowing
entire
row
sparsely
populated
walls
implement
algorithm
greedy
previous
sec-
tion
approximating
maximization
line
algo-
rithm
approximation
done
sampling
environ-
ments
distribution
used
randvaried
policy
observation
setting
1000
samples
ﬁrst
drawn
consistent
set
using
hit-and-run
sampler
mcmc
method
uniformly
sampling
high-
dimensional
convex
sets
polynomial
time
samples
also
used
estimate
volume
tra-
jectory
setting
ﬁrst
sample
trajectories
environ-
ment
use
arbitrary
proxy
examining
results
see
greedy
converges
sig-
niﬁcantly
quicker
either
non-adaptive
approaches
rounds
experimentation
policy
observa-
tion
setting
greedy
attains
error
0.2687
±0.0302
best
non-adaptive
approach
attains
0.9691
±0.24310
greedy
requires
rounds
reach
similar
error
0.9678
±0.0701
note
performance
greedy
seems
continue
improve
non-
adaptive
approaches
appear
stagnate
could
due
fact
certain
number
rounds
non-
adaptive
approaches
received
information
avail-
able
environments
typically
sampled
dis-
tributions
order
make
progress
must
receive
new
information
contrast
greedy
designed
ac-
tively
select
environments
finally
greedy
runs
selecting
sequence
environ-
ments
resulting
observations
selects
using
thus
regularization
parameter
free
parameter
greedy
took
equal
0.5
results
figure
conclude
experi-
mentally
analyzing
sensitivity
greedy
choice
parameter
well
randuniform
randvaried
also
select
according
increased
eventually
over-regularizes
optimized
taking
setting
begins
occur
cid:126
begin
see
pathological
behavior
figure
problem
occurs
standard
irl
one
approach
select
large
lambda
transition
hence
choice
0.5.
however
even
signiﬁcantly
smaller
results
qualitatively
similar
figure
figure
ﬁnd
long
large
results
sensitive
choice
0.05
figure
result
repeated
experimentation
algorithms
using
large
small
regularization
parameter
conclusions
provide
number
contributions
work
first
separate
causes
unidentiﬁability
irl
problems
two
classes
representational
experimental
argue
representational
unidentiﬁability
superﬁcial
leading
redeﬁne
problem
identiﬁcation
irl
according
deﬁnition
previous
work
distinguish
two
classes
demonstrate
algorithms
designed
eliminate
exper-
imental
unidentiﬁability
providing
formal
guarantees
along
way
derive
new
model
irl
learner
observe
behavior
multiple
environments
model
believe
interesting
right
also
key
eliminating
experimental
unidentiﬁability
give
algorithm
powerful
learner
ob-
serve
agent
behavior
environment
show
algorithm
-identiﬁes
agent
reward
deﬁned
states
observing
behavior
log
d/
environments
weaken
learner
model
realistic
settings
learner
might
restricted
types
envi-
ronments
may
choose
may
able
elicit
small
number
demonstrations
agent
derive
simple
adaptive
greedy
algorithm
select
nearly
optimal
respect
reducing
volume
possible
reward
function
set
environments
value
solution
found
greedy
algorithm
com-
parable
optimal
algorithm
uses
logarithmic
factor
fewer
number
experiments
finally
implement
algorithm
simple
maze
en-
vironment
nevertheless
demonstrates
value
elim-
inating
experimental
unidentiﬁability
signiﬁcantly
outper-
forming
methods
attempt
perform
irl
single
environment
references
abbeel
coates
quigley
application
reinforcement
learning
aerobatic
helicopter
ﬂight
advances
neural
information
processing
systems
19:1
2007
abbeel
apprenticeship
learning
via
inverse
reinforcement
learning
proceedings
twenty-ﬁrst
international
conference
machine
learning
page
acm
2004
chajewska
koller
parr
making
rational
decisions
using
adaptive
utility
elicitation
aaai/iaai
pages
363–369
2000
coates
abbeel
learning
control
multiple
demonstrations
proceedings
25th
international
conference
machine
learning
pages
144–151
acm
2008.
approach
apprenticeship
learning
advances
neural
information
processing
systems
pages
1449–1456
2007
von
neumann
morgenstern
theory
games
economic
behavior
60th
anniversary
commemorative
edition
princeton
university
press
2007
ziebart
maas
bagnell
dey
maximum
entropy
inverse
reinforcement
learning
aaai
pages
1433–1438
2008.
appendix
proof
theorem
theorem
cid:48
cid:48
coates
abbeel
apprenticeship
canonicalized
representation
learning
helicopter
control
communications
acm
:97–105
2009
golovin
krause
adaptive
submodularity
new
approach
active
learning
stochastic
optimization
colt
pages
333–345
2010
guillory
bilmes
interactive
submodular
set
cover
proceedings
international
conference
machine
learning
2010
lopes
melo
montesano
active
learning
reward
estimation
inverse
reinforcement
learning
machine
learning
knowledge
discovery
databases
pages
31–46
springer
2009
lov´asz
hit-and-run
mixes
fast
mathematical
programming
:443–461
1999
russell
algorithms
inverse
reinforcement
learning
icml
pages
663–670
2000
ramachandran
amir
bayesian
inverse
reinforcement
learning
urbana
51:61801
2007
ratliﬀ
bagnell
zinkevich
maximum
margin
planning
proceedings
23rd
international
conference
machine
learning
pages
729–736
acm
2006
regan
boutilier
regret-based
reward
elicitation
markov
decision
processes
proceedings
twenty-fifth
conference
uncertainty
artiﬁcial
intelligence
pages
444–451
auai
press
2009
regan
boutilier
robust
policy
computation
reward-uncertain
mdps
using
nondominated
policies
aaai
2010
regan
boutilier
eliciting
additive
reward
functions
markov
decision
processes
ijcai
proceedings-international
joint
conference
artiﬁcial
intelligence
volume
page
2159
2011
rothkopf
dimitrakakis
preference
elicitation
inverse
reinforcement
learning
machine
learning
knowledge
discovery
databases
pages
34–48
springer
2011
smart
kaelbling
eﬀective
reinforcement
learning
mobile
robots
robotics
automation
2002.
proceedings
icra
ieee
international
conference
volume
pages
3404–3410
ieee
2002
syed
schapire
game-theoretic
one
cid:48
proof
deﬁnition
canonicalized
representation
reward
function
attained
scaling
translation
therefore
theorem
cid:48
canonicalized
cid:48
therefore
cid:48
direction
suppose
cid:48
canonical-
respectively
cid:54
cid:48
ized
cid:48
theorem
cid:48
cid:48
thus
prove
theorem
suﬃcient
argue
cid:48
behaviorally
equivalent
cid:48
cid:126
straight-
forward
show
behaviorally
equivalent
thus
focus
case
cid:48
cid:126
consider
three
cases
first
suppose
cid:54
cid:48
state
cid:48
diﬀerent
minimally-rewarding
states
without
loss
generality
sup-
pose
cid:48
furthermore
let
cid:48
con-
sider
environment
two
actions
cid:48
action
deterministically
transitions
state
state
action
cid:48
determininstically
transitions
state
cid:48
state
let
policy
always
takes
ac-
however
opt
tion
opt
cid:48
means
cid:48
therefore
policies
opt
thus
opt
cid:54
opt
cid:48
cid:48
behaviorally
equivalent
next
suppose
cid:54
cid:48
diﬀerent
maximally-rewarding
states
analagously
previous
case
suppose
without
loss
generality
cid:48
state
cid:54
cid:126
deﬁne
cid:48
environment
way
previous
case
time
opt
cid:48
let
cid:48
exists
since
cid:48
cid:48
opt
cid:54
cid:48
finally
suppose
cid:48
share
maximally
minimally
rewarding
states
exists
cid:54
cid:48
let
state
cid:48
let
state
cid:48
without
loss
generality
suppose
cid:48
let
environment
two
actions
let
real
number
cid:48
every
state
action
transitions
state
probabiity
state
remaining
probability
every
state
action
transtions
state
deterministically
reward
taking
action
state
either
reward
function
action
gives
reward
cid:48
concluding
proof
thus
opt
cid:48
cid:48
πap
cid:54
opt
cid:48
proof
greedy
performance
given
set
component
use
denote
union
singleton
set
begin
redeﬁning
cid:90
volµ
volµ
upper
bound
volµ
−rmax
rmax
let
set
possible
observations
trajectory
trajectory
setting
policy
policy
setting
let
space
possible
environments
wwe
ﬁrst
establish
indeed
submodular
lemma
submodular
non-decreasing
function
2u×o
proof
fix
2u×o
cid:54
deﬁnition
vol
vol
cid:54
cid:90
cid:90
cid:54
establishes
submodularity
since
arbitrary
right-hand-side
second
equality
non-zero
also
monotone
let
denote
set
functions
mapping
environments
observations
overload
∪e∈s
suppose
environments
labeled
according
consider
algorithm
knowing
selects
fewest
number
environments
given
algorithm
deﬁne
general
identiﬁcation
cost
identiﬁes
worst-
possible
labelling
strategy
particular
gicα
max
t∈t
min
s⊂u
|s|
recall
deﬁnition
main
body
optn
maxan
min
min
e∈c
largest
algorithm
guarantee
make
environments
environments
con-
sistently
labeled
let
algorithm
satis-
fying
max
lemma
gicoptn
proof
fix
consider
two
cases
first
sup-
pose
exists
|s|
inconsistent
labeling
deﬁntion
optn
since
|s|
lemma
proven
otherwise
must
|s|
con-
sistent
labeling
deﬁnition
optn
running
labels
provided
guaranteed
result
sequence
environments
|s∗|
satisfying
optn
witness
mins⊂u
≥optn
|s|
given
environment
true
reward
let
denote
set
possible
observations
either
policy
trajectory
setting
lemma
optn
ex-
ists
environment
min
r∈k
min
o∈o
optn−f
/gicoptn
proof
suppose
every
environment
exists
optn
/gicoptn
let
cid:48
deﬁned
cid:48
cid:44
arg
min
o∈o
optn
/gicoptn
deﬁnition
gic
min
s⊂u
cid:48
≥optn
|s|
gicoptn
exists
set
environments
|s|
gicoptn
cid:48
optn
monotonicity
know
cid:48
optn
let
|s|/gicoptn
however
despite
cid:48
optn
repeatedly
ap-
plying
submodularity
applying
equation
implies
cid:88
cid:48
cid:48
e∈s
|s|
optn
/gicoptn
optn
γoptn
optn
establishes
contradiction
prove
main
theorem
theorem
returned
greedy
environment
se-
lection
algorithm
satisﬁes
optn
optn/
proof
let
denote
subsequence
consisting
ﬁrst
environment
observation
pairs
optn
nothing
prove
otherwise
applying
lemma
deﬁnition
algorithm
know
ei−1
optn
ei−1
/gicoptn
implies
optn
optn
ei−1
1/gicoptn
using
fact
e−x
conclude
optn
optn
exp
−b/gicoptn
applying
lemma
substituting
completes
proof
