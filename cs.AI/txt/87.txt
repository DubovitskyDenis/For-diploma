causal
models
data-driven
debugging
decision
making
cloud
computing
philipp
geiger
max
planck
institute
intelligent
systems
tübingen
germany
pgeiger
tuebingen.mpg.de
lucian
carata
univeristy
cambridge
cambridge
united
kingdom
lc525
cam.ac.uk
bernhard
schölkopf
max
planck
institute
intelligent
systems
tübingen
germany
tuebingen.mpg.de
abstract
cloud
computing
involves
complex
technical
economical
systems
interactions
brings
various
challenges
two
debugging
control
optimize
performance
computing
systems
help
sandbox
experiments
prediction
cost
spot
resources
decision
making
cloud
clients
paper
formalize
debugging
counterfactual
probabilities
control
post-
soft-
interventional
probabilities
prove
coun-
terfactuals
approximately
calculated
stochastic
graphical
causal
model
originally
deﬁned
deterministic
func-
tional
causal
models
based
sketch
data-driven
approach
address
problem
address
problem
formalize
bidding
post-
soft-
interventional
probabilities
present
simple
mathematical
result
approximate
integration
incomplete
conditional
probability
distributions
show
used
cloud
clients
trade
privacy
predictability
outcome
bidding
actions
toy
scenario
report
experiments
simulated
real
data
introduction
recent
years
paradigm
business
model
cloud
computing
become
increasingly
popular
allows
rent
computing
resources
on-demand
use
eﬃciently
sharing
smart
way
particular
using
auctions
sell
unused
resources
several
new
challenges
arise
paradigm
cloud
computing
technical
level
problem
understand
control
debug
involved
computing
systems
size
several
data
centers
much
automation
possible
optimize
performance
detail
section
4.1.
economical
level
auctions
spot
resources
help
providers
use
resources
eﬃciently
unpredictability
prices
performance
complicates
bidding
buying
decisions
clients
detail
section
5.1.
absence
exact
models
natural
try
address
problems
using
data-driven
methods
however
standard
machine
learning
usually
applies
settings
underlying
system
invariant
often
based
assumption
samples
i.i.d.
make
predictions
eﬀect
interventions
important
though
debugging
decision
making
integration
heterogeneous
data
1.1
contributions
present
paper
takes
ﬁrst
steps
towards
addressing
challenges
cloud
computing
using
causal
models
inferring
causal
models
observational
data
notoriously
hard
convincing
applications
causal
modeling
real
world
problems
scarce
present
paper
exception
main
focus
conceptual
rather
empirical
main
contributions
present
two
theoretical
results
approximations
causal
modeling
proposi-
tions
relevance
subsequent
cloud
problems
possibly
beyond
section
needs
emphasized
practicability
theoretical
results
remains
proved
section
suggest
ﬁrst
steps
towards
causal
models
approximate
coun-
terfactuals
principled
data-driven
approach
addressing
cloud
control
performance
debugging
problems
integrating
sandbox
experiments
section
use
approximate
integration
causal
knowledge
enable
cloud
clients
better
predict
performance
costs
preserving
privacy
toy
setting
1.2
structure
remainder
paper
structured
follows
section
give
brief
introductions
causal
models
cloud
computing
section
contains
deﬁnition
counterfactuals
addition
two
theoretical
results
section
contains
simplistic
real-world
simulated
experiments
two
approaches
well
preliminary
causal
model
realistic
cloud
system
section
discuss
related
work
conclude
paper
section
background
2.1
causal
models
give
succinct
deﬁnitions
detailed
introduction
causal
models
refer
reader
paper
generally
assume
variables
discrete
although
results
may
also
hold
continuous
case
let
set
variables
graphical
causal
model
gcm
consists
directed
acyclic
graph
dag
node
set
called
causal
diagram
causal
dag
conditional
probability
density
px|pax=pax
pax
domain
pax
pax
parents
functional
causal
model
fcm
special
gcm
includes
observed
variable
hidden
root
i.e.
parentless
background
variable
arrow
function
i.e.
fully
determined
pax
fcm
induces
cgm
dropping
background
variables
causal
models
refer
fcms
well
cgms
given
causal
model
tuple
variables
post-interventional
causal
model
mdo
z=z
deﬁned
follows
drop
variables
incoming
arrows
causal
diagram
value
variables
corresponding
entry
remaining
conditional
densities
based
deﬁne
post-interventional
density
setting
relative
denoted
|do
z=z
density
mdo
z=z
note
use
expressions
like
x|y
shorthand
px|y
x|y
non-mathematical
level
consider
correct
causal
model
part
reality
correctly
predicts
outcomes
interventions
part
reality
clearly
reasonable
deﬁnitions
causation
2.2
cloud
computing
traditionally
businesses
individuals
used
dedicated
local
computers
computer
networks
storing
managing
processing
data
however
ineﬃcient
several
ways
overhead
maintaining
infrastructure
high
one
needs
buy
enough
computers
handle
peak
loads
normal
operation
remain
unutilized
cloud
computing
signiﬁcantly
changes
allowing
computing
resources
rented
demand
company
cloud
provider
responsible
running
hardware
keeping
upgraded
sharing
amongst
multiple
clients
infrastructure
run
highly
eﬃcient
manner
tens
hundreds
virtual
machines
vms
i.e.
emulations
computer
systems
chartered
diﬀerent
clients
run
single
physical
server
share
resources
central
processing
units
cpus
memory
network
note
refer
system
production
system
actual
work
clients
visitors
contracts
met
w.r.t
system
contrast
e.g.
experimental
system
two
approximations
causal
modeling
3.1
structural
counterfactuals
approximation
let
fcm
set
variables
let
denote
set
independent
background
variables
let
sets
variables
structural
counterfactual
probability
given
evidence
deﬁned
based
as1
ydo
x=x
y|e
y|do
u|e
even
though
computer
systems
deterministic
many
systems
due
interactions
environment
missing
information
usually
one
infer
gcm
fcm
computer
system
without
fcm
though
counterfactual
probabilities
equation
generally
uniquely
determined
i.e.
derived
gcm
let
give
example
example
gcms
determine
counterfactual
probabilities
let
binary
consider
gcm
dag
conditionals
0|x
induced
two
diﬀerent
fcms
one
hand
fcm
structural
equations
nif
orm
nif
orm
denotes
uniform
distribution
induces
hand
fcm
xor
structural
equations
1note
deﬁnition
uses
functions
instead
deterministic
conditionals
nif
orm
induces
ydo
x=1
0|x
0|do
0|uy
ydo
x=1
0|x
0|do
0|x
gives
extreme
example
counterfactual
probabilities
determined
gcm
detailed
discussion
phenomenon
refer
reader
show
nonetheless
counterfactual
probabilities
calculated
approximately
one
know
gcm
wrong
approximation
average
important
approach
debugging
section
belief
areas
well
let
gcm
let
set
root
variables
variables
parents
causal
dag
sets
variables
deﬁne
approximate
structural
counterfactual
approximate
counterfactual
as2
y|do
w|e
ydo
x=x
y|e
proposition
let
fcm
induces
gcm
let
denote
root
variables
sets
variables
ydo
x=x|e
k˜p
ydo
x=x|e
e|z
ydo
x=x|e
deﬁned
w.r.t
ydo
x=x|e
w.r.t
prove
using
monotonicity
divergence
properties
entropy
generalization
proposition
proposition
section
a.1.3
example
give
intuition
approximate
counterfactual
proposition
let
ﬁrst
consider
following
two
special
cases
already
fcm
sense
variables
completely
determined
root
nodes
e|z
thus
based
equation
quantities
coincide
seems
natural
evidence
comprises
root
nodes
approximation
amounts
simple
conditional
y|do
part
corresponds
similar
evidence
background
variables
fcm
note
example
approximate
counterfactual
help
much
calculated
ydo
x=1
0|x
0|do
2the
idea
counterfactual
deﬁnition
based
gcm
mentioned
section
7.2.2
investigated
depending
speciﬁc
setting
available
information
may
suitable
approximations
encode
counterfactual-like
probabilities
3note
chose
set
proposition
close
causal
diagram
possible
could
yield
better
approximations
simply
letting
root
nodes
done
ydo
x=x
y|e
leave
question
future
work
easy
see
implies
divergence
ydo
x=1|x
true
ydo
x=1|x
example
divergence
coincides
upper
bound
divergence
proposition
since
example
practical
meaningfulness
approximate
counterfactual
probability
particular
decision
making
remains
subject
debate
brieﬂy
comment
remark
3.2
approximate
integration
causal
knowledge
following
result
important
section
since
used
preserve
amount
privacy
consider
random
variables
typical
causal
structure
satisﬁes
assumptions
make
depicted
figure
page
13.
introduce
seen
approximation
transportability
introduced
bareinboim
pearl
following
simple
case
would
like
know
know
mechanism
z|x0
plus
diﬀerent
source
know
deﬁne
approximation
z|x0
xk|c
proposition
c|x0
k¯p
...
xk|c
coincide
reﬂected
byp
note
based
proposition
know
wrong
approximation
using
available
information
xk|c
proof
using
monotonicity
divergence
properties
entropy
found
section
a.2
example
get
intuition
consider
case
fully
determined
xk|c
already
mentioned
example
causal
model
implies
condition
proposition
depicted
figure
page
13.
apply
proposition
predictability-privacy
problem
section
generally
applicable
joint
distributions
available
particular
section
focus
approximate
integration
privacy
reasons
even
frequent
reason
may
insuﬃcient
marginals
known
keep
mind
stronger
statements
set
possible
available
information
may
exist
e.g.
based
ideas
problem
models
control
debugging
approach
start
problem
statement
section
4.1
followed
approach
section
4.2
illustrate
approach
detail
based
several
toy
scenarios
discuss
advantages
previous
work
section
4.3
4.1
problem
statement
cloud
computing
involves
technical
systems
highest
complexity
controlled
debugged
ideally
semi-
automatic
way
speciﬁcally
control
problem
stated
follows
operation
cloud
server
many
decisions
automatically
made
regarding
resources
complete
computers
parts
cpu
time
allocated
among
various
applications
virtual
machines
vms
clients
goal
optimize
automatic
decision
making
based
given
utility
function
encoding
e.g.
energy
consumption
guarantees
given
customers
simply
proﬁt
performance
debugging
problem
closely
related
performance
attribution
formulated
follows
general
goal
understand
component
system
contributes
extent
measured
performance
based
decided
components
modiﬁed
perform
gradient
step
towards
optimal
performance
give
example
cloud
computing
client
may
wonder
whether
high
latency
web
server
caused
concurrent
programs
within
could
directly
intervene
concurrent
vms
physical
cloud
sever
come
back
example
section
4.3
address
toy
scenario
well
section
6.2
give
example
preliminary
realistic
causal
model
help
situation
note
presently
focus
debugging
individual
observations
i.e.
unit-level
usually
plenty
heterogeneous
knowledge
data
available
involved
systems
expert
knowledge
formal
program
code
system
speciﬁcations
often
containing
non-
causal
associational
knowledge
data
system
similar
ones
data
sandbox
experiments
4.2
outline
approach
sketch
several
steps
uniﬁed
approach
based
causal
models
potentially
help
address
control
debugging
problem
follows
refer
cloud
system
production
i.e.
fully
conﬁgured
system
speciﬁc
set
applications
target
system
note
depending
speciﬁc
setup
steps
may
canceled
4.2.1
step
inference
causal
diagram
mechanisms
given
various
information
sources
described
procedure
keep
mind
inference
procedure
describe
usually
based
target
system
since
details
speciﬁc
vms
running
varying
quickly
instead
past
experience
systems
equal
similar
conﬁguration
particular
usually
details
target
system
known
step
mechanisms
stay
underdetermined
inferred
later
step
usual
main
sources
causal
inference
randomized
interventional
experiments
observational
data
expert
knowledge
necessary
condition
harness
ﬁrst
two
sources
decision
performance
measurements
system
propose
use
tools
discussed
carata
snee
note
important
fact
many
aspects
computer
systems
hardware
software
design
modular
i.e.
separable
individually
manipulable
input-output
mechanisms
central
assumption
causal
models
give
simple
example
see
erroneous
behavior
caused
network
one
unplug
network
cable
check
error
occurs
nonetheless
procedure
generally
would
change
mechanism
cpu
keyboard
furthermore
similar
mechanisms
occur
diﬀerent
systems
helpful
extrapolation
experiments
note
additional
source
information
speciﬁc
computer
systems
lot
knowledge
non-causal
associations
program
calls
program
execution
available
often
well-formatted
way
e.g
program
code
system
architecture
speciﬁcations
information
could
translated
hypotheses
causal
association
used
measurement
selection
semi-
automatic
way
output
procedure
causal
diagram
target
system
together
mechanisms
i.e.
conditionals
causal
model
target
system
inferred
based
past
experience
mechanisms
known
based
past
experience
target
system
revealed
e.g.
speciﬁc
vms
running
explored
directly
target
system
either
since
tentative
conﬁgurations
may
violate
contracts
clients
discuss
integration
sandbox
experiments
step
complete
causal
model
4.2.2
step
design
integration
sandbox
experiments
given
additional
cloud
system
experimental
system
equivalent
hardware
target
system
causal
diagram
target
system
variable
e.g
performance
identity
e.g.
properties
mechanism
produces
whose
unknown
properties
inferred
experiment
procedure
knowledge
allows
integrate
sandbox
experiments
principled
way
derive
direct
inﬂuences
i.e.
parents
pax
could
include
resources
cpu
time
size
requests
received
internet
design
sandbox
experiment
experimental
system
experimental
system
conditional
x|pax
mechanism
e.g.
simply
running
experimental
system
planned
run
target
system
variables
pax
randomly
varied
based
gathered
data
regress
pax
plug
inferred
conditional
x|pax
x|do
pax
mechanism
possible
since
parents
intervened
regressed
upon
without
going
detail
needs
mentioned
transfer
conditional
experimental
target
system
seen
simple
example
transportation
causal
relations
deﬁned
pearl
bareinboim
4.2.3
step
control
given
causal
model
target
system
utility
variable
function
one
several
variables
variable
e.g
concurrent
workload
cpu
time
network
bandwidth
controlled
optimize
procedure
predicts
eﬀect
modifying
mechanisms
used
ﬁnd
mechanism
policy
x|pax
x|pax
maximizes
4.2.4
step
observation-level
performance
debugging
given
causal
model
target
system
variable
measures
performance
want
optimize
performance
debugging
query
individual
observation
contains
observables
besides
since
move
level
individual
observations
instead
populations
term
step
observation-level
performance
debugging
procedure
performance
debugging
query
assume
following
form
current
situation
would
improve
performance
current
would
set
given
side
information
side
information
may
contain
observation
stated
way
seems
natural
translate
query
query
structural
counterfactual
probability
ydo
x=x0
y0|y
based
section
3.1
particular
proposition
calculate
approximate
answer
ydo
x=x0
y0|y
gcm
e|z
small
set
root
nodes
remark
value
approximate
counterfactuals
performance
debugging
remark
due
regarding
notion
counterfactual
application
performance
debugging
narrow
sense
counterfactual
statement
always
statement
past
neither
falsiﬁable
help
falsiﬁable
recommendations
regarding
future
decision
contrast
mind
broader
notion
counterfactual
situation
one
observes
system
poor
performance
asks
performance
could
debugged
system
remains
state
visits
similar
states
language
causal
models
state
means
tuple
background
variables
question
relevant
situations
debugging
action
performed
quickly
observation
poor
performance
ones
assumes
state
changes
comparably
slowly
i.e.
state
varies
smoothly
time.5
alternatively
question
4clearly
ways
formalize
attribution
debugging
5it
seems
thorough
analysis
argument
might
fruitful
could
theoretically
justify
frequent
usage
counterfactual
reasoning
everyday
life
leave
future
work
relevant
one
good
subjective
judgement
similarity
state
two
points
time
judgement
based
pbjective
observables
though
non-counterfactual
form
reasoning
may
appropriate
situations
counterfactual
reasoning
may
useful
arise
particular
whenever
one
assume
know
population-level
distribution
state
well
enough
one
beliefs
structural
equations
instance
varies
time
instead
one
wants
reason
observation-level
i.e.
unit-level
population-level
better
ways
decision
making
counterfactual
reasoning
see
step
propose
one
way
formalize
performance
debugging
questions
answer
based
one
possible
formalization
counterfactual
probabilities
proposed
pearl
remains
open
question
whether
better
formalizations
debugging
questions
consider
whether
general
notion
counterfactual
probability
well
formalization
pearl
sensible
discussion
see
also
note
additional
issue
able
settle
close
approxi-
mation
counterfactual
comes
true
counterfactual
practice
4.3
application
toy
scenarios
discussion
potential
advantages
previous
approaches
researchers
familiar
causal
inference
steps
described
may
seem
trivial
however
current
approaches
described
problems
aware
lacking
principled
formal
language
concepts
causal
suﬃciency
things
integration
sandbox
experiments
performance
debugging
give
toy
examples
make
approach
outlined
step
step
concrete
simultaneously
show
advantages
approach
based
causal
models
previous
approaches
examples
applications
step
see
sections
6.1
6.2
keep
mind
clearly
approach
outlined
completely
solve
problem
inference
knowledge
relies
remains
challenge
approaches
however
approach
may
less
prone
errors
data-eﬃcient
step
integrating
sandbox
experiments
without
principled
approach
lead
errors
e.g.
parents
direct
causes
variable
varied
experiment
regressed
upon
afterwards
say
regressed
causal
children
methodology
include
reasoning
concepts
causal
eﬀect
causal
suﬃciency
randomization
prone
mistakes
let
give
toy
example
approach
works
sandbox
experiments
approaches
wrong
terms
variation
regression
example
design
integration
sandbox
experiments
possible
mistakes
imagine
cloud
provider
want
decide
whether
put
cloud
server
already
concurrent
vms
running
let
denote
performance
main
application
running
inside
denoting
good
bad
performance
instance
could
denote
latency
assume
figure
depicts
correct
causal
dag
target
system
i.e.
would
running
mentioned
cloud
sever
particular
performance
depends
two
factors
say
amount
requests
coming
internet
one
hand
usage
cpu
cloud
sever
concurrent
vms
stands
low
high
turn
depend
may
denote
state
internet
users
send
requests
potentially
also
concurrent
vms
therefore
also
inﬂuence
alternatively
could
denote
parameter
behaviour
internet
users
i.e.
distribution
states
assume
true
mechanism
underlying
denotes
logical
i.e.
performance
bad
serve
many
requests
time
cpu
usage
concurrent
vms
high
furthermore
assume
target
system
figure
causal
diagram
running
target
system
varying
regressing
sandbox
experiment
lead
wrong
predictions
performance
target
system
especially
hidden
source
say
internet
users
introduces
strong
correlations
instance
could
due
fact
concurrent
vms
serve
internet
users
time
zone
additionally
assume
suppose
inferred
causal
dag
figure
based
step
want
infer
mechanism
underlying
performance
following
step
taking
would
perform
sandbox
experiment
would
vary
afterwards
regress
would
correctly
infer
mechanism
additionally
knowing
say
previous
experience
reports
cloud
clients
would
correctly
predict
probability
bad
performance
target
system
1|r
contrast
without
principled
approach
two
things
happen
sandbox
experiment
varied
regressed
upon
kept
constant
properly
inferred
communicated
inﬂuence
factor
simply
experimental
system
concurrent
vms
emulated
would
wrongly
predicted
1|r
even
sandbox
experiment
would
varied
according
correct
target
system
e.g.
concurrent
vms
target
system
would
emulated
well
experimental
system
one
would
forget
regressing
still
one
would
wrongly
predict
1|r
clearly
simplistic
toy
example
best
knowledge
author
problems
thematized
literature
yet
step
causal
models
provide
principled
tool
control
cloud
systems
allows
integrate
various
forms
information
results
sandbox
experiments
obtained
4.2.2.
furthermore
compared
e.g.
based
adaptive
control
advantage
using
causal
models
allow
encode
integrate
knowledge
mechanisms
vary
stay
invariant
example
control
based
causal
models
consider
example
performance
recall
inferred
mechanism
assume
consider
diﬀerent
target
system
example
namely
system
involves
policy
r|s
controls
amount
cpu
occupied
vms
depict
causal
dag
figure
figure
causal
diagram
running
system
controlled
policy
similar
system
figure
except
inﬂuenced
choice
policy
well
current
serves
input
policy
therefore
add
diagram
draw
arrow
note
handling
policy
rather
parameter
variable
way
similar
use
so-called
selection
diagrams
mechanisms
vary
marked
special
nodes
arrows
suppose
goal
follows
keep
probability
poor
performance
i.e.
1|π
allocating
little
cpu
possible
i.e.
minimizing
0|π
cpu
used
vms
furthermore
assume
using
causal
dag
plugging
knowledge
mechanisms
easy
see
optimal
policy
1|r
i.e.
always
occupy
cpu
vms
example
1|π
1|r
s|r
1|0
1|1
goal
w.r.t
performance
still
met
shows
causal
models
provide
principled
tool
integrate
sandbox
experiments
based
step
perform
control
proposed
step
corresponds
let
mention
potential
advantage
control
based
causal
models
case
cloud
systems
time-varying
assume
denotes
parameter
behavior
internet
users
indicated
meaning
example
4.2.2
suppose
varies
reason
say
due
campaign
unpredictable
way
know
behavior
internet
users
inﬂuences
via
since
rest
cloud
system
aﬀected
internet
knowledge
encoded
causal
dag
figure
based
l|r
l|r
formally
reasoned
even
varies
mechanism
l|r
stays
hence
derive
new
optimal
policy
one
infer
new
plug
equation
optimize
furthermore
certain
identiﬁed
new
system
new
optimal
policy
given
assumptions
correct
sort
reasoning
analyzed
general
level
pearl
bareinboim
apply
control
settings
contrast
approaches
adaptive
control
cloud
computing
based
modularity
reasoning
may
try
infer
complete
information
well
l|r
scratch
upon
variation
assuming
completely
new
environment
even
approaches
utilize
invariant
l|r
variation
one
way
another
usually
missing
language
reason
identiﬁability
new
system
variation
based
causal
models
figure
causal
diagram
observation-level
performance
debugging
toy
setting
unobserved
nonetheless
assume
l|r
known
may
provider
publishes
client
knows
sandbox
experiments
needs
emphasized
considered
overly
simplistic
scenario
complex
realistic
scenarios
much
mechanisms
involved
could
potentially
vary
stay
invariant
respectively
see
section
6.2
example
causal
dag
realistic
still
simple
cloud
system
step
give
example
observation-level
debugging
performed
based
step
approach
seen
complementary
methods
problem
errors
may
arise
confusing
causation
correlation
diﬃcult
integrate
heterogeneous
knowledge
sandbox
experiments
example
observation-level
performance
debugging
note
toy
scenario
assumptions
make
example
regarding
known/observed
close
realistic
similar
example
consider
performance
latency
running
cloud
system
denoting
amount
incoming
requests
amount
say
cpu
time
allocated
concurrent
vms
stands
low
high
denote
contrast
example
assume
causal
dag
depicted
figure
furthermore
let
structural
equation
given
may
suppose
seen
encoding
prior
belief
assume
client
belongs
wonders
whether
would
improve
latency
desired
current
situation
observes
decreased
amount
incoming
requests
lower
level
i.e.
set
note
current
situation
include
nearby
future
unobserved
variables
vary
comparably
slowly
see
remark
observe
due
neither
cloud
provider
clients
publishing
information
realistic
assumption
cloud
computing
based
step
translates
question
query
counterfactual
probability
ldo
r=0
0|r
suppose
published
l|r
known
may
provider
publishes
client
knows
sandbox
experiments
l|r
give
structural
equation
al-
though
structural
equation
would
needed
calculate
counterfactual
ldo
r=0
0|r
exactly
see
example
calculate
approxi-
mate
counterfactual
probability
deﬁned
equation
ldo
r=0
0|r
0|do
s|r
0|do
2|r
s|r
2|r
plugged
set
root
variables
yields
evidence
took
value
based
concludes
probability
setting
helps
decreasing
latency
rather
small
current
situation
note
true
counterfactual
probability
equation
section
3.1
speciﬁc
case
given
ldo
r=0
0|r
0|do
ul|r
0|do
ul|r
0|do
1|r
0|do
0|r
would
lead
even
stronger
conclusion
setting
decreasing
would
work
note
upper
bound
proposition
takes
value
l|r
l|r
l|r
recall
picked
approximation
would
even
better
bound
smaller
i.e.
rather
strong
noise
less
noise
note
generally
one
could
try
learn
sense
machine
learning
things
perform
integrate
experiment
one
would
always
rely
prior
assumptions
may
diﬃcult
encode
problem
cost
predictability
versus
privacy
approach
start
problem
statement
section
5.1
followed
approach
section
5.2
present
toy
example
section
5.3
additional
remarks
section
5.4
5.1
problem
statement
consider
economical
aspect
cloud
computing
currently
one
common
way
clients
purchase
cloud
resources
provider
via
auction
mechanism
spot
i.e.
short-term
resources
described
simpliﬁed
way
follows
customer
enters
bid
e.g.
hour
usage
price
determined
provider
based
supply
demand
private
factors
drops
bid
customer
gets
resource
usually
long
bid
exceeds
price
within
hour
approach
several
advantages
particular
provider
sell
resources
unused
ﬂuctuate
lot
due
guarantees
given
dedicated
on-demand
customers
clients
proﬁt
well
spot
resources
usually
signiﬁcantly
cheaper
long-term
dedicated
resources
obvious
drawback
spot
resources
kind
mechanism
comes
high
uncertainty
clients
hard
tell
prices
evolve
future
particular
purchased
resources
terminated
unforeseeable
way
extent
due
unpredictability
clients
therefore
client
want
take
risks
signiﬁcantly
harm
his/her
business
often
avoid
mechanism
figure
causal
diagram
hidden
5.2
sketch
approach
follows
present
ﬁrst
step
towards
addressing
problem
based
available
observational
data
causal
models
assume
one
provider
clients
stakeholders
refer
provider
clients
together
point
time
say
beginning
hour
let
denote
client
demand
next
hour
cloud
product
client
buys
provider
information
based
client
decides
demand
e.g.
hour
day
may
always
fully
known
though
policy
determining
cloud
product
buy
given
demand
let
denote
provider
pricing
parameter
time
point
may
depend
e.g.
energy
costs
let
denote
outcome
provider
mechanism
applied
generally
include
price
well
say
termination
spot
resources
simplicity
let
denote
cost/price
moment
comprise
indirect
costs
resulting
loss
visitors
termination
assume
following
simple
mechanism
simpliﬁed
version
auction
described
clients
always
get
product
want
subsequent
price
vector
varies
known
advance
causal
diagram
complete
causal
structure
case
depicted
figure
role
explained
denotes
hidden
part
confounder
approach
uncertainty
problem
towards
predictable
prices
subsequent
reduced
costs
based
idea
clients
may
want
share
willing
share
information
speciﬁcally
propose
following
two-step
procedure
allows
clients
trade
privacy
versus
predictability
interests
jointly
picking
variable
xk|c
allows
approximate
prediction
still
preserves
privacy.6
5.2.1
step
jointly
picking
first
stakeholders
pick
candidates
possibly
based
given
list
privacy
budget
balancing
privacy
interests
minimizing
xk|c
intersection
candidates
non-empty
reveal
xk|c
joint
xk|c
optimize
predictability
based
proposition
candidates
c.7
pick
minimizesp
5.2.2
step
prediction
individual
decision
clients
reveal
xk|c
assumed
common
knowledge
furthermore
yk|xk
either
known
priori
based
possible
products
provider
oﬀers
6an
extreme
approach
would
directly
infer
joint
model
clients
joint
data
i.e.
considering
clients
single
client
assume
possible
due
heterogeneous
data
privacy
interests
etc
7if
intersection
empty
procedure
canceled
without
result
stakeholders
proceed
classical
non-collaborative
way
revealed
provider
reveals
z|x0
x0|c
z|π1
calculated
based
equation
speciﬁcally
z|π1
...
z|x0
xk|c
z|x0
yk|xk
...
...
k=1
k=0
xk|c
k=0
based
proposition
clients
narrow
set
possible
z|π1
z|π1
k¯p
z|π1
xk|c
based
constraint
z|π1
client
decides
strategy
e.g.
based
game-theoretic
considerations
5.3
application
toy
scenario
illustrate
approach
let
give
example
example
cloud
provider
clark
oﬀers
clients
alice
bob
monthly
dedicated
large
resources
rather
expensive
hourly
spot
small
large
resources
usually
cheaper
however
alice
bob
happen
order
large
spot
resource
hour
cost
signiﬁcantly
higher
hourly
rate
monthly
large
resource
since
clark
may
buy
new
resource
may
cancel
one
client
applications
causing
loss
web
site
visitors
assume
alice
bob
step
pick
hourly
weather
forecast
sunny
cloudy
since
public
information
anyway
web
sites
weather
related
alice
runs
website
outdoor
activities
bob
one
indoor
activities
region
remaining
uncertainty
w.r.t
demand
small
high
i.e.
xk|c
small
causal
diagram
scenario
depicted
figure
based
alice
bob
conclude
rarely
require
large
resource
time
spot
resources
respective
dominant
strategies
5.4
discussion
cases
provider
could
infer
joint
distribution
based
past
data
would
contain
relevant
information
however
complete
system
complex
unlikely
stationary
note
step
already
information
revealed
transparent
stakeholders
limitations
approach
clients
may
even
willing
reveal
may
predictable
model
may
wrong
although
humans
organizations
usually
plan
ahead
needs
emphasized
completely
ignore
strategic
aspects
lead
problems
proposed
approach
aspects
could
analyzed
e.g.
based
game
theory
experiments
6.1
control
debugging
problem
simple
real
cloud
system
test
small
parts
approach
section
4.2
simple
real
cloud
system
physical
server
running
speciﬁc
application
web
server
together
concurrent
workload
another
web
server
system
consider
causal
dag
two
examples
section
4.3.
scenarios
generally
similar
system
consider
simpler
experimental
purposes
figure
causal
diagram
·108
0.5
figure
x-axis
number
simultaneous
requests
y-axis
99th
percentile
prediction
l|do
dashed
blue
close
99th
percentile
solid
blue
ground
truth
test
data
l|do
subsample
gray
source
keeps
sending
simultaneous
request
application
concurrent
workload
drawn
multivariate
correlated
poisson
distribution
received
application
concurrent
workload
request
latency
performance
application
measured
nanoseconds
examine
well
step
works
first
infer
causal
diagram
depicted
figure
well
estimate
observational
samples
system
based
step
together
denote
incomplete
using
back-door
adjustment
derive
prediction
l|do
l|do
besides
step
tests
applicability
step
thinking
simple
controller
outputs
constant
e.g
putting
application
another
machine
concurrent
workload
well
step
relies
post-interventional
distributions
updated
model
though
outcome
depicted
figure
use
99th
percentile
statistic
common
cloud
computing
shows
prediction
close
ground
truth
test
data
magnitude
trend
6.2
example
realistic
cloud
system
experiments
previous
section
6.1
performed
overly
simplistic
system
want
give
example
preliminary
partial
causal
model
causal
dag
plus
knowledge
mechanisms
e.g.
additivity
realistic
system
approach
section
4.2
particular
performance
debugging
step
meant
applied
note
merely
illustration
purposes
test
hypothesis
here.8
consider
cloud
sever
running
serveral
vms
focus
one
speciﬁc
call
moment
inside
web
sever
speciﬁcally
lighttpd
runs
8the
inference
causal
model
application
approach
complex
system
turned
diﬃcult
expected
therefore
evaluation
approach
applied
system
reported
stage
figure
example
preliminary
causal
dag
cloud
system
variables
left
side
measured
within
runs
together
vms
cloud
server
right
side
contains
measurements
outside
hypervisor
program
responsible
allocating
cloud
server
resources
among
vms
objective
minimize
latency
web
server
running
denoted
srv_lat
keeping
utilization
vms
denoted
concurrent_vm_count
high
possible
possible
manipulations
include
reducing
workload
within
denoted
local_load
versus
changing
number
concurrent
vms
causal
model
good
help
pick
optimal
manipulations
ﬁgure
taken
also
gives
descriptions
remaining
variables
described
consider
following
observed
hidden
variables
among
others
measured
inside
outside
req_size
size
ﬁle
requested
internet
user
web
server
local_load
resource-consuming
activity
applications
besides
concurrent_vm_count
number
vms
running
concurrently
physical
sever
outside
srv_lat
latency
web
server
seen
part
objective
needs
minimized
depict
partial
causal
model
figure
taken
carata
also
gives
descriptions
variables
ﬁgure
discussed
note
model
experimental
system
system
production
variables
local_load
concurrent_vm_count
could
inﬂuences
hidden
common
cause
similar
previous
experiment
section
6.1.
model
inferred
described
step
iterative
sequential
way
based
non-causal
associational
knowledge
program
execution
structure
known
program
code
well
general
system
architecture
expert
knowledge
independence
tests
sampled
data
seen
often
integrated
knowledge
allows
9in
cloud
computing
important
distinguish
inside
outside
since
privacy
reasons
often
things
inside
known
client
belongs
srv_latconcurrent_vm_countreq_sizelocal_loadxen_block
xen_yield
xen_sch_out_cyc+k_cycusr_sch_outconcwait+usr_xen_outk_xen_outk_sch_out+usr_cyc+local
measurementshypervisor
measurements+measuredinferredcausal+causal
additive+known
causal
function
0.4
0.2
0.1
0.2
0.3
0.4
0.5
figure
x-axis
parameter
higher
inﬂuence
y-axis
dashed
k¯p
blue
close
solid
blue
even
gets
strong
weakens
solid
red
xx|c
dashed
red
draw
conclusions
additivity
mechanisms
based
fact
runtime
one
program
essentially
sum
runtimes
subroutines
model
could
help
decision
making
various
ways
instance
performance
debugging
problem
mentioned
sections
4.1
4.3
cloud
client
owner
may
observe
high
latency
srv_lat
web
server
together
variables
wonders
situation
high
latency
caused
programs
within
local_load
concurrent
vms
concurrent_vm_count
running
physical
cloud
sever
simply
large
requests
req_size
coming
moment
based
could
conclude
whether
intervene
local_load
may
simplest
rather
intervene
concurrent_vm_count
say
changing
another
cloud
product
dedicated
server
may
expensive
6.3
predictability-privacy
problem
simulated
data
approach
section
5.2
work
approximate
reasonably
well
examine
extent
case
simulated
version
toy
example
section
5.3
additionally
testing
tight
bound
proposition
compared
toy
example
restrict
spot
resources
i.e.
assume
following
speciﬁc
mechanisms
policy
simply
purchase
demand
clark
pricing
cheap
versus
expensive
one
since
want
large
particular
furthermore
xor
xor
nxk
confounder
alice
bob
want
reveal
0.5
draw
1000
samples
bernoulli
0.5
bernoulli
ﬁnd
wrong
gets
increasing
confounder
revealed
adjusted
nxk
bernoulli
0.2
0.2r
also
examine
little
variation
noise
strength
outcome
depicted
figure
shows
good
estimate
simple
setting
also
due
fact
already
alone
reveal
something
also
shows
setting
bound
proposition
may
improvable
dashed
red
line
far
away
solid
red
line
related
work
regarding
section
approximations
non-identiﬁable
quantities
causal
models
examined
balke
pearl
technique
seem
directly
applicable
setup
proposition
may
allow
derive
stronger
statements
i.e.
narrowing
set
possible
proposition
could
examined
future
work
discussed
related
work
section
i.e.
control
debugging
problem
section
4.3.
additionally
maybe
work
closest
investigation
section
lemeire
suggests
use
causal
models
performance
modeling
programs
consider
counterfactuals
complex
computing
systems
generally
utilization
modularity
based
causal
models
section
strongly
inspired
theory
transportability
causal
relations
developed
pearl
bareinboim
however
theory
applied
cloud
computing
problems
far
relation
causality
control
also
considered
bottou
regarding
section
work
angel
seen
related
allow
provider
hide
exact
costs
still
making
information
costs
available
others
work
mcsherry
talwar
investigates
privacy-preserving
mechanisms
consider
integration
revealed
information
estimate
causal
model
conclusions
paper
assayed
causal
inference
principle
help
technological
economical
problems
cloud
computing
guided
problems
presented
two
theoretical
results
approximate
causal
inference
reported
initial
experimental
results
application
causal
inference
domain
best
knowledge
ﬁrst
kind
believe
potential
area
signiﬁcant
applications
methodological
work
problems
computing
systems
often
require
sophisticated
interventions
bring
system
closer
optimum
rarely
classical
settings
machine
learning
excels
particular
issues
integration
sandbox
experiments
formally
reasoning
concepts
causation
causal
suﬃciency
randomization
seems
crucial
methodology
neglects
classical
machine
learning
may
prone
errors
another
concept
plays
important
role
causal
modeling
course
also
areas
identiﬁability
helps
critically
reason
inferred
based
given
used
control
problem
cases
modules
system
vary
next
step
would
extend
experimtens
real
cloud
systems
system
preliminary
model
derived
section
6.2
based
advance
approach
sketched
section
another
future
step
would
use
aspects
game
theory
mechanism
design
extend
approach
predictability-privacy
trade-oﬀ
section
appendix
present
proofs
section
a.1
generalized
version
proof
proposition
start
stating
proving
generalization
proposition
proposition
generalization
proposition
let
fcm
discrete
variables
induces
gcm
let
triple
sets
variables
i.e.
d-separated
inﬂuence
let
arbitrary
set
variables
let
ydo
x=x
y|e
y|do
w|e
ydo
x=x|e
kpz
ydo
x=x|e
e|z
ydo
x=x|e
deﬁned
w.r.t
ydo
x=x|e
w.r.t
generalization
proposition
see
let
denote
set
root
nodes
implies
ydo
x=x
y|e
deﬁned
ydo
x=x
y|e
deﬁned
section
3.1.
proposition
applied
ydo
x=x
y|e
coincides
proposition
ydo
x=x
y|e
ydo
x=x
y|e
proof
let
set
tuple
background
variables
inﬂuence
ydo
x=x
y|e
y|do
w|e
y|do
w|e
u0|do
y|do
w|e
u0|w
y|do
w|e
equation
due
fact
distribution
invariant
inﬂuence
written
function
x=x
equation
due
fact
deﬁnition
hand
y|do
u|e
y|do
u1|e
w|do
u1|e
ydo
x=x
y|e
w|do
u1|e
y|do
w|do
u1|e
y|do
w|do
u1|e
y|do
w|u1
u1|e
y|do
w|u0
u1|e
y|do
u1|e
y|do
u1|e
y|do
y|do
u0|e
u1|e
y|do
u0|e
equation
due
markovianity
implies
u1|z
thus
u1|w
mdo
x=x
equation
follows
fact
inﬂuence
equation
follows
fact
already
determines
note
w|e
implies
u0|e
therefore
ydo
x=x|e
kpw
ydo
x=x|d
deﬁned
calculate
ydo
x=x|e
kpz
ydo
x=x|e
ydo
x=x|e
kpz
ydo
x=x|e
u0|e
w|e
log
u0|e
w|e
log
u0|z
e|z
e|u0
inequality
follows
monotonicity
follows
chain
rule
kullback-leibler
divergence
together
equations
equation
chain
rule
mutual
information
due
inﬂuencing
markovianity
note
chose
set
proposition
close
causal
diagram
possible
could
yield
better
approximations
ydo
x=x
y|e
simply
letting
root
nodes
done
ydo
x=x
y|e
leave
question
future
work
a.2
proof
proposition
give
proof
proposition
proof
calculate
k¯p
πkp
xk|c
x0|c
x1|x0
···
xk|x0
xk−1
πkp
xk|c
log
x0|c
x0|c
x1|x0
x1|c
x2|x0
x2|c
···
xk|x0
xk−1
...
xk|c
log
...
x0|c
x0|c
x0|c
x1|c
x0|c
x1|c
x2|c
x1|c
···
xk−1|c
xk|c
xk−1|c
x0|c
x1|c
xk−1|c
x1|c
x2|c
xk|c
inequality
follows
monotonicity
follows
chain
rule
kullback-leibler
divergence
references
angel
ballani
karagiannis
shea
thereska
end-to-end
perfor-
mance
isolation
virtual
datacenters
11th
usenix
symposium
operating
systems
design
implementation
osdi
pages
233–248
2014
armbrust
fox
griﬃth
joseph
katz
konwinski
lee
patterson
rabkin
stoica
view
cloud
computing
communications
acm
:50–58
2010
balke
pearl
counterfactual
probabilities
computational
methods
bounds
applications
proceedings
tenth
international
conference
uncertainty
artiﬁcial
intelligence
pages
46–54
morgan
kaufmann
publishers
inc.
1994
bareinboim
pearl
transportability
causal
eﬀects
completeness
results
proceedings
26th
national
conference
artiﬁcial
intelligence
aaai
pages
698–704
aaai
press
menlo
park
ca.
2012
bottou
peters
quinonero-candela
charles
chickering
portu-
galy
ray
simard
snelson
counterfactual
reasoning
learning
systems
example
computational
advertising
journal
machine
learning
research
:3207–3260
2013
carata
provenance-based
computing
phd
dissertation
university
cambridge
2016
carata
chick
snee
sohan
rice
hopper
resourceful
ﬁne-grained
resource
accounting
explaining
service
variability
technical
report
university
cambridge
computer
laboratory
2014
chiang
hwang
huang
wood
matrix
achieving
predictable
vir-
tual
machine
performance
clouds
11th
international
conference
autonomic
computing
icac
pages
45–56
2014
cover
thomas
elements
information
theory
wileys
series
telecom-
munications
new
york
1991
lemeire
dirkx
verbist
causal
analysis
performance
modeling
computer
programs
scientiﬁc
programming
:121–136
2007
mcsherry
talwar
mechanism
design
via
diﬀerential
privacy
foundations
computer
science
2007.
focs
48th
annual
ieee
symposium
pages
94–103
ieee
2007
ostrowski
mann
sandler
diagnosing
latency
multi-tier
black-box
services
5th
workshop
large
scale
distributed
systems
middleware
ladis
2011
volume
page
2011
padala
k.-y
hou
shin
zhu
uysal
wang
singhal
merchant
automated
control
multiple
virtualized
resources
proceedings
4th
acm
european
conference
computer
systems
pages
13–26
acm
2009
pearl
causality
cambridge
university
press
2000
pearl
causal
inference
statistics
overview
statistics
surveys
3:96–146
2009
pearl
bareinboim
transportability
causal
statistical
relations
formal
approach
data
mining
workshops
icdmw
2011
ieee
11th
international
conference
pages
540–547
ieee
2011
peters
janzing
schölkopf
elements
causal
inference
foundations
learning
algorithms
mit
press
2017
snee
carata
chick
sohan
faragher
rice
hop-
per
soroban
attributing
latency
virtualized
environments
proceedings
7th
usenix
conference
hot
topics
cloud
computing
pages
11–11
usenix
association
2015
spirtes
glymour
scheines
causation
prediction
search
mit
cambridge
2nd
edition
2000
zheng
bianchini
janakiraman
santos
turner
justrunit
experiment-based
management
virtualized
data
centers
proc
usenix
annual
technical
conference
2009
