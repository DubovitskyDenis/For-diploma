quantifying
vanishing
gradient
long
distance
dependency
problem
recursive
neural
networks
recursive
lstms
phong
willem
zuidema
institute
logic
language
computation
university
amsterdam
netherlands
p.le
zuidema
uva.nl
abstract
recursive
neural
networks
rnn
recently
proposed
extension
recur-
sive
long
short
term
memory
networks
rlstm
models
compute
rep-
resentations
sentences
recursively
combining
word
embeddings
according
externally
provided
parse
tree
models
thus
unlike
recurrent
networks
explicitly
make
use
hierarchical
structure
sentence
paper
demonstrate
rnns
nevertheless
suf-
fer
vanishing
gradient
long
distance
dependency
problem
rlstms
greatly
improve
rnn
problems
present
artiﬁcial
learning
task
allows
quantify
severity
problems
mod-
els
show
ratio
gra-
dients
root
node
focal
leaf
node
highly
indicative
success
backpropagation
optimizing
relevant
weights
low
tree
paper
thus
provides
explanation
existing
supe-
rior
results
rlstms
tasks
sentiment
analysis
suggests
beneﬁts
including
hierarchical
structure
including
lstm-style
gating
complementary
introduction
recursive
neural
network
rnn
model
be-
came
popular
since
work
socher
2010
employed
tackle
several
nlp
tasks
syntactic
parsing
socher
al.
2013a
machine
translation
liu
al.
2014
word
embedding
learning
luong
al.
2013
like
traditional
recurrent
neural
net-
however
works
rnn
seems
suffer
vanish-
ing
gradient
problem
error
signals
prop-
agating
root
parse
tree
child
nodes
shrink
quickly
moreover
encoun-
ters
difﬁculties
capturing
long
range
dependen-
cies
information
propagating
child
nodes
deep
parse
tree
obscured
reach-
ing
root
node
recurrent
neural
network
world
long
short
term
memory
lstm
architecture
hochre-
iter
schmidhuber
1997
often
used
so-
lution
two
problems
natural
extension
lstm
deﬁned
tree
structures
call
recursive
lstm
rlstm
pro-
posed
independently
tai
2015
zhu
2015
zuidema
2015
how-
ever
intensive
research
showing
lstm
architecture
overcome
two
problems
compared
traditional
recurrent
models
e.g.
gers
schmidhuber
2001
research
knowledge
still
absent
comparison
rnns
rlstms
there-
fore
current
paper
investigate
fol-
lowing
two
questions
rlstm
capable
capturing
long
range
dependencies
rnn
rlstm
overcome
vanishing
gradient
problem
effectively
rnn
supervised
learning
requires
annotated
data
often
expensive
collect
result
ex-
amining
model
natural
data
many
different
aspects
difﬁcult
portion
data
ﬁts
speciﬁc
aspect
could
sufﬁcient
moreover
studying
individual
aspects
separately
hard
since
many
aspects
often
correlated
unfortunately
true
case
answering
two
questions
requires
evaluate
examined
models
datasets
dif-
ferent
tree
depths
key
nodes
contain
decisive
information
parse
tree
must
identiﬁed
using
available
annotated
corpora
stanford
sentiment
treebank
socher
al.
2013b
penn
treebank
thus
inap-
propriate
small
purpose
10k
40k
trees
respectively
compared
240k
trees
experiments
key
nodes
marked
solution
artiﬁcial
task
sentences
parse
trees
randomly
gener-
ated
arbitrary
constraints
tree
depth
key
node
position
background
rnn
rlstm
model
in-
stances
general
framework
takes
sen-
tence
syntactic
tree
vector
representations
words
sentence
input
applies
composition
function
recursively
compute
vec-
tor
representations
phrases
tree
complete
sentence
technically
speaking
given
production
representing
compute
composition
function
rnn
one-layer
feed-forward
neu-
ral
network
rlstm
node
repre-
sented
vector
resulting
concatenating
vector
representing
phrase
node
covers
memory
vector
could
lstm
combine
two
con-
catenation
vectors
structure-lstm
zhu
al.
2015
tree-lstm
tai
al.
2015
lstm-rnn
zuidema
2015
cur-
rent
paper
use
implementation1
zuidema
2015
experiments
examine
two
problems
van-
ishing
gradient
problem
problem
capture
long
range
dependencies
affect
rlstm
model
rnn
model
propose
following
artiﬁcial
task
re-
quires
model
distinguish
useful
signals
noise
deﬁne
sentence
sequence
tokens
integer
numbers
range
10000
sentence
contains
one
one
keyword
token
integer
number
smaller
1000
1https
//github.com/lephong/lstm-rnn
sentence
labeled
integer
result-
ing
dividing
keyword
100.
instance
keyword
607
label
way
classes
ranging
task
predict
class
sentence
given
binary
parse
tree
figure
label
sentence
determined
solely
keyword
two
models
need
identify
keyword
parse
tree
allow
information
leaf
node
keyword
affect
root
node
worth
noting
task
resembles
sentiment
analysis
simple
cases
sentiment
whole
sentence
determined
one
key-
word
e.g
like
movie
simulating
com-
plex
cases
involving
negation
composition
etc
straightforward
future
work
believe
current
task
adequate
answer
two
questions
raised
section
two
models
rlstm
rnn
im-
plemented
dimension
vector
represen-
tations
vector
memories
50.
following
socher
2013b
used
tanh
activation
function
initialized
word
vectors
randomly
sampling
value
uniform
distribution
−0.0001
0.0001
trained
two
models
using
adagrad
method
duchi
al.
2011
learning
rate
0.05
mini-batch
size
rnn
rlstm
de-
velopment
sets
employed
early
stopping
training
halted
accuracy
de-
velopment
set
improved
consecutive
epochs
3.1
experiment
randomly
generated
datasets
generate
sentence
length
shufﬂe
list
randomly
chosen
non-keywords
one
keyword
i-th
dataset
contains
12k
sentences
lengths
10i−
tokens
10i
tokens
split
train
dev
test
sets
sizes
10k
sentences
parsed
sentence
randomly
generating
binary
tree
whose
number
leaf
nodes
equals
sentence
length
test
accuracies
two
models
datasets
shown
figure
dataset
run
model
times
reported
high-
est
accuracy
rnn
model
distribu-
tion
accuracies
via
boxplot
rlstm
model
see
rnn
model
per-
forms
reasonably
well
short
sentences
figure
example
binary
tree
artiﬁcial
task
number
enclosed
box
keyword
sentence
figure
test
accuracies
rnn
best
among
runs
rlstm
boxplots
datasets
different
sentence
lengths
less
tokens
however
sentence
length
exceeds
rnn
performance
drops
quickly
difference
random
guess
performance
negligible
trying
different
learning
rates
mini-batch
sizes
values
dimension
vectors
give
signiﬁcant
differences
hand
rlstm
model
achieves
ac-
curacy
sentences
shorter
tokens
performance
drops
sentence
length
in-
creases
still
substantially
better
ran-
dom
guess
sentence
length
ex-
ceed
70.
sentence
length
exceeds
rlstm
rnn
perform
similarly
3.2
experiment
experiment
clear
whether
tree
size
keyword
depth
main
factor
rapid
drop
rnn
performance
ex-
periment
kept
tree
size
ﬁxed
vary
keyword
depth
generated
pool
sentences
lengths
tokens
parsed
randomly
generating
binary
trees
created
datasets
12k
trees
10k
training
development
testing
i-th
dataset
consists
trees
dis-
tances
keywords
roots
stop
networks
exploiting
keyword
depths
directly
figure
shows
test
accuracies
two
mod-
els
datasets
similarly
experiment
dataset
run
model
times
reported
highest
accuracy
rnn
model
distribution
accuracies
rlstm
model
see
rnn
model
achieves
high
accuracies
keyword
depth
exceed
performance
drops
rapidly
gets
close
performance
60002757077590006095060700058460584500059820401500548400189304571007450000458204993025021-1011-2021-3031-4041-5051-6061-7071-8081-9091-1000246810datasetu
minlength-maxlength
020406080100wrnn
suaccuracyaverageukeywordudepthaccuracydepth
figure
test
accuracies
rnn
best
among
runs
rlstm
boxplots
datasets
different
keyword
depths
random
guess
evidence
rnn
model
difﬁculty
capturing
long
range
de-
pendencies
contrast
rlstm
model
per-
forms
accuracy
depth
keyword
reaches
difﬁculty
dealing
larger
depths
performance
always
better
random
guess
3.3
experiment
examine
whether
two
models
en-
counter
vanishing
gradient
problem
looked
back-propagation
phase
model
experiment
third
dataset
one
containing
sentences
lengths
tokens
tree
calculated
ra-
tio
cid:107
cid:107
∂xkeyword
cid:107
∂xroot
cid:107
numerator
norm
error
vector
keyword
node
denominator
norm
error
vector
root
node
ratio
gives
intuition
error
signals
develop
propagating
backward
leaf
nodes
ratio
cid:28
vanishing
gradient
problem
occurs
else
ratio
cid:29
observe
exploding
gradient
problem
figure
reports
ratios
w.r.t
keyword
node
depth
epoch
training
rnn
model
ratios
ﬁrst
epoch
always
small
following
epoch
rnn
model
successfully
lifts
ratios
steadily
see
figure
clear
picture
keyword
depth
clear
decrease
depth
becomes
larger
observable
rlstm
model
see
figure
story
somewhat
different
ratios
two
epochs
rapidly
even
exploding
error
signals
sent
back
leaf
nodes
subsequently
remain
stable
substantially
less
explod-
ing
error
signals
interestingly
concurrent
performance
rlstm
model
development
set
see
figure
seems
rlstm
model
one
epoch
quickly
locates
keyword
node
tree
relates
root
building
strong
bond
via
error
signals
correlation
keyword
label
root
found
tries
stabilize
training
reducing
error
signals
sent
back
keyword
node
comparing
two
models
aligning
figure
figure
figure
figure
see
rlstm
model
capable
transmitting
error
signals
leaf
nodes
worth
noting
see
vanish-
ing
gradient
problem
happening
training
rnn
model
figure
figure
suggests
problem
become
less
serious
long
enough
training
time
might
depth
still
manageable
rnn
model
notice
stanford
sentiment
treebank
three
quarters
leaf
nodes
depths
less
fact
rnn
model
still
doesnot
perform
better
random
guessing
explained
using
arguments
given
ben-
gio
1994
show
trade-off
1-22-33-44-55-66-77-88-99-1010-11020406080100dataset
mindepth-maxdepth
accuracy
figure
ratios
norms
error
vectors
keyword
nodes
norms
error
vectors
root
nodes
w.r.t
keyword
node
depth
epoch
training
rnn
gradients
gradually
vanish
greater
depth
figure
ratios
norms
error
vectors
keyword
nodes
different
depths
norms
error
vectors
root
nodes
rlstm
many
gradients
explode
epoch
stabilize
later
gradients
vanish
even
depth
02468101214012345depthepoch
102468101214012345depthepoch
202468101214012345depthepoch
302468101214012345depthepoch
402468101214012345depthepoch
502468101214012345depthepoch
602468101214012345depthepoch
102468101214012345depthepoch
202468101214012345depthepoch
302468101214012345depthepoch
402468101214012345depthepoch
502468101214012345depthepoch
rnn
rlstm
development
accuracies
figure
ratios
depth
epoch
training
rnn
rlstm
02468101200.511.522.53epoch024681012020406080100epoch
00.511.522.53accuracyaccuracy
richard
socher
john
bauer
christopher
manning
andrew
2013a
parsing
compo-
proceedings
sitional
vector
grammars
51st
annual
meeting
association
compu-
tational
linguistics
pages
455–465
richard
socher
alex
perelygin
jean
jason
chuang
christopher
manning
andrew
christopher
potts
2013b
recursive
deep
mod-
els
semantic
compositionality
sentiment
treebank
proceedings
emnlp
kai
sheng
tai
richard
socher
christopher
manning
2015.
improved
semantic
representa-
tions
tree-structured
long
short-term
memory
networks
proceedings
53rd
annual
meet-
ing
association
computational
linguistics
7th
international
joint
conference
natu-
ral
language
processing
volume
long
papers
pages
1556–1566
beijing
china
july
association
computational
linguistics
xiaodan
zhu
parinaz
sobhani
hongyu
guo
2015.
long
short-term
memory
recursive
structures
proceedings
international
confer-
ence
machine
learning
july
avoiding
vanishing
gradient
problem
capturing
long
term
dependencies
train-
ing
traditional
recurrent
networks
conclusions
experimental
results
show
rlstm
superior
rnn
terms
overcoming
vanishing
gradient
problem
capturing
long
term
dependencies
parallel
general
conclusions
power
lstm
archi-
tecture
compared
traditional
recurrent
neural
networks
future
work
focus
com-
plex
cases
involving
negation
composition
etc
references
yoshua
bengio
patrice
simard
paolo
frasconi
1994.
learning
long-term
dependencies
gra-
dient
descent
difﬁcult
neural
networks
ieee
transactions
:157–166
john
duchi
elad
hazan
yoram
singer
2011.
adaptive
subgradient
methods
online
learning
stochastic
optimization
journal
ma-
chine
learning
research
pages
2121–2159
felix
gers
j¨urgen
schmidhuber
2001.
lstm
recurrent
networks
learn
simple
context-free
neural
networks
context-sensitive
languages
ieee
transactions
:1333–1340
sepp
hochreiter
j¨urgen
schmidhuber
1997.
neural
computation
long
short-term
memory
:1735–1780
phong
willem
zuidema
2015.
compositional
distributional
semantics
long
short
term
mem-
ory
proceedings
joint
conference
lex-
ical
computational
semantics
*sem
associ-
ation
computational
linguistics
shujie
liu
nan
yang
ming
zhou
2014.
recursive
recurrent
neural
network
statistical
machine
translation
proceedings
52nd
an-
nual
meeting
association
computational
linguistics
volume
long
papers
pages
1491–
1500
baltimore
maryland
june
association
computational
linguistics
minh-thang
luong
richard
socher
christo-
pher
manning
2013.
better
word
representa-
tions
recursive
neural
networks
morphol-
ogy
conll-2013
104.
richard
socher
christopher
manning
an-
drew
2010.
learning
continuous
phrase
representations
syntactic
parsing
recursive
neural
networks
proceedings
nips-2010
deep
learning
unsupervised
feature
learning
workshop
