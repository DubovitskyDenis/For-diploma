technical
report
str
2016-2
february
2016
energetics
brain
anders
sandberg
sapience
project
2016
synopsis
energy
requirements
human
brain
give
energy
constraints
give
reason
doubt
feasibility
artificial
intelligence
report
review
relevant
estimates
brain
bioenergetics
analyze
methods
estimating
brain
emulation
energy
re-
quirements
turning
reasons
believe
energy
requirements
novo
little
correlation
brain
emulation
energy
requirements
since
cost
could
depend
merely
cost
processing
higher-level
representations
rather
billions
neural
fir-
ings
unless
one
thinks
human
way
thinking
optimal
easily
imple-
mentable
way
achieving
software
intelligence
expect
novo
make
use
different
potentially
compressed
fast
processes
acm
computing
classification
system
ccs
hardware~cellular
neural
networks
hard-
ware~emerging
technologies~biology-related
information
processing~neural
systems
london
united
kingdom
sapience
project
thinktank
dedicated
study
disruptive
intelligent
computing
charter
identify
extrapolate
anticipate
disruptive
long-lasting
possibly
unintended
consequences
progressively
intelligent
computation
economy
society
syndicate
focus
reports
mitigation
strategies
board
vic
callaghan
university
essex
jack
copeland
university
canterbury
amnon
eden
sapience
project
jim
moor
dartmouth
college
david
pearce
bltc
research
steve
phelps
kings
college
london
anders
sandberg
oxford
university
tony
willson
helmsman
services
sapience
project
recently
major
enthusiasm
artificial
intelligence
concerns
might
pose
major
risks
humanity
bostrom
2014
number
high-profile
re-
searchers
think
safety
high
priority
future
life
institute
2015
also
significant
disagreement
much
risk
poses
especially
true
questions
human-level
beyond
lawrence
krauss
krauss
2015
worried
risk
much
complacency
based
particular
view
trustworthiness
level
common
sense
exhibited
pos-
sible
future
pretty
impossible
criticise
makes
particular
claim
first
let
make
one
thing
clear
even
exponential
growth
computer
storage
processing
power
past
years
thinking
computers
require
digital
architecture
bears
little
resemblance
current
computers
likely
become
competitive
consciousness
near
term
simple
physics
thought
experiment
supports
claim
given
current
power
consumption
electronic
computers
computer
storage
processing
capability
human
mind
would
require
excess
terawatts
power
within
factor
two
current
power
consumption
humanity
however
human
brain
uses
watts
power
means
mismatch
factor
1012
million
million
past
decade
doubling
time
megaflops/watt
years
even
assuming
moore
law
continues
unabated
means
take
doubling
times
120
years
reach
comparable
power
dissipation
moreover
doubling
efficiency
requires
relatively
radical
change
technology
extremely
unlikely
dou-
blings
could
achieved
without
essentially
changing
way
computers
compute
claim
several
problems
first
developers
think
must
stay
current
architectures
second
importantly
community
concerned
superintelli-
gence
risk
generally
agnostic
soon
smart
could
developed
happen
soon
tough
problem
need
solution
given
hard
value
alignment
problem
seems
third
consciousness
likely
irrelevant
instrumental
intelligence
maybe
word
used
stand-in
equally
messy
term
like
mind
common
sense
human
intelligence
interesting
issue
however
energy
requirements
computational
power
tells
human
machine
intelligence
vice
versa
energy
major
constraint
cog-
nition
way
constraining
predictions
claims
future
artificial
minds
computer
brain
emulation
energy
use
earlier
looked
energy
requirements
singularity
sandberg
2015
sum
current
computers
energy
hogs
requiring
2.5
power
globally
average
cost
around
per
operation
efficient
processors
certainly
possible
many
ones
current
use
old
suboptimal
example
current
gpus
consume
hundred
watts
1010
transistors
reaching
performance
100
gflops
range
one
per
flop
energetics
brain
better1
koomey
law
states
energy
cost
per
operation
halves
every
1.57
years
years
krauss
says
koomey
2011
far
growth
computing
capacity
grown
pace
energy
efficiency
making
two
trends
cancel
end
landauer
principle
gives
lower
bound
per
irreversible
operation
landauer
1961
one
circumvent
using
reversible
quantum
computation
costs
error
correction
unless
use
extremely
slow
cold
systems
current
era
com-
putation
energy-intensive
sure
brain
model
krauss
bases
estimate
tw/25
4·1020
opera-
tions
per
second
using
slightly
efficient
gpus
ups
1022
flops
looking
esti-
mates
brain
computational
capacity
appendix
old
roadmap
sandberg
bostrom
2008
higher
estimate
seem
ballpark
thagard
2002
argues
number
computational
elements
brain
far
greater
number
neurons
possibly
even
individual
protein
molecules
strong
claim
say
least
especially
since
current
gpus
somewhat
credible
job
end-to-end
speech
recognition
transcription
catanzaro
2015
corresponds
small
part
brain
hardly
10-11
brain
generally
assuming
certain
number
operations
per
second
brain
calculating
energy
cost
give
answer
want
people
argue
really
matters
tiny
conscious
bandwidth
maybe
bits/s
less
lifetime
may
learn
gigabit
landauer
1986
used
1022
1025
flops
safe
side
one
post
sandberg
2009
aiimpacts.org
collected
several
estimates
getting
median
estimate
1018
grace
2015b
also
argued
favor
using
teps
traversed
edges
per
second
rather
flops
suggesting
around
1014
teps
human
brain
level
soon
within
reach
systems
grace
2015a
many
apples-to-oranges
comparisions
single
processor
operation
may
may
correspond
floating-point
operation
let
alone
gpu
teps
however
since
land
order-of-magnitude
estimates
may
matter
much
brain
energy
use
turn
things
around
energy
use
human
brains
tell
com-
putational
capacity
ralph
merkle
calculated
back
1989
given
watts
usable
energy
per
human
brain
cost
jump
past
node
ranvier
costs
5·10-15
producing
2·1015
op-
erations
estimated
equal
number
synaptic
operations
ending
1013–1016
operations
per
second
merkle
1989
calculation
due
karlheinz
meier
argued
brain
uses
power
100
billion
neurons
firing
per
second
uses
10-10
per
action
potential
plus
1015
synapses
receiving
signals
nvidia
titan
reaches
6.2
tflops
using
250
0.04
per
flop
sapience
project
uses
10-14
per
synaptic
transmission
one
also
bottom
top
109
atp
molecules
per
action
potential
105
needed
synaptic
transmission
10-19
per
atp
gives
10-10
per
action
potential
10-14
per
synaptic
transmission
estimates
converge
rough
numbers
used
meier
argue
need
much
better
hardware
scaling
ever
want
get
level
detail
digging
deeper
neural
energetics
maintaining
resting
potentials
neurons
glia
ac-
count
total
brain
metabolic
cost
respectively
actual
spiking
activity
transmitter
release/recycling
plus
calcium
movement
contributes
lennie
2003
note
far
equipartition
meier
estimate
looking
total
brain
metabolism
constrains
neural
firing
rate
3.1
spikes
per
second
per
neuron
would
consume
energy
brain
normally
consumes
likely
optimistic
estimate
brain
simply
afford
firing
neurons
time
likely
relies
rather
sparse
representations
unmyelinated
axons
require
nj/cm
transmit
action
potentials
crotty
sangrey
levy
2006
general
brain
gets
around
current
optimization
alle
roth
geiger
2009
myelinisation
also
speeds
transmission
price
increased
error
rate
likely
many
clever
coding
strategies
biology
clearly
strongly
energy
con-
strained
addition
cooling
brain
bloodflow
750-1000
ml/min
rela-
tively
tight
given
arterial
blood
already
body
temperature
divided
1.3·10-21
landauer
limit
body
temperature
suggests
limit
1.6·1022
irreversible
operations
per
second
huge
number
orders
higher
many
estimates
juggling
far
say
operations
distributed
across
100
billion
neurons
least
within
order
magnitude
real
number
azevedo
2009
get
160
billion
operations
per
second
per
neuron
in-
stead
treat
synapses
8000
per
neuron
loci
get
million
operations
per
second
per
synapse
running
full
hodgkin-huxley
neural
model
resolution
requires
1200
flops
1.2
million
flops
per
second
simulation
per
compartment
izhikevich
2004
treat
synapse
compartment
16.6
times
landauer
limit
neural
simulation
multiple
digit
precision
erased
per
operation
would
bump
landauer
limit
straight
away
synapses
actually
fairly
computationally
efficient
least
body
temperature
cryogenically
cooled
computers
could
course
much
better
izikievich
originator
1200
flops
estimate
likes
point
model
requires
flops
izhikevich
2004
maybe
need
model
ion
currents
like
hodgkin-hux-
ley
get
right
behavior
hence
shave
two
orders
magnitude
information
dissipation
neural
networks
much
information
lost
neural
processing
brain
autonomous
dynamical
system
changing
internal
state
complicated
way
let
ignore
sensory
inputs
current
discussion
start
state
somewhere
within
energetics
brain
predefined
volume
state-space
time
state
move
states
initial
uncertainty
grow
eventually
possible
volume
find
state
doubled
lost
one
bit
information
things
bit
complicated
since
dynamics
contract
along
dimensions
diverge
along
others
described
lyapunov
exponents
trajectory
exponent
direction
nearby
trajectories
diverge
like
ݔ௔ሺݐሻ
ݔ௕ሺݐሻ
ݔ௔ሺ0ሻ
ݔ௕ሺ0ሻ݁ఒ௧
direction
dissipative
dynamical
system
sum
exponents
negative
total
trajectories
move
towards
attractor
set
however
least
one
exponents
posi-
tive
strange
attractor
trajectories
endlessly
approach
yet
locally
diverge
gradually
mix
measure
fixed
precision
point
time
tell
certainly
trajectory
contraction
due
negative
exponents
thrown
away
starting
location
information
exactly
attractor
future
positive
exponents
amplifying
current
uncertainty
measure
information
loss
kolmogorov-sinai
entropy
sinai
2009
bounded
sum
positive
lyapunov
exponents
therefore
calculate
ks-entropy
neural
system
estimate
much
information
thrown
away
per
unit
time
hence
get
hint
energy
dissipation
ఒ೔வ଴
monteforte
wolf
looked
one
simple
neural
model
theta-neuron
monteforte
wolf
2010
found
ks-entropy
roughly
bit
per
neuron
spike
large
range
parameters
given
estimates
one
spike
per
second
per
neuron
gives
overall
information
loss
1011
bits/s
brain
1.3·10-10
landauer
limit
account
orders
magnitude
away
thermodynamic
per-
fection
picture
regard
action
potential
corresponding
roughly
one
irreversible
yes/no
decision
perhaps
unreasonable
claim
worth
noticing
one
look
cognition
system
large-scale
dynamics
one
entropy
corresponding
shifting
different
high-level
mental
states
mi-
croscale
dynamics
different
entropy
corresponding
neural
information
pro-
cessing
safe
bet
biggest
entropy
costs
microscale
fast
numerous
simple
states
macroscale
slow
complex
states
computing
system
dynamics
mimicking
brain
microscale
expect
dissipate
10-10
weak
bound
energy
leave
regards
energy
requirements
artificial
intelligence
assuming
amount
energy
needed
human
machine
cognitive
task
mistake
sapience
project
first
izikievich
neuron
demonstrates
izhikevich
2004
might
judicious
ab-
straction
easily
saves
two
orders
magnitude
computation/energy
special
purpose
hard-
ware
also
save
one
two
orders
magnitude
krizhevsky
sutskever
hinton
2012
2015
park
2015
using
general-purpose
processors
fixed
computations
inefficient
gpus
present
useful
neural
processing
many
cases
want
perform
action
many
pieces
data
rather
different
actions
piece
careful
design
may
reduce
demands
even
cao
chen
khosla
2014
panda
sengupta
roy
2015
importantly
level
task
implemented
matters
sorting
summing
list
thousand
elements
fast
computer
operation
done
memory
microseconds
hour-long
task
human
mental
architecture
need
represent
information
far
redundant
slow
way
mention
perform
individual
actions
seconds
time-scale
computer
sort
uses
tight
representation
like
low-level
neural
circuitry
doubt
one
could
string
together
biological
neurons
per-
form
sort
sum
operation
quickly
c.f
ditto
2003
cognition
happens
higher
general
level
system
intriguing
speculations
idiot
savants
aside
reason
admire
brains
also
unable
perform
certain
useful
computations
artificial
neural
networks
often
employ
non-local
matrix
operations
like
inversion
calculate
optimal
weights
toutounian
ataei
2009
computations
possible
perform
locally
distributed
manner
gradient
descent
algorithms
backpropagation
unrealistic
biological
sense
clearly
successful
deep
learn-
ing
shortage
papers
describing
various
clever
approximations
would
allow
biologically
realistic
system
perform
similar
operations
fact
brains
may
well
artificial
systems
perform
directly
using
low-level
hardware
intended
efficiently
deep
learning
system
learns
object
recognition
week2
beats
human
baby
many
months
learns
analogies
1.6
billion
text
snippets
minutes
pen-
nington
socher
manning
2014
beats
human
children
years
small
domains
yet
domains
important
humans
presumably
develop
quickly
efficiently
possible
children
biology
many
advantages
robustness
versatility
mention
energy
efficiency
nevertheless
also
fundamentally
limited
built
cells
particular
kind
metabolism
fact
organisms
need
build
inside
need
solving
problems
exist
particular
biospheric
environment
time
train
deep
neural
network
imagenet
database
individual
computers
gpu
support
order
days
liu
2016
energetics
brain
conclusion
unless
one
thinks
human
way
thinking
optimal
easily
implementable
way
expect
novo
make
use
different
potentially
compressed
fast
processes3
hence
costs
brain
computation
merely
proof
existence
systems
effective
mental
tasks
could
well
done
far
less
far
efficient
systems
end
may
try
estimate
fundamental
energy
costs
cognition
bound
energy
use
human-like
cognition
takes
certain
number
bit
erasures
per
second
would
get
bound
using
landauer
ignoring
reversible
computing
however
discussion
showed
may
actual
computational
cost
needed
higher-level
representations
rather
billions
neural
firings
actually
understand
intelligence
say
point
question
moot
anyway
many
people
intuition
cautious
approach
always
state
things
work
however
mixes
cautious
conservative
even
reactionary
better
cau-
tious
approach
recognize
things
may
work
start
checking
possible
con-
sequences
want
reassuring
constraint
certain
things
happen
need
tighter
energy
estimates
acknowledgments
earlier
version
essay
previously
published
personal
blog
thanks
am-
non
eden
adapting
whitepaper
luke
muelhauser
2015
carl
shulman
originally
pointing
lawrence
krauss
essay
lawerence
krauss
stimulating
writing
essay
would
also
like
thank
eric
drexer
katja
grace
useful
comments
references
alle
roth
geiger
2009
energy-efficient
action
potentials
hippocampal
mossy
fibers
science
325
5946
1405–8
doi:10.1126/science.1174331
azevedo
frederico
ludmila
carvalho
lea
grinberg
josé
marcelo
farfel
renata
ferretti
renata
leite
wilson
jacob
filho
roberto
lent
suzana
herculano-
houzel
2009
equal
numbers
neuronal
nonneuronal
cells
make
human
brain
isometrically
scaled-up
primate
brain
journal
comparative
neurology
513
532–41
doi:10.1002/cne.21974
bostrom
nick
2014.
superintelligence
paths
dangers
strategies
first
edition
oxford
oxford
university
press
brain
emulation
makes
sense
one
either
figure
else
achieve
one
wants
copy
extant
brains
properties
individuality
sapience
project
cao
yongqiang
yang
chen
deepak
khosla
2014
spiking
deep
convolutional
neural
networks
energy-efficient
object
recognition
international
journal
computer
vi-
sion
113
54–66
doi:10.1007/s11263-014-0788-3
catanzaro
bryan
2015
deep
speech
accurate
speech
recognition
gpu-accelerated
deep
learning
parallel
forall
february
25.
https
//devblogs.nvidia.com/parallel-
forall/deep-speech-accurate-speech-recognition-gpu-accelerated-deep-learning/
crotty
patrick
thomas
sangrey
william
levy
2006
metabolic
energy
cost
action
potential
velocity
journal
neurophysiology
1237–46
doi:10.1152/jn.01204.2005
ditto
william
2003
chaos
neural
systems
epilepsy
neurocomputing
en-
gineering
medicine
biology
society
2003.
proceedings
25th
annual
international
conference
ieee
4:3830–33
ieee
http
//ieeexplore.ieee.org/xpls/abs_all.jsp
ar-
number=1280997
future
life
institute
2015
autonomous
weapons
open
letter
robotics
re-
searchers
july
28.
http
//futureoflife.org/ai/open_letter_autonomous_weapons
grace
katja
2015a
impacts
brain
performance
teps
may
http
//aiim-
pacts.org/brain-performance-in-teps/
———
2015b
impacts
brain
performance
flops
july
26.
http
//aiimpacts.org/brain-
performance-in-flops/
izhikevich
eugene
2004
model
use
cortical
spiking
neurons
ieee
trans-
actions
neural
networks
1063–70
doi:10.1109/tnn.2004.832719
koomey
j.g.
berard
sanchez
wong
2011
implications
historical
trends
electrical
efficiency
computing
ieee
annals
history
computing
46–
54.
doi:10.1109/mahc.2010.28
krauss
lawrence
2015
worry
share
concerns
artificial
intelligence
ieet
blog
may
28.
http
//edge.org/response-detail/26163
krizhevsky
alex
ilya
sutskever
geoffrey
hinton
2012
imagenet
classification
deep
convolutional
neural
networks
advances
neural
information
processing
systems
1097–1105
http
//papers.nips.cc/paper/4824-imagenet-classification-w.
landauer
rolf
1961
irreversibility
heat
generation
computing
process
ibm
jour-
nal
research
development
183–91
doi:10.1147/rd.53.0183
landauer
thomas
1986
much
people
remember
estimates
quantity
learned
information
long-term
memory
cognitive
science
477–93
doi:10.1207/s15516709cog1004_4
lennie
peter
2003
cost
cortical
computation
current
biology
493–97
doi:10.1016/s0960-9822
00135-0.
liu
liu
2016
convnet
deep
convolutional
networks
february
http
//lib-
ccv.org/doc/doc-convnet/
junjie
young
arel
holleman
2015
tops/w
analog
deep
machine-learn-
ing
engine
floating-gate
storage
0.13
x00b5
cmos
ieee
journal
solid-
state
circuits
270–81
doi:10.1109/jssc.2014.2356197
merkle
ralph
1989
brain
limits
foresight
update
august
http
//www.merkle.com/brain-
limits.html
monteforte
michael
fred
wolf
2010
dynamical
entropy
production
spiking
neuron
physical
review
letters
state
105
networks
doi:10.1103/physrevlett.105.268104
balanced
muehlhauser
luke
2015
krauss
long-term
impacts
lukemuehlhauser.com
may
28.
http
//lukemuehlhauser.com/krauss-on-long-term-ai-impacts/
energetics
brain
panda
priyadarshini
abhronil
sengupta
kaushik
roy
2015
conditional
deep
learning
energy-efficient
enhanced
pattern
recognition
arxiv:1509.08971
septem-
ber
http
//arxiv.org/abs/1509.08971
park
seong-wook
junyoung
park
kyeongryeol
bong
dongjoo
shin
jinmook
lee
sungpill
choi
hoi-jun
yoo
2015
energy-efficient
scalable
deep
learning/inference
processor
tetra-parallel
mimd
architecture
big
data
applications
ieee
trans-
actions
biomedical
circuits
systems
838–48
doi:10.1109/tbcas.2015.2504563
pennington
jeffrey
richard
socher
christopher
manning
2014
glove
global
vectors
word
representation.
emnlp
14:1532–43
http
//llcao.net/cu-deeplearn-
ing15/presentation/nn-pres.pdf
sandberg
anders
2009
andart
*really*
green
sustainable
humanity
andart
march
30.
http
//www.aleph.se/andart/archives/2009/03/a_really_green_and_sustainable_hu-
manity.html
———
2015
energy
requirements
singularity
andart
february
http
//aleph.se/andart2/megascale/energy-requirements-of-the-singularity/
sandberg
anders
nick
bostrom
2008
whole
brain
emulation
roadmap
2008-3.
fu-
http
//www.philoso-
ture
phy.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf
institute
oxford
humanity
university
sinai
yakov
2009
kolmogorov-sinai
entropy
scholarpedia
2034.
doi:10.4249/scholar-
pedia.2034
thagard
paul
2002
molecules
matter
mental
computation
philosophy
science
497–518
doi:10.1086/342452
toutounian
ataei
2009
new
method
computing
moore–penrose
inverse
412–17
matrices
journal
computational
applied
mathematics
228
doi:10.1016/j.cam.2008.10.008
