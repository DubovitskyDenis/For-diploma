learning
tuning
meta-heuristics
plan
space
planning
shashank
shekhar
deepak
khemani
department
computer
science
engineering
iit
madras
chennai
tamil
nadu
india
600
036
sshekhar
khemani
cse.iitm.ac.in
abstract
recent
years
planning
community
observed
techniques
learning
heuristic
functions
yielded
im-
provements
performance
one
approach
use
ofﬂine
learning
learn
predictive
models
existing
heuristics
domain
dependent
manner
learned
models
de-
ployed
new
heuristic
functions
learned
models
turn
tuned
online
using
domain
independent
error
correction
approach
enhance
informativeness
online
tuning
approach
domain
independent
in-
stance
speciﬁc
contributes
improved
performance
individual
instances
planning
proceeds
consequently
effective
larger
problems
paper
mention
two
approaches
applicable
par-
tial
order
causal
link
pocl
planning
also
known
plan
space
planning
first
endeavour
enhance
performance
pocl
planner
giving
algorithm
supervised
learning
second
discuss
online
error
minimization
approach
pocl
framework
minimize
step-error
associated
ofﬂine
learned
models
thus
en-
hancing
informativeness
evaluation
shows
learning
approaches
scale
performance
planner
standard
benchmarks
specially
larger
problems
introduction
recent
international
planning
competitions
ipc
state-space
based
total-order
planners
like
lama
richter
westphal
2010
fast
downward
stone
soup
helmert
r¨oger
karpas
2011
fast
downward
stone
soup
2014
r¨oger
pommerening
seipp
2014
performed
well
planners
efﬁcient
generate
consistent
states
fast
use
powerful
state-based
heuristics
often
commit
early
ordering
actions
giving
ﬂexibility
generated
plans
contrast
pocl
framework
coles
2010
generates
ﬂexible
plans
general
computationally
intensive
state-space
based
approaches
pocl
framework
found
applicability
multi-agent
planning
kvarnstr¨om
2011
temporal
planning
benton
coles
coles
2012
researchers
recently
investigated
applicability
state-space
heuristic
learning
approaches
sapena
on-
copyright
cid:13
2015
association
advancement
artiﬁcial
intelligence
www.aaai.org
rights
reserved
aindıa
torreno
2014
coles
2010
pocl
plan-
ning
revival
interest
due
idea
delayed
commitment
repop
nguyen
kambhampati
2001
vhpop
younes
simmons
2003
paper
investigate
adaptation
state
space
approaches
pocl
planners
yielding
quality
plans
even
larger
problems
general
due
diverse
nature
planning
problems
characterized
degree
inter-
actions
subgoals
heuristic
function
al-
ways
work
well
planning
domains
various
techniques
devised
increase
informativeness
heuris-
tics
state
space
arena
one
approach
strengthens
delete
relaxation
heuristic
incrementing
lower
bounds
tighten
admissibility
heuristic
repeatedly
solv-
ing
relaxed
versions
planning
problem
haslum
2012
another
approach
circumvent
trade-offs
combin-
ing
multiple
heuristics
decision
rule
used
selecting
heuristic
function
given
state
active
online
learning
technique
applied
learn
model
given
decision
rule
domshlak
karpas
markovitch
2010
present
couple
machine
learning
techniques
inﬂuenced
arfaee
zilles
holte
2011
thayer
dionne
ruml
2011
samadi
felner
schaeffer
2008
virseda
borrajo
alc´azar
2013
improve
effectiveness
heuristics
pocl
framework
first
apply
domain
wise
regression
techniques
supervised
manner
using
existing
pocl
heuristics
features
tar-
get
generation
use
planner
named
regpocl
based
vhpop
uses
grounded
actions
speed
planning
process
next
give
another
tech-
nique
pocl
framework
enhances
informa-
tiveness
ofﬂine
learned
models
technique
adapted
version
online
heuristic
adjustment
approach
based
temporal
difference
learning
sutton
1988
thayer
dionne
ruml
2011
extend
domain
independent
approach
learning
instance
speciﬁc
details
corrects
error
associated
learned
models
thus
making
informed
regpocl
planner
employs
two
approaches
evaluation
shows
efﬁcient
benchmarks
conﬁned
evaluation
non-temporal
strips
domains
rest
paper
structured
follows
look-
ing
motivation
exploring
pocl
approach
describe
learning
approaches
used
along
experi-
ment
design
followed
experimental
results
concluding
discussion
pocl
planning
pocl
planner
starts
search
null
partial
plan
progresses
space
partial
plans
adding
re-
solver
partial
plan
remove
ﬂaw
use
heuristics
younes
simmons
2003
selecting
ad-
verse
ﬂaw
resolve
selected
partial
plan
pocl
framework
certain
advantages
forward
state-space
search
fsss
fsss
problem
premature
commit-
ment
ordering
two
actions
reduces
plan
ﬂexibility
avoid
mutual
interference
actions
though
may
pocl
plan-
ning
avoids
committing
unnecessarily
ordering
actions
fsss
faces
problems
solving
instances
dead-
lines
deadlines
may
arise
within
interval
one
durative
actions
general
actions
may
produce
delayed
effects
may
ramiﬁcations
deadlines
well
creates
deadlines
relative
starting
points
coles
2010
fsss
also
suffers
signiﬁcant
backtracking
may
explore
possible
plan
permutations
absence
effective
guidance
pocl
framework
also
several
advantages
temporal
planning
specially
planning
complex
temporal
con-
straints
beyond
actions
duration
limitations
fsss
motivate
explore
pocl
framework
example
suppose
required
add
four
actions
plan
dependent
dependent
interference
two
actions
apart
dependencies
case
fsss
gives
ordering
timestamp
makespan
whilst
delayed
commitment
strategy
would
give
choices
ﬂexibility
orderings
like
parallel
execution
allowed
makespan
would
another
action
depen-
dent
introduced
plan
fsss
allot
timestamp
whereas
delayed-commitment
strat-
egy
could
allot
however
ignore
absence
ﬂexibility
action
parallelism
fsss
fast
recovering
situation
would
arise
due
committing
wrong
choices
planning
fsss
advantage
faster
search
state
generation
powerful
state-based
heuristics
learning
approaches
used
propose
two
fold
approach
learn
better
heuristic
functions
first
existing
heuristic
function
combined
process
ofﬂine
learning
generates
learned
predictive
models
followed
online
technique
adjust-
ing
step-error
associated
models
par-
tial
plan
reﬁnement
divide
section
two
parts
ﬁrst
describes
ofﬂine
learning
techniques
perform
regression
second
technique
strength-
ening
informativeness
learned
predictive
model
ofﬂine
learning
ofﬂine
learning
attractive
approach
generating
training
sets
planning
domains
fast
simple
process
learning
process
three
phases
dataset
preparation
training
iii
testing
training
in-
stances
gathered
solving
small
problems
become
in-
puts
used
regression
techniques
e.g
linear
regres-
sion
m5p
described
later
learn
effective
ways
combining
existing
heuristic
functions
testing
phase
used
validate
best
ways
combining
known
heuristic
functions
algorithm
described
embodies
complete
training
process
algorithm
algorithm
used
training
phase
input
attribute
selection
training
dataset
problem
set
learning
technique
heuristic
set
regpocl
planner
output
learned
predictive
model
procedure
training-algorithm
domain
speciﬁc
dataset-prep
regpocl
training
instances
apply
return
apply
training
instances
end
procedure
πloc
solve
regpocl
reﬁnes
completely
check
target
value
set
seed
partial
plans
null
partial
plan
random
procedure
dataset-prep
regpocl
end
procedure
πloc
ins
comp-inst
regpocl
ins
end
bounded
number
iterations
end
return
end
dataset
preparation
procedure
dataset-prep
line
algorithm
used
solve
set
plan-
ning
problems
consider
small
problems
gathered
planning
domain
selected
previ-
ous
ipcs
output
algorithm
trained
predictive
model
shown
line
consider
problem
dataset
preparation
domain
line
algorithm
seed
partial
plan
new
partial
plan
gets
generated
due
possible
reﬁnement
selected
par-
tial
plan
select
seed
set
seed
partial
plans
line
provided
regpocl
fur-
ther
reﬁnements
regpocl
able
generate
one
consis-
tent
solution
reﬁning
completely
using
solve
func-
tion
line
ﬂag
true
capture
newly
generated
partial
plans
local
instance
called
πloc
target
captures
number
new
actions
get
added
calculated
planner
reﬁnes
completely
using
heuristics
note
heuristic
value
number
new
actions
added
dur-
ing
reﬁnement
process
value
also
plan
length
found
might
optimal
since
reﬁned
completely
πloc
updated
line
line
com-
putes
training
instance
ins
using
comp-inst
function
given
planner
generates
training
instance
form
cid:104
cid:104
cid:105
cid:105
feature
heuristics
maintain
consistency
update
training
set
line
regpocl
reﬁnes
current
seed
completely
complete
reﬁnement
possible
new
seeds
πloc
dropped
even
though
might
case
πloc
contains
consistent
seeds
particularly
case
time
maintain
diversity
given
domain
randomly
select
ﬁxed
number
seeds
complete
reﬁnement
process
line
execute
algorithm
selected
domain
given
set
feature
heuristics
note
learning
guarantee
optimal
predictive
models
even
though
opti-
mal
targets
used
training
virseda
borrajo
alc´azar
2013
algorithm
hunts
well
informed
heuristic
using
learning
bother
admis-
sibility
since
state-of-the-art
pocl
heuristics
optimal
nature
younes
simmons
2003
usage
regpocl
generating
training
instances
affect
performance
regpocl
large
problems
testing
phase
selection
regpocl
generating
training
sets
might
deteriorate
actual
target
values
targets
calculated
regpocl
optimal
general
thus
possibility
learning
inaccurate
predictive
models
training
phase
might
reduce
infor-
mativeness
models
enhance
informativeness
models
correcting
step-error
associated
using
online
heuristic
tuning
approach
training
algorithm
generates
enough
number
training
instances
given
domain
moves
next
step
line
deﬁne
regression
model
training
set
set
real
num-
bers
following
general
model
training
strategy
use
weka
hall
2009
remove
irrelevant
redundant
attributes
line
training
set
reduces
effort
planner
planner
must
calculate
selected
feature
heuristics
step
planning
pro-
cess
next
apply
model
training
process
line
feed
training
instances
different
machine
learning
ap-
proaches
learn
different
predictive
models
testing
test
predictive
models
large
problems
models
directly
compared
current
best
heuris-
tics
pocl
framework
using
machine
learning
ap-
proaches
planning
efﬁciently
select
best
learned
regression
models
test
efﬁciency
regpocl
using
different
benchmarks
models
help
selecting
suitable
partial
plan
reﬁnement
ofﬂine
learning
learns
better
model
terms
search
guidance
accuracy
online
learning
samadi
fel-
ner
schaeffer
2008
thayer
dionne
ruml
2011
ofﬂine
learned
predictor
accurate
on-
line
one
ofﬂine
case
complete
training
set
available
another
alternative
approaches
would
bootstrapping
methods
arfaee
zilles
holte
2011
set
problems
solved
using
base
heuris-
tic
within
speciﬁed
time
limit
later
solutions
ob-
tained
learning
used
generate
new
informed
heuristic
function
online
error
adjustment
predictive
model
ofﬂine
approach
assumes
learned
knowledge
effectively
transferred
training
instances
test
instances
always
true
planning
problems
always
similar
even
though
belong
domain
also
training
instances
required
training
process
planning
domain
dataset
preparation
calculation
features
compu-
tationally
hard
calculating
actual
number
new
actions
needed
relatively
expensive
online
learning
attempts
avoid
limitations
hybrid
approach
use
small
instances
ofﬂine
training
thus
saving
time
followed
online
learning
improve
heuristic
estimate
on-the-ﬂy
online
error
tuning
based
temporal
difference
learning
sutton
1988
figure
pocl
framework
reﬁnement
starts
goes
solution
plan
πsol
step
selected
partial
plan
many
reﬁnements
possible
like
reﬁnements
lead
cid:48
cid:48
cid:48
best
child
shown
horizontal
reﬁnements
online
heuristic
adjustment
approach
inspired
recent
work
presented
technical
communica-
tion
shekhar
khemani
2015
tested
planning
domains
develop
approach
provide
complete
derivation
proof
appendix
the-
orem
used
cited
work
assume
predictive
model
given
partial
plan
estimates
actual
number
new
actions
required
added
reﬁne
completely
solution
plan
denoted
whilst
minimum
number
new
actions
required
purpose
nguyen
kambhampati
2001
pocl
framework
computationally
expensive
predict
since
also
likely
uninformed
adjust-
ments
observing
single-step-error
on-the-ﬂy
minimum
number
total
reﬁnements
needed
partial
plan
make
solution
plan
goes
best
child
πi+1
obtained
reﬁnement
child
πi+1
best
child
lowest
prediction
number
new
actions
needed
complete
reﬁne-
ment
among
siblings
figure
break
ties
fa-
vor
minimum
number
actions
children
partial
plans
set
successors
partial
plan
potentially
inﬁnite
due
introduction
loops
plan
simply
achieve
destroy
subgoals
example
cid:104
stack
unstack
cid:105
cid:104
pickup
putdown
cid:105
blocksworld
domain
looping
common
reﬁnement
process
specially
heuristic
well
informed
avoid
looping
explicitly
crucial
real
world
scenarios
example
pair
actions
like
cid:104
load-truck
obj
truck
loc
unload-truck
obj
truck
loc
cid:105
driverlog
domain
could
expensive
general
plan
space
planning
backtracking
reﬁnement
partial
plan
leads
different
node
plan
space
necessitates
consider
looping
explicitly
ideally
cost
πi+1
non-ideal
cases
generating
expensive
as-
sume
cost
πi+1
fig-
ure
given
partial
plan
suppose
minimum
reﬁnement
steps
required
make
solution
plan
assume
stage
reﬁnement
process
best
child
chosen
estimate
adding
new
actions
consider
unit
cost
actions
ideal
case
πi+1
generally
commits
error
reﬁnement
step
called
single-step-error
single-step-error
as-
sociated
gets
reﬁned
algorithm
using
denoted
cid:0
πi+1
cid:1
using
derive
proof
appendix
fol-
lowing
non-ideal
case
theorem
given
learned
predictive
model
partial
plan
figure
leads
solution
plan
πsol
certain
reﬁnement
steps
enhanced
version
predictive
model
cid:48
cid:48
cid:32
πsol
cid:32
πsol
path
figure
captures
partial
plan
cid:48
along
path
πsol
path
includes
excludes
πsol
term
single-
step-error
associated
reﬁnement
enhancement
predictive
model
theorem
online
approximated
version
ap-
proximation
uses
parent
best
child
relationships
measure
step-error
reﬁnement
path
correct
evaluations
reﬁnements
using
theorem
figure
calculate
average-step-error
associated
denoted
avg
along
path
πsol
cid:88
cid:46
avg
cid:48
cid:32
πsol
cid:48
cid:88
rewriting
cid:88
cid:48
cid:32
πsol
cid:48
avg
using
simpliﬁes
avg
simpliﬁcation
yields
avg
cid:46
another
possible
expansion
using
inﬁnite
geometric
pro-
gression
would
cid:88
avg
use
regpocl
test
effectiveness
pocl
framework
selects
best
partial
plan
i=0
experiment
design
section
describe
evaluation
phase
settings
includes
heuristics
selected
features
domains
selected
feature
heuristics
learning
features
used
learning
non-temporal
heuristics
literature
pocl
planning
considering
ap-
plicability
pocl
heuristics
litera-
ture
younes
simmons
2003
nguyen
kambham-
pati
2001
select
six
different
heuristic
functions
heuristics
informed
informativeness
varies
different
planning
domains
aim
learn
informed
combination
individual
heuris-
tics
six
heuristics
value
hg-val
returns
number
actions
selected
partial
plan
counting
two
dummy
actions
signiﬁes
far
search
progressed
starting
state
number
open
conditions
hoc
total
num-
ber
unsupported
causal
links
present
partial
plan
hoc
|oc|
nguyen
kambhampati
2001
additive
heuristic
heuristic
hadd
haslum
geffner
2000
adds
steps
required
individual
open
goal
younes
sim-
mons
2003
use
adapted
version
additive
heuristic
pocl
planning
ﬁrst
time
additive
heuristic
effort
hadd
estimate
similar
hadd
considers
cost
action
number
preconditions
action
plus
linking
cost
action
supports
unsupported
causal
link
younes
simmons
2003
call
hadd
notation
used
earlier
signiﬁes
extra
work
required
accounting
positive
interaction
add
returns
estimate
takes
account
positive
interac-
tions
subgoals
ignoring
negative
inter-
actions
represented
add
variant
hadd
younes
simmons
2003
hadd
additive
accounting
positive
interaction
effort
add
similar
heuristic
considers
to-
tal
effort
required
younes
simmons
2003
standard
notation
heuristic
also
used
literature
domains
selected
consider
following
domains
logistics
gripper
ipc
logistics
elevator
ipc
rovers
zenotravel
ipc
rovers
ipc
exper-
iments
consider
domains
com-
petitions
either
state-of-the-art
heuristics
able
create
enough
training
instances
learning
reg-
pocl
support
domain
deﬁnition
language
fea-
tures
ipc
domains
selected
since
planner
able
generate
enough
instances
initiate
ofﬂine
learn-
ing
domains
ipc
later
supported
regpocl
representations
use
action
costs
ﬂuents
hard
soft
constraints
included
preprocessing
like
removal
actions
cost
domain
description
ﬁles
selected
domain
consider
problems
represented
strips
style
select
small
sized
prob-
lems
learning
test
learned
predictive
models
large
sized
problems
domain
total
109
small
sized
problems
selected
domains
last
four
feature
heuristics
previous
subsection
used
calculating
targets
domain
means
generate
four
different
datasets
selected
do-
main
best
two
selected
choose
satisﬁc-
ing
track
problems
generating
training
instances
training
set
preparation
time
limit
minutes
upper
limit
500,000
node
generation
gen-
erate
thousand
training
instances
except
zeno-
travel
domain
total
instances
950.
avoid
overﬁtting
pick
training
instances
250
350
larger
training
sets
selected
learning
approaches
section
discuss
brief
procedure
feature
selection
dataset
training
regression
models
different
regression
techniques
references
feature
selection
general
training
sets
contain
irrelevant
redun-
dant
attributes
six
selected
heuristics
re-
duce
training
effort
increase
efﬁciency
planner
discard
training
set
planner
bound
calculate
selected
features
stage
reﬁnement
correlation
based
feature
selection
tech-
nique
hall
2000
used
ﬁnd
correlated
features
regression
techniques
use
following
regression
techniques
learn
predic-
tive
models
techniques
applied
planning
learning
recent
years
samadi
felner
schaeffer
2008
thayer
dionne
ruml
2011
virseda
borrajo
alc´azar
2013
linear
regression
regression
model
learns
linear
function
minimizes
sum
squared
error
training
instances
bishop
2006
m5p
m5p
gives
ﬂexibility
due
nature
capturing
non
linear
relationships
m5p
technique
learns
regression
tree
quinlan
others
1992
approximates
class
value
m5rules
similar
m5p
generates
rules
instead
modeling
regression
trees
quinlan
others
1992
least
median
squared
lms
lms
similar
median
squared
error
functions
generated
subsamples
data
least
squared
error
function
usu-
ally
model
lowest
median
squared
error
se-
lected
rousseeuw
leroy
2005
multilayer
perceptron
mlp
mlp
learn
com-
plex
relationships
compared
four
regression
techniques
bishop
2006
techniques
discussed
used
learn
mod-
els
weka
hall
2009
using
10-fold
cross-
validation
domain
regression
technique
called
svmreg
implements
support
vector
machine
regres-
sion
purposes
used
work
due
techni-
cal
difﬁculty
however
much
inﬂuenced
planning
processes
past
virseda
borrajo
alc´azar
2013
experimental
evaluation
use
mc-loc
mw-loc
younes
simmons
2003
ﬂaw
selecting
heuristics
reﬁning
partial
plan
give
higher
preference
local
ﬂaws
present
par-
tial
plan
employ
greedy
best
first
search
algorithm
selecting
next
partial
plan
reﬁnement
environment
perform
experiments
intel
core
quad
2.83
ghz
64-bit
processor
4gb
ram
evaluate
effectiveness
learned
models
correct
single-
step-error
associated
models
time
limit
min-
utes
node
generation
limit
million
used
evaluations
use
regpocl
compare
performances
of-
ﬂine
predictive
models
corresponding
enhanced
models
last
four
base
features
also
compared
recent
effective
state-space
based
heuristics
approaches
introduced
later
exclude
ﬁrst
two
base
features
comparison
since
weak
heuristics
regpocl
solve
suf-
ﬁcient
problems
using
however
useful
working
jointly
informed
heuristics
next
dis-
cuss
observations
made
training
phase
training
using
algorithm
prepare
datasets
learn
different
predictive
models
applying
various
regression
techniques
discussed
earlier
select
last
four
features
solve
set
problems
target
value
plan
length
found
regpocl
using
base
features
dataset
preparation
phase
took
less
two
pocl
heuristics
domain
gripper-1
rovers-3
rovers-5
logistics-1
elevator-2
logistics-2
zenotravel-3
150
hadd
hadd
148
add
state-of-the-art
hadd
hadd
add
150
add
add
add
+36
via
learning
approaches
add
+39
add
+32
150
+16
add
+39
add
+32
150
+16
add
150
+16
add
+39
add
+32
150
+16
evaluation
using
fast
downward
lazy
eager
150
cea
lm-cut
mhfs
cea
lm-cut
mhfs
150
150
150
150
cea
lm-cut
mhfs
cea
lm-cut
mhfs
150
150
150
lama
150
table
number
solved
problems
using
heuristic
number
problems
domain
state-of-the-art
pocl
heuristics
compared
learned
ones
left
half
state-based
heuristics
cea
lm-cut
combination
using
mhfs
strategy
also
compared
pocl
heuristics
last
column
captures
performance
lama11
best
results
shown
bold
number
mark
e.g.+36
shows
competitive
performance
learned
models
enhancements
base
heuristic
similar
representations
followed
tables
hours
average
domain
four
fea-
tures
enough
instances
begin
training
process
domain
best
two
datasets
four
sufﬁcient
training
points
selected
model
train-
ing
visualizing
distribution
training
points
using
weka
note
figure
different
heuristics
cal-
culating
target
values
prefer
different
paths
therefore
four
base
features
generate
four
different
datasets
attribute
selection
strategy
allows
select
subset
attributes
training
set
removing
correlated
redundant
features
learn
total
domains
datasets
regression
techniques
regression
models
training
phase
took
20ms
milliseconds
using
270ms
using
lms
600ms
using
mlp
82ms
using
m5rule
58ms
using
m5p
average
per
model
learned
models
high
accuracy
fastest
followed
m5p
m5rule
next
test
models
different
benchmarks
testing
test
effectiveness
approaches
se-
lecting
partial
plans
reﬁnement
using
regpocl
assume
informed
heuristic
leads
minimal
possible
reﬁnements
needed
next
comparison
com-
pute
score1
ipc
satisﬁcing
track
problems
bet-
ter
score
standard
signiﬁes
better
performance
compare
performance
learned
models
add
selected
base
features
hadd
hadd
comparison
done
basis
number
solved
problems
score
obtained
plan
quality
execu-
tion
time
nodes
partial
plan
visited
makespan
quality
add
learned
add
words
regpocl
dataset
prepared
using
add
calculating
target
values
dataset
uses
add
enhanced
add
using
online
heuristic
ad-
justment
approach
expected
informed
add
similar
learned
heuristics
models
applied
pocl
framework
selecting
suitable
partial
plan
followed
heuristic
mw-
loc
younes
simmons
2003
selecting
ad-
verse
ﬂaw
example
ofﬂine
learned
model
add
also
compare
approaches
state-space
based
1https
//helios.hud.ac.uk/scommv/ipc-14/
approaches
basis
number
problems
solved
score
obtained
plan
quality
total
exe-
cution
time
select
fast
forward
heuristic
hoff-
mann
nebel
2001
context-enhanced
additive
heuris-
tic
cea
helmert
geffner
2008
landmark-cut
heuristic
lm-cut
helmert
domshlak
2011
also
use
heuristics
together
applying
multi-
heuristic
ﬁrst
solution
strategy
mhfs
r¨oger
helmert
2010
general
strategy
performs
better
alternat-
ing
usage
different
heuristics
instead
combining
also
compare
effectiveness
techniques
lama11
richter
westphal
helmert
2011
win-
ner
ipc-2011
sequential
satisﬁcing
track
lama11
applies
lm-count
richter
helmert
westphal
2008
heuristics
together
using
multi-queue
search
set
minute
time
limit
evaluating
lama11
domains
since
internal
time
limit
minutes
invariant
synthesis
part
translator
state-based
approaches
evaluated
using
greedy
best
first
search
algorithm
fast
downward
planning
system
helmert
2006
use
eager
greedy
types
evaluations
preferred
operators
regression
models
selected
chosen
domains
trained
using
m5p
gripper-
elevator-2
rovers-3
rovers-5
logistics-2
zenotravel-3
iii
m5rule
logistics-1
table
compare
base
features
ofﬂine
learned
models
enhanced
versions
iii
state-space
based
heuristics
cea
lm-cut
strate-
gies
used
mhfs
lama11
basis
num-
ber
problems
solved
table
regpocl
solves
equal
number
problems
lama11
gripper-1
rovers-3
elevator-2
logistics-2
using
learned
heuristics
enhanced
versions
base
features
per-
formed
well
domains
consistent
over-
rovers-5
approaches
solved
problem
less
lama11
beat
state-space
based
competi-
tors
comprehensively
also
learned
model
per-
formed
better
base
features
logistics-2
competitive
lama11
solve
least
problems
good
heuristics
like
cea
lm-cut
zenotravel-3
regpocl
solved
problems
ap-
plying
approaches
loses
state-based
competi-
tors
second
approach
improves
performance
domain
gripper-1
rovers-3
rovers-5
logistics-1
elevator-2
logistics-2
zenotravel-3
time
score
hadd
16.0
17.3
25.7
hadd
24.3
142.8
33.8
4.6
204.4
state-of-the-art
hadd
14.9
16.2
26.3
hadd
0.8
11.8
10.7
3.7
76.5
add
0.7
18.1
28.0
add
31.2
144.9
34.9
8.5
197.7
pocl
heuristics
via
learning
approaches
add
0.7
16.9
30.2
add
26.1
49.7
30.7
8.8
148.6
add
20.0
17.8
+33.1
add
+31.2
142.7
38.1
+13.5
+272.9
add
20.0
+18.6
+36.3
add
+31.2
+144.9
+36.3
+13.7
+260.8
add
20.0
17.8
30.1
add
20.9
142.7
38.1
+13.5
+258.6
add
20.0
+18.6
+33.6
add
32.0
+144.9
+36.3
+13.7
+264.3
15.45
17.1
20.8
23.7
117.1
33.72
17.3
255.6
evaluation
using
fast
downward
lazy
eager
15.5
12.5
13.5
cea
lm-cut
mhfs
14.9
15.5
18.8
15.9
23.4
26.0
cea
lm-cut
mhfs
27.4
30.3
116.9
112.0
29.5
33.3
18.6
17.7
280.0
233.8
19.7
117.1
32.4
14.7
228.0
15.5
18.1
22.7
27.0
144.1
34.1
18.1
259.6
15.5
13.9
15.7
cea
lm-cut
mhfs
14.9
15.5
19.0
16.9
27.3
28.3
cea
lm-cut
mhfs
24.6
30.8
141.0
136.0
35.6
18.8
14.3
271.3
222.5
15.1
144.2
33.7
223.0
lama
20.0
19.8
39.8
30.8
148.2
37.8
19.2
137
table
scores
plan
quality
overall
time
compare
state-of-the-art
pocl
heuristics
learned
ones
effec-
tiveness
pocl
heuristics
compared
latest
state
based
approaches
last
row
demonstrates
overall
time
score
heuristic
numerical
representations
column
details
similar
table
domain
gripper-1
rovers-3
rovers-5
logistics-1
elevator-2
logistics-2
zenotravel-3
hadd
7.0
8.6
8.2
hadd
5.1
41.4
24.5
0.0
state-of-the-art
hadd
14.0
8.3
14.7
hadd
0.6
8.2
6.5
1.0
add
0.0
12.6
13.5
add
16.5
58.9
23.4
2.4
via
learning
approaches
add
+18.0
13.3
31.4
add
29.9
145.0
+33.4
7.4
add
19.7
13.6
31.4
add
29.9
+135.0
35.3
11.07
add
+18.0
13.3
24.7
add
20.7
145.0
+33.4
7.4
add
19.7
13.6
31.4
add
29.9
+135.0
35.3
11.07
add
0.0
19.0
30.9
add
21.6
39.7
26.7
7.7
table
results
reﬁnement
score
number
nodes
vis-
ited
regpocl
using
heuristic
compare
state-
of-the-art
heuristics
learned
models
enhancements
best
results
bold
domain
gripper-1
rovers-3
rovers-5
logistics-1
elevator-2
logistics-2
zenotravel-3
hadd
16.0
17.9
26.1
hadd
21.8
147.1
26.3
3.7
hadd
9.8
15.5
24.2
hadd
0.9
11.7
7.0
2.7
add
0.5
17.8
28.4
add
25.2
149.1
33.0
7.5
add
0.5
15.8
26.5
add
20.8
50.5
19.3
7.7
add
20.0
17.4
30.5
add
+30.4
149.1
+33.5
+12.1
add
20.0
17.7
+29.8
add
+30.4
146.8
35.4
13.5
add
20.0
17.4
+27.6
add
17.6
149.1
+33.4
+12.1
add
20.0
17.7
30.5
add
30.5
146.8
35.4
13.5
table
makespan
quality
score
heuristics
columns
identical
corresponding
ones
table
learned
models
rovers-5
solving
problems
logistics-1
solves
problems
approach
could
increase
coverage
domains
lama11
wins
basis
total
number
problems
solved
domain
table
compare
score
obtained
plan
qual-
ity
base
features
learned
models
corresponding
enhancements
state-space
based
heuris-
tics
techniques
lama11
anytime
planner
gives
solution
close
optimal
takes
time
compute
shorter
plans
gripper-1
logistics-2
reg-
pocl
using
learned
heuristics
solves
equal
number
problems
ﬁnds
shorter
plans
compared
lama11
logistics-1
regpocl
lost
lama11
problems
number
problems
solved
obtained
higher
score
problems
solved
produces
shorter
plans
using
add
effectiveness
wins
domain
best
plan
quality
add
also
in-
creases
plan
quality
score
20.9
obtained
add
add
add
add
add
32.0
using
online
error
correcting
strategy
however
logistics-2
performance
decreased
af-
ter
enhancements
add
general
performance
regpocl
using
either
ofﬂine
learned
models
enhancements
often
better
using
base
features
cases
online
error
adjustment
approach
enhanced
performance
learned
models
last
row
table
gives
score
obtained
total
time
taken
process
planner
takes
less
second
solving
problem
gets
full
score
total
time
score
winner
cea
lazy
evaluation
learned
models
enhanced
versions
obtained
better
scores
competitors
except
cea
scores
close
winning
score
almost
twice
lama11
table
compare
score
obtained
number
nodes
regpocl
visits
solving
planning
problems
obtained
base
features
learned
heuristics
enhanced
versions
models
obtained
of-
ﬂine
learning
informed
toward
goals
reﬁnes
fewer
partial
plans
score
obtained
learned
mod-
els
increased
good
factor
zenotravel-3
us-
ing
error
correcting
approach
elevator-2
error
correcting
approach
shown
negative
effect
continues
table
table
demonstrate
score
obtained
makespan
quality
higher
score
sig-
niﬁes
smaller
makespan
ﬂexibility
gener-
ated
solution
plan
elevator-2
rovers-5
scores
add
add
decreased
due
negative
effects
error
adjustment
approach
score
obtained
add
almost
1.5
times
score
add
logistics-1
general
ofﬂine
learned
models
generated
ﬂexible
plans
shorter
makespan
base
features
qualities
improved
using
enhanced
ver-
sions
models
discussion
already
discussed
advantages
approaches
also
limitations
ofﬂine
approach
bound
poor
generalization
learning
heuristics
current
literature
supports
idea
selecting
large
feature
set
accurate
learning
roberts
2008
accuracy
also
improved
using
empirical
performance
model
components
portfolio
de-
cide
component
pick
next
fawcett
2014
work
large
feature
set
may
drawbacks
example
computing
features
reﬁnement
step
planning
process
computationally
expensive
online
error
adjustment
approach
could
also
perform
poorly
certain
domains
figure
orientation
objects
domain
πi+1
larger
may
accurate
inaccuracy
compounded
condition
holds
beginning
planning
process
results
inaccurate
avg
value
leading
wrong
selection
partial
plan
reﬁne
next
consequently
planner
may
end
ﬁnding
longer
less
ﬂexible
plans
another
limitation
reﬁne-
ment
may
change
existing
priorities
partial
plans
set
due
single-step-error
adjustment
considering
time
factor
avoid
changing
decided
priorities
partial
plans
may
also
lead
inaccuracy
avg
approaches
utilize
advantage
strategies
like
alternation
queue
candidate
selection
using
con-
cept
pareto
optimality
r¨oger
helmert
2010
re-
cently
planning
community
tried
coming
effective
portfolios
heuristics
planners
techniques
generating
good
portfolios
new
theoretical
ma-
chine
learning
follow
work
done
past
combin-
ing
multiple
heuristics
online
streeter
golovin
smith
2007
one
could
form
portfolio
different
algorithms
reduce
total
makespan
set
jobs
solve
streeter
smith
2008
authors
provide
bound
perfor-
mance
portfolio
approaches
example
execu-
tion
greedy
schedule
algorithms
exceed
four
times
optimal
schedule
planning
sequential
portfolio
planners
heuris-
tics
aims
optimize
performance
metrics
general
conﬁgurations
automatically
generate
sequential
order-
ings
best
planning
algorithms
portfolio
par-
ticipants
allotted
timestamp
participate
solv-
ing
problems
ordering
similar
approach
used
seipp
2015
authors
outline
procedure
optimal
satisﬁcing
planning
procedure
used
work
starts
set
planning
algorithms
time
bound
uses
another
procedure
optimize
focuses
marginal
improvements
performance
quality
portfolio
bounded
1/e
opt
running
time
exceed
opt
components
allowed
act
round-robin
fashion
gerevini
saetti
vallati
2014
state-of-the-art
planners
exhibit
variations
runtime
given
problem
instance
planner
always
dominates
others
good
approach
would
select
planner
given
instance
looking
processing
time
done
building
empirical
performance
model
epm
planner
epm
derived
sets
planning
problems
performance
observation
predicts
whether
planner
could
solve
given
instance
fawcett
2014
authors
consider
large
set
instance
fea-
tures
show
runtime
predictor
often
superior
individual
planners
performance
wise
sorting
com-
ponents
portfolio
also
possible
n´u˜nez
borrajo
l´opez
2015
portfolio
sorted
probabil-
ity
performance
portfolio
maximum
time
experiments
show
performance
greedy
strat-
egy
enhanced
near
optimal
time
last
two
paragraphs
cover
recent
literature
brief
explain
previous
strategies
combining
different
base
methods
literature
shows
performed
well
different
benchmarks
current
settings
capture
ideas
combining
different
components
heuristics
direct
comparison
mentioned
works
therefore
scope
current
work
concerned
work-
ing
unit
cost
based
pocl
heuristics
isolation
hand
suspect
many
strategies
adapted
form
would
likely
beneﬁcial
pocl
framework
summary
future
work
demonstrate
use
different
regression
models
combine
different
heuristic
values
arrive
consistently
better
estimates
range
planning
domains
prob-
lems
extend
recent
attempts
learn
combinations
heuristic
functions
state-space
based
planning
pocl
planning
also
show
learned
models
fur-
ther
enhanced
online
error
correction
approach
future
intend
explore
online
learning
continue
experiments
combining
heuristic
func-
tions
also
aim
explore
use
optimizing
planner
tandem
bootstrapping
methods
apart
giving
complete
generalization
current
learning
approaches
temporal
planning
plan-
ning
deadlines
references
arfaee
zilles
holte
2011
arfaee
zilles
holte
2011.
learning
heuristic
functions
large
state
spaces
artiﬁcial
intelligence
175
16-17
:2075–2098
benton
coles
coles
2012
benton
coles
coles
2012.
temporal
planning
preferences
time-dependent
continuous
costs
proc
icaps
2012
bishop
2006
bishop
2006.
pattern
recognition
machine
learning
volume
springer
aug.
2006
coles
2010
coles
coles
fox
long
2010.
forward-chaining
partial-order
planning
proc
icaps
domshlak
karpas
markovitch
2010
domshlak
karpas
markovitch
2010.
max
max
online
learning
speeding
optimal
planning
proc
aaai
2010
1071–1076
fawcett
2014
fawcett
vallati
hutter
hoffmann
hoos
leyton-brown
2014.
improved
features
runtime
prediction
domain-
independent
planners
proc
icaps
2014
355–359
gerevini
saetti
vallati
2014
gerevini
saetti
vallati
2014.
planning
automatic
portfo-
lio
conﬁguration
pbp
approach
jair
2014
639–696
hall
2009
hall
frank
holmes
pfahringer
reutemann
witten
2009.
weka
data
mining
software
update
sigkdd
explorations
2009
volume
hall
2000
hall
2000.
correlation-based
feature
se-
lection
discrete
numeric
class
machine
learning
proc
icml
haslum
geffner
2000
haslum
geffner
2000.
admissible
heuristics
optimal
planning
proc
aips
2000
140–149
haslum
2012
haslum
2012.
incremental
lower
bounds
additive
cost
planning
problems
proc
icaps
2012
74–82
helmert
domshlak
2011
helmert
domshlak
2011.
lm-cut
optimal
planning
landmark-cut
heuristic
seventh
ipc
helmert
geffner
2008
helmert
geffner
2008.
unifying
causal
graph
additive
heuristics
proc
icaps
2008
helmert
r¨oger
karpas
2011
helmert
r¨oger
karpas
2011.
fast
downward
stone
soup
baseline
building
planner
portfolios
proc
icaps
pal
2011
28–35
citeseer
helmert
2006
helmert
2006.
fast
downward
plan-
ning
system
artif
intell
res
jair
2006
26:191–246
hoffmann
nebel
2001
hoffmann
nebel
2001.
planning
system
fast
plan
generation
heuristic
search
jair
2001
kvarnstr¨om
2011
kvarnstr¨om
planning
loosely
coupled
agents
using
partial
order
forward-chaining
proc
icaps
2011
nguyen
kambhampati
2001
nguyen
kamb-
hampati
2001.
reviving
partial
order
planning
proc
ijcai
2001
459–466
n´u˜nez
borrajo
l´opez
2015
n´u˜nez
borrajo
l´opez
2015.
sorting
sequential
portfolios
au-
tomated
planning
proc
ijcai
2015
1638–1644
aaai
press
quinlan
others
1992
quinlan
1992.
learn-
ing
continuous
classes
proc
australian
joint
con-
ference
artiﬁcial
intelligence
volume
343–348
richter
westphal
2010
richter
westphal
2010.
lama
planner
guiding
cost-based
anytime
planning
landmarks
jair
2010
39:127–177
richter
helmert
westphal
2008
richter
helmert
westphal
2008.
landmarks
revisited
proc
aaai
2008
volume
975–982
richter
westphal
helmert
2011
richter
west-
phal
helmert
2011.
lama
2008
2011.
seventh
ipc
2011
117–124
roberts
2008
roberts
howe
wilson
desjardins
2008.
makes
planners
predictable
proc
icaps
2008
288–295
aaai
r¨oger
helmert
2010
r¨oger
helmert
2010
2011.
merrier
combining
heuristic
estimators
satisﬁcing
planning
proc
icaps
2010
246–249
r¨oger
pommerening
seipp
2014
r¨oger
pom-
merening
seipp
2014.
fast
downward
stone
soup
2014.
eighth
ipc
2014
rousseeuw
leroy
2005
rousseeuw
leroy
2005.
robust
regression
outlier
detection
volume
589.
john
wiley
sons
samadi
felner
schaeffer
2008
samadi
felner
schaeffer
2008.
learning
multiple
heuris-
tics
proc
aaai
2008
357–362
sapena
onaindıa
torreno
2014
sapena
on-
aindıa
torreno
2014.
combining
heuristics
proc
icaps
accelerate
forward
partial-order
planning
coplas
2014
seipp
2015
seipp
sievers
helmert
hutter
2015.
automatic
conﬁguration
sequential
plan-
ning
portfolios
proc
aaai
2015
3364–3370
shekhar
khemani
2015
shekhar
khemani
2015.
improving
heuristics
on-the-ﬂy
effective
search
plan
space
2015
advances
artiﬁcial
intelligence
38th
annual
german
conference
302–308
streeter
smith
2008
streeter
smith
2008.
new
techniques
algorithm
portfolio
design
proc
uai
2008
519–527
streeter
golovin
smith
2007
streeter
golovin
smith
2007.
combining
multiple
heuristics
online
proc
aaai
2007
sutton
1988
sutton
1988.
learning
predict
methods
temporal
differences
machine
learning
3:9–
44.
dionne
thayer
dionne
ruml
2011
thayer
ruml
2011.
learning
inadmissible
heuris-
tics
search
proc
icaps
2011
virseda
borrajo
alc´azar
2013
virseda
borrajo
alc´azar
2013.
learning
heuristic
functions
cost-based
planning
proc
icaps
pal
2013.
citeseer
younes
simmons
2003
younes
sim-
mons
2003.
vhpop
versatile
heuristic
partial
order
planner
jair
2003
20:405–430
appendix
proof
theorem
theorem
given
learned
predictive
model
partial
plan
figure
leads
solution
plan
πsol
certain
reﬁnement
steps
enhanced
version
predictive
model
cid:48
cid:88
cid:48
cid:32
πsol
cid:32
πsol
path
figure
captures
partial
plan
cid:48
along
path
πsol
path
includes
excludes
πsol
term
single-
step-error
associated
reﬁnement
proof
use
principle
mathematical
induction
prove
theorem
basis
assume
needs
one
reﬁnement
become
πsol
would
also
best
child
best
child
always
keeps
lowest
estimate
requirement
new
actions
reﬁnement
among
siblings
one
ri−−→
πsol
using
possible
reﬁnement
say
term
single-step-error
associated
estimates
total
effort
required
reﬁne
com-
pletely
unit
cost
reﬁnements
cost
computed
cost
πi+1
partial
plan
πi+1
also
πsol
therefore
πi+1
using
together
get
therefore
base
step
holds
base
case
assume
reﬁnement
step
best
child
unsupported
causal
link
present
πi+1
threat
resolved
planner
immediately
make
solution
plan
unsupported
causal
link
must
existing
action
πi+1
support
case
estimate
requirement
new
actions
still
hypothesis
select
arbitrary
partial
plan
πi+1
assume
holds
proof
step
show
holds
cost
πi+1
cost
πi+1
cid:88
cid:48
cid:48
induction
hypothesis
cid:48
πi+1
cid:32
πsol
cid:88
cid:88
cid:48
πi+1
cid:32
πsol
cid:48
cid:48
cid:32
πsol
therefore
relationship
holds
parent
partial
plan
well
thus
induction
partial
plans
cid:4
assumption
correct
