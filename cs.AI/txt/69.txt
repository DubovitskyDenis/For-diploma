workshop
track
iclr
2016
minimalistic
approach
sum-product
net-
work
learning
real
applications
viktoriya
krakovna
department
statistics
harvard
university
vkrakovna
fas.harvard.edu
moshe
looks
google
madscience
google.com
abstract
sum-product
networks
spns
class
expressive
yet
tractable
hierarchical
graphical
models
learnspn
structure
learning
algorithm
spns
uses
hierarchical
co-clustering
simultaneously
identifying
similar
entities
simi-
lar
features
original
learnspn
algorithm
assumes
variables
discrete
missing
data
introduce
practical
simpliﬁed
version
learnspn
minispn
runs
faster
handle
missing
data
hetero-
geneous
features
common
real
applications
demonstrate
performance
minispn
standard
benchmark
datasets
two
datasets
google
knowledge
graph
exhibiting
high
missingness
rates
mix
discrete
continuous
features
introduction
sum-product
network
spn
poon
domingos
2011
tractable
interpretable
deep
model
advantage
spns
graphical
models
bayesian
networks
allow
efﬁcient
exact
inference
linear
time
network
size
spn
represents
multivari-
ate
probability
distribution
directed
acyclic
graph
consisting
sum
nodes
clusters
instances
product
nodes
partitions
features
leaf
nodes
univariate
distributions
features
shown
figure
standard
algorithms
learning
spn
structure
assume
discrete
data
missingness
mostly
test
set
benchmark
data
sets
satisfy
criteria
rooshenas
lowd
2014
reasonable
assumption
dealing
messy
data
sets
real
applications
google
knowledge
graph
semantic
network
facts
based
freebase
bollacker
al.
2008
used
generate
knowledge
panels
google
search
data
quite
heterogeneous
lot
missing
since
much
known
entities
graph
others
high
missingness
rates
also
worsen
impact
discretizing
continuous
variables
structure
learning
results
losing
already
scarce
covariance
information
applications
like
common
need
spn
learning
algorithm
handle
kind
data
paper
present
minispn
simpliﬁcation
state-of-the-art
spn
learning
algorithm
improves
applicability
performance
speed
demonstrate
performance
minispn
data
standard
benchmark
data
sets
variation
learnspn
algorithm
learnspn
gens
domingos
2013
greedy
algorithm
performs
co-clustering
recur-
sively
partitioning
variables
approximately
independent
sets
partitioning
training
data
clusters
similar
instances
shown
figure
variable
instance
partitioning
steps
applied
data
slices
subsets
instances
variables
produced
previous
steps
variable
partition
step
uses
pairwise
independence
tests
variables
approximately
independent
sets
connected
components
resulting
dependency
graph
instance
clustering
step
uses
naive
bayes
mixture
model
clusters
variables
cluster
assumed
independent
clusters
learned
using
hard
restarts
avoiding
overﬁtting
using
exponential
prior
number
clusters
splitting
process
continues
data
workshop
track
iclr
2016
figure
example
spn
structure
ﬁgure
zhao
2015
figure
recursive
partitioning
process
learnspn
algorithm
ﬁgure
gens
domingos
2013
slice
instances
test
independence
point
variables
slice
considered
independent
end
result
tree-structured
spn
standard
learnspn
algorithm
assumes
variables
discrete
missing
data
hyperparameter
values
cluster
penalty
independence
test
critical
value
determined
using
grid
search
clustering
step
seems
unnecessarily
complex
involving
penalty
prior
restarts
hyperparameter
tuning
far
complicated
part
algorithm
way
seems
difﬁcult
justify
likely
time-consuming
due
restarts
hyperparameter
tuning
propose
variation
learnspn
called
minispn
handles
missing
data
performs
lazy
discretization
continuous
data
variable
partition
step
simpliﬁes
model
instance
clustering
step
require
hyperparameter
search
simplify
naive
bayes
mixture
model
instance
clustering
step
attempting
split
two
clusters
given
point
eliminating
cluster
penalty
prior
results
greedy
approach
learnspn
require
restarts
hyperparameter
tuning
seems
like
natural
choice
simpliﬁcation
extension
greedy
approach
used
top
level
learnspn
algorithm
compare
partition
univariate
leaves
mixture
two
partitions
univariate
leaves
generated
using
hard
split
succeeds
two-cluster
version
higher
validation
set
likelihood
split
succeeds
apply
two
resulting
data
slices
move
variable
partition
step
clustering
step
fails
greedy
approach
similar
one
used
spn-b
method
vergari
al.
2015
however
alternates
variable
instance
splits
default
thus
building
even
deeper
spns
variable
partition
step
perform
independence
test
using
subset
rows
variables
missing
conclude
independence
number
rows
threshold
apply
binary
binning
continuous
variable
using
median
data
slice
cutoff
compare
pareto
algorithm
previously
used
learning
spn
models
inspired
work
grosse
2012
produces
pareto-optimal
set
models
trading
degrees
freedom
validation
set
log
likelihood
score
iteration
production
rules
randomly
applied
add
partition
mixture
splits
models
current
model
set
new
models
added
model
set
model
model
set
lower
degrees
freedom
higher
log
likelihood
score
another
model
inferior
model
removed
set
algorithm
returns
model
pareto
model
set
highest
validation
log
likelihood
also
compare
hybrid
method
pareto
algorithm
initialized
minispn
summary
experiments
use
two
data
sets
knowledge
graph
people
collection
professions
data
set
variables
boolean
indicators
whether
person
belongs
particular
profession
boolean
variables
continuous
variables
dates
data
set
continuous
variables
representing
dates
life
events
person
spouse
workshop
track
iclr
2016
table
average
log
likelihood
runtime
comparison
data
sets
best
performing
methods
shown
bold
test
set
log
likelihood
runtime
seconds
pareto
hybrid
minispn
pareto
hybrid
minispn
data
set
professions-10k
-10.2
professions-100k
-6.61
-8.66
dates-10k
dates-100k
-17.1
-6.09
-6.44
-8.68
-16.5
-6.2
-6.53
-8.53
-16.7
0.4
7.2
0.26
5.4
3.7
131
2.4
566
5.3
1.7
table
average
log
likelihood
runtime
comparison
literature
data
sets
best
performing
methods
shown
bold
data
set
nltcs
msnbc
kddcup
plants
audio
jester
netﬂix
accidents
retail
pumsb-star
dna
kosarek
msweb
book
eachmovie
webkb
reuters-52
newsgroup
bbc
test
set
log
likelihood
runtime
seconds
pareto
hybrid
minispn
learnspn
pareto
hybrid
minispn
learnspn
-6.33
-6.54
-2.17
-17.3
-41.9
-54.6
-59.5
-40.4
-11.1
-40.8
-98.1
-11.2
-10.7
-35.1
-55
-161
-92
-156
-258
-52.3
-6.1
-6.11
-2.21
-13
-40.5
-53.4
-57.3
-30.3
-11.09
-25
-89
-11
-10.26
-36.4
-52.5
-162
-86.5
-160.5
-250
-22
2400
400
160
955
1190
1230
330
100
350
300
200
260
350
220
900
2900
28000
900
300
-6.03
-6.4
-2.13
-13.1
-39.9
-52.9
-56.7
-32.5
-11
-28.4
-91.5
-10.8
-9.94
-34.7
-52.3
-155
-85.2
-152
-250
-49.5
-6.12
-6.61
-2.14
-13.2
-40
-53
-56.8
-32.6
-11.1
-28.3
-93.9
-10.9
-10.1
-34.7
-52.2
-155
-84.7
-152
-249
-49.2
212
2080
780
556
193
766
1140
1100
537
572
181
218
169
656
1190
123
1.4
5.6
6.7
7.3
139
4.8
152
6.3
181
around
data
missing
use
subsets
10000
100000
instances
data
sets
randomly
split
data
sets
training
test
set
data
sets
compare
minispn
pareto
hybrid
algorithms
able
apply
standard
learnspn
algorithm
data
sets
since
contain
missing
data
table
shows
log
likelihood
performance
test
set
runtime
performance
minispn
better
pareto
terms
log
likelihood
runtime
hybrid
performs
comparably
minispn
usually
slowest
three
use
benchmark
data
sets
literature
exactly
ones
used
learnspn
paper
gens
domingos
2013
compare
performance
minispn
standard
learn-
spn
algorithm
particularly
interested
effect
minispn
simple
two-cluster
instance
split
relative
complex
instance
split
exponential
prior
restarts
used
standard
learnspn
table
shows
log
likelihood
performance
test
set
runtime
per-
formance
like
data
ﬁnd
minispn
uniformly
outperforms
pareto
performs
similarly
hybrid
learnspn
runs
much
faster
time-intensive
data
set
news-
group
minispn
takes
minutes
learnspn
takes
hours
conclusion
sum-product
networks
receiving
increasing
attention
researchers
due
expres-
siveness
efﬁcient
inference
interpretability
many
learning
algorithms
developed
past
years
recent
developments
mostly
focused
improving
performance
benchmark
data
sets
variation
classical
learning
algorithm
simple
yet
large
impact
usability
improving
speed
making
possible
apply
messy
real
data
sets
workshop
track
iclr
2016
references
kurt
bollacker
colin
evans
praveen
paritosh
tim
sturge
jamie
taylor
freebase
collab-
oratively
created
graph
database
structuring
human
knowledge
proceedings
2008
acm
sigmod
international
conference
management
data
1247–1250
acm
2008.
robert
gens
pedro
domingos
learning
structure
sum-product
networks
pro-
ceedings
30th
international
conference
machine
learning
icml
2013
atlanta
usa
16-21
june
2013
873–880
2013.
roger
grosse
ruslan
salakhutdinov
william
freeman
joshua
tenenbaum
exploit-
proceedings
28th
ing
compositionality
explore
large
space
model
structures
conference
uncertainty
uai
2012.
hoifung
poon
pedro
domingos
sum-product
networks
new
deep
architecture
uai
2011
proceedings
twenty-seventh
conference
uncertainty
artiﬁcial
intelligence
barcelona
spain
july
14-17
2011
337–346
2011.
amirmohammad
rooshenas
daniel
lowd
learning
sum-product
networks
direct
indirect
variable
interactions
tony
jebara
eric
xing
eds
proceedings
31st
international
conference
machine
learning
icml-14
710–718
jmlr
workshop
conference
proceedings
2014.
antonio
vergari
nicola
mauro
floriana
esposito
simplifying
regularizing
strength-
ening
sum-product
network
structure
learning
machine
learning
knowledge
discov-
ery
databases
european
conference
ecml
pkdd
2015
porto
portugal
september
7-11
2015
proceedings
part
343–358
2015.
han
zhao
mazen
melibari
pascal
poupart
relationship
sum-product
net-
works
bayesian
networks
proceedings
32nd
international
conference
machine
learning
icml
2015
lille
france
6-11
july
2015
116–124
2015
