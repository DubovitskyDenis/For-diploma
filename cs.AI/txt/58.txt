learning
communicate
solve
riddles
deep
distributed
recurrent
q-networks
jakob
foerster1
yannis
assael1
nando
freitas1,2,3
shimon
whiteson1
1university
oxford
united
kingdom
2canadian
institute
advanced
research
cifar
ncap
program
3google
deepmind
jakob.foerster
cs.ox.ac.uk
yannis.assael
cs.ox.ac.uk
nandodefreitas
google.com
shimon.whiteson
cs.ox.ac.uk
abstract
propose
deep
distributed
recurrent
networks
ddrqn
enable
teams
agents
learn
solve
communication-based
co-
ordination
tasks
tasks
agents
given
pre-designed
communication
pro-
tocol
therefore
order
successfully
com-
municate
must
ﬁrst
automatically
develop
agree
upon
communication
proto-
col.
present
empirical
results
two
multi-
agent
learning
problems
based
well-known
riddles
demonstrating
ddrqn
success-
fully
solve
tasks
discover
elegant
com-
munication
protocols
knowl-
edge
ﬁrst
time
deep
reinforcement
learning
succeeded
learning
communica-
tion
protocols
addition
present
ablation
experiments
conﬁrm
main
components
ddrqn
architecture
crit-
ical
success
introduction
recent
years
advances
deep
learning
in-
strumental
solving
number
challenging
reinforce-
ment
learning
problems
including
high-dimensional
robot
control
levine
al.
2015
assael
al.
2015
wat-
ter
al.
2015
visual
attention
al.
2015
atari
learning
environment
ale
guo
al.
2014
mnih
al.
2015
stadie
al.
2015
wang
al.
2015
schaul
al.
2016
van
hasselt
al.
2016
al.
2015
belle-
mare
al.
2016
nair
al.
2015
above-mentioned
problems
involve
single
learning
agent
however
recent
work
begun
address
multi-agent
deep
competitive
settings
deep
learn-
†these
authors
contributed
equally
work
ing
maddison
al.
2015
silver
al.
2016
recently
shown
success
cooperative
settings
tampuu
2015
adapted
deep
q-networks
mnih
al.
2015
allow
two
agents
tackle
multi-agent
exten-
sion
ale
approach
based
independent
learning
shoham
al.
2007
shoham
leyton-brown
2009
zawadzki
al.
2014
agents
learn
q-functions
independently
parallel
however
approaches
assume
agent
fully
observe
state
environment
dqn
also
extended
address
partial
observability
hausknecht
stone
2015
single-agent
settings
considered
knowledge
work
deep
reinforcement
learning
yet
considered
settings
partially
observable
multi-agent
problems
challenging
important
cooperative
case
multiple
agents
must
coordinate
be-
haviour
maximise
common
payoff
fac-
ing
uncertainty
hidden
state
envi-
ronment
teammates
observed
thus
act
problems
arise
naturally
variety
settings
multi-robot
systems
sensor
networks
matari
1997
fox
al.
2000
gerkey
matari
2004
olfati-saber
al.
2007
cao
al.
2013
paper
propose
deep
distributed
recurrent
networks
ddrqn
enable
teams
agents
learn
ef-
fectively
coordinated
policies
challenging
prob-
lems
show
naive
approach
simply
train-
ing
independent
dqn
agents
long
short-term
memory
lstm
networks
hochreiter
schmidhuber
1997
in-
adequate
multi-agent
partially
observable
problems
therefore
introduce
three
modiﬁcations
key
ddrqn
success
last-action
inputs
supplying
agent
previous
action
input
next
time
step
agents
approximate
action-observation
his-
tories
inter-agent
weight
sharing
single
network
learning
communicate
solve
riddles
deep
distributed
recurrent
q-networks
weights
used
agents
network
conditions
agent
unique
enable
fast
learning
also
allowing
diverse
behaviour
disabling
experience
replay
poorly
suited
non-stationarity
aris-
ing
multiple
agents
learning
simultaneously
evaluate
ddrqn
propose
two
multi-agent
rein-
forcement
learning
problems
based
well-known
riddles
hats
riddle
prisoners
line
must
determine
hat
colours
switch
riddle
prisoners
must
determine
vis-
ited
room
containing
single
switch
riddles
used
interview
questions
companies
like
google
goldman
sachs
environments
require
convolutional
net-
works
perception
presence
partial
observabil-
ity
means
require
recurrent
networks
deal
complex
sequences
single-agent
works
hausknecht
stone
2015
al.
2015
language-
based
narasimhan
al.
2015
tasks
addition
partial
observability
coupled
multiple
agents
op-
timal
policies
critically
rely
communication
agents
since
communication
protocol
given
pri-
ori
reinforcement
learning
must
automatically
develop
coordinated
communication
protocol
results
demonstrate
ddrqn
successfully
solve
tasks
outperforming
baseline
methods
dis-
covering
elegant
communication
protocols
along
way
knowledge
ﬁrst
time
deep
reinforcement
learning
succeeded
learning
communication
proto-
cols
addition
present
ablation
experiments
con-
ﬁrm
main
components
ddrqn
ar-
chitecture
critical
success
background
section
brieﬂy
introduce
dqn
multi-
agent
recurrent
extensions
2.1.
deep
q-networks
single-agent
fully-observable
reinforcement
learning
setting
sutton
barto
1998
agent
observes
cur-
rent
state
discrete
time
step
chooses
action
according
potentially
stochastic
policy
observes
reward
signal
transitions
new
state
st+1
objective
maximize
expectation
discounted
return
γrt+1
γ2rt+2
···
discount
factor
q-function
policy
rt|st
optimal
maxπ
obeys
bellman
optimality
equation
action-value
function
cid:105
cid:48
cid:48
cid:48
cid:48
max
cid:104
deep
q-networks
mnih
al.
2015
dqns
use
neural
networks
parameterised
represent
dqns
optimised
minimising
following
loss
function
iteration
cid:20
cid:16
cid:17
cid:21
ydqn
cid:48
target
ydqn
max
cid:48
cid:48
cid:48
weights
target
network
frozen
number
iterations
updating
online
net-
work
gradient
descent
dqn
uses
experience
replay
lin
1993
mnih
al.
2015
learning
agent
builds
dataset
experiences
st+1
across
episodes
q-network
trained
sampling
mini-batches
experiences
uniformly
random
experience
replay
helps
prevent
divergence
breaking
correlations
among
samples
also
enables
reuse
past
experiences
learning
thereby
reducing
sample
costs
2.2.
independent
dqn
dqn
extended
cooperative
multi-agent
set-
tings
agent
observes
global
se-
receives
team
reward
lects
individual
action
shared
among
agents
tampuu
2015
ad-
dress
setting
framework
combines
dqn
independent
q-learning
applied
two-player
pong
agents
independently
simultaneously
learn
q-functions
indepen-
dent
q-learning
principle
lead
convergence
prob-
lems
since
one
agent
learning
makes
environment
ap-
pear
non-stationary
agents
strong
empir-
ical
track
record
shoham
al.
2007
shoham
leyton-
brown
2009
zawadzki
al.
2014
2.3.
deep
recurrent
q-networks
dqn
independent
dqn
assume
full
observabil-
ity
i.e.
agent
receives
input
contrast
par-
tially
observable
environments
hidden
instead
agent
receives
observation
correlated
general
disambiguate
hausknecht
stone
2015
propose
deep
recurrent
network
drqn
architecture
address
single-agent
par-
tially
observable
settings
instead
approximating
learning
communicate
solve
riddles
deep
distributed
recurrent
q-networks
feed-forward
network
approximate
recurrent
neural
network
maintain
in-
ternal
state
aggregate
observations
time
modelled
adding
extra
input
ht−1
represents
hidden
state
network
yielding
ht−1
thus
drqn
outputs
time
step
drqn
tested
partially
observable
version
ale
portion
input
screens
blanked
2.4.
partially
observable
multi-agent
work
consider
settings
mul-
tiple
agents
partial
observability
agent
receives
private
time
step
maintains
inter-
nal
state
however
assume
learning
occur
centralised
fashion
i.e.
agents
share
parameters
etc.
learning
long
policies
learn
con-
dition
private
histories
words
consider
centralised
learning
decentralised
policies
interested
settings
multiple
agents
partial
observability
coexist
agents
incentive
communicate
communi-
cation
protocol
given
priori
agents
must
ﬁrst
au-
tomatically
develop
agree
upon
protocol
knowledge
work
deep
considered
settings
work
demonstrated
deep
successfully
learn
communication
protocols
ddrqn
straightforward
approach
deep
par-
tially
observable
multi-agent
settings
simply
combine
drqn
independent
q-learning
case
agent
q-network
represents
t−1
conditions
agent
individual
hidden
state
well
observation
approach
call
naive
method
performs
poorly
show
section
instead
propose
deep
distributed
recurrent
q-networks
ddrqn
makes
three
key
modiﬁcations
naive
method
ﬁrst
last-action
input
involves
pro-
viding
agent
previous
action
input
next
time
step
since
agents
employ
stochastic
policies
sake
exploration
general
condi-
tion
actions
action-observation
histories
observation
histories
feeding
last
action
input
allows
rnn
approximate
action-observation
histories
second
inter-agent
weight
sharing
involves
tying
weights
agents
networks
effect
one
network
learned
used
agents
however
agents
still
behave
differently
receive
different
ob-
servations
thus
evolve
different
hidden
states
ad-
dition
agent
receives
index
input
mak-
algorithm
ddrqn
initialise
episode
agent
initial
state
cid:54
terminal
agent
probability
pick
random
else
t−1
get
reward
next
state
st+1
arg
maxa
t−1
cid:46
reset
gradient
agent
cid:110
terminal
else
j+1
maxa
accumulate
gradients
j−1
θi+1
α∇θ
cid:46
update
parameters
θi+1
i+1
j−1
cid:46
update
target
network
t−1
t−1
ing
easier
agents
specialise
weight
sharing
dra-
matically
reduces
number
parameters
must
learned
greatly
speeding
learning
third
disabling
experience
replay
simply
involves
turning
feature
dqn
although
experience
re-
play
helpful
single-agent
settings
multiple
agents
learn
independently
environment
appears
non-
stationary
agent
rendering
experience
ob-
solete
possibly
misleading
given
modiﬁcations
ddrqn
learns
q-function
note
form
condition
due
weight
sharing
t−1
portion
history
action
whose
value
q-network
estimates
algorithm
describes
ddrqn
first
initialise
tar-
get
q-networks
episode
also
initialise
state
internal
state
agents
next
time
step
pick
action
agent
-greedily
w.r.t
q-function
feed
previous
t−1
agent
index
along
observa-
action
t−1
tion
agents
taken
action
query
environment
state
update
reward
information
reach
ﬁnal
time
step
terminal
state
proceed
bellman
updates
agent
time
step
calculate
target
q-value
using
observed
reward
discounted
target
network
also
accumulate
gradients
regressing
q-value
estimate
j−1
target
q-value
previous
internal
state
action
chosen
j−1
learning
communicate
solve
riddles
deep
distributed
recurrent
q-networks
lastly
conduct
two
weight
updates
ﬁrst
direc-
tion
accumulated
gradients
target
network
direction
although
riddle
single
action
observation
prob-
lem
still
partially
observable
given
none
agents
observe
colour
hat
multi-agent
riddles
section
describe
riddles
evalu-
ate
ddrqn
4.1.
hats
riddle
hats
riddle
described
follows
execu-
tioner
lines
100
prisoners
single
ﬁle
puts
red
blue
hat
prisoner
head
every
prisoner
see
hats
people
front
line
hat
anyone
behind
execu-
tioner
starts
end
back
asks
last
prisoner
colour
hat
must
answer
red
blue.
answers
correctly
allowed
live
gives
wrong
answer
killed
instantly
silently
ev-
eryone
hears
answer
one
knows
whether
answer
right
night
line-up
prisoners
confer
strategy
help
poundstone
2012
figure
illustrates
setup
4.2.
switch
riddle
switch
riddle
described
follows
one
hun-
dred
prisoners
newly
ushered
prison
warden
tells
starting
tomorrow
placed
isolated
cell
unable
communicate
amongst
day
warden
choose
one
prisoners
uniformly
random
replacement
place
central
interrogation
room
containing
light
bulb
toggle
switch
prisoner
able
observe
current
state
light
bulb
wishes
toggle
light
bulb
also
option
an-
nouncing
believes
prisoners
visited
in-
terrogation
room
point
time
announce-
ment
true
prisoners
set
free
false
prisoners
executed
warden
leaves
pris-
oners
huddle
together
discuss
fate
agree
protocol
guarantee
freedom
2002
figure
hats
prisoner
hear
answers
pre-
ceding
prisoners
left
see
colour
hats
front
right
must
guess
hat
colour
optimal
strategy
prisoners
agree
com-
munication
protocol
ﬁrst
prisoner
says
blue
number
blue
hats
even
red
otherwise
vice-versa
remaining
prisoners
deduce
hat
colour
given
hats
see
front
responses
heard
behind
thus
everyone
except
ﬁrst
prisoner
deﬁnitely
answer
correctly
formalise
hats
riddle
multi-agent
task
deﬁne
state
space
total
number
agents
blue
red
agent
hat
colour
blue
red
action
took
m-th
step
time
steps
agent
take
null
action
m-th
time
step
agent
observation
am−1
sm+1
re-
ward
zero
except
end
episode
total
number
agents
correct
action
label
relevant
observation
cid:80
action
agent
omitting
time
index
figure
switch
every
day
one
prisoner
gets
sent
inter-
rogation
room
see
switch
choose
actions
tell
none
number
strategies
song
2012
2002
analysed
inﬁnite
time-horizon
version
prob-
lem
goal
guarantee
survival
one
well-
known
strategy
one
prisoner
designated
counter
allowed
turn
switch
prisoner
turns
thus
counter
turned
switch
n−1
times
tell
formalise
switch
riddle
deﬁne
state
space
swt
irt
swt
position
switch
current
visitor
interrogation
room
tracks
agents
already
interrogation
room
time
step
agent
observes
irt
swt
irt
irt
swt
swt
agent
interrogation
room
null
otherwise
agent
interrogation
room
actions
tell
none
otherwise
action
none
episode
ends
agent
chooses
tell
maximum
time
step
reached
reward
213456
red
answers
prisoners
hats
red
observed
hats
day
13231offonoffonoffonday
2day
3day
4switch
action
onnonenonetelloffonprisoner
learning
communicate
solve
riddles
deep
distributed
recurrent
q-networks
hk−1
hk−1
lstma
outputs
added
element-wise
passed
lstm
network
subsequently
follow
similar
pro-
cedure
hats
observed
deﬁning
finally
last
values
two
lstms
lstm
networks
ym−1
used
approximate
q-values
action
mlp
128
ym−1
||yn
network
trained
mini-
batches
episodes
furthermore
use
adaptive
variant
curriculum
learning
bengio
al.
2009
pave
way
scalable
strategies
better
training
performance
sample
ex-
amples
multinomial
distribution
curricula
corresponding
different
current
bound
raised
every
time
performance
becomes
near
optimal
probability
sampling
given
inversely
proportional
performance
gap
compared
normalised
maxi-
mum
reward
performance
depicted
figure
ﬁrst
evaluate
ddrqn
compare
tabular
q-learning
tabular
q-learning
feasible
agents
since
state
space
grows
exponentially
addition
separate
tables
agent
precludes
gen-
eralising
across
agents
figure
shows
results
ddrqn
substan-
tially
outperforms
tabular
q-learning
addition
ddrqn
also
comes
near
performance
optimal
strategy
de-
scribed
section
4.1.
ﬁgure
also
shows
results
ablation
experiment
inter-agent
weight
sharing
removed
ddrqn
results
conﬁrm
inter-agent
weight
sharing
key
performance
since
agent
takes
one
action
hats
riddle
essentially
single
step
problem
therefore
last-action
figure
results
hats
riddle
agents
com-
paring
ddrqn
without
inter-agent
weight
sharing
tabular
q-table
hand-coded
optimal
strategy
lines
depict
average
runs
conﬁdence
intervals
figure
hats
agent
observes
answers
preceding
agents
hat
colour
front
variable
length
sequences
processed
rnns
first
answers
heard
passed
two
single-
mlp
mlp
outputs
layer
mlps
added
element-wise
passed
lstm
net-
hk−1
work
similarly
observed
lstma
hk−1
hats
deﬁne
yk−1
lstms
last
val-
ues
two
lstms
ym−1
used
approximate
||yn
mlp
ym−1
action
chosen
hk−1
except
unless
agent
chooses
tell
case
agents
interrogation
room
otherwise
experiments
section
evaluate
ddrqn
multi-agent
riddles
experiments
prisoners
select
actions
using
-greedy
policy
1−0.5
hats
riddle
0.05
switch
riddle
latter
discount
factor
set
0.95
target
networks
described
section
update
0.01
cases
weights
optimised
using
adam
kingma
2014
learning
rate
10−3
proposed
architectures
make
use
rectiﬁed
linear
units
lstm
cells
details
network
implementations
described
supplementary
material
source
code
published
online
5.1.
hats
riddle
figure
shows
architecture
use
apply
ddrqn
hats
riddle
select
network
fed
input
am−1
sm+1
well
answers
heard
passed
two
mlp
mlp
single-layer
mlps
qmm
na1+++m
nam-1m
n+++m
nsm+1snm
nskm
naklstm
unrolled
m-1
stepslstm
unrolled
n-m
steps…………za1zsm+1zakzskzam-1zsnha1hsm+1hakhskyam-1ysn20k40k60k80k100k
epochs0.50.60.70.80.91.0norm
optimal
ddrqn
tied
weightsddrqn
w/o
tied
weightsq-tableoptimal
learning
communicate
solve
riddles
deep
distributed
recurrent
q-networks
table
percent
agreement
hats
riddle
ddrqn
optimal
parity-encoding
strategy
agreement
100.0
100.0
79.6
52.6
50.8
52.5
inputs
disabling
experience
replay
play
role
need
ablated
consider
compo-
nents
switch
riddle
section
5.2.
compare
strategies
ddrqn
learns
optimal
strategy
computing
percentage
trials
ﬁrst
agent
correctly
encodes
parity
observed
hats
answer
table
shows
encoding
almost
perfect
agents
encode
parity
learn
different
distributed
solu-
tion
nonetheless
close
optimal
believe
qualitatively
solution
corresponds
agents
communicating
information
hats
answers
instead
ﬁrst
agent
5.2.
switch
riddle
figure
illustrates
model
architecture
used
switch
riddle
agent
modelled
recurrent
neural
network
lstm
cells
unrolled
dmax
time-steps
denotes
number
days
episode
experiments
limit
dmax
order
keep
experiments
computationally
tractable
inputs
t−1
processed
mlp
128
128
2-layer
mlp
figure
hats
using
curriculum
learning
ddrqn
achieves
good
performance
...
agents
compared
op-
timal
strategy
irm
room
last
action
agents
figure
switch
agent
receives
input
state
swt
inputs
processed
2-layer
mlp
mlp
swt
irm
ding
lstm
action-observation
history
finally
output
used
step
compute
switch
t−1
step
t−1
onehot
embed-
t−1
used
approximate
agent
lstm
passed
lstm
network
onehot
mlp
passed
lstm
network
t−1
onehot
embed-
onehot
128
ding
t−1
used
approximate
lstm
128
agent
action-observation
history
finally
output
lstm
used
step
approximate
q-values
action
using
2-layer
mlp
mlp
128
128
128
128
128
hats
riddle
curriculum
learning
used
training
figure
shows
results
shows
ddrqn
learns
optimal
policy
beating
naive
method
hand
coded
strategy
tell
last
day
veri-
ﬁes
three
modiﬁcations
ddrqn
substantially
improve
performance
task
following
paragraphs
analyse
importance
individual
modiﬁcations
analysed
strategy
ddrqn
discovered
looking
1000
sampled
episodes
figure
shows
decision
tree
constructed
samples
corre-
sponds
optimal
strategy
allowing
agents
col-
lectively
track
number
visitors
interrogation
room
prisoner
visits
interrogation
room
day
two
two
options
either
one
two
pris-
oners
may
visited
room
three
prisoners
third
prisoner
would
already
ﬁnished
game
two
remaining
options
encoded
via
position
respectively
order
carry
strategy
prisoner
learn
keep
track
whether
visited
cell
day
currently
figure
compares
performance
ddrqn
variant
switch
disabled
around
3,500
episodes
two
diverge
performance
hence
clearly
identiﬁable
point
learning
prison-
20k40k60k80k100k120k140k160k
epochs0.50.60.70.80.91.0norm
optimal
n=3
c.l.n=5
c.l.n=8
c.l.n=12
c.l.n=16
c.l.n=20
c.l.n=20optimalq2m
sw1m
ir1m
a0m
sw3m
ir3m
a2m
swdmaxm
irdmaxm
admax
-1m
…………q1mqtmqdmaxmq3m……h1mh2mhtmhdmax
-1my1my2my3mytmydmaxmz1mz2mz3mztmzdmaxm
learning
communicate
solve
riddles
deep
distributed
recurrent
q-networks
figure
switch
ddrqn
outperforms
naive
method
simple
hand
coded
strategy
tell
last
day
achieving
oracle
level
performance
lines
depict
av-
erage
runs
conﬁdence
interval
figure
switch
3.5k
episodes
ddrqn
line
clearly
sep-
arates
performance
line
switch
test
start
exceeding
tell
last
day
point
agents
start
dis-
cover
strategies
evolve
communication
using
switch
lines
depict
mean
runs
conﬁdence
interval
ers
start
learning
communicate
via
switch
note
switch
enabled
ddrqn
outperform
hand-coded
tell
last
day
strategy
thus
communi-
cation
via
switch
required
good
performance
figure
shows
performance
runs
ddrqn
clearly
beats
hand-coded
tell
last
day
strategy
ﬁnal
performance
approaches
or-
acle
however
remaining
runs
ddrqn
fails
signiﬁcantly
outperform
hand-coded
strategy
analysing
learned
strategies
suggests
prisoners
typically
encode
whether
prisoners
room
via
positions
switch
respec-
tively
strategy
generates
false
negatives
i.e.
4th
prisoner
enters
room
always
tells
generates
false
positives
around
time
example
strategies
included
supplementary
material
furthermore
figure
shows
results
ablation
ex-
periments
modiﬁcations
ddrqn
removed
one
one
results
show
three
modiﬁcations
contribute
substantially
ddrqn
perfor-
mance
inter-agent
weight
sharing
far
im-
portant
without
agents
essentially
unable
learn
task
even
last-action
inputs
also
play
signiﬁcant
role
without
performance
substantially
exceed
tell
last
day
strat-
egy
disabling
experience
replay
also
makes
difference
performance
replay
never
reaches
optimal
even
af-
ter
50,000
episodes
result
surprising
given
non-stationarity
induced
multiple
agents
learning
par-
allel
non-stationarity
arises
even
though
agents
track
action-observation
histories
via
rnns
within
given
episode
since
memories
reset
episodes
learning
performed
agents
appears
non-stationary
perspective
figure
switch
ddrqn
manages
discover
per-
fect
strategy
visualise
decision
tree
figure
day
position
switch
encodes
prison-
ers
visited
interrogation
room
encodes
one
prisoner
figure
10.
switch
runs
using
curriculum
learning
cases
able
ddrqn
ﬁnd
strategies
out-
perform
tell
last
day
10k20k30k40k50k
epochs1.00.50.00.51.0norm
oracle
ddrqnnaive
methodhand-codedoracleoffhas
onyesnononehas
yesnoswitch
ononofftellonday123+10k20k30k40k50k
epochs0.00.20.40.60.81.0norm
oracle
ddrqnw
disabled
switchhand-codedoraclesplit100k200k300k400k500k
epochs0.60.70.80.91.0norm
oracle
n=4
c.lhand-codedoracle
learning
communicate
solve
riddles
deep
distributed
recurrent
q-networks
deal
high
dimensional
complex
problems
ale
partial
observability
artiﬁcially
in-
troduced
blanking
fraction
input
screen
hausknecht
stone
2015
deep
recurrent
reinforcement
learning
also
applied
text-
based
games
naturally
partially
observable
narasimhan
al.
2015
recurrent
dqn
also
suc-
cessful
email
campaign
challenge
al.
2015
however
examples
apply
recurrent
dqn
single-agent
domains
without
combination
multiple
agents
partial
observability
need
learn
communication
protocol
essential
feature
work
conclusions
future
work
paper
proposed
deep
distributed
recurrent
q-networks
ddrqn
enable
teams
agents
learn
solve
communication-based
coordination
tasks
order
suc-
cessfully
communicate
agents
tasks
must
ﬁrst
au-
tomatically
develop
agree
upon
communi-
cation
protocol
presented
empirical
results
two
multi-agent
learning
problems
based
well-known
rid-
dles
demonstrating
ddrqn
successfully
solve
tasks
discover
elegant
communication
protocols
addition
presented
ablation
experiments
conﬁrm
main
components
ddrqn
architecture
critical
success
future
work
needed
fully
understand
improve
scalability
ddrqn
architecture
large
num-
bers
agents
e.g.
switch
riddle
also
hope
explore
local
minima
structure
coordination
strategy
space
underlies
riddles
another
avenue
improvement
extend
ddrqn
make
use
various
multi-agent
adaptations
q-learning
tan
1993
littman
1994
lauer
riedmiller
2000
panait
luke
2005
beneﬁt
using
deep
models
efﬁciently
cope
high
dimensional
perceptual
signals
inputs
future
tested
replacing
binary
repre-
sentation
colour
real
images
hats
applying
ddrqn
scenarios
involve
real
world
data
input
advanced
new
proposal
using
riddles
test
ﬁeld
multi-agent
partially
observable
reinforce-
ment
learning
communication
also
hope
research
spur
development
interesting
challenging
domains
area
figure
11.
switch
tied
weights
last
action
input
key
performance
ddrqn
experience
replay
prevents
agents
reaching
oracle
level
experiment
executed
lines
depict
average
runs
conﬁdence
interval
however
particularly
important
communication-
based
tasks
like
riddles
since
value
function
communication
actions
depends
heavily
interpreta-
tion
messages
agents
turn
set
q-functions
related
work
plethora
work
multi-agent
rein-
forcement
learning
communication
e.g.
tan
1993
melo
al.
2011
panait
luke
2005
zhang
lesser
2013
maravall
al.
2013
however
work
assumes
pre-deﬁned
communication
protocol
one
ex-
ception
work
kasai
2008
tabular
q-learning
agents
learn
content
message
solve
predator-prey
task
approach
similar
q-table
benchmark
used
section
5.1.
contrast
ddrqn
uses
recurrent
neural
networks
al-
low
memory-based
communication
generalisation
across
agents
another
example
open-ended
communication
learning
multi-agent
task
given
giles
jim
2002
how-
ever
evolutionary
methods
used
learning
com-
munication
protocols
rather
using
deep
shared
weights
enable
agents
develop
dis-
tributed
communication
strategies
allow
faster
learning
via
gradient
based
optimisation
furthermore
planning-based
methods
em-
ployed
include
messages
integral
part
multi-agent
reinforcement
learning
challenge
spaan
al.
2006
however
far
work
extended
10k20k30k40k50k
epochs0.40.20.00.20.40.60.81.0norm
oracle
ddrqnw/o
last
actionw/o
tied
weightsw
experience
replayhand-codedoracle
learning
communicate
solve
riddles
deep
distributed
recurrent
q-networks
acknowledgements
work
supported
oxford-google
deepmind
graduate
scholarship
epsrc
references
assael
j.-a
wahlstr¨om
sch¨on
deisen-
roth
data-efﬁcient
learning
feedback
policies
image
pixels
using
deep
dynamical
models
arxiv
preprint
arxiv:1510.02173
2015.
mnih
kavukcuoglu
multiple
object
recognition
visual
attention
iclr
2015.
bellemare
ostrovski
guez
thomas
munos
increasing
action
gap
new
operators
reinforcement
learning
aaai
2016.
bengio
louradour
collobert
weston
curriculum
learning
icml
41–48
2009.
cao
ren
chen
overview
re-
cent
progress
study
distributed
multi-agent
co-
ordination
ieee
transactions
industrial
informatics
:427–438
2013.
fox
burgard
kruppa
thrun
prob-
abilistic
approach
collaborative
multi-robot
localiza-
tion
autonomous
robots
:325–344
2000.
gerkey
b.p
matari
m.j.
formal
analysis
tax-
onomy
task
allocation
multi-robot
systems
inter-
national
journal
robotics
research
:939–954
2004.
giles
jim
learning
communication
multi-agent
systems
innovative
concepts
agent-
based
systems
377–390
springer
2002.
guo
singh
lee
lewis
wang
deep
learning
real-time
atari
game
play
using
ofﬂine
monte-carlo
tree
search
planning
nips
3338–
3346
2014.
hausknecht
stone
deep
recurrent
learning
partially
observable
mdps
arxiv
preprint
arxiv:1507.06527
2015.
hochreiter
schmidhuber
long
short-term
mem-
ory
neural
computation
:1735–1780
1997.
kasai
tenmoto
kamiya
learning
com-
munication
codes
multi-agent
reinforcement
learning
problem
ieee
conference
soft
computing
in-
dustrial
applications
1–6
2008.
kingma
adam
method
stochastic
optimization
arxiv
preprint
arxiv:1412.6980
2014.
lauer
riedmiller
algorithm
distributed
reinforcement
learning
cooperative
multi-agent
sys-
tems
icml
2000.
levine
finn
darrell
abbeel
end-to-
end
training
deep
visuomotor
policies
arxiv
preprint
arxiv:1504.00702
2015.
gao
chen
deng
recurrent
reinforcement
learning
hybrid
approach
arxiv
preprint
1509.03044
2015.
lin
l.j
reinforcement
learning
robots
using
neu-
ral
networks
phd
thesis
school
computer
science
carnegie
mellon
university
1993.
littman
markov
games
framework
multi-
agent
reinforcement
learning
international
confer-
ence
machine
learning
icml
157–163
1994.
maddison
huang
sutskever
silver
move
evaluation
using
deep
convolutional
neu-
ral
networks
iclr
2015.
maravall
lope
domnguez
coordina-
tion
communication
robot
teams
reinforcement
learning
robotics
autonomous
systems
:661–
666
2013.
matari
m.j.
reinforcement
learning
multi-robot
do-
main
autonomous
robots
:73–83
1997.
melo
spaan
witwicki
querypomdp
pomdp-based
communication
multiagent
systems
multi-agent
systems
189–204
2011.
mnih
kavukcuoglu
silver
rusu
ve-
ness
bellemare
graves
riedmiller
fidjeland
ostrovski
petersen
beattie
sadik
antonoglou
king
kumaran
wier-
stra
legg
hassabis
human-level
con-
trol
deep
reinforcement
learning
nature
518
7540
:529–533
2015.
nair
srinivasan
blackwell
alcicek
fearon
maria
panneershelvam
suley-
man
beattie
petersen
legg
mnih
kavukcuoglu
silver
massively
paral-
deep
lel
methods
deep
reinforcement
learning
learning
workshop
icml
2015.
narasimhan
kulkarni
barzilay
lan-
guage
understanding
text-based
games
using
deep
re-
inforcement
learning
emnlp
2015.
guo
lee
lewis
singh
action-conditional
video
prediction
using
deep
net-
works
atari
games
nips
2845–2853
2015.
learning
communicate
solve
riddles
deep
distributed
recurrent
q-networks
olfati-saber
fax
j.a.
murray
r.m
consensus
cooperation
networked
multi-agent
systems
pro-
ceedings
ieee
:215–233
2007.
wang
freitas
lanctot
dueling
network
arxiv
architectures
deep
reinforcement
learning
preprint
1511.06581
2015.
watter
springenberg
boedecker
ried-
miller
embed
control
locally
linear
latent
dynamics
model
control
raw
images
nips
2015.
100
prisoners
lightbulb
technical
report
ocf
berkeley
2002.
zawadzki
lipson
leyton-brown
empir-
ically
evaluating
multiagent
learning
algorithms
arxiv
preprint
1401.8074
2014.
zhang
lesser
coordinating
multi-agent
rein-
forcement
learning
limited
communication
vol-
ume
1101–1108
2013.
panait
luke
cooperative
multi-agent
learning
state
art
autonomous
agents
multi-agent
systems
:387–434
2005.
poundstone
smart
enough
work
google
fiendish
puzzles
impossible
interview
questions
world
top
companies
oneworld
publications
2012.
schaul
quan
antonoglou
silver
priori-
tized
experience
replay
iclr
2016.
shoham
leyton-brown
multiagent
systems
algorithmic
game-theoretic
logical
foundations
cambridge
university
press
new
york
2009.
shoham
powers
grenager
multi-agent
learning
answer
question
artiﬁcial
intelligence
171
:365–377
2007.
silver
huang
maddison
c.j.
guez
sifre
van
den
driessche
schrittwieser
antonoglou
panneershelvam
lanctot
dieleman
grewe
nham
kalchbrenner
sutskever
lillicrap
leach
kavukcuoglu
graepel
has-
sabis
mastering
game
deep
neural
networks
tree
search
nature
529
7587
:484–489
2016.
song
100
prisoners
light
bulb
technical
report
university
washington
2012.
spaan
gordon
vlassis
decentralized
planning
uncertainty
teams
communicating
agents
international
joint
conference
autonomous
agents
multiagent
systems
249–256
2006.
stadie
levine
abbeel
incentivizing
ex-
ploration
reinforcement
learning
deep
predictive
models
arxiv
preprint
arxiv:1507.00814
2015.
sutton
barto
introduction
reinforce-
ment
learning
mit
press
1998.
tampuu
matiisen
kodelja
kuzovkin
kor-
jus
aru
aru
vicente
multiagent
coop-
eration
competition
deep
reinforcement
learn-
ing
arxiv
preprint
arxiv:1511.08779
2015.
tan
multi-agent
reinforcement
learning
independent
vs.
cooperative
agents
icml
1993.
van
hasselt
guez
silver
deep
reinforce-
ment
learning
double
q-learning
aaai
2016
