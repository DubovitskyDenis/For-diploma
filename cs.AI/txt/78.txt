reinforcement
learning
approach
real
time
strategy
games
battle
city
harshit
sethya
amit
patelb
acto
gymtrekker
fitness
private
limited
mumbai
india
email
hsethy1
gmail.com
bassistant
professor
department
computer
science
engineering
rgukt
iiit
nuzvid
krishna-521202
india
email
amtptl93
gmail.com
paper
proposed
reinforcement
learning
algorithms
generalized
reward
function
proposed
method
use
q-learning
sarsa
algorithms
generalised
reward
function
train
rein-
forcement
learning
agent
evaluated
performance
proposed
algorithms
two
real-time
strategy
games
called
battlecity
two
main
advantages
approach
compared
works
rts
ignore
concept
simulator
often
game
speciﬁc
usually
hard
coded
type
rts
games
system
learn
interaction
opponents
quickly
change
strategy
according
opponents
need
human
traces
used
previous
works
keywords
reinforcement
learning
machine
learning
real
time
strategy
artiﬁcial
intelligence
introduction
existence
good
artiﬁcial
intelligence
technique
background
game
one
major
factor
fun
re-play
ability
commercial
computer
games
although
applied
successfully
several
games
chess
backgammon
checkers
comes
real-time
games
pre-deﬁned
scripts
usually
used
simulate
artiﬁcial
intelli-
gence
chess
backgammon
etc
seem
work
real-time
games
decisions
made
real-time
well
search
space
huge
contain
true
learning
traditional
planning
approaches
diﬃcult
case
rts
games
various
factors
like
huge
decision
spaces
adversarial
domains
partially-
observable
non-deterministic
real-time
real
time
means
deciding
best
actions
game
continues
running
states
change
simul-
taneously
1.1.
real
time
strategy
games
today
game
developing
companies
started
showing
interest
rts
games
unlike
turn
based
strategy
games
one
ability
take
ones
time
real
time
strategy
games
movement
construction
combat
etc.
occurring
real
time
typical
rts
game
screen
contains
map
area
consists
game
world
buildings
units
terrain
usually
several
players
rts
game
players
various
game
entities
called
participants
units
structures
control
players
players
need
save
assets
and/or
destroy
assets
opponent
players
making
use
control
entities
using
rts
games
battlecity
game
evaluation
snapshot
two
rts
games
called
battlecity
given
figure
1.2.
battlecity
game
battlecity
multidirectional
shooter
video
game
played
using
two
basic
ac-
tions
move
fire
player
controlling
tank
must
destroy
enemy
tanks
enemy
base
also
protect
base
player
move
tank
four
directions
left
right
ﬁre
bullets
whichever
direction
tank
last
moved
bases
static
tank
three
types
obstacle
brick
wall
destroy
ﬁring
type
wall
marble
tank
cant
destroy
ﬁring
water
wall
harshit
sethy
amit
patel
bodies
tank
ﬁre
tank
cant
pass
obstacle
brick
wall
destroyed
tank
destroying
tank
pass
figure
snapshot
game
snapshot
battlecity
game
1.3.
game
real-time
strategy
game
players
goal
remain
alive
destroying
rest
players
four
basic
actions
game
harvest
i.e.
gather
resources
gold
wood
build
build
buildings
bar-
rack
blacksmith
tower
etc
train
produce
troops
archers
footmen
catapults
knights
at-
tack
attacking
enemy
paper
structured
follows
apart
introduction
ﬁve
sections.in
sec-
tion
highlights
review
related
works
section
discuss
reinforcement
learning
techniques
real-time-strategy
games
out-
line
various
learning
algorithms
used
rein-
forcement
learning
section
outline
im-
plementation
details
related
proposed
re-
inforcement
learning
algorithms
gener-
alized
reward
function
two
real-time-strategy
games
battlecity
game
section
discusses
experimental
result
related
proposed
work
battlecity
conclude
section
related
work
one
major
works
using
online
case-
based
planning
techniques
real
time
strat-
egy
games
published
on-line
case-
based
planning
revises
case
based
planning
strategic
real-time
domains
involving
on-line
planning
case-based
planning
system
called
introduced
play
rts
darmok2
games
introduced
set
algorithms
used
learn
plans
represented
petri-nets
one
human
demon-
strations
another
work
authors
uses
darmok2
addresses
issues
plan
acquisition
on-line
plan
execution
inter-
leaved
planning
execution
on-line
plan
adaptation
authors
summarize
work
ex-
ploring
use
ﬁrst
order
inductive
learning
foil
algorithm
learning
rules
used
represent
opponent
strategies
authors
improve
darmok2
using
information
related
sensors
game
refer
work
pr-model
paper
pr-model
capable
learning
play
rts
games
observing
human
demonstrations
us-
ing
human
traces
pr-model
makes
plans
play
games
prioritize
plan
according
feed-
back
game
feedbacks
decided
us-
ing
rule
depends
sensors
game
reinforcement
learning
approach
real
time
strategy
games
like
battle
city
drawbacks
case
based
learning
ap-
proaches
mentioned
requires
expert
demonstrations
making
plans
training
done
learning
takes
place
cover
large
state
spaces
would
require
large
number
rules
plan
base
ex-
ploration
optimal
solution
follows
hu-
man
traces..
stefan
wender
uses
reinforce-
ment
learning
city
site
selection
turn-
based
strategy
game
civilization
civiliza-
tion
strategy
game
turn-based
game
battle
city
real
time
game
stefan
wender
uses
reinforcement
learning
city
site
selection
turn-based
strat-
egy
game
civilization
civilization
strategy
game
similar
turn-based
game
real
time
multi
agent
game
paper
aim
away
hard
coded
simulator
propose
learning
approach
based
reinforcement
learning
wherein
sensor
information
current
game-state
used
select
best
action
reinforcement
learning
used
be-
cause
advantages
previous
strategies
speciﬁcally
cuts
need
man-
ually
specify
rules
agents
learn
simply
playing
game
human
players
even
agents
large
state
spaces
combined
function
approxima-
tor
neural
network
approximate
evaluation
function
agent
always
explores
optimal
solution
reach
goal
applied
widely
many
ﬁelds
robotics
board
games
turn
based
games
single
agent
games
great
results
hardly
ever
rts
multi-agent
games
reinforcement
learning
reinforcement
learning
ﬁeld
ma-
chine
learning
deals
map
situations
actions
maximize
numerical
reward
signal.the
learner
know
actions
take
forms
machine
learning
instead
must
discover
actions
gives
reward
applying
interesting
challenging
cases
actions
may
aﬀect
immediate
reward
also
next
situation
subsequent
rewards
comparing
reinforcement
learning
rts
game
environment
player
learns
interacting
environment
observing
feed-backs
interactions
fundamental
way
humans
animals
learn
human
perform
ac-
tions
observe
results
actions
environment
way
rl-agent
inter-
acts
environment
observes
re-
sult
assign
reward
penalty
state
state-action
pair
according
desirability
resultant
state
figure
reinforcement
learning
archi-
tecture
reinforcement
learning
3.1.
reinforcement
learning
architecture
architecture
two
main
characteristics
one
learning
playing
harshit
sethy
amit
patel
learnt
experiences
initially
rlearner
knowledge
game
random
actions
observe
resultant
state
using
sensor
information
game
give
feedback
form
reward
used
calculate
q-values
state-action
pairs
q-table
action
previous
state
according
desirability
current
state
q-values
state-action
pairs
known
q-table
deﬁne
policy
every
action
policy
updates
q-values
state
action
pairs
q-table
policy
used
predict
best
action
playing
game
agent
learns
playing
gives
feedback
whole
process
going
till
end
game
3.2.
basic
components
reinforcement
learning
contains
ﬁve
basic
com-
ponents
listed
set
environment
states
set
actions
rules
transitioning
states
rules
determine
scalar
immediate
reward
transition
reward
functions
rules
describe
agent
observes
value
functions
3.2.1.
reward
function
scalar
value
represents
degree
state
action
desirable
known
reward
scalar
reward
assigned
ac-
tion
particular
transition
resultant
state
game
resultant
state
desir-
able
safe
positive
scalar
value
reward
assigned
action
otherwise
state
safe
undesirable
negative
scalar
value
negative
reward
assigned
action
using
types
reward
function
conditional
reward
function
generalised
reward
function
3.2.2.
value
function
value
functions
used
mapping
states
state-action
pairs
real
numbers
value
state
represents
long-
term
reward
achieved
starting
state
state-action
executing
particular
policy
estimates
good
particular
action
given
state
return
ac-
tion
expected
two
type
value
functions
value
state
policy
expected
return
starting
following
thereafter
value
taking
action
state
policy
expected
re-
turn
starting
taking
action
thereafter
following
policy
two
methods
deﬁne
value
functions
monte
carlo
method
method
agent
would
need
wait
ﬁnal
reward
received
state-action
pair
values
updated
ﬁnal
reward
received
path
taken
reach
ﬁnal
state
would
need
traced
back
value
updated
state
visited
time
reward
time
constant
parameter
temporal
diﬀerence
method
used
estimate
value
functions
step
estimate
ﬁnal
reward
cal-
culated
state
state-action
value
updated
every
step
way
reﬂects
realistic
assignment
rewards
actions
compared
updates
actions
end
directly
learning
nothing
combination
dynamic
programming
monte
carlo
method
formula
related
learning
given
rt+1+γv
st+1
rt+1
observed
reward
time
t+1
reinforcement
learning
approach
real
time
strategy
games
like
battle
city
3.3.
sensor
representation
bat-
tlecity
game
figure
snapshot
battlecity
games
current
maps
using
two
types
sensor
information
assigning
reward
battle
city
game
explained
follows
enemyinline
enemy
position
directly
line
player
without
block
wall
sensor
represented
number
wall
block
enemy
player
sensor
represented
num-
ber
enemy
position
line
player
sensor
enemybaseinline
sensor
information
represented
way
instead
taking
consideration
position
enemy
position
enemy-base
taken
account
enemy-base
position
di-
rectly
line
player
without
block
wall
sensor
represented
num-
ber
wall
block
enemy-base
player
sensor
repre-
sented
number
enemy-base
position
line
player
sensor
sensor
information
game
get
current
map
store
two
dimensional
array
gold
wood
sensors
retrieved
current
game-state
number
peasant
footmen
entities
enemies
player
retrieved
entities
state
update
two
dimensional
array
static
entities
like
goldmine
position
buildings
far
outlined
method
obtain-
ing
sensor
information
related
two
real-time
strategy
games
battlecity
3.4.
action
selection
policies
following
action
selections
policies
used
select
desired
action
accord-
ing
behavior
particular
policy
−greedy
time
action
highest
estimated
reward
cho-
sen
called
greediest
action
small
probability
action
selected
random
ensure
optimal
actions
dis-
covered
−soft
similar
−greedy
best
action
selected
probability
rest
time
random
action
chosen
uniformly
softmax
one
drawback
methods
select
random
actions
probability
case
worst
possible
action
selected
second
best
softmax
remedies
harshit
sethy
amit
patel
assigning
rank
weight
actions
according
action-value
es-
timate
worst
actions
unlikely
chosen
3.5.
steps
learning
rlearner
observes
input
game
state
rlearner
creates
new
policy
based
dimensions
world
set
parameters
number
episodes
rlearner
start
learn-
ing
start
running
epochs
optionally
run
epoch
individually
one
epoch
contains
following
steps
action
determined
decision
mak-
ing
function
e.g
−greedy
action
performed
rlearner
receives
scalar
reward
re-
inforcement
environment
accord-
ing
reward
function
information
reward
given
state
action
pair
recorded
update
q-values
q-table
according
learning
algorithm
e.g
q-learning
sarsa
proposed
learning
algorithm
section
outline
proposed
learn-
ing
algorithms
integrated
two
rts
games
battlecity
also
provide
implementation
details
related
selection
parameters
reward
functions
4.1.
parameters
section
contains
information
regarding
reward
algorithms
parameters
use
two
game
battlecity
learning
rate
learning
rate
determines
fraction
old
estimate
updated
new
estimate
stop
rl-agent
learning
anything
com-
pletely
change
previous
values
new
one
discount
factor
discount
factor
determines
fraction
upcoming
reward
values
considered
evaluation
upcom-
ing
rewards
ignored
means
rl-agent
consider
current
upcoming
rewards
equal
weightage
exploration
rate
action
selec-
tion
policies
one
policy
called
greedy
method
uses
exploration
rate
determining
ratio
exploration
exploitation
using
greedy
method
selecting
best
action
maintain
balance
exploration
exploitation
4.2.
reward
function
battlecity
algorithm
reward
function
calcu-
lating
reward
performing
action
current
state
according
result
action
re-
ward
penalty
assigned
steps
get
positions
x-y
co-ordinates
player
en-
emy
enemy
base
map
steps
game
winner
rl-agent
player
add
reward
total
reward
newreward
else
deduct
penalty
total
reward
steps
enemy
line
rl-agent
deduct
penalty
total
reward
always
tries
line
enemy
steps
enemy
base
line
rl-agent
calculate
distance
enemy
base
rl-agent
deduct
times
reward
add
total
reward
pushes
rl-agent
come
closer
enemy
base
steps
gives
generalized
reward
function
makes
rl-agent
quickly
at-
tack
enemy
base
prevent
attack
enemy
4.3.
reward
function
algorithm
step
get
sensors
related
total
gold
total
wood
size
troops
reinforcement
learning
approach
real
time
strategy
games
like
battle
city
algorithm
calcreward
battlecity
input
state
contains
positions
entities
reward
penalty
sensorslist
contains
sensors
game
domain
gamestate
contains
state
game
running
output
reward
layerx
null
layery
null
enemyx
null
enemyy
null
enemybasex
null
enemybasey
null
winner
null
newreward
distance
layerx
getpositionx
state
player
layery
getpositiony
state
player
enemyx
getpositionx
state
enemy
enemyy
getpositiony
state
enemy
enemybasex
getpositionx
state
enemybase
enemybasey
getpositiony
state
enemybase
gamestate
end
winner
getwinner
winner
player
newreward
newreward
reward
else
newreward
newreward
penalty
else
sensorlist
enemyinline
==2
newreward
newreward
penalty
sensorlist
enemybaseinline
==2
distance
cid:112
enemybasex
layerx
enemybasey
layery
distance
cid:112
enemyx
layerx
enemyy
layery
newreward
newreward
reward
distance
newreward
newreward
distance
newreward
newreward
distance
return
newreward
player
enemy
steps
game
winner
rl-agent
player
add
reward
total
reward
newreward
else
deduct
penalty
total
reward
steps
gold
wood
player
greater
enemy
add
reward
total
reward
otherwise
deduct
penalty
total
reward
always
tries
increase
gold
wood
compare
enemy
steps
player
troop
bigger
enemy
troop
add
twice
reward
total
reward
newreward
else
deduct
twice
penalty
total
reward
pushes
rl-agent
attack
build
army
increase
size
troop
compared
enemy
step
return
total
reward
experimental
results
previous
section
discussed
successfully
applied
reinforcement
learning
two
real-time
strategy
games
called
battlecity
section
outline
experi-
mental
results
related
reinforcement
learning
battlecity
5.1.
battlecity
evaluated
performance
rl-agent
help
various
maps
e.g
bridge-26x18
bridge-metal-26x18
bridges-34x26
well
harshit
sethy
amit
patel
algorithm
calcreward
input
state
contains
positions
entities
reward
penalty
global
access
gamestate
contains
state
game
running
output
reward
sensorslist
contains
sensors
game
domain
layerg
layerw
enemyg
enemyw
enemyt
rooplength
layert
rooplength
winner
null
newreward
layerg
player.getgold
layerw
player.getwood
enemyg
enemy.getgold
enemyw
enemy.getwood
enemyt
rooplength
enemytroop.size
layert
rooplength
playertroop.size
gamestate
end
winner
getwinner
winner
player
newreward
newreward
reward
else
newreward
newreward
penalty
else
layerg
enemyg
newreward
newreward
reward
else
newreward
newreward
penalty
layerw
enemyw
newreward
newreward
reward
else
newreward
newreward
penalty
layert
rooplength
enemyt
rooplength
newreward
newreward
2*reward
else
newreward
newreward
2*penalty
return
newreward
two
types
opponents
called
ai-random
ai-follower
map
observed
reinforcement
learning
agent
games
played
op-
ponents
ai-random
ai-follower
sim-
ple
maps
played
ai-random
complex
maps
played
ai-follower
com-
plex
maps
statistics
performance
sarsa
q-learning
darmok2
various
maps
represented
form
graphs
observed
performance
rl-
agent
sarsa
learning
algorithm
better
techniques
also
rl-agent
trained
sarsa
algorithm
takes
less
time
win
game
performed
evaluation
battlecity
game
two
opponents
ai-random
ai-
follower
three
diﬀerent
maps
ai-random
built-in
selects
random
action
always
ai-follower
tough
compete
be-
cause
always
follows
opponent
ﬁres
clear
experimental
results
reinforcement
learning
agent
sarsa
algorithm
performs
better
techniques
like
q-learning
online
case
based
learn-
reinforcement
learning
approach
real
time
strategy
games
like
battle
city
ing
based
darmok2
statistics
related
performance
given
form
graphs
statistics
represented
using
two
types
graphs
one
time
milliseconds
taken
win
game
versus
episodes
x-axis
repre-
sents
number
episode
y-axis
represents
time
milliseconds
number
games
versus
episode
also
x-axis
rep-
resents
number
episodes
y-axis
rep-
resents
total
number
games
till
episode
5.1.1.
map
bridge-26x18
map
size
26x18
refer
figure
to-
tal
state
space
map
total
combination
co-ordinates
player
en-
emy
262x182
map
marble
wall
tank
destroy
ﬁring
advantage
tank
hide
opponents
attack
opponents
enters
side
figure
map
bridge-26x18
ai-
follower
figure
map
bridge-26x18
5.1.2.
map
bridges-34x24
complex
map
refer
figure
among
performed
eval-
uation
size
structure
34x24
map
342x242
search
spaces
contains
many
brick
wall
water
bodies
brick
wall
destroyed
ﬁring
size
water
bodies
makes
diﬃcult
complex
map
figure
map
bridge-26x18
ai-
random
harshit
sethy
amit
patel
figure
map
bridge-metal-34x24
time
versus
episodes
graph
refer
figure
plot
refer
figure
show-
ing
time
win
game
strategies
varies
every
episodes
map
wa-
ter
bodies
diﬃcult
learn
strategy
win
quickly
ai-random
performance
strategies
close
case
ai-follower
sarsa
performs
well
wins
game
compared
q-learning
darmok2
figure
map
bridge-metal-26x18
ai-
random
figure
map
bridge-metal-26x18
ai-
follower
figure
10.
map
bridges-34x24
ai-
random
reinforcement
learning
approach
real
time
strategy
games
like
battle
city
still
available
also
sends
catapults
attack
enemies
ai-rush
built-in-ai
builds
bar-
rack
starting
two
peas-
ants
starting
harvesting
gold
wood
building
barrack
ai-rush
trains
footmen
two
trained
footmen
starts
attacking
figure
11.
map
bridges-34x24
ai-
follower
5.2.
maps
related
complex
battlecity
evaluated
ap-
proach
various
maps
several
built-
player
experiments
built
agent
game
using
relative
reward
func-
tion
q-learning
sarsa
approach
discussed
earlier
rl-agent
learn
playing
games
built-in-ai
called
ai-catapult-rush
simple
map
nwtr1
refer
figure
us-
ing
two
approaches
q-learning
sarsa
state-action
pair
values
q-values
updated
playing
learning
discussed
earlier
rl-agent
also
learns
playing
using
updated
q-values
rl-agent
plays
games
ai-catapult-rush
well
another
type
built-
in-ai
called
ai-rush
ai-catapult-rush
built-in-ai
builds
barracks
lumber-mills
starting
two
peasants
har-
vesting
gold
two
harvesting
wood
starts
building
catapults
nonstop
also
attacks
some-
time
increases
number
peasants
starts
building
second
barrack
also
looks
goldmines
gold
figure
12.
snapshot
game
map
gow
experiment
used
three
type
maps
refer
figure
according
diﬃ-
culty
level
easy-nwtr2
medium-nwtr6
diﬃcult-gow
performed
experiments
ﬁve
games
two
built-in-ai
wherein
two
approaches
q-learning
sarsa
map
comparison
statistics
given
table
observed
rl-agent
sarsa
wins
games
q-learning
previous
approach
darmok2
performs
almost
better
sarsa
also
sarsa
gives
best
results
table
shows
results
comparison
ana-
lyzing
results
shown
table
see
maps
sarsa
drawn
game
maps
lost
found
built-in-ai
quick
attacker
rl-
agent
able
produce
enough
number
harshit
sethy
amit
patel
table
comparison
sarsa
q-learning
darmok2
map
approach
epoch
epoch
epoch
epoch
epoch
ai-catapult
sarsa
nwtr2
nwtr2
q-learning
darmok2
nwtr2
nwtr6
sarsa
nwtr6
q-learning
nwtr6
darmok2
sarsa
gow
gow
q-learning
gow
darmok2
sarsa
nwtr2
nwtr2
q-learning
darmok2
nwtr2
nwtr6
sarsa
nwtr6
q-learning
darmok2
nwtr6
sarsa
gow
gow
q-learning
gow
darmok2
lost
lost
draw
lost
lost
draw
lost
draw
draw
lost
lost
lost
lost
lost
draw
draw
lost
draw
lost
draw
ai-rush
draw
draw
lost
lost
draw
lost
lost
lost
lost
lost
lost
draw
troops
defend
enemy
attack-
ing
agent
basically
trying
ﬁnd
way
enter
wall
trees
maps
shown
results
drawn
means
resources
like
wood
gold
player
enemy
got
ﬁnished
peasants
left
sides
anything
without
gold
wood
compared
previous
research
darmok2
pre-prepared
strategies
used
play
game
plan
adaption
mod-
ule
used
switch
strategies
research
rl-agent
quickly
switches
strategies
playing
even
though
used
simple
map
training
rl-agent
conclusions
paper
proposed
reinforcement
learning
model
real-time
strategy
games
order
achieve
end
make
use
two
re-
inforcement
learning
algorithms
sarsa
learning
idea
get
best
action
using
one
algorithms
make
use
traces
generated
players
previous
works
real-time
strategy
games
using
line
case
based
learning
human
traces
form
im-
portant
component
learning
process
proposed
method
making
use
previous
knowledge
like
traces
therefore
follow
unsupervised
approach
research
regard
getting
best
action
using
two
algorithms
sarsa
q-learning
comes
reinforcement
learning
without
traces
generated
player
proposed
previous
work
line
case
based
learning
us-
ing
darmok2
another
major
contribution
work
reward
function
rewards
cal-
culated
two
types
reward
functions
called
conditional
generalized
reward
function
sensor
information
related
game
used
cal-
culating
rewards
reward
values
fur-
ther
used
two
algorithms
sarsa
q-learning
algorithms
make
policies
according
reward
state-action
pair
agent
choose
action
us-
ing
policies
evaluated
approach
successfully
two
diﬀerent
game
domains
bat-
tlecity
observed
reinforce-
ment
learning
performs
better
previous
ap-
reinforcement
learning
approach
real
time
strategy
games
like
battle
city
10.
santiago
aaai-2008
pages
1-2.
aaai
press
2008.
doc-
umentation
2010
http
//heanet.dl.sourceforge.net/project/dar-
mok2/d2documentation.pdf
villar
1-6
may
onta˜n´on
pages
11.
marc
ponsen
pieter
spronck
improving
adaptive
game
evolutionary
learn-
ing
computer
games
artiﬁcial
intelli-
gence
design
education
pages
389-396
2004
12.
bhaskara
marthi
stuart
russell
david
latham
carlos
guestrin
concurrent
hi-
erarchical
reinforcement
learning
turn-based
strategy
game
civilization
interna-
tional
joint
conference
artiﬁcial
intelli-
gence
edinburgh
scotland
pages
1652-1653
2005
13.
pranay
game
simulator
learner
darmok2
university
hyderabad
m.tech
thesis
2013.
harshit
sethy
co-
founder
chief
technology
oﬃcer
gymtrekker
fitness
private
limited
mumbai
in-
dia
received
masters
artiﬁcial
degree
intel-
ligence
university
hyderabad
assistant
currently
amit
patel
professor
rajiv
gandhi
university
knowledge
technologies
iiit
nuzvid
krishna
obtained
bachelor
technology
uttar
pradesh
technical
university
received
masters
degree
artiﬁcial
intelligence
university
hyderabad
hyderabad
proaches
terms
learning
time
winning
ratio
particular
sarsa
algorithm
takes
lesser
time
learn
start
winning
quickly
q-learning
complex
maps
references
sutton
barto
reinforcement
learning
introduction
book
publisher
mit
press
1998
katie
long
genter
using
ﬁrst
order
inductive
learning
alternative
simulator
game
arﬁcial
intelligence
under-graduate
the-
sis
georgia
institute
technology
pages
1–2
may
2009
katie
long
genter
santiago
onta˜n´on
ashwin
ram
learning
opponent
strategies
ﬁrst
order
induction
flairs
conference
pages
1–2
2011
gomez-martin
llanso
gomez-martin
santiago
onta˜n´on
ash-
win
ram
mmpm
generic
platform
case-based
planning
research
iccbr
2010
workshop
case-based
reasoning
computer
games
pages
45-54
july
2010
stefan
wender
ian
watson
using
re-
inforcement
learning
city
site
selection
turn-based
strategy
game
civiliza-
tion
computational
intelligence
games
cig-2008
pages
372-377
2008
janet
kolodner
introduction
case-
based
reasoning
artiﬁcial
intelligence
review
pages
3-34
1992
santi
onta˜n´on
kinshuk
mishra
neha
sugandh
ashwin
ram
on-line
case-
based
planning
computational
intelli-
gence
pages
84-119
2010
santiago
onta˜n´on
long
genter
k.bonnette
p.mahindrakar
gomez-martin
katie
j.radhakrishnan
r.shah
ashwin
ram
learning
human
demonstrations
real-time
case-
based
planning
struck-09
workshop
colocated
ijcai
pages
2-3
2011
neha
sugandh
santiago
onta˜n´on
ash-
win
ram
on-line
case-based
plan
adaptation
real-time
strategy
games
association
advancement
artiﬁcial
intelligence
