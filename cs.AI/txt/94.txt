hierarchical
linearly-solvable
markov
decision
problems
extended
version
supplementary
material
anders
jonsson
vicenc¸
g´omez
department
information
communication
technologies
universitat
pompeu
fabra
roc
boronat
138
08018
barcelona
spain
anders.jonsson
vicen.gomez
upf.edu
abstract
present
hierarchical
reinforcement
learning
framework
formulates
task
hierarchy
special
type
markov
decision
process
bellman
equa-
tion
linear
analytical
solution
problems
type
called
linearly-solvable
mdps
lmdps
interest-
ing
properties
exploited
hierarchical
setting
efﬁcient
learning
optimal
value
function
task
compositionality
proposed
hierarchical
approach
also
seen
novel
alternative
solving
lmdps
large
state
spaces
derive
hierarchical
version
so-
called
z-learning
algorithm
learns
different
tasks
simul-
taneously
show
empirically
signiﬁcantly
outper-
forms
state-of-the-art
learning
methods
two
classical
hierarchical
reinforcement
learning
domains
taxi
domain
autonomous
guided
vehicle
task
introduction
hierarchical
reinforcement
learning
hrl
general
framework
addressing
large-scale
reinforcement
learn-
ing
problems
exploits
task
action
structure
problem
considering
policies
temporally
extended
actions
typically
involve
reduced
subset
state
components
example
maxq
approach
dietterich
2000
decomposes
markov
decision
process
mdp
value
function
hierarchy
smaller
mdps
value
function
target
mdp
corresponds
additive
combination
value
functions
smaller
mdps
another
example
options
approach
different
tasks
learned
simultaneously
online
fashion
sutton
precup
1998
hrl
methods
also
used
explain
human
animal
behavior
botvinick
niv
barto
2009
independently
class
stochastic
optimal
control
prob-
lems
introduced
actions
cost
func-
tion
restricted
ways
make
bellman
equation
linear
thus
efﬁciently
solvable
todorov
2006
kappen
2005
class
problems
known
dis-
crete
setting
linearly-solvable
mdps
lmdps
continuous
setting
path-integral
control
gener-
ally
kullback-leibler
control
kappen
g´omez
opper
2012
optimal
control
computation
class
problems
equivalent
kl-divergence
minimization
original
lmdp
formulation
considers
single
action
changes
stochastic
laws
environment
al-
ternative
interpretation
adopt
work
con-
sider
stochastic
policy
deterministic
actions
lmdps
many
interesting
properties
example
optimal
con-
trol
laws
lmdps
linearly
combined
derive
composite
optimal
control
laws
efﬁciently
todorov
2009a
also
power
iteration
method
used
solve
lmdps
equivalent
popular
belief
propagation
algorithm
used
probabilistic
inference
dynamic
graphical
mod-
els
kappen
g´omez
opper
2012
optimal
value
function
lmdps
learned
efﬁciently
using
off-
policy
learning
algorithm
z-learning
operates
directly
state
space
instead
product
space
states
actions
todorov
2009b
continuous
setting
kl-divergence
reduces
familiar
quadratic
energy
cost
widely
used
robotic
ap-
plications
examples
applications
include
robot
nav-
igation
kinjo
uchibe
doya
2013
motor
skill
re-
inforcement
learning
theodorou
buchli
schaal
2010
class
problems
also
relevant
disciplines
cognitive
science
decision
making
theory
fris-
ton
2013
ortega
braun
2013
however
gen-
eral
application
lmdps
real-world
problems
chal-
lenging
mainly
due
curse
dimensionality
abbasi-
yadkori
2015
matsubara
g´omez
kappen
2014
todorov
2009c
paper
propose
combine
hrl
lmdp
frameworks
formulate
reinforcement
learn-
ing
problem
hierarchy
lmdps
surprisingly
despite
lmdps
introduced
already
ten
years
ago
unifying
framework
combines
methodologies
pro-
posed
yet
beneﬁts
combination
two-fold
one
hand
hrl
problems
expressed
way
beneﬁt
properties
lmdps
enjoy
example
one
use
z-learning
efﬁcient
alternative
state-
of-the-art
hrl
methods
another
example
task
composi-
tionality
composite
task
learned
cost
given
optimal
solution
different
composing
tasks
useful
tasks
several
terminal
states
show
later
hand
lmdps
also
ben-
eﬁt
hrl
framework
example
addressing
curse
dimensionality
alternative
way
pre-
viously
mentioned
approaches
simultaneous
intra-task
learning
hrl
paper
organized
follows
review
hrl
lmdps
section
main
contribution
work
hierarchical
formulation
lmdps
present
section
empirically
illustrate
beneﬁts
two
benchmarks
section
conclude
work
sec-
tion
preliminaries
section
introduce
preliminaries
notation
ﬁrst
deﬁne
mdps
semi-mdps
explain
idea
be-
hind
maxq
decomposition
ﬁnally
describe
linearly-
solvable
mdps
satisfying
cid:80
2.1
mdps
semi-mdps
mdp
cid:104
cid:105
consists
set
states
set
actions
transition
probability
distribution
cid:48
cid:48
state-action
pair
expected
reward
function
aim
learn
optimal
policy
i.e
mapping
states
actions
maximizes
expected
future
reward
mdps
usually
solved
deﬁning
value
function
estimates
expected
future
reward
state
undiscounted
case
optimal
value
obtained
solving
bellman
optimality
equation
cid:48
cid:48
max
a∈a
cid:40
cid:41
max
a∈a
cid:48
cid:48
cid:88
cid:48
bound
optimal
value
function
consider
ﬁrst
exit
problems
deﬁne
set
terminal
states
function
deﬁnes
ﬁnal
reward
terminal
state
alternative
value
function
one
deﬁne
action-value
function
estimates
expected
future
reward
state-action
pair
bellman
optimality
equation
solved
globally
using
algorithms
value
iteration
policy
itera-
tion
however
large
state
spaces
feasible
al-
ternative
algorithms
make
local
updates
value
func-
tion
online
arguably
popular
online
algorithm
mdps
q-learning
watkins
1989
given
transition
st+1
state
state
st+1
taking
ac-
tion
receiving
reward
q-learning
makes
fol-
lowing
update
estimate
optimal
action-value
function
max
st+1
learning
rate
semi-mdp
generalizes
mdp
including
actions
take
one
time-step
complete
case
figure
task
graph
taxi
domain
bellman
optimality
equation
becomes
cid:41
τ|s
cid:48
cid:48
cid:88
cid:48
cid:48
τ|s
cid:48
cid:40
cid:88
cid:88
cid:48
max
a∈a
max
a∈a
duration
action
expected
cid:80
reward
applied
lasts
steps
cid:48
τ|s
cid:80
probability
transitioning
cid:48
steps
deﬁning
cid:48
cid:48
τ|s
cid:48
cid:40
cid:48
τ|s
get
cid:41
max
a∈a
cid:48
cid:48
cid:88
cid:48
i.e
semi-mdp
treated
solved
mdp
2.2
maxq
decomposition
maxq
decomposition
dietterich
2000
decomposes
mdp
cid:104
cid:105
ﬁnite
set
tasks
root
task
i.e
solving
equiva-
lent
solving
task
cid:104
˜ri
cid:105
consists
termination
set
action
set
pseudo-reward
˜ri
task
primitive
subtasks
i.e
primitive
task
corresponds
action
original
mdp
deﬁned
always
applica-
ble
terminates
one
time
step
pseudo-reward
everywhere
non-primitive
task
applied
non-terminal
states
i.e
states
terminating
state
produces
pseudo-reward
˜ri
corresponds
semi-mdp
action
set
i.e
actions
tasks
maxq
deﬁnes
task
graph
tasks
nodes
edge
nodes
i.e
action
task
avoid
inﬁnite
recursion
task
graph
acyclic
figure
shows
simpliﬁed
task
graph
taxi
domain
commonly
used
illustrate
maxq
decomposition
aim
maxq
decomposition
learn
hier-
archical
policy
i.e
separate
policy
individual
task
task
deﬁnes
value
function
state
estimates
expected
cumulative
reward
terminates
reward
associated
applying
action
state
task
equals
value
rootpickupnavigate
putdownnorthsoutheastwest
cid:41
cid:40
cid:88
cid:48
i.e
hence
bellman
optimality
equation
decomposes
max
mj∈ai
cid:48
cid:48
cid:48
probability
transitioning
cid:48
applying
possibly
composite
action
primitive
task
corresponding
action
original
mdp
value
expected
immediate
reward
i.e
pseudo-reward
˜ri
used
learning
policy
contribute
value
function
dietterich
2000
proposed
online
algorithm
maxq
decomposition
called
maxq-q
learning
al-
gorithm
maintains
two
value
functions
task
estimate
ˆvi
value
function
deﬁned
estimate
˜vi
expected
cumulative
reward
includes
pseudo-reward
˜ri
estimate
˜vi
deﬁnes
policy
estimate
ˆvi
passed
reward
parent
tasks
maxq-q
learning
achieves
recursive
optimal-
ity
i.e
policy
locally
optimal
respect
dietterich
1999
also
showed
use
state
abstraction
simplify
learning
maxq
decomposition
2.3
linearly-solvable
mdps
linearly-solvable
mdps
lmdps
ﬁrst
introduced
todorov
2006
2009b
original
formulation
ex-
plicit
actions
control
consists
changing
predeﬁned
uncontrolled
probability
distribution
next
states
al-
ternative
interpretation
view
resulting
probability
distribution
stochastic
policy
deterministic
actions
todorov
idea
transform
discrete
optimization
problem
actions
continuous
optimization
problem
transition
probabilities
convex
analyti-
cally
tractable
formally
lmdp
cid:104
cid:105
consists
set
states
uncontrolled
transition
probability
distribution
cid:48
cid:48
state
expected
reward
function
given
state
next
state
distribution
deﬁne
set
next
states
cid:48
cid:48
ﬁrst-exit
problems
lmdps
also
subset
terminal
states
control
lmdps
probability
distribution
·|s
next
states
given
next
state
cid:48
cid:48
non-zero
cid:48
non-zero
reward
applying
control
state
satisfying
cid:80
·|s
cid:107
·|s
cid:48
·|s
log
cid:20
cid:21
cid:48
cid:48
non-positive
reward
associated
state
·|s
cid:107
·|s
kullback-leibler
diver-
gence
penalizing
controls
sig-
niﬁcantly
different
typically
random
walk
acts
temperature
parameter
large
values
high
temperature
lead
solutions
stochas-
tic
since
deviating
random
dynamics
penalized
conversely
small
values
low
temperature
result
deterministic
policies
since
state-dependent
term
dominates
immediate
cost
lmdps
sense
re-
place
deterministic
policies
deﬁned
stochastic
actions
stochastic
policies
deﬁned
deterministic
actions
follows
unless
otherwise
stated
next
state
cid:48
always
drawn
distribution
·|s
deﬁne
bellman
optimality
equation
cid:21
cid:20
max
a∈a
cid:48
cid:48
cid:48
log
cid:48
cid:48
cid:48
cid:48
cid:48
max
inputs
satisfy
cid:80
max
a∈a
given
state
set
consists
control
cid:48
cid:48
cid:48
cid:48
cid:48
bound
values
absense
discount
factor
terminal
states
ab-
sorbing
i.e
t|t
cid:21
introducing
obtain
log
cid:48
cid:48
obtain
divergence
right-hand
side
introduce
cid:48
cid:48
cid:48
in-
cid:21
cid:48
cid:20
cid:20
normalization
term
cid:80
cid:20
cid:13
cid:13
cid:13
cid:13
·|s
min
log
min
cid:48
cid:48
cid:48
sert
bellman
equation
cid:48
cid:48
min
·|s
cid:18
cid:19
cid:21
cid:48
cid:48
log
log
term
achieves
minimum
distribu-
tions
equal
i.e
optimal
policy
cid:48
cid:48
cid:48
exponentiating
bellman
equation
gives
/λg
write
equation
matrix
form
ωπz
diagonal
matrix
terms
diagonal
transition
probability
matrix
de-
rived
distribution
unlike
bellman
optimality
equation
system
linear
equations
since
equation
linear
solve
eigenvec-
tor
problem
using
example
power
iteration
method
alternative
todorov
2006
2009b
proposed
on-
line
learning
algorithm
lmdps
called
z-learning
simi-
lar
q-learning
mdps
idea
z-learning
fol-
low
trajectory
record
transitions
perform
incremental
updates
value
function
since
lmdps
explicit
actions
transition
st+1
consists
state
next
state
st+1
reward
recorded
transition
z-learning
main-
tains
estimate
optimal
value
estimate
updated
transition
αert/λ
st+1
learning
rate
naive
z-learning
samples
transitions
passive
dy-
namics
essentially
amounts
random
walk
leads
slow
learning
better
alternative
use
impor-
tance
sampling
guide
exploration
sampling
transitions
informed
distribution
natural
choice
estimated
optimal
policy
derived
resulting
following
corrected
update
rule
todorov
2006
αert/λ
st+1
wˆa
st+1
wˆa
st+1
st+1|st
st+1|st
note
importance
weight
wˆa
st+1
requires
ac-
cess
passive
dynamics
2.4
lmdps
transition-dependent
rewards
original
formulation
lmdps
reward
state-
dependent
develop
hierarchical
framework
based
lmdps
account
fact
task
may
accumulate
different
amounts
reward
hence
reward
transition-dependent
depending
current
state
also
next
state
section
extend
lmdps
transition-dependent
reward
i.e
expected
re-
ward
function
deﬁned
pairs
states
reward
applying
control
state
cid:48
cid:20
cid:48
·|s
cid:107
·|s
cid:48
log
cid:48
cid:48
cid:21
cid:48
bellman
equation
becomes
max
cid:20
cid:48
cid:48
cid:48
cid:48
log
max
cid:48
cid:48
letting
cid:48
cid:48
yields
cid:48
cid:21
cid:20
normalizing
cid:80
min
cid:48
log
cid:21
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
yields
log
results
policy
cid:48
cid:48
cid:48
cid:48
exponentiating
bellman
equation
gives
written
matrix
form
entry
equals
cid:48
cid:48
cid:48
solve
equation
either
apply
power
iter-
ation
method
z-learning
trivial
extend
z-learning
lmdps
transition-dependent
reward
transi-
tion
still
triplet
st+1
difference
reward
depends
next
state
st+1
well
current
state
st.
compare
target
value
see
update
rule
equa-
tion
causes
tend
towards
optimal
value
using
uncontrolled
distribution
sample
transitions
update
importance
sampling
equation
also
directly
applied
lmdps
transition-dependent
re-
ward
hierarchical
lmdps
section
formalize
framework
hierarchical
lmdps
based
maxq
decomposition
assume
exists
underlying
lmdp
cid:104
cid:105
set
tasks
root
task
task
cid:104
˜ri
cid:105
consists
termination
set
set
subtasks
pseudo-reward
˜ri
state
absorbing
produces
reward
˜ri
clarity
presentation
ﬁrst
assume
task
deterministic
i.e
state
terminates
unique
state
later
show
extend
hierarchical
lmdps
non-
deterministic
tasks
maxq
decomposition
since
actions
origi-
nal
mdp
cid:104
cid:105
included
primitive
tasks
action
set
task
contains
subset
actions
analogy
hierarchical
lmdps
task
contains
subset
allowed
transitions
original
lmdp
cid:104
cid:105
i.e
transitions
non-zero
prob-
ability
according
intuitively
optimal
control
task
viewed
stochastic
policy
selects
deterministic
tasks
deterministic
next
states
associate
task
lmdp
cid:104
cid:105
transition-dependent
rewards
task
primitive
state
let
subset
next
states
original
lmdp
also
present
let
set
subtasks
applicable
i.e
terminal
state
clearly
primitive
given
state
passive
dynamics
immediate
reward
task
lmdp
deﬁned
cid:80
terms
transitions
due
cid:48
cid:48
cid:48
∈ns
cid:48
cid:48
|ns|
|ns|
|as|
cid:48
cid:48
cid:48
cid:48
transitions
due
|ns|
|as|
∀lj
∀lj
maxq
decomposition
reward
associated
applying
subtask
state
equals
value
3.1
task
compositionality
non-deterministic
tasks
subsection
extend
deﬁnition
hierarchical
lmdps
non-deterministic
tasks
associate
task
lmdp
cid:104
cid:105
important
difference
subtasks
one
terminal
state
primitive
subtasks
transitions
ad-
dressed
omit
clarity
thus
deﬁne
passive
dynamics
immediate
rewards
non-
primitive
subtasks
one
terminal
state
denote
|tj|
k-th
termi-
nal
state
subtask
deﬁne
counterparts
equa-
tions
multiple
terminal
states
|ns|
|as|
∀lj
∀lj
transition
probability
subtask
state
terminal
state
value
function
expressed
using
compositionality
optimal
control
laws
lmdps
todorov
2009a
described
be-
low
note
different
immediate
transition
probabilities
subtask
total
transition
probability
subtask
still
|ns|
|as|
distributed
among
possible
terminal
states
accord-
ing
terminal
state
task
de-
ﬁne
separate
task
new
tasks
identical
|tj|
terminal
states
differ
pseudo-
rewards
˜rj
task
pseudo-reward
goal
zero
remaining
|tj|−
terminal
states
negative
pseudo-reward
e.g
˜rj
˜rj
cid:54
·|s
optimal
value
individual
tasks
using
compo-
sitionality
original
task
multiple
terminal
states
expressed
weighted
sum
individual
tasks
particular
composite
optimal
policy
todorov
2009a
consider
optimal
policy
|tj|
cid:88
·|s
|tj|
|tj|
cid:88
k=1
k=1
·|s
mixing
weights
composing
tasks
uniform
equals
1/|tj|
since
task
assigns
pseudo-reward
non-goal
terminal
states
value
function
equation
given
log
figure
lmdp
task
graph
taxi
domain
function
i.e
transi-
tion
associated
subtask
uniform
probability
transitions
probabilities
pro-
portional
produce
reward
original
lmdp
task
value
function
estimates
ex-
pected
cumulative
reward
terminates
deﬁnes
immediate
reward
higher-level
tasks
write
bellman
optimality
equation
max
cid:48
cid:48
cid:48
log
cid:20
cid:21
cid:48
cid:48
task
graph
hierarchical
lmdps
deﬁned
maxq
decomposition
acyclic
figure
shows
lmdp
task
graph
taxi
domain
compared
figure
primitive
actions
longer
appear
tasks
new
sink
nodes
e.g
navigate
correspond
primitive
tasks
hierarchical
lmdp
deﬁnitions
implicitly
consider
following
assumptions
differ
maxq
formulation
required
hierarchical
lmdps
first
assume
terminal
states
subtasks
mu-
tually
exclusive
overlap
next
states
reason
lmdp
allowed
one
transition
two
states
different
rewards
happens
optimal
policy
iden-
tiﬁable
since
one
collapse
transitions
one
determine
resulting
immediate
reward
ill-deﬁned
problem
another
difference
lmdp
task
needs
equivalent
no-op
action
i.e
s|s
corresponding
markov
chain
aperiodic
needed
con-
vergence
power-iteration
method
finally
unlike
maxq
decomposition
value
function
equation
includes
terms
due
differences
control
uncontrolled
dynamics
reward
also
includes
subtask
depen-
dent
terms
consequently
value
function
root
task
includes
terms
tasks
although
introduces
approximation
con-
trol
relative
importance
terms
adjusting
value
rootpickupnavigate
putdownnorthsoutheastwest
transition
probability
equation
deﬁned
re-
cursively
states
|tj
|tj
cid:48
cid:48
cid:54
cid:88
cid:48
deﬁnes
hierarchical
lmdp
framework
non-
deterministic
tasks
note
individual
task
still
deterministic
purpose
avoid
terminating
state
different
3.2
hierarchical
learning
algorithms
aim
hierarchical
lmdp
learn
estimated
hi-
erarchical
control
policy
cid:104
ˆa0
ˆan
cid:105
i.e
individual
control
policy
ˆai
task
similar
maxq
decomposition
two
main
alternatives
learning
hierarchical
policy
learn
individual
policy
ˆai
separately
bottom-up
fashion
learn
policies
simultaneously
using
hierarchical
ex-
ecution
top-down
fashion
implementing
algorithm
ﬁrst
type
straightfor-
ward
since
individual
task
lmdp
us-
ing
power
iteration
method
z-learning
since
sub-
tasks
solved
rewards
known
ﬁxed
solving
implement
algorithm
second
type
similar
maxq-q
learning
start
root
task
sample
subtask
execute
using
current
estimate
ˆa0
policy
execute
termination
possibly
applying
subtasks
along
way
terminates
return
control
root
task
another
subtask
sampled
using
ˆa0
continues
reach
absorbing
state
process
value
function
estimates
task
updated
using
z-learning
maxq-q
learning
task
pseudo-rewards
different
learn
two
estimates
value
function
one
estimate
ˆvi
optimal
value
func-
tion
excludes
pseudo-reward
˜ri
another
es-
timate
˜vi
includes
pseudo-reward
˜ri
estimate
˜vi
deﬁnes
policy
ˆai
ˆvi
passed
reward
parent
tasks
3.3
hierarchical
mdps
aim
learn
separate
policy
individual
task
since
q-learning
off-policy
al-
gorithm
possible
use
transitions
recorded
one
task
learn
policy
another
task
intra-task
learn-
ing
known
converge
faster
sutton
precup
1998
section
describe
algorithm
intra-task
learning
intra-task
z-learning
described
subsection
2.3
use
importance
sampling
improve
exploration
let
st+1
tran-
sition
sampled
using
estimated
policy
ˆaj
task
consider
update
estimated
value
ˆzi
another
task
even
though
sample
distribution
ˆaj
different
estimated
policy
ˆai
consider
update
equation
ˆzi
ˆzi
αert/λ
ˆzi
st+1
wˆai
st+1
see
update
rule
correct
simply
substitute
expressions
wˆai
ˆai
ˆzi
ˆzi
αert/λ
ˆzi
st+1
st+1|st
ˆai
st+1|st
ˆzi
αert/λ
st+1|st
ˆzi
st+1
st+1|st
ˆzi
st+1
ˆzi
ˆzi
αert/λg
ˆzi
words
instead
moving
ˆzi
direction
ert/λ
ˆzi
st+1
update
rule
moves
ˆzi
direction
ert/λg
ˆzi
precisely
desired
value
ˆzi
particular
importance
weight
shared
different
tasks
intra-task
learning
lmdps
transition
costs
update
rule
used
substituting
expressions
wˆai
ˆai
leads
slightly
different
result
ˆzi
ˆzi
αert/λ
ˆzi
st+1
st+1|st
ˆai
st+1|st
ˆzi
st+1|st
ert/λ
ˆzi
st+1
ˆzi
st+1|st
st+1
ˆzi
st+1
ˆzi
ˆzi
recall
st+1
st+1
expectation
ˆzi
results
fact
observed
reward
expected
reward
st+1
may
different
equal
expectation
lmdps
transition
costs
ˆzi
desired
value
ˆzi
3.4
state
abstraction
hierarchical
lmdps
apply
forms
state
abstraction
maxq
decomposition
dietterich
1999
common
form
state
abstraction
projection
max
node
irrelevance
form
state
abstraction
as-
sumes
state
factored
i.e
s1×···×sk
domains
state
variables
max
node
irrelevance
identiﬁes
state
variables
irrelevant
given
task
implying
values
state
variables
remain
completion
task
irrelevant
state
variables
ignored
learning
value
function
dietterich
1999
identiﬁed
conditions
safe
perform
state
abstraction
one
condition
leaf
figure
taxi
problem
results
primitive
tasks
navi-
gate
figure
taxi
problem
results
abstract
task
root
irrelevance
apply
hierarchical
lmdps
since
ac-
tions
longer
included
leaves
task
graph
an-
condition
result
distribution
irrelevance
apply
hierarchical
lmdps
two
states
transition
probabilities
respect
given
task
need
estimate
single
value
ˆvi
˜vi
states
experiments
evaluate
proposed
framework
two
tasks
com-
monly
used
hierarchical
mdps
taxi
domain
di-
etterich
2000
autonomous
vehicle
guided
task
agv
ghavamzadeh
mahadevan
2007
compare
following
methods
z-learning
using
naive
sampling
i.e
random
walk
without
correction
term
equation
z-is
z-learning
importance
sampling
without
intra-task
learning
equation
z-is-il
z-learning
importance
sampling
intra-
task
learning
equation
q-g
-greedy
q-learning
without
intra-task
learning
q-g-il
-greedy
q-learning
intra-task
learning
z-learning
variants
evaluated
using
task
lmdps
described
section
compare
q-learning
variants
task
lmdp
construct
traditional
mdp
following
methodology
todorov
2009b
resulting
traditional
mdp
guaranteed
optimal
value
function
original
lmdp
following
todorov
2006
use
dynamic
learning
rates
de-
cay
optimized
separately
algorithm
current
trial
parameter
q-learning
also
optimized
best
performance
compare
performance
calculate
iteration
cid:96
1-norm
differences
learned
optimal
value
function
computed
exactly
tasks
consider
details
experi-
ments
described
supplementary
material
4.1
taxi
domain
taxi
domain
deﬁned
grid
four
distinguished
locations
passenger
one
four
locations
passenger
wishes
transported
one
three
locations
also
taxi
must
navigate
passenger
location
pick
passenger
navigate
destination
location
put
passenger
use
variant
taxi
domain
dietterich
2000
much
larger
state
space
grid
decompose
taxi
domain
shown
figure
like
dietterich
2000
apply
state
abstraction
form
projection
navigate
tasks
ignoring
passenger
location
destination
results
state
spaces
sizes
625
3,125
navigation
full
task
respectively
primitive
tasks
navigate
contain
state
transi-
tions
associated
navigation
actions
north
south
east
west
idle
i.e
no-op
action
four
primitive
tasks
one
location
corner
grid
corresponding
lmdps
similar
grid
example
todorov
2006
passive
dynamics
random
walk
state-cost
term
zero
termi-
nal
states
corresponding
corner
elsewhere
figure
shows
performance
different
learning
methods
primitive
task
domain
best
results
obtained
z-learning
importance
sampling
intra-task
learning
z-is-il
z-learning
variants
out-
perform
q-learning
variants
mainly
z-learning
unlike
q-learning
need
maximization
operator
state-action
values
todorov
2006
remarkably
z-is
with-
intra-task
learning
still
performs
better
q-learning
intra-task
learning
naive
z-learning
performs
bet-
ter
greedy
q-learning
q-g
particular
task
random
exploration
still
useful
learn
location
one
corner
grid
full
task
composed
four
possible
naviga-
tion
subtasks
plus
transitions
resulting
applying
original
pickup
putdown
actions
idle
transition
figure
shows
results
full
task
since
one
task
intra-task
learning
ap-
ply
case
random
exploration
converges
slowly
0246810x
10400.511.522.5x
104time−steperrortaxi
domain
primitive
tasks
q−gq−g−ilzz−isz−is−il00.511.522.5x
1050123456x
104time−steperrortaxi
domain
high−level
task
q−gzz−is
figure
agv
problem
results
primitive
tasks
nav-
igate
figure
agv
problem
comparison
q-learning
z-learning
terms
throughtput
curve
indicates
also
advantage
z-learning
importance
sampling
-greedy
q-learning
less
pronounced
primitive
tasks
results
conclude
proposed
ex-
tensions
z-learning
outperform
state-of-the-art
learn-
ing
methods
domain
4.2
autonomous
guided
vehicle
agv
domain
second
domain
consider
variant
agv
do-
main
ghavamzadeh
mahadevan
2007
problem
agv
transport
parts
machines
ware-
house
different
parts
arrive
warehouse
uncertain
times
parts
loaded
warehouse
delivered
speciﬁc
machines
process
as-
semble
machine
terminates
avg
pick
assembly
bring
unload
location
warehouse
state
space
full
problem
nine
components
three
components
position
agv
an-
gle
one
type
part
agv
carrying
ﬁve
represent
different
parts
available
pick
warehouse
assembly
locations
convert
overall
problem
ﬁrst-exit
task
allow
new
parts
arrive
warehouse
task
assemble
parts
deliver
unload
station
details
see
supplementary
material
important
feature
problem
avg
navigate
using
transitions
corresponding
prim-
itive
actions
forward
turn-left
turn-right
stay
unlike
taxi
domain
signiﬁcantly
constrains
trajectories
required
navigate
one
location
an-
warehouse
similar
taxi
domain
de-
ﬁne
six
primitive
navigate
tasks
navigating
six
dropoff
pickup
locations
warehouse
apply
state
abstraction
form
projection
tasks
ignoring
location
parts
assemblies
figure
shows
result
different
learning
methods
navigate
tasks
similar
taxi
domain
although
q-learning
intra-task
learning
per-
forms
comparatively
better
naive
z-learning
performs
comparatively
worse
latter
result
explained
need
guided
exploration
navigating
domain
since
total
number
states
large
also
apply
state
abstraction
form
result
distribution
irrelevance
overall
task
since
navigate
tasks
always
terminate
predictable
state
necessary
maintain
value
function
dropoff
pickup
locations
also
implemented
online
algorithm
similar
maxq-q
learning
instead
using
value
function
subtasks
estimate
transition
rewards
execute
subtask
termination
recording
reward
along
way
reward
accumulated
subtask
used
observed
immediate
reward
abstract
task
performance
measure
throughput
i.e
num-
ber
assemblies
delivered
unload
station
per
time
step
figure
shows
relative
performance
z-learning
importance
sampling
q-learning
maxq-
variant
omit
naive
z-learning
since
throughput
random
walk
constant
time
number
time
steps
includes
primitive
transitions
including
navigate
subtasks
ﬁgure
shows
z-learning
con-
verges
quickly
suboptimal
policy
compared
learning
illustrating
beneﬁts
hierarchical
lmdps
conclusions
presented
framework
hierarchical
reinforce-
ment
learning
combines
maxq
decomposition
formulates
task
linearly-solvable
mdp
frame-
work
illustrated
two
domains
hi-
erarchical
intra-task
z-learning
algorithm
outperforms
state-of-the-art
methods
hierarchical
mdps
00.511.522.5x
105012345678x
104time−steperroragv
domain
primitive
tasks
q−gq−g−ilzz−isz−is−il12345x
1061.0e−035.0e−031.0e−021.5e−02time−stepthroughputagv
domain
high−level
task
q−gz−is
hierarchical
linearly-solvable
markov
decision
problems
supplementary
material
anders
jonsson
vicenc¸
g´omez
department
information
communication
technologies
universitat
pompeu
fabra
roc
boronat
138
08018
barcelona
spain
anders.jonsson
vicen.gomez
upf.edu
experimental
setup
compare
q-learning
variants
task
lmdp
construct
traditional
mdp
following
methodology
todorov
2009b
state
de-
ﬁne
symbolic
action
transition
probability
distribu-
tion
matching
optimal
·|s
computed
using
power
iteration
method
original
lmdp
also
de-
ﬁne
many
symbolic
actions
number
possible
states
following
transition
probabilities
obtained
·|s
circular
shifting
reward
symbolic
actions
λkl
·|s
||p
·|s
resulting
tradi-
tional
mdp
guaranteed
optimal
value
function
original
lmdp
following
todorov
2006
use
dynamic
learning
rates
decay
constant
optimized
separately
algorithm
cur-
rent
trial
parameter
q-learning
also
optimized
best
performance
compare
performance
calculate
iteration
cid:96
1-norm
differences
learned
optimal
value
function
a.1
taxi
domain
taxi
domain
deﬁned
grid
four
distinguished
locations
passenger
one
four
locations
passenger
wishes
transported
one
three
locations
also
taxi
must
navigate
passenger
location
pick
passenger
navigate
destination
location
put
passenger
use
variant
taxi
domain
dietterich
2000
much
larger
state
space
15×15
grid
shown
figure
state
space
composed
horizontal
vertical
coordinates
taxi
location
location
passenger
different
pickup
locations
passenger
taxi
like
dietterich
2000
apply
state
abstrac-
tion
form
projection
navigate
tasks
ignoring
passenger
location
destination
results
state
spaces
sizes
625
3,125
navigation
full
figure
taxi
domain
used
experiments
color
value
function
composite
task
navigate
task
respectively
primitive
tasks
navigate
composed
state
transitions
corresponding
navigation
actions
namely
north
south
east
west
idle
i.e
no-op
action
four
primitive
tasks
one
location
corner
grid
corresponding
lmdps
similar
grid
example
2006
passive
dynamics
random
walk
state-cost
term
zero
terminal
states
corresponding
corner
elsewhere
terms
scalability
bottleneck
algorithm
numerical
precision
required
computing
exact
op-
timal
value
function
precision
strongly
depends
absolute
difference
maximum
minimum
values
matrices
order
obtain
good
es-
composite
value
function
24681012142468101214−42−41−40−39−38−37−36−35pickup
locationsdestination
location
part
type
available
warehouse
part
type
available
warehouse
convert
overall
problem
ﬁrst-exit
task
allow
new
parts
arrive
warehouse
task
assemble
parts
deliver
unload
station
resulting
task
approximately
000
states
bottleneck
algorithm
numerical
pre-
cision
error
calculating
optimal
value
function
using
power
iteration
method
references
abbasi-yadkori
2015
abbasi-yadkori
bartlett
chen
malek
2015.
large-scale
markov
decision
problems
control
cost
application
crowdsourcing
32nd
international
conference
ma-
chine
learning
icml
2015
1053–1062
botvinick
niv
barto
2009
botvinick
niv
barto
2009.
hierarchically
organized
behav-
ior
neural
foundations
reinforcement
learning
per-
spective
cognition
113
:262–280
dietterich
1999
dietterich
1999.
state
abstraction
maxq
hierarchical
reinforcement
learning
advances
neural
information
processing
systems
nips
994–
1000
dietterich
2000
dietterich
2000.
hierarchical
rein-
forcement
learning
maxq
value
function
decom-
position
journal
artiﬁcial
intelligence
research
13:227–
303
friston
2013
friston
schwartenbeck
fitzger-
ald
moutoussis
behrens
dolan
2013.
anatomy
choice
active
inference
agency
fron-
tiers
human
neuroscience
7:598
ghavamzadeh
mahadevan
2007
ghavamzadeh
2007.
hierarchical
average
reward
mahadevan
journal
machine
learning
reinforcement
learning
research
8:2629–2669
kappen
g´omez
opper
2012
kappen
g´omez
opper
2012.
optimal
control
graphical
model
inference
problem
machine
learning
:159–
182
kappen
2005
kappen
2005.
linear
theory
con-
trol
nonlinear
stochastic
systems
physical
review
letters
95:200201
kinjo
uchibe
doya
2013
kinjo
uchibe
doya
2013.
evaluation
linearly
solvable
markov
decision
process
dynamic
model
learning
mobile
robot
navigation
task
frontiers
neurorobotics
7:1–13
matsubara
g´omez
kappen
2014
matsubara
g´omez
kappen
latent
kull-
back
leibler
control
continuous-state
systems
using
30th
conference
probabilistic
graphical
models
uncertainty
artiﬁcial
intelligence
uai
583–592
auai
press
ortega
braun
2013
ortega
braun
2013.
thermodynamics
theory
decision-making
2014.
figure
agv
domain
used
experiments
timates
one
needs
set
sufﬁciently
small
mentioned
todorov
2006
increases
required
numerical
precision
although
obtained
correct
policies
larger
grids
grids
started
numerical
prob-
lems
since
threshold
required
check
convergence
power
iteration
method
10−300
a.2
autonomous
guided
vehicle
agv
domain
second
domain
consider
variant
agv
do-
main
ghavamzadeh
mahadevan
2007
problem
agv
transport
parts
machines
ware-
house
different
parts
arrive
warehouse
uncertain
times
parts
loaded
warehouse
delivered
speciﬁc
machines
process
as-
semble
machine
terminates
avg
pick
assembly
bring
unload
location
warehouse
simpliﬁed
original
problem
reducing
num-
ber
machines
setting
processing
time
machines
make
task
fully
observable
see
fig-
ure
state
space
full
problem
following
com-
ponents
coordinate
agv
position
coordinate
agv
position
orientation
agv
right
left
num
parts
input
buffer
machine
num
parts
output
buffer
machine
num
parts
input
buffer
machine
num
parts
output
buffer
machine
information-processing
costs
proceedings
royal
society
london
volume
469
20120683.
royal
society
sutton
precup
1998
sutton
precup
1998.
intra-option
learning
temporally
abstract
ac-
tions
proceedings
15th
international
conference
machine
learning
icml
556–564
morgan
kaufman
theodorou
buchli
schaal
2010
theodorou
buchli
schaal
2010.
generalized
path
integral
journal
control
approach
reinforcement
learning
machine
learning
research
11:3137–3181
todorov
2006
todorov
2006.
linearly-solvable
markov
decision
problems
advances
neural
information
pro-
cessing
systems
nips
1369–1376
todorov
2009a
todorov
2009a
compositionality
optimal
control
laws
advances
neural
information
pro-
cessing
systems
nips
1856–1864
todorov
2009b
todorov
2009b
efﬁcient
computation
optimal
actions
proceedings
national
academy
sciences
united
states
america
106
:11478–
11483
todorov
2009c
todorov
2009c
eigenfunction
approx-
imation
methods
linearly-solvable
optimal
control
prob-
lems
proceedings
2nd
ieee
symposium
adap-
tive
dynamic
programming
reinforcement
learning
161–168
watkins
1989
watkins
1989.
learning
delayed
rewards
ph.d.
dissertation
king
college
cam-
bridge
