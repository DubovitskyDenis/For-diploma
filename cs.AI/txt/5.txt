november
2015
joint
learning
ontology
semantic
parser
text
janez
starc
a,1
dunja
mladeni
joˇzef
stefan
international
postgraduate
school
slovenia
abstract
semantic
parsing
methods
used
capturing
representing
se-
mantic
meaning
text
meaning
representation
capturing
concepts
text
may
always
available
may
sufﬁciently
complete
ontologies
provide
structured
reasoning-capable
way
model
content
collection
texts
work
present
novel
approach
joint
learning
ontology
semantic
parser
text
method
based
semi-automatic
induction
context-free
grammar
semantically
annotated
text
grammar
parses
text
semantic
trees
grammar
semantic
trees
used
learn
ontology
several
levels
classes
instances
taxonomic
non-taxonomic
relations
approach
evaluated
ﬁrst
sentences
wikipedia
pages
describing
people
keywords
ontology
learning
semantic
parsing
grammar
induction
context-free
grammar
introduction
one
ultimate
goals
natural
language
processing
nlp
machine
reading
automatic
unsupervised
understanding
text
one
way
pursuing
machine
reading
semantic
parsing
transforms
text
meaning
representation
however
capturing
meaning
ﬁnal
goal
meaning
representation
needs
predeﬁned
structured
way
supports
reasoning
ontologies
provide
common
vocabulary
meaning
representations
support
reasoning
vital
understanding
text
enable
ﬂexibility
encountering
new
concepts
relations
text
machine
reading
want
able
learn
extend
ontology
reading
traditional
methods
ontology
learning
2,3
concerned
discovering
salient
concepts
text
thus
work
macro-reading
fashion
goal
extract
facts
large
collection
texts
necessarily
opposed
micro-reading
fashion
goal
extract
every
fact
input
text
semantic
parsers
operate
micro-reading
fashion
consequently
ontologies
salient
concepts
enough
semantic
parsing
fur-
thermore
traditional
methods
learn
ontology
particular
domain
text
used
tool
hand
ontologies
used
tool
repre-
sent
meaning
semantic
parsing
setting
developing
semantic
parser
1corresponding
author
janez
starc
joˇzef
stefan
international
postgraduate
school
jamova
1000
ljubljana
slovenia
e-mail
janez.starc
ijs.si
november
2015
trivial
get
best
meaning
representation
observed
text
especially
content
known
yet
semantic
parsing
datasets
created
either
selecting
texts
expressed
given
meaning
representation
like
free917
dataset
manually
deriving
meaning
representation
given
text
like
atis
dataset
datasets
unit
text
corresponding
meaning
representation
free917
uses
freebase
big
multi-domain
ontology
possible
represent
arbitrary
sentence
freebase
existing
ontology
paper
propose
novel
approach
joint
learning
ontology
seman-
tic
parsing
designed
homogeneous
collections
text
fact
usually
stated
therefore
rely
data
redundancy
approach
text-driven
semi-automatic
based
grammar
induction
presented
fig-
ure
1.the
input
seed
ontology
together
text
annotated
concepts
seed
ontology
result
process
ontology
extended
instances
classes
taxonomic
non-taxonomic
relations
semantic
parser
transform
basic
units
text
i.e
sentences
semantic
trees
compared
trees
structure
sentences
based
syntactic
information
nodes
semantic
trees
contain
semantic
classes
like
location
profession
color
etc
approach
rely
syntactic
analysis
text
like
part-of-speech
tagging
dependency
parsing
grammar
induction
method
works
premise
curriculum
learning
parser
ﬁrst
learns
parse
simple
sentences
proceeds
learn
complex
ones
induction
method
iterative
semi-automatic
based
frequent
patterns
context-free
grammar
cfg
induced
text
represented
several
layers
semantic
annotations
motivation
use
cfg
suitable
proposed
alternating
usage
top-down
bottom-up
parsing
new
rules
induced
previously
un-
parsable
parts
furthermore
shown
cfgs
expressive
enough
model
almost
every
language
phenomena
induction
based
greedy
iter-
ative
procedure
involves
minor
human
involvement
needed
seed
rule
deﬁnition
rule
categorization
experiments
show
although
grammar
ambiguous
scalable
enough
parse
large
dataset
sentences
grammar
semantic
trees
serve
input
new
ontology
classes
instances
taxonomic
relations
constructed
grammar
also
propose
method
discovering
less
frequent
instances
classes
supervised
method
learn
relations
instances
methods
work
semantic
trees
experimentation
ﬁrst
sentences
wikipedia
pages
describing
people
taken
dataset
sentences
already
annotated
links
pages
also
instances
dbpedia
knowledge
base
using
relations
dbpedia
training
set
several
models
predict
relations
trained
evaluated
rest
paper
organized
following
way
grammar
induction
ap-
proach
presented
section
ontology
induction
approach
follows
section
section
present
conducted
experiments
grammar
induction
instance
relation
extraction
examine
related
work
section
conclude
discussion
section
november
2015
figure
proposed
approach
ontology
grammar
induction
gets
collection
text
seed
ontology
input
outputs
new
ontology
collection
text
ﬁrst
annotated
different
levels
including
annotations
concepts
existing
ontology
annotated
text
used
grammar
induction
text
represented
semantic
trees
used
together
grammar
induce
new
ontology
grammar
induction
section
propose
semi-automatic
bootstrapping
procedure
grammar
in-
duction
searches
frequent
patterns
constructs
new
production
rules
one
main
challenges
make
induction
way
minimizes
human
involvement
maximizes
quality
semantic
trees
input
process
illustrated
figure
set
predeﬁned
seed
grammar
rules
see
section
2.5
sample
sentences
layered
representation
see
section
2.1
dataset
output
process
larger
set
rules
forming
induced
grammar
one
rule
added
grammar
iteration
beginning
iteration
sentences
parsed
top-down
parser
output
parsing
single
sentence
semantic
tree
set
semantic
nodes
connected
tree
distinguish
two
possible
outcomes
parsing
sentence
completely
parsed
ﬁnal
goal
least
one
part
sentence
parsed
perspective
parser
second
scenario
happens
node
parsed
rules
name
nodes
null
nodes
serve
input
next
step
rule
induction
step
several
rules
constructed
generalization
null
nodes
generalization
see
section
2.4
based
utilization
semantic
annotations
bottom-up
composition
existing
rules
induced
rules
rule
highest
frequency
one
generalized
highest
number
null
nodes
added
grammar
improve
quality
grammar
rules
marked
called
property
instructs
parser
use
rule
eg.
parsing
induction
property
vitally
affects
result
parsing
following
iterations
potentially
causing
huge
semantic
drift
rest
process
consequently
human
user
needs
mark
seedontologyinduced
ontologylayered
text
representationgrammarsemantic
treesannotationinductionsemantic
parsinginstance
relation
extractionclass
instance
taxonomy
extractionextended
november
2015
property
rule
iterative
process
runs
predeﬁned
stopping
criteria
met
criteria
either
connected
quality
grammar
time
limitation
sake
transparency
experiments
human
involved
be-
ginning
seed
rules
created
later
rule
properties
speciﬁed
however
another
setting
user
could
also
deﬁne
new
rules
middle
bootstrapping
procedure
following
sections
describe
component
process
details
figure
grammar
induction
2.1.
textual
data
representation
input
textual
data
needs
properly
structured
order
work
best
pro-
posed
algorithms
shallow
nlp
tools
like
sentence
splitting
word
tokenization
named
entity
recognition
might
help
obtaining
structure
basic
unit
sentence
rep-
resented
several
layers
example
presented
table
layer
consists
several
tokens
span
one
words
basic
layer
lexical
layer
token
represents
single
word
layers
created
annota-
tions
annotations
like
named-entities
may
span
several
words
words
may
annotation
thus
given
null
token
crucial
algorithms
aware
deal
particular
layer
instance
parser
must
break
apart
multi-word
annotation
layers
may
derived
others
using
seed
ontology
example
instance
layer
contains
annotations
instances
ontology
derived
class
layer
represents
classes
annotations
also
ontology
annotation
layers
valuable
provide
good
means
generalization
connection
ontology
term
subpart
sentence
deﬁned
starting
ending
position
sentence
different
interpretation
parsingrule
inductionproperty
assignmentgrammarlayered
text
representationseed
rulesunparsablenodesbest
ruleincompletebest
rulesemantic
trees
november
2015
tokens
layer
lexical
small-caps
named-entity
person
musician
musician
phil
madeira
phil
madeira
nashville
nashville
location
nashville
tennessee
location
class
table
layered
representation
sentence
null
tokens
expressed
musical
artist
phil
madeira
profession
instance
person
layer
interpretation
breaks
tokens
valid
instance
term
representing
madeira
valid
named-entity
layer
table
breaks
person
2.2.
grammar
deﬁnition
context-free
grammar
deﬁned
5-tuple
set
non-terminals
non-terminal
represents
semantic
class
e.g
cid:104
person
cid:105
cid:104
color
cid:105
cid:104
organization
cid:105
also
universal
non-terminal
cid:104
cid:105
replaced
non-terminal
non-terminal
replaces
occurrences
rule
used
represent
several
rules
notation
grammar
still
context-free
see
seed
rule
examples
section
2.5
set
terminals
terminal
existing
non-null
token
sentence
layer
denote
terminal
value
layer
instance
location
named-entity
phil
madeira
instance
terminal
lexical
layer
layer
skipped
denotation
set
production
rules
represents
relation
example
relation
person
life
role
starting
non-terminal
symbol
since
non-terminals
represent
semantic
classes
starting
symbol
chosen
based
semantic
class
input
ex-
amples
input
examples
sentences
appropriate
category
may
cid:104
relation
cid:105
input
examples
noun
phrases
starting
symbol
may
speciﬁc
category
like
cid:104
job
title
cid:105
set
properties
positive
neutral
negative
non-inducible
property
controls
usage
rule
parsing
rule
induction
phase
details
given
following
subsections
2.3.
parser
parsing
recursive
descent
parser
backtracking
developed
top-
parser
ﬁrst
looks
higher
level
sentence
structure
proceeds
parse
tree
identify
low
level
details
sentence
advantage
top-
parsing
ability
partially
parse
sentences
detect
unparsable
parts
sentences
parser
takes
layered
sentence
input
returns
semantic
tree
output
see
figure
recursive
structure
program
closely
follows
structure
november
2015
parse
tree
recursive
function
parse
see
algorithm
takes
term
non-terminal
input
returns
parse
node
output
parse
node
contains
class
node
non-terminal
rule
parsed
node
term
list
children
nodes
order
rule
parse
node
left-hand
side
must
match
input
non-terminal
right-hand
side
must
match
layered
input
pattern
matching
function
match
line
right
hand
side
rule
treated
like
regular
expression
non-terminals
present
wildcard
characters
match
least
one
word
terminals
treated
literal
characters
matched
layer
deﬁnes
result
successfully
matched
pattern
list
terms
term
represents
non-terminal
pattern
due
ambiguity
pattern
matching
might
several
matches
term
non-terminal
pair
every
list
parse
function
recursively
called
line
left
side
child
nodes
size
term
list
pattern
right
hand
side
ambiguous
lists
match
pattern
foreach
term
list
ambiguous
lists
parse
phrase
non-terminal
output
parse
node
nodes
foreach
rule
grammar
nodes
empty
ﬁnal
node
node
type
null
else
ﬁnal
node
argmaxn∈nodes
ﬁnal
node
fully
parsed
return
ﬁnal
node
add
ﬁnal
node
induction
nodes
add
node
type
child
nodes
nodes
child
node
parse
term
listi
pattern.non
terminalsi
add
child
node
child
nodes
algorithm
pseudocode
main
function
parse
top-down
parser
since
grammar
ambiguous
term
parsed
multiple
ways
two
types
ambiguity
two
rules
expand
term
one
rule
expand
term
one
way
ambiguity
one
node
created
best
node
according
reliability
measure
selected
result
line
reliability
measure
1−t
node
fully
parsed
node
partially
parsed
|c|·
c∈c
c∈c
|c|
node
null

november
2015
figure
example
semantic
tree
result
parsing
input
table
tree
fully
parsed
node
three
rows
class
rule
term
interpreted
lexical
layer
trigger
probability
rule
parsed
node
predeﬁned
weight
set
children
|c|
length
term
node
trigger
probability
rule
probability
right-hand
side
rule
pattern
matches
random
term
dataset
estimated
rule
induced
range
measure
measure
deﬁned
way
text
node
parses
higher
reliability
second
summand
middle
row
hand
nodes
rules
frequently
matched
lower
reliability
penalizes
rules
loosely
deﬁned
ﬁrst
summand
middle
row
parameter
set
0.05
using
grid
search
average
score
relation
extraction
experiment
section
4.4
performance
measure
none
rules
match
term
null
node
created
added
list
nodes
later
used
grammar
induction
line
note
even
null
node
discarded
reliable
still
used
grammar
induction
step
node
fully
parsed
node
descendants
parsed
node
parsed
least
one
descendants
parsed
node
partially
parsed
nodes
fully
parsed
added
list
induction
since
ambiguity
grammar
may
make
parsing
computationally
infeasible
several
optimization
techniques
used
memoization
used
reduce
com-
plexity
exponential
time
length
sentence
parser
support
productions
mainly
grammar
induction
produce
patterns
contain
terminals
ambiguous
two
non-terminals
allowed
maximal
length
term
corresponds
ﬁrst
non-terminal
three
tokens
argue
huge
limitation
since
relation
relation
person
liferole
phil
madeira
musician
nashville
person
person
person
named-entity
phil
madeira
liferole
musician
nashville
liferole
liferole
liferole
location
musician
nashville
liferole
liferole
profession
class
musician
location
location
location
class
nashville
november
2015
way
human
languages
structured
usually
two
longer
terms
connected
word
like
comma
verb
furthermore
way
induction
works
connectors
get
generalized
become
terminal
rule
at-
tempt
introduce
rules
negative
property
whenever
rule
fully
parses
node
indicates
current
parsing
path
incorrect
allows
parser
back-
track
sooner
also
prevents
adding
null
sister
nodes
null
sister
nodes
case
usually
wrong
rule
induction
however
turned
negative
rules
actually
slow
parsing
since
grammar
gets
bigger
better
mark
rules
neutral
therefore
added
grammar
2.4.
rule
induction
goal
rule
induction
step
convert
null
nodes
parsing
step
rules
rules
frequent
one
promoted
term
null
node
generalized
form
right
side
rule
class
non-terminal
null
node
present
left
side
rule
recently
induced
rule
parse
nodes
induced
following
iterations
additionally
rules
may
parse
children
nodes
generalization
generalization
done
two
steps
first
terms
generalized
layer
level
output
process
sequence
tokens
might
different
layers
position
term
single
layer
selected
according
predeﬁned
layer
order
beginning
term
generalized
ﬁrst
layer
non-null
tokens
layer
taken
part
generalized
term
positions
term
generalized
attempted
generalized
next
layer
etc
last
layer
without
null-tokens
therefore
position
term
assigned
layer
usually
lexical
layer
example
top
part
table
shows
generalization
term
table
layer
list
constructed
manually
good
layers
generalization
typically
express
semantic
classes
individual
terms
preferably
types
general
loss
information
speciﬁc
larger
grammar
next
step
generalization
tokens
generalized
using
greedy
bottom-up
parser
using
rules
grammar
right
sides
rules
matched
input
token
term
match
matched
sub-term
re-
placed
left
side
rule
actually
iteration
disjunct
matches
replaced
get
disjunct
matches
overlapping
matches
discarded
greed-
ily
longer
matches
priority
process
repeated
rules
match
term
example
presented
lower
part
table
bottom-up
parsing
algorithm
needs
fast
number
unexpanded
nodes
high
due
ambiguities
top-down
parsing
consequently
al-
gorithm
greedy
instead
exhaustive
yields
one
result
aho-corasick
string
matching
algorithm
selected
matching
ability
match
rules
si-
multaneously
like
top-down
parser
parser
generates
partial
parses
bottom-up
parser
never
fully
parse
output
non-terminal
type
unexpanded
node
would
generate
cyclical
rule
i.e
class
class
however
never
happens
top-down
parser
would
already
expand
null
node
november
2015
layered
generalization
profession
location
small-caps
small-caps
class
small-caps
class
bottom-up
parsing
small-caps
small-caps
life
role
non-terminal
person
class
person
class
table
two
step
generalization
term
table
phil
madiera
musician
nashville
layer
list
constructed
reverse
order
table
example
rule
life
role
profession
location
rule
used
bottom-up
parsing
property
assignment
last
step
iteration
assigning
property
newly
induced
rule
property
controls
role
rule
parsing
induction
default
property
positive
deﬁnes
default
behavior
rule
procedures
rules
neutral
prop-
erty
used
procedure
also
re-induced
rules
good
parsing
may
introduce
errors
induction
rules
given
non-
inducible
property
instance
rule
date
number
candidate
non-inducible
property
since
years
represented
single
number
contrary
every
number
date
experiments
assignment
done
manually
human
user
sees
induced
rule
examples
null
nodes
induced
provide
enough
information
user
decide
seconds
property
assign
stopping
criteria
met
iterative
procedure
continue
automati-
cally
assigning
positive
property
rule
initial
experimenting
showed
single
mistake
assignment
cause
huge
drift
making
rules
wrong
2.5.
seed
rules
start
list
seed
rules
may
needed
order
grammar
induction
successful
since
step
done
manually
reasonable
list
seed
rules
short
efﬁcient
seed
rules
divided
three
groups
domain
independent
linguistic
rules
class
rules
top-level
domain
rules
domain
independent
linguistic
rules
relation
relation
parse
top
mid-level
nodes
applied
many
different
datasets
class
rules
connect
class
tokens
like
named-entity
tokens
non-terminals
example
location
location
named-entity
date
date
named-entity
location
location
class
film
film
class
parse
leaf
nodes
trees
hand
top-level
domain
rules
deﬁne
basic
structure
sentence
example
relation
person
life
role
november
2015
name
suggests
parse
nodes
close
root
altogether
rule
groups
parse
levels
tree
may
already
enough
parse
basic
sen-
tences
importantly
provide
basis
learning
parse
complex
sentences
decision
many
seed
rules
deﬁned
relies
human
judgment
whether
current
set
seed
rules
powerful
enough
ignite
bootstrap-
ping
procedure
judgment
may
supported
running
one
iteration
inspecting
top
induced
rules
ontology
induction
section
describes
utilize
grammar
manipulate
semantic
trees
dis-
cover
ontology
components
textual
data
3.1.
ontology
induction
grammar
propose
procedure
mapping
grammar
components
ontology
components
particular
classes
instances
taxonomic
relations
extracted
first
distinguish
instances
classes
grammar
classes
represented
non-terminals
terminals
come
layer
populated
classes
example
named-entity
layer
class
layer
table
instances
might
already
exist
instance
layer
created
rules
whose
right
hand
side
contains
tokens
lexical
layer
tokens
represent
label
new
instance
instance
rule
profession
software
engineer
candidate
in-
stance
extraction
furthermore
distinguish
class
instance
rules
class
rules
single
symbol
representing
class
right-hand
side
class
rules
map
subclassof
relations
ontology
rule
positive
class
right
side
sub-
class
class
left
side
instance
rule
organization
company
yields
relation
subclassof
company
organization
hand
instance
rules
one
symbols
representing
instance
right
side
deﬁne
isa
relation
rule
positive
instance
right
side
member
class
left
side
instance
rule
profession
software
engineer
yields
relation
isa
softwareengineer
profession
class
instance
rule
neutral
relation
treated
false
note
many
relations
may
inferred
combing
newly
induced
relations
relations
seed
ontology
instance
induced
relation
subclassof
new-class
seed-class
seed
relation
isa
seed-class
seed-instance
used
infer
new
relation
isa
new-class
seed-instance
section
described
discover
relations
taxonomic
level
next
section
describe
discover
relations
instances
3.2.
relation
extraction
semantic
trees
propose
method
learning
relations
semantic
trees
tries
solve
problem
classical
relation
extraction
methods
given
dataset
positive
relation
examples
represent
one
relation
type
e.g
birthplace
goal
discover
new
unseen
relations
november
2015
semantic
tree
training
entity
nodes
semantic
tree
train-
ing
en-
tity
nodes
variable
sub-tree
present
positions
relation
figure
given
relation
semantic
tree
relation
semantic
tree
variable
sub-tree
extracted
method
based
assumption
relation
entities
expressed
shortest
path
semantic
tree
input
training
sentences
layered
representation
corresponding
parse
trees
relation
examples
given
relation
training
set
ﬁrst
try
identify
sentence
containing
entity
relation
relation
one
two
even
entities
entity
matched
layer
corresponds
entity
type
example
strings
matched
lexical
layer
ontology
entities
matched
layer
containing
entities
result
successfully
matched
entity
sub-term
sentence
next
step
corresponding
semantic
tree
searched
node
contains
sub-term
point
entity
corresponding
entity
node
otherwise
relation
discarded
learning
process
given
entity
nodes
minimum
spanning
tree
containing
extracted
one
entity
node
resulting
subtree
path
node
root
node
extracted
sub-tree
con-
verted
variable
tree
different
semantic
trees
variable
sub-
trees
example
see
figure
semantic
nodes
sub-tree
converted
variable
nodes
retaining
class
rule
node
well
places
children
original
tree
entity
nodes
also
position
relation
memo-
rized
variable
tree
extracted
relation
positive
example
training
process
negative
examples
sub-trees
present
relations
converted
variable
trees
variable
node
represents
one
feature
therefore
classiﬁcation
algorithm
logistic
regression
used
training
predicting
possible
sub-trees
semantic
tree
predicted2
sub-
tree
predicted
positive
terms
leaf
nodes
represent
arguments
relation
2the
number
leaf
nodes
sub-trees
must
match
number
arguments
relation
also
relation
two
mode
arguments
predicted
several
times
time
different
position
numbers
entity
nodes
a2a1b2b1
november
2015
experiments
section
present
experiments
evaluating
proposed
approach
con-
ducted
experimentation
wikipedia–dbpedia
dataset
section
4.1
first
in-
duced
grammar
wikipedia
dataset
section
4.2
present
characteristics
scalability
approach
next
experiment
present
method
discover-
ing
less
prominent
instances
section
4.3
last
experiment
demonstrates
one
appli-
cation
semantic
parsing
supervised
learning
dbpedia
relations
section
4.4
4.1.
datasets
datasets
experiments
constructed
english
wikipedia
knowledge
bases
dbpedia
freebase
dbpedia
provides
structured
information
wikipedia
articles
scraped
infoboxes
first
sentences
wikipedia
pages
describing
people
taken
textual
dataset
dbpedia
relations
ex-
pressing
facts
people
taken
dataset
supervised
relation
learning
note
dbpedia
instance
wikipedia
page
set
person
in-
stances
identiﬁed
querying
dbpedia
instances
person
class
textual
dataset
wikipedia
pages
representing
entities
parsed
in-
house
wikipedia
markup
parser3
convert
markup
plain
text
furthermore
links
wikipedia
pages
retained
example
sentence
plain
text
victor
francis
hess
june
1883
december
1964
austrian-american
physicist
nobel
laureate
physics
discovered
cosmic
rays.
using
standford
opennlp
plain
texts
obtained
sentence
token
splits
named-entity
annotation
notice
ﬁrst
sentence
page
retained
converted
proposed
layered
representation
see
section
2.1
layered
representation
contains
ﬁve
layers
lexical
plain
text
named-entity
named
entity
recognizer
wiki-link
wikipedia
page
link
dbpedia
instance4
dbpedia-class
class
wikipedia
page
dbpedia
freebase-class
class
wikipedia
page
freebase
freebase
also
contains
classes
wikipedia
pages
last
two
layers
might
several
classes
per
wikipedia
page
one
selected
using
short
priority
list
classes
none
categories
list
category
chosen
random
comparing
dbpedia-class
freebase-class
layers
freebase-class
utilized
experiments
wiki-link
tokens
class
freebase-class
layer
dbpedia-class
layer
almost
1.1
million
sentences
collection
average
length
sentence
18.3
words
median
length
13.8
words
2.3
links
per
sentence
dataset
supervised
relation
learning
contains
relations
person
instance
appears
subject
dbpedia
relation
example
dbpedia
victor
francis
hess
dbpedia-owl
birthdate
1883-06-24
3the
markup
parsing
wikipedia
markup
non-trivial
extract
plain
text
many
elements
like
images
tables
discarded
well
elements
appear
middle
sentence
like
pronunciation
citation
4each
wikipedia
page
corresponding
dbpedia
instance
november
2015
119
different
relation
types
unique
predicates
rela-
tions
million
relations
since
dbpedia
freebase
available
rdf
format
used
rdf
store
querying
storage
existing
new
relations
4.2.
grammar
induction
experiments
grammar
induced
10.000
random
sentences
taken
dataset
described
section
4.1.
first
list
seed
nodes
constructed
domain
in-
dependent
linguistic
rules
category
rules
top-level
rules
property
assign-
ment
done
authors
every
iteration
best
rule
shown
together
number
nodes
induced
ten
nodes
together
sentences
appear
goal
set
stop
iterative
process
two
hours
believe
right
amount
time
still
expect
quality
feedback
human
user
689
new
rules
created
sample
presented
table
ta-
ble
presents
distributions
properties
around
rules
used
parsing
non
neutral
rules
together
seed
rules
297
rules
used
parsing
different
properties
evenly
dispersed
across
iterations
using
procedure
conversion
grammar
rules
taxonomy
presented
section
classes
subclassof
relations
instances
isa
relations
generated
grammar
also
tested
parsing
sample
100.000
test
sentences
statistic
presented
table
quarter
sentences
fully
parsed
meaning
null
leaf
nodes
coverage
represents
fraction
words
sentence
parsed
words
null-nodes
number
operations
shows
many
times
parse
function
called
parsing
sentences
highly
correlated
time
spend
parsing
sentence
average
0.16ms
measurement
done
single
cpu
core
consequently
feasible
parse
collection
million
sentences
like
dataset
statistics
also
calculated
training
set
numbers
similar
test
set
fully
parsed
coverage
even
slightly
lower
test
set
statistics
calculated
iteration
non
neutral
rule
created
graphs
figure
show
statistics
changed
course
grammar
induction
graph
shows
coverage
fraction
fully
parsed
sentences
correlated
grow
rapidly
beginning
growth
starts
slow
indicates
long
tail
unparsed
nodes/sentences
following
section
present
concept
learning
method
deals
long
tail
furthermore
number
operations
per
sentence
also
slows
see
graph
number
rules
gives
positive
sign
retaining
computational
feasibility
growth
grammar
graph
somewhat
elaborates
dynamics
grammar
induction
earlier
phase
induction
many
rules
deﬁne
upper
structure
tree
induced
rules
rapidly
increase
depth
number
null
nodes
like
rule
rule
also
explain
spikes
graph
addition
grammar
causes
rules
emerge
top
list
signiﬁcantly
higher
frequency
rules
induced
frequency
gets
back
previous
values
slowly
decreases
long
run
5rule1
liferole
liferole
location
rule2
liferole
liferole
organization
november
2015
number
101
201
301
401
501
601
rule
personattr
born
date
liferole
born
location
location
location
date
person
location
date
orderofchivalry
personattr
liferole
action
event
none
date
university
property
none
none
neutral
neutral
neutral
neutral
table
sample
induced
rules
grammar
positive
rules
non-inducible
rules
neutral
rules
parsing
fully
parsed
sentences
avg
coverage
avg
tree
depth
avg
number
leaf
nodes
avg
number
null
leaf
nodes
avg
number
operations
avg
parsing
time
231
437
25.63
78.52
6.96
6.69
1.98
320.3
0.16
table
statistics
test
set
4.3.
instance
extraction
section
present
experiment
method
discovering
new
instances
appear
long
tail
null
nodes
note
majority
instances
already
placed
ontology
method
section
3.1.
less
prominent
in-
stances
extracted
increase
coverage
semantic
parsing
term
class
null
node
form
isa
relation
class
node
represents
class
relation
terms
converted
instances
ﬁrst
generalized
layer
level
see
section
2.1
goal
exclude
non-atomic
terms
repre-
sent
instances
therefore
terms
consisting
one
wiki-link
token
exclusively
lexical
tokens
retained
relations
sorted
according
frequency
observe
accuracy
relations
drops
frequency
therefore
relations
occurred
less
three
times
excluded
number
accuracy
six
classes
reported
table
classes
less
accurate
class
accuracy
manually
evaluated
random
sample
100
instance
relations
taking
account
estimated
accuracy
13.000
correct
isa
relations
4.4.
relation
extraction
section
present
experiment
relation
extraction
methods
presented
section
3.2.
input
supervision
dbpedia
relation
dataset
sec-
tion
4.1.
subject
ﬁrst
argument
every
relation
person
dbpedia
instance
november
2015
coverage/fully
parsed
tree
dimensions
operations
rule
frequencies
figure
statistics
iterative
process
class
relations
accuracy
life
role
person
order
chivalry
date
action
field
study
15356
5427
1319
1310
967
table
instance
relations
per
category
person
wikipedia
page
beginning
ﬁrst
sentence
wikipedia
page
identiﬁed
textual
dataset
object
last
argument
relation
matches
sub-term
sentence
relation
eligible
experiments
distinguish
three
types
values
objects
dbpedia
resources
matched
wiki-link
layer
dates
get
converted
format
used
english
wikipedia
matched
lexical
layer
string
objects
relation
types
200
eligible
relations
retained
119
relations
macro
average
number
eligible
relations
per
relation
type
17.7
micro
average
23.8
meaning
roughly
quarter
dbpedia
person
relations
expressed
ﬁrst
sentence
wikipedia
page
50100150200250rules0.30.40.50.60.70.8coveragecoverage/fully
parsedcoveragefully
parsed0.000.050.100.150.200.250.30fully
parsed50100150200250rules2.02.22.42.62.83.03.23.4null
leaf
nodesrule
1rule
2tree
dimensionsnull
leaf
nodestree
depth4.55.05.56.06.57.07.5tree
depth50100150200250rules050100150200250300350number
operationsoperations50100150200250rules101102103104frequencyrule
frequencies
november
2015
model
basic
net
lrc
lrcl
precision
72.2
61.6
71.9
72.4
76.2
table
performance
various
relation
extraction
models
converted
recall
48.3
77.6
84.2
84.4
85.5
converted
52.1
63.3
77.0
77.3
80.0
recall
28.2
42.5
50.2
50.3
50.7
36.4
46.9
55.5
55.7
57.1
rest
section
stated
averages
micro-averages
prediction
problem
designed
following
way
given
predicate
re-
lation
type
ﬁrst
argument
relation
person
model
predicts
sec-
ond
argument
relation
object
relations
functional
like
instance
child
relation
several
values
per
predicate–person
pair
average
1.1.
since
one
argument
relation
predicted
variable
trees
pre-
sented
section
3.2
paths
root
single
node
analysis
variable
tree
extraction
shows
average
60.8
eligible
relations
successfully
con-
verted
variable
trees
object
term
exactly
matches
term
node
others
converted
8.2
terms
split
nodes
30.9
terms
sub-terms
nodes
instead
complete
terms
measuring
diversity
variable
trees
shows
distinct
variable
tree
appeared
2.7
times
average
several
models
based
variable
trees
trained
solving
classiﬁcation
problem
basic
basic
model
model
contains
positive
trained
variable
trees
prediction
test
variable
tree
matches
one
trees
model
example
predicted
positive
net
automaton
model
positive
variable
trees
paths
start
end
points
model
merged
net
acts
deterministic
automaton
automaton
accepts
test
variable
tree
predicted
positive
example
automaton
model
presented
figure
logistic
regression
logistic
regression
model
trained
positive
negative
examples
nodes
variable
trees
represents
features
lrc
logistic
regression
context
nodes
leaf
nodes
siblings
nodes
variable
tree
added
model
lrcl
logistic
regression
context
nodes
lexical
tokens
tokens
lexical
layer
entity
nodes
added
lrc
features
training
maximum
10.000
eligible
relations
taken
relation
types
10-fold
cross
validation
performed
evaluation
re-
sults
presented
table
converted
recall
converted
score
presents
re-
call
converted
examples
one
relations
success-
fully
converted
variable
trees
performance
increases
model
however
interpretability
decreases
also
compared
method
conditional
random
ﬁelds
crf
crf
method
tokens
layers
window
size
taken
features
sequence
prediction
converted
examples
crf
achieved
score
80.8
comparable
best
model
lrcl
score
80.0.
november
2015
figure
automaton
model
institution
relation
bottom
node
fraction
training
variable
trees
contain
node
displayed
related
work
many
known
approaches
ontology
learning
semantic
parsing
however
best
knowledge
ﬁrst
work
jointly
learn
ontology
seman-
tic
parser
following
sections
make
comparisons
work
semantic
parsing
ontology
learning
grammar
induction
others
5.1.
semantic
parsing
goal
semantic
parsing
map
text
meaning
representations
several
ap-
proaches
used
combinatory
categorial
grammar
ccg
lambda
calculus
meaning
representation
16,17
ccg
grammar
closely
connects
syntax
semantics
lexicon
entry
consist
term
syntactical
category
lambda
statement
similarly
context-free
grammar
contains
production
rules
rules
contain
lexical
tokens
grammar
lexicalized
gives
ability
express
relations
single
rule
instance
parse
jazz
drummer
rule
musician
type
musical
genre
musician
type
used
directly
express
relation
determines
genre
musician
lambda
calculus
may
provide
formal
meaning
representation
semantic
trees
lexicon
ccg
requires
http
//dbpedia.org/ontology/institution
organization
organization
named-entity
0.697674
relation
relation
.1.000000
relation
person
liferole
0.813953
relation
person
liferole
0.069767
relation
person
liferole
0.116279
liferole
liferole
organization
0.813953
0.488372
0.232558
person
person
personattr
0.116279
organization
organization
class
0.186047
liferole
liferole
organization
0.069767
person
person
order_of_chivalry
0.116279
order_of_chivalry
organization
0.116279
november
2015
mappings
lambda
statements
approaches
use
dependency-based
compositional
semantics
ungrounded
graphs
etc
meaning
representations
early
semantic
parsers
trained
datasets
geoquery
atis
map
sentences
domain-speciﬁc
databases
later
datasets
question
an-
swering
based
freebase
created
free917
webquestions
datasets
contain
short
questions
multiple
domains
since
meaning
repre-
sentations
formed
freebase
concepts
allow
reasoning
freebase
on-
tology
much
richer
databases
geoquery
atis
datasets
constructed
either
forming
sentences
given
meaning
representation
vice-
versa
consequently
systems
trained
evaluated
datasets
might
work
sentences
represented
underlying
ontology
overcome
limitation
developed
open
vocabulary
semantic
parser
approach
uses
ccg
parser
questions
labmda
statements
besides
freebase
vocabulary
contain
underspeciﬁed
predicates
lambda
statements
together
answers
freebase
entities
used
learn
low-dimensional
probabilistic
database
used
answer
ﬁll-in-the-blank
natural
language
questions
similar
fashion
deﬁnes
underspeciﬁed
entities
types
relations
corresponding
concept
exist
freebase
contrast
purpose
method
identify
new
concepts
ground
ontology
5.2.
ontology
learning
many
ontology
learning
approaches
address
ontology
components
ap-
proach
however
goal
learn
salient
concepts
particular
domain
goal
learn
concepts
including
instances
like
particular
organiza-
tions
used
meaning
representation
survey
sum-
marizes
learning
mechanisms
based
either
statistics
linguistics
logic
approach
unique
part
ontology
constructed
grammar
many
approaches
use
lexico-syntactic
patterns
ontology
learning
often
based
dependency
parses
like
3,24
approach
rely
linguistic
preprocessing
makes
suitable
non-standard
texts
poorly
resourced
languages
ap-
proach
also
build
patterns
however
form
grammar
rules
instead
lexico-syntactic
patterns
contain
linguistic
classes
approach
models
semantic
patterns
contain
semantic
classes
like
person
color
patterns
constructed
ad-
vance
sometimes
difﬁcult
constructor
always
aware
phenomena
expressed
input
text
approach
allows
create
small
number
seed
patterns
advance
explore
patterns
process
gram-
mar
learning
similar
bootstrapping
semi-automatic
approach
ontology
learning
developed
user
validates
lexicalizations
particular
relation
learn
new
instances
user
validates
newly
identiﬁed
terms
approach
user
validates
grammar
rules
learn
composition
whole
sentences
similar
approach
combining
dbpedia
wikipedia
superised
learning
taken
however
focus
lexicalization
relations
classes
5.3.
grammar
induction
goal
develop
semi-automatic
method
induces
grammar
suitable
scenario
ontology
extracted
text
parsed
semantic
trees
november
2015
survey
compares
several
papers
grammar
induction
according
classiﬁcation
method
falls
unsupervised
text-based
negative
examples
sentences
methods
many
methods
induce
context-free
grammars
however
focus
learning
syntactic
structures
rather
semantic
evident
evaluation
strategies
parse
trees
compared
golden
parse
trees
treebanks
like
penn
treebank
annotated
according
syntactic
policies
furthermore
grammar
limited
speciﬁc
form
like
instance
chom-
sky
normal
form
greibach
normal
form
instead
may
contain
arbitrary
context-free
rules
several
algorithms
like
employ
greedy
strategy
grammar
induction
grammar
updated
best
decision
step
whereas
method
adds
rule
sentences
parsed
incremental
parsing
algorithm
up-
dates
grammar
sentence
also
done
adios
method
shown
order
sentences
affects
grammar
method
employs
frequency
analysis
human
supervision
control
grammar
construction
others
use
minimum
description
length
principle
clustering
sequences
signiﬁcance
word
co-occurrences
5.4.
approaches
related
work
linking
short
terms
ontology
concepts
designed
similarly
approach
terms
bootstrapping
procedure
induce
patterns
instead
induc-
ing
context-free
grammar
production
rules
suggestions
rewrite
rules
transform
text
directly
ontology
language
provided
another
bootstrapping
semi-automatic
approach
developed
knowledge
base
population
task
knowledge
base
population
concerned
extracting
instances
relations
given
on-
tology
work
also
extract
backbone
ontology
classes
taxo-
nomic
relations
also
many
approaches
focus
one
aspect
knowledge
extraction
like
taxonomy
extraction
37,38
relation
extraction
14,39
combining
approaches
lead
cumbersome
concept
matching
problems
problem
also
observed
system
ontousp
tries
overcome
unsupervised
inducing
populating
probabilistic
grammar
solve
question
answering
problem
however
result
logical-form
clusters
connected
isa
hierarchy
grounded
concepts
connected
existing
ontology
discussion
presented
approach
joint
ontology
learning
semantic
parsing
approach
evaluated
building
ontology
representing
biographies
people
ﬁrst
sentences
person
wikipedia
pages
combination
dbpedia
freebase
used
dataset
dataset
suitable
approach
text
equipped
human
tagged
annotations
already
linked
ontology
cases
named
entity
disambiguation
would
needed
obtain
annotations
next
trait
dataset
suitable
approach
homogeneous
style
writing
otherwise
style
heterogeneous
users
would
participate
iterations
achieve
level
coverage
participation
users
may
seen
cost
hand
allows
learn
november
2015
dataset
without
reading
users
learn
much
speciﬁc
facts
learn
second
order
information
like
types
relations
expressed
distribution
semantic
trees
offer
compact
tree-structured
meaning
representation
could
exploited
scenarios
covered
paper
like
relation
type
discovery
question
answering
furthermore
used
interpretable
representation
meaning
like
automaton
representation
figure
compared
meth-
ods
like
one
based
neural
networks
approach
may
superior
one
speciﬁc
part
ontology
learning
rather
provides
integrated
approach
learning
several
levels
ontology
also
approach
use
syntac-
tic
analysis
like
part
speech
tags
dependency
parsing
makes
approach
language
independent
useful
non-standard
texts
analysis
available
hand
looking
integrating
syntactic
analysis
future
work
one
scenario
automatically
detect
property
rule
another
idea
future
work
integrate
ideas
grammar
induction
methods
detect
meaningful
patterns
without
relying
annotation
text
acknowledgements
work
supported
slovenian
research
agency
ict
programme
xlike
fp7-ict-288342-strep
xlime
fp7-ict-611346
references
oren
etzioni
michele
banko
michael
cafarella
machine
reading
aaai
volume
pages
1517–1519
2006
philipp
cimiano
johanna
v¨olker
text2onto
natural
language
processing
information
systems
pages
227–238
springer
2005
amal
zouaq
roger
nkambou
evaluating
generation
domain
ontologies
knowledge
puzzle
project
ieee
trans
knowl
data
eng.
:1559–1572
november
2009
tom
mitchell
justin
betteridge
andrew
carlson
estevam
hruschka
richard
wang
populating
semantic
web
macro-reading
internet
text
proceedings
8th
international
semantic
web
conference
pages
998–1002
2009
qingqing
cai
alexander
yates
large-scale
semantic
parsing
via
schema
matching
lexicon
proceedings
annual
meeting
association
computational
linguistics
extension
acl
2013
deborah
dahl
madeleine
bates
michael
brown
william
fisher
kate
hunicke-smith
david
pallett
christine
pao
alexander
rudnicky
elizabeth
shriberg
expanding
scope
atis
task
atis-3
corpus
proceedings
workshop
human
language
technology
pages
43–48
1994
kurt
bollacker
colin
evans
praveen
paritosh
tim
sturge
jamie
taylor
freebase
collabora-
tively
created
graph
database
structuring
human
knowledge
proceedings
2008
acm
sigmod
international
conference
management
data
pages
1247–1250
2008
yoshua
bengio
j´erˆome
louradour
ronan
collobert
jason
weston
curriculum
learning
pro-
ceedings
26th
annual
international
conference
machine
learning
pages
41–48
2009
laura
kallmeyer
parsing
beyond
context-free
grammars
springer
publishing
company
incorpo-
rated
1st
edition
2010.
jens
lehmann
robert
isele
max
jakob
anja
jentzsch
dimitris
kontokostas
pablo
mendes
se-
bastian
hellmann
mohamed
morsey
patrick
van
kleef
s¨oren
auer
christian
bizer
dbpedia
large-scale
multilingual
knowledge
base
extracted
wikipedia
semantic
web
journal
:167–
195
2015.
november
2015
peter
norvig
techniques
automatic
memoization
applications
context-free
parsing
comput
linguist.
:91–98
march
1991
richard
frost
rahmatullah
haﬁz
new
top-down
parsing
algorithm
accommodate
ambiguity
left
recursion
polynomial
time
sigplan
not.
:46–54
may
2006
alfred
aho
margaret
corasick
efﬁcient
string
matching
aid
bibliographic
search
commun
acm
:333–340
june
1975
razvan
bunescu
raymond
mooney
shortest
path
dependency
kernel
relation
extraction
proceedings
conference
human
language
technology
empirical
methods
natural
language
processing
pages
724–731
2005
christopher
manning
mihai
surdeanu
john
bauer
jenny
finkel
steven
bethard
david
mcclosky
stanford
corenlp
natural
language
processing
toolkit
proceedings
52nd
annual
meeting
association
computational
linguistics
system
demonstrations
pages
55–60
2014
tom
kwiatkowski
luke
zettlemoyer
sharon
goldwater
mark
steedman
lexical
generalization
ccg
grammar
induction
semantic
parsing
proceedings
conference
empirical
methods
natural
language
processing
pages
1512–1523
2011.
jayant
krishnamurthy
tom
mitchell
learning
compositional
semantics
freebase
open
predicate
vocabulary
transactions
association
computational
linguistics
3:257–270
2015
percy
liang
michael
jordan
dan
klein
learning
dependency-based
compositional
semantics
computational
linguistics
:389–446
2013
siva
reddy
mirella
lapata
mark
steedman
large-scale
semantic
parsing
without
question-answer
pairs
transactions
association
computational
linguistics
2:377–392
2014.
john
zelle
raymond
mooney
learning
parse
database
queries
using
inductive
logic
pro-
proceedings
national
conference
artiﬁcial
intelligence
pages
1050–1055
gramming
1996.
jonathan
berant
andrew
chou
roy
frostig
percy
liang
semantic
parsing
freebase
question-answer
pairs
emnlp
pages
1533–1544
2013
eunsol
choi
tom
kwiatkowski
luke
zettlemoyer
scalable
semantic
parsing
partial
ontolo-
gies
proceedings
2015
association
computational
linguistics
july
2015
wilson
wong
wei
liu
mohammed
bennamoun
ontology
learning
text
look
back
future
acm
computing
surveys
csur
:20
2012.
johanna
v¨olker
pascal
hitzler
philipp
cimiano
acquisition
owl
axioms
lexical
resources
semantic
web
research
applications
pages
670–685
springer
2007
wei
liu
albert
weichselbraun
arno
scharl
elizabeth
chang
semi-automatic
ontology
extension
using
spreading
activation
journal
universal
knowledge
management
1:50–58
2005
christopher
brewster
fabio
ciravegna
yorick
wilks
user-centred
ontology
learning
knowl-
edge
management
natural
language
processing
information
systems
pages
203–207
springer
2002
sebastian
walter
christina
unger
philipp
cimiano
atoll—a
framework
automatic
induction
ontology
lexica
data
knowledge
engineering
94:148–162
2014
arianna
ulizia
fernando
ferri
patrizia
grifoni
survey
grammatical
inference
methods
natural
language
learning
artiﬁcial
intelligence
review
:1–27
2011
mitchell
marcus
mary
ann
marcinkiewicz
beatrice
santorini
building
large
annotated
corpus
english
penn
treebank
computational
linguistics
:313–330
1993
yoav
seginer
fast
unsupervised
incremental
parsing
annual
meeting-association
computa-
tional
linguistics
volume
page
384
2007
zach
solan
david
horn
eytan
ruppin
shimon
edelman
unsupervised
learning
natu-
ral
languages
proceedings
national
academy
sciences
united
states
america
102
:11629–11634
2005
stephen
watkinson
suresh
manandhar
psychologically
plausible
computationally
effective
approach
learning
syntax
proceedings
2001
workshop
computational
natural
language
learning-volume
page
2001
alexander
clark
unsupervised
induction
stochastic
context-free
grammars
using
distributional
clus-
tering
proceedings
2001
workshop
computational
natural
language
learning-volume
page
2001
christian
h¨anig
stefan
bordag
uwe
quasthoff
unsuparse
unsupervised
parsing
unsuper-
november
2015
vised
part
speech
tagging
lrec
2008.
janez
starc
dunja
mladeni´c
semi-automatic
rule
construction
semantic
linking
relation
arguments
proceedings
17th
international
multiconference
information
society
2014
2013
travis
wolfe
mark
dredze
james
mayﬁeld
paul
mcnamee
craig
harman
tim
finin
benjamin
van
durme
interactive
knowledge
base
population
arxiv
preprint
arxiv:1506.00301
2015
roberto
navigli
paola
velardi
stefano
faralli
graph-based
algorithm
inducing
lexical
tax-
onomies
scratch
proceedings
twenty-second
international
joint
conference
artiﬁcial
intelligence
volume
volume
three
ijcai
pages
1872–1877
2011
rion
snow
daniel
jurafsky
andrew
semantic
taxonomy
induction
heterogenous
evidence
proceedings
21st
international
conference
computational
linguistics
44th
annual
meeting
association
computational
linguistics
pages
801–808
2006
dmitry
zelenko
chinatsu
aone
anthony
richardella
kernel
methods
relation
extraction
mach
learn
res.
3:1083–1106
march
2003
hoifung
poon
pedro
domingos
unsupervised
ontology
induction
text
proceedings
48th
annual
meeting
association
computational
linguistics
acl
pages
296–305
2010
phil
blunsom
nando
freitas
edward
grefenstette
karl
moritz
hermann
deep
architecture
semantic
parsing
proceedings
acl
2014
workshop
semantic
parsing
2014
