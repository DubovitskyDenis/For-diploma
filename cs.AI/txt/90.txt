learning
shared
representations
value
functions
multi-task
reinforcement
learning
diana
borsa
thore
graepel
john
shawe-taylor
university
college
london
dept
computer
science
csml
london
wc1e
6ea
diana.borsa
gmail.com
thore
google.com
j.shawe-taylor
cs.ucl.ac.uk
abstract
investigate
paradigm
multi-task
rein-
forcement
learning
mt-rl
agent
placed
environment
needs
learn
perform
series
tasks
within
space
since
environment
change
potentially
lot
common
ground
amongst
tasks
learning
solve
individually
seems
extremely
wasteful
paper
explicitly
model
learn
shared
structure
arises
state-action
value
space
show
one
jointly
learn
optimal
value-functions
modifying
popular
value-
iteration
policy-iteration
procedures
ac-
commodate
shared
representation
assump-
tion
leverage
power
multi-task
super-
vised
learning
finally
demonstrate
proposed
model
training
procedures
able
infer
good
value
functions
even
low
samples
regimes
addition
data
efï¬ciency
show
analysis
learning
ab-
stractions
state
space
jointly
across
tasks
leads
robust
transferable
representations
potential
better
generalization
introduction
reinforcement
learning
gained
lot
popular-
ity
seen
remarkable
successes
last
years
exploiting
beneï¬ting
greatly
recent
develop-
ments
general
functional
approximators
neural
networks
mnih
al.
2015
least
part
suc-
cess
seems
linked
ability
universal
functional
approximators
distill
meaningfully
represen-
tations
bengio
2009
high-dimensional
input
states
enabled
scale
complex
environ-
ments
scenarios
previously
prohibited
re-
quired
great
amount
feature
engineering
shown
mnih
al.
2015
silver
al.
2016
thus
learning
good
abstraction
given
environment
agent
role
seems
key
component
developing
com-
plex
optimal
control
mechanisms
lot
progress
made
improving
learn-
ing
individual
single
tasks
seems
lot
less
work
trying
re-use
efï¬ciently
transfer
in-
formation
one
task
another
taylor
stone
2009b
nevertheless
natural
assume
differ-
ent
tasks
agent
needs
learn
life
share
lot
structure
in-build
redundancy
potentially
could
leverage
speed-up
learning
work
propose
way
address
aspect
learning
ro-
bust
transferable
abstractions
environment
gen-
eralize
set
tasks
value
functions
central
ideas
reinforcement
learn-
ing
sutton
barto
1998
successfully
used
conjunction
functional
approximators
generalize
large
state-action
spaces
concise
way
readily
assess
goodness
state
learnt
efï¬ciently
even
off-policy
fashion
enables
decouple
data
gathering
learning
process
importantly
allows
re-use
past
experi-
ences
collected
arbitrary
exploratory
policies
sut-
ton
barto
1998
recently
value
functions
shown
exhibit
nice
compositional
structure
respect
state
space
goal
states
schaul
al.
2015
consistent
earlier
studies
sutton
al.
2011
suggest
value
functions
capture
represent
knowledge
beyond
current
goal
leveraged
re-used
similar
structures
iden-
tiï¬ed
hierarchical
reinforcement
learning
literature
dietterich
2000
sutton
al.
1999
mo-
tivated
choice
explicitly
modelling
presence
shared
structure
state-action
value
space
learning
share
representations
value
functions
multi-task
using
multi-task
formulation
following
re-
cent
work
done
calandriello
al.
2014
ï¬rstly
outline
two
general
ways
learning
tasks
jointly
sharing
knowledge
across
extending
two
popular
procedures
learning
value
function
fitted
q-iteration
ernst
al.
2005
fitted
policy
iteration
antos
al.
2007
accommodate
shared
structure
assumption
furthermore
taking
advantage
multi-
task
methods
developed
supervised
settings
extend
work
calandriello
al.
2014
account
task-
speciï¬c
components
also
show
empirically
lead
over-
improvement
policies
inferred
well
de-
crease
number
samples
per
task
needed
achieve
good
performance
explore
nature
repre-
sentation
learnt
potential
transferability
new
related
tasks
show
learning
able
infer
com-
pressed
structure
nevertheless
captures
lot
trans-
ferable
knowledge
similar
option-like
transition
models
sutton
al.
1999
without
ever
specifying
parti-
tion
desirable
states
subgoals
finally
argue
way
learning
leads
robust
reï¬ned
representations
deemed
crucial
learning
planning
complex
environments
proposed
model
2.1.
background
notation
deï¬ne
markov
decision
process
mdp
tuple
set
states
set
actions1
transition
dynamics
cid:48
provides
probability
next
state
cid:48
reward
signal
as-
sumed
bounded
âˆƒrmax
s.t
rmax
discount
factor
given
mdp
policy
de-
ï¬ne
state-action
value
function
dis-
counted
cumulative
reward
agent
expected
collect
starting
state
taking
action
act
accordingly
policy
Î³trt|s
cid:35
cid:34
cid:88
t=0
expectation
trajectories
starting
obtained
interacting
environment
fol-
lowing
behaviour
policy
goal
learn
optimal
behaviour
respect
expected
cumulative
reward
thus
looking
s.t
arg
max
Ï€qÏ€
1in
work
ï¬nite
set
denote
optimal
value
function
qÏ€âˆ—
note
ï¬nding
automatically
gives
optimal
policy
acting
greedily
respect
values
following
denote
greedy
operation
gqâˆ—
2.2.
problem
formulation
consider
scenario
agent
resides
placed
environment
needs
perform
series
tasks
overall
goal
learn
suc-
ceed
tasks
environment
described
state-action
space
transition
kernel
cid:48
tasks
speciï¬ed
different
rewards
signals
one
task
formally
gives
rise
mdps
share
lot
structure
thus
ï¬nd
way
leverage
structure
expect
aid
learning
process
lead
better
generalization
taylor
stone
2009a
2.3.
shared
value
function
representation
propose
model
shared
structured
found
deï¬ned
mdp-s
shared
embedding
state-action
space
build
individ-
t=1
considered
ual
optimal
value
functions
tasks
potentially
new
ones
thus
paper
interested
learning
shared
embedding
well
ulti-
mately
optimal
behaviour
tasks
consid-
ered
following
present
one
extend
two
popular
paradigms
learning
value
func-
tions
fitted
q-iteration
fitted
policy
iteration
in-
corporate
shared
structure
assumption
come
employing
multi-task
learning
procedure
target-ï¬tting
step
q-iteration
policy
evaluation
step
policy-iteration
multi-task
fitted
value
iteration
section
outline
general
framework
using
approximate
value
iteration
infer
optimal
q-values
optimal
policies
set
tasks
given
en-
vironment
following
mt-rl
setup
previously
intro-
duced
proposed
algorithm
extension
fitted
q-iteration
fqi
allows
joint
learning
transfer
across
tasks
following
recipe
fqi
step
iteration
loop
sample
experience
set
cid:48
cid:48
.|s
compute
one-
step
target
based
current
estimate
value
function
treating
estimates
ground
truth
obtain
regression
problem
state-action
space
onto
targets
really
place-holders
true
value
function
case
mt-rl
ob-
tain
regression
problem
task
could
principle
solve
regression
problems
in-
learning
share
representations
value
functions
multi-task
figure
proposed
model
enforcing
shared
representation
state-action
space
used
modelling
value
functions
across
set
tasks
dependently
task
would
amount
apply-
ing
fqi
individually
task
assumption
shared
structure
tasks
would
like
make
use
common
ground
aid
learning
process
arrive
robust
abstractions
input
space
thus
propose
solving
regression
problems
jointly
accounting
building
upon
common
repre-
sentation
detailed
description
proposed
procedure
outlined
algorithm
algorithm
multi-task
fitted
q-iteration
require
t=1dt
set
experi-
ences/episodes
task
initialize
parameters
converged
||k
maxiter
k+1
compute
targets
k+1
maxa
cid:48
cid:48
cid:48
cid:48
multi-task
learning
t=1y
k+1
mtl
cid:107
k+1
cid:107
t=1dt
end
return
t=1
fÎ¸t
note
spirit
generalization
specify
particular
algorithm
multi-task
learning
step
mtl
algorithm
extensive
literature
deal
multi-task
inference
exploit
shared
structure
tasks
purely
supervised
settings
take
look
instantiations
step
throughout
work
multi-task
fitted
policy
iteration
similar
argument
one
presented
last
sec-
tion
mt-fqi
extend
framework
general
policy
iteration
mt-rl
scenario
policy
iteration
al-
gorithms
rely
alternating
procedure
policy
evaluation
step
policy
improvement
step
ex-
tend
framework
multi-task
case
deï¬ning
current
set
policies
t=1
one
task
evolve
set
policies
jointly
itera-
tion
please
ï¬nd
outline
proposed
procedure
algorithm
implement
policy
improve-
ment
step
acting
greedily
respect
current
es-
timates
value
function
step
done
individually
task
hand
allow
joint
learning
sharing
knowledge
policy
evaluation
step
gives
rise
general
procedure
call
multi-task
policy
evalua-
tion
mt-pe
see
algorithm
mt-pe
given
set
policies
t=1
one
task
col-
lection
experiences
t=1dt
aim
algorithm
approximate
corresponding
value
func-
tions
qÏ€t
associated
acting
policy
task
original
stateğ‘†ğ‘¡ğ´ğ‘¡actionğ‘†ğ‘¡+1ğ´ğ‘¡+1ğ‘†ğ‘¡+2ğ´ğ‘¡+2ğœ™
ğ‘†ğ‘¡+3ğœ™
ğ‘†ğ‘¡+1
ğ´ğ‘¡+1
ğ‘†ğ‘¡+2
ğ´ğ‘¡+2
ğ‘¡ğ‘ğ‘ ğ‘˜ğ‘ ğ‘—=1
ğ‘‡ğ‘„ğ‘—
ğ‘ ğ‘¡+1
ğ‘ğ‘¡+1
ğ‘ ğ‘¡+2
ğ‘ğ‘¡+2
learning
share
representations
value
functions
multi-task
algorithm
multi-task
policy
iteration
mt-pi
require
t=1dt
set
experiences
task
initialize
algorithm
multi-task
policy
evaluation
mt-pe
require
t=1dt
set
experiences
task
t=1
task
need
set
policies
evaluated
convergence
reached
policy
evaluation
compute
mt-pe
via
algorithm
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Î¸Ï€t
Â·Â·Â·
Î¸Ï€t
Î¸Ï€1
policy
improvement
a|s
k+1
arg
maxa
fÎ¸Ï€t
arg
maxa
end
return
t=1
policies
t=1
note
general
step
policy
evaluation
requires
on-policy
data
policy
task
could
quite
demanding
inefï¬cient
data-wise
numbers
tasks
grow
mention
inner
loop
another
iterative
algorithm
mt-pi
work
opt
implementation
policy
evalua-
tion
step
circumvents
problem
making
use
bellman
expectation
equation
compute
regression
targets
approximating
qÏ€t
using
experience
form
cid:48
previously
collected
case
fitted-q
iteration
cid:48
cid:48
cid:48
therefore
reduced
original
problem
set
regression
problems
solved
jointly
shared
input
space
representation
similar
multi-task
learning
step
employed
mt-fqi
shared
structure
learnt
model
input
set
policies
t=1
rather
optimal
ones
nev-
ertheless
constantly
improving
set
policies
presented
mt-pe
step
eventually
able
convergence
optimal
policies
thus
point
policy
evaluation
step
able
recover
shared
structure
amongst
optimal
value
functions
multi-task
representation
learning
section
look
couple
methods
plug
algorithms
step
assume
linear
parametrization
state-action
value
space
-i.e
assume
s.t
value
function
interest
well
approximated
linear
combination
set
features
case
ï¬tted
value
iteration
want
set
features
well
intermediate
targets
ultimately
initialize
convergence
reached
cid:48
cid:48
cid:48
i+1
compute
targets
i+1
multi-task
learning
i+1
mtl
cid:107
i+1
cid:107
t=1dt
t=1y
end
return
cid:16
Ë†qÏ€t
t=1
fÎ¸t
qÏ€t
cid:17
interested
set
features
well
optimal
value
functions
see
turns
small
subspace
original
feature
space
cid:104
cid:105
case
policy
iteration
evaluation
step
interested
feature
space
well
approxi-
mates
value
function
corresponding
current
poli-
cies
thus
looking
s.t
cid:105
policies
improve
cid:104
end
ï¬tting
optimal
near-optimal
value
functions
certainly
regression
step
done
perfectly
approximation
error
policy
iteration
continue
im-
prove
policies
limit
converge
op-
timal
value
functions
thus
representation
come
learning
procedure
similar
ones
learned
value-iteration
procedures
conse-
quently
ultimately
want
terms
representation
low-dimensional
features
space
spans
optimal
value
functions
interest
5.1.
multi-task
feature
learning
cid:105
cid:17
cid:104
cid:80
cid:16
Ë†qt
joint
problem
try-
terms
planning
ing
solve
formalized
inferring
t=1
arg
minw
regularizer
weight
vectors
encourages
fea-
ture
sharing
time
wish
learn
compact
abstraction
state-action
space
shared
among
tasks
make
bit
formal
let
cid:104
cid:105
learning
share
representations
value
functions
multi-task
i=1
cid:80
assumption
expressed
small
set
features
Î±tiÏˆi
cid:104
cid:105
Ïˆi-s
form
basis
relevant
low-dim
subspace
thus
task
trying
solve
jointly
fol-
lowing
optimization
problem
cid:34
cid:88
cid:16
cid:17
cid:35
arg
min
cid:104
cid:105
ldt
cid:107
cid:107
argyriou
al.
2008
shown
equivalent
solved
efï¬ciently
alternating
minimization
procedure
cid:16
cid:104
cid:105
cid:17
ldt
cid:35
arg
min
cid:34
cid:88
Â·Â·Â·
Â·Â·Â·
assumed
sparse
take
Î³||a||2
2,1
5.2.
allowing
task
speciï¬city
procedure
used
construct
infor-
mative
shared
features
shown
calandriello
al.
2014
experimental
section
however
lot
scenarios
tasks
beneï¬t
small
sparse
set
features
represent
particularities
in-
dividual
task
top
low-dimensional
shared
subspace
deï¬nitely
case
many
practical
applications
observed
purely
supervised
settings
well
simply
restricted
constrain
tasks
us-
ing
single
shared
structure
thus
researchers
come
various
ways
incorporating
task-speciï¬c
com-
ponents
see
zhou
al.
2011
jalali
al.
2010
chen
al.
2012
reference
therein
showed
modelling
explicitly
improve
learning
accuracy
speed
interpretability
resulting
representations
work
choose
one
formulations
introduced
ando
zhang
2005
learn
low-dimensional
shared
representation
well
task
speciï¬c
vector
place
strong
sparsity
constraint
encourage
common
features
still
identiï¬ed
shared
note
zero
matrix
treating
task
completely
independent
hand
zero
tasks
recover
previous
formulation
furthermore
place
orthogonality
condition
set
shared
features
inferred
enforcing
resulting
optimization
problem
form
cid:34
cid:88
cid:16
cid:17
cid:35
Ë†qt
arg
min
t=1
solved
alternating
structure
optimization
aso
see
ando
zhang
2005
experiments
assess
performance
behaviour
proposed
model
learning
procedures
4-room
navigation
task
sutton
al.
1999
state
space
described
valid
positions
agent
might
take
position
grid
wall
agent
access
four
actions
consider
deterministic
dynamics
directions
walls
considered
elastic
bumping
walls
effect
state
tasks
speciï¬ed
target
locations
environment
agent
need
navigate
sampled
random
valid
states
environment
specify
starting
state
agents
need
learn
navigate
selected
goal
position
part
environment
agent
transitions
goal
states
collects
positive
reward
reward
signal
provided
since
proposed
methods
run
off-policy
thus
decoupling
experience
gathering
learning
sample
modest
amount
experience
front
considered
tasks
done
principle
behaviour
policy
data-gathering
employ
uniformly
random
exploration
data
gathered
provided
proceed
learning
experiments
conducted
re-
strictive
sample
budget
|dt|
500
750
1000
firstly
would
like
compare
proposed
joint-representation
learning
single
tasks
counterparts
fqi
fpi
see
effects
enforcing
learning
shared
representation
would
assess
quality
inferred
greedy
policies
amount
reward
able
produce
random
starts
proxy
real
value
function
Ë†qt
cid:34
cid:88
cid:35
emp
es0âˆ¼Âµ
Î³krk|s0
k=0
depending
selection
starting
states
difï¬-
culty
tasks
thus
amount
reward
achievable
may
vary
ease
interpretation
report
normalized
value
estimate
respect
optimal
value
function
starting
states
example
results
ï¬rst
training
tasks
displayed
figure
obtained
training
randomly
sampled
tasks
500
samples
experience
per
task
see
learning
share
representations
value
functions
multi-task
figure
quality
inferred
greedy
policies
trained
individually
jointly
tasks
sample
budget
500
sam-
ples/task
show
average
cumulative
reward
achieved
agent
random
initial
positions
values
normalized
emp
see
cases
respect
optimal
cumulative
reward
achievable
starting
positions
single-task
learning
struggles
sample
regime
whereas
joint-learning
methods
able
discover
much
better
policies
even
recover
optimal
ones
emp/v
joint-learning
procedures
manage
learn
good
poli-
cies
quite
close
optimal
ones
substantially
outper-
form
single-task
learning
please
note
proposed
extension
allow
task-speciï¬c
features
cases
im-
proves
performance
even
considering
small
set
common
features
dshared
also
gives
much
faster
convergence
shared
subspace
in-
deed
behaviour
seems
consistent
lower
sam-
ples
sizes
although
worth
mentioning
divergence
occur
often
extrem
conditions
samples
regularization
parameters
might
ensure
convergence
calandriello
al.
2014
provide
solution
often
worse
even
single
task
outside
extreme
cases
policy
value
iteration
methods
perform
similarly
see
figure
tend
converge
solution
get
better
idea
average
task
performance
obtain
changes
training
look
average
distance
estimate
value
functions
iteration
optimal
ones
small
environment
computed
analytically
results
500
1000
samples
budgets
displayed
figure
observe
quite
big
difference
single-task
multi-task
procedures
terms
re-
covering
true
optimal
value
functions
convergence
better
mse
happens
much
faster
get
even
asymp-
totic
superior
solution
nevertheless
closeness
opti-
mal
value
functions
euclidean
space
may
necessar-
ily
imply
relation
policy
space
plot
quality
policies
function
value/policy
iterations
cid:80
||v
emp
t||/||v
available
figure
report
normalized
see
average
regret
policies
general
converge
much
faster
value
functions
comparing
q-value
con-
vergence
figure
please
also
note
multi-task
fitted
policy-iteration
procedures
inherit
speedy
convergence
present
single-task
counterpart
6.1.
learnt
shared
representations
probably
interesting
phenomenon
encountered
learning
shared
representations
nature
low
dimensional
representations
inferred
visualize
inferred
set
shared
features
figure
respec-
tive
weights
value-function
figure
produced
via
mt-fqi
aso
con-
straint
shared
subspace
5-dimensions
even
seem
permissive
actually
ob-
tain
strong
activations
top
features
inferred
Ïˆ1:3
presented
figure
thus
learnt
representation
low
dimensional
time
expressive
enough
effectively
approximate
optimal
value
functions
6.2.
transferring
knowledge
new
tasks
learnt
representations
resemble
option-like
features
sutton
al.
1999
essentially
inform
agent
across
tasks
navigate
efï¬ciently
rooms
negotiate
narrow
hallways
indeed
eas-
ily
transferable
skills
use
learning
new
task
test
hypothesis
augmenting
represen-
task
id12345678910normalized
cummulative
reward
achieved
qÏ€t/q*00.10.20.30.40.50.60.70.80.91joint
afpijoint
afpi
asosingle
task
afpijoint
afqijoint
afqi
asosingle
task
afqi
learning
share
representations
value
functions
multi-task
figure
convergence
optimal
value
function
assessed
euclidean
norm
||qâˆ—
different
sample
complexities
500
samples/task
top
750
samples/task
middle
1000
samples/task
bottom
different
methods
proposed
report
average
tasks
shaded
area
corre-
sponds
variances
mean
note
1000
sample/task
joint-representation
learning
algorithms
obtain
convergence
true
optimal
value
functions
tasks
also
note
join-learning
methods
second
method
allowing
task-speciï¬city
red
lines
afpi-aso
afqi-aso
yields
better
approximations
cid:80
||v
||/||v
figure
evaluate
quality
policy
learnt
produce
different
empirical
estimate
sample
complexities
500
samples/task
top
750
samples/task
middle
1000
samples/task
bottom
different
methods
proposed
seen
convergence
plot
1000
samples
multi-task
methods
reliably
recover
optimal
value
functions
implicitly
optimal
policies
single-task
methods
lot
tasks
time
half
budget
500
samples
multi-task
learning
already
able
recover
optimal
policies
single-task
methods
converge
suboptimal
value
function
blue
lines
plot
learning
share
representations
value
functions
multi-task
figure
read
row-wise
ï¬rst
three
relevant
shared
features
Ïˆ1:3
corresponding
top
three
eigen-
values
learnt
via
afpi-mtfl
tasks
randomly
sampled
four
rooms
please
note
already
enable
navi-
gation
pair
rooms
figure
weighting
coefï¬cients
Î±1:3
Î±2:3
three
prominent
shared
features
see
val-
ues
ï¬rst
feature
clearly
dominated
tasks
bottom
rescaled
version
Î±2:3
see
activation
two
prominent
features
blue
corresponds
negative
activation
red
positive
ones
given
nature
fea-
tures
one
readily
read
looking
sign
weight
room
task
goal
state
instance
look
second
task
negative
activation
north-side
environment
west-side
environment
goal
indeed
located
position
top-left
room
figure
average
performance
set
new
tasks
without
transfer
shared
features
assessed
normal-
ized
average
cumulative
reward
collected
random
starts
environment
value
functions
new
tasks
produced
single-task
fqi
original
feature
transfer
respectively
augmented
features
space
Ïˆaso
transfer
tation
new
task
shared
subspace
in-
vestigate
beneï¬ts
learnt
shared
subspace
set
training
tasks
terms
transferring
knowledge
optimizing
new
task
augment
feature
space
new
task
learnt
features
assess
effect
modiï¬cation
learning
new
task
figure
present
empir-
ical
evaluation
cumulative
regret
agent
in-
cur
inferred
greedy
policy
trained
original
representation
versus
augmented
represen-
tation
seeing
varying
amount
samples
200
300
500
700
1000
2000.
see
augmented
representation
able
produce
good
per-
formance
smaller
sample
sizes
general
learn-
ing
based
transferred
representation
able
pro-
duce
policy
equivalent
ones
could
learn
without
transfer
twice
much
data
behaviour
consistent
convergence
6.3.
connection
options
previously
learnt
shared
representation
seems
ac-
count
general
topology
dynamics
envi-
ronment
value
functions
nicely
partition
environment
relevant
regions
facilitate
global
navigation
local
neighbourhood
goal
features
characteristic
options
sutton
al.
1999
skills
konidaris
barto
2007
macro-actions
lit-
erature
dietterich
2000
hve
potential
dras-
tically
improve
efï¬ciency
scalability
meth-
ods
barto
mahadevan
2003
hengst
2002
246810246810Ïˆ1
246810246810Ïˆ1
246810246810Ïˆ1
246810246810Ïˆ2
246810246810Ïˆ2
246810246810Ïˆ2
246810246810Ïˆ2
246810246810Ïˆ3
246810246810Ïˆ3
246810246810Ïˆ3
246810246810Ïˆ3
246810246810coefficients
read
columns
task
51015202530weights
new
features
0.511.522.533.5-300-250-200-150-100-500coefficients
read
columns
task
51015202530weights
new
features
Ïˆ2:3
0.511.522.5-40-30-20-10010203040
learning
share
representations
value
functions
multi-task
ton
al.
1999
given
cid:80
tion
corresponding
newly
deï¬ned
semi-mdp
sut-
cid:48
cid:48
const
semi-mdp
run
fqi
indeed
see
able
construct
value
function
based
solely
learnt
5-dim
feature
space
suc-
cessfully
completes
speciï¬ed
task
results
navigation
options
available
figure
cid:48
following
would
like
investigate
connection
fur-
ther
following
formulation
sutton
al.
1999
op-
tion
cid:104
cid:105
generalization
primitive
actions
temporally
extended
course
action
initiation
set
option
available
policy
going
follow
options
trig-
gered
probability
termination
case
value
function
take
form
ï£®ï£¯ï£¯ï£¯ï£¯ï£°
cid:88
k=0
cid:124
Î³krt
cid:123
cid:122
cid:125
+Î³k+1
cid:88
cid:48
qÏ€t
cid:48
cid:48
cid:48
cid:80
cid:48
termination
state
options
denote
cid:48
cid:48
probability
options
terminate
state
cid:48
exactly
steps
note
term
accounts
transition
dynamics
policy
option
termination
criteria
task-invariant
moreover
note
generally
unless
option
happens
hit
goal
thus
equation
simpliï¬es
qÏ€t
cid:124
cid:123
cid:122
cid:125
cid:48
task
independent
cid:48
cid:124
cid:123
cid:122
cid:125
task
dependent
ï£®ï£¯ï£°
cid:88
cid:48
ï£¹ï£ºï£ºï£ºï£ºï£»
ï£¹ï£ºï£»
linear
combination
option
transition
cid:48
Ï†Âµo
cid:48
termination
set
subgoals
models
option
independent
task
depends
weighted
value
function
termination
states
tasks
incorpo-
rates
dependency
task
individual
policy
em-
ployed
option
terminated
similar
parametrization
assumed
suggest
learnt
representation
able
capture
repre-
sent
efï¬ciently
option-like
transition
models
without
specifying
subgoals
policies
initial
states
hy-
pothesize
learnt
shared
space
actually
com-
pressed
basis
option-transition
models
order
test
hypothesis
consider
intuitive
set
op-
tions
like
navigation
particular
room
test
cid:48
option
learnt
basis
span
successfully
represent
option-policy
deï¬ne
option
navigating
speciï¬c
room
say
room
initialization
set
io1
set
states
outside
room
termination
set
state
desired
room
also
deï¬ne
mdp
main-
tains
transition
dynamics
state
action
space
reward
signal
zero
outside
target
room
constant
positive
reward
desired
ter-
mination
states
cid:48
room
note
value
func-
figure
learned
greedy
policies
indicated
arrows
value
functions
maxa
enabling
navigation
four
rooms
based
share
feature
subspace
discovered
multi-task
value
function
learning
goals
randomly
sampled
environment
value
functions
learnt
using
single-task
fqi
top
features
Ïˆaso
re-
quired
200
samples
recover
option-like
policies
enable
agent
reach
desired
room
please
note
deï¬ned
options
quite
ex-
tended
ones
simpler
ones
would
include
making
way
outside
particular
room
along
lines
options
deï¬ned
sutton
al.
1999
stolle
pre-
cup
2002
easily
recovered
well
ac-
tually
simpler
options
require
sam-
ples
obtain
desired
behaviour
10-30
samples
al-
though
might
optimal
please
consult
supple-
mentary
material
details
fact
able
express
whole
variety
intuitively
deï¬ned
options
much
dimensionality
common
sub-
space
building
clear
indication
expressiveness
shared
representation
po-
tential
transferability
aiding
learning
new
tasks
within
2this
actually
true
mild
assumption
agent
cid:48
leave
room
reward
â†“â†’â†’â†’â†‘â†‘â†“â†’â†’â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†“â†â†‘â†â†â†‘â†“â†“â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†â†‘â†“â†’â†â†‘â†‘â†‘â†“â†â†â†‘â†‘â†‘â†â†‘â†‘â†‘â†â†“â†â†‘â†“â†â†â†‘â†‘â†â†“â†“â†â†‘â†“â†â†‘â†‘â†‘â†‘â†â†“â†â†â†“â†â†â†â†‘â†policy
task
11234567891012345678910â†’â†’â†’â†’â†“â†’â†’â†’â†’â†’â†’â†’â†’â†’â†“â†’â†’â†’â†’â†’â†’â†’â†’â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†“â†“â†’â†‘â†“â†â†’â†‘â†‘â†‘â†“â†“â†’â†“â†“â†“â†’â†“â†“â†“â†“â†’â†‘â†’â†’â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†“â†â†‘â†â†‘â†‘â†“â†â†‘â†‘â†‘â†‘â†“â†â†â†â†‘â†‘policy
task
21234567891012345678910â†’â†’â†’â†’â†“â†’â†’â†’â†’â†“â†“â†“â†“â†“â†“â†“â†“â†â†â†“â†“â†“â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†â†’â†“â†“â†â†‘â†‘â†‘â†“â†“â†“â†“â†“â†â†â†‘â†‘â†‘â†â†â†â†â†“â†â†â†â†â†‘â†â†â†â†â†“â†â†â†â†â†‘â†â†â†â†â†“â†â†â†â†â†‘policy
task
31234567891012345678910â†’â†’â†’â†’â†“â†’â†’â†’â†’â†’â†’â†’â†’â†’â†“â†’â†’â†’â†’â†’â†“â†“â†’â†“â†“â†“â†’â†‘â†‘â†‘â†“â†“â†’â†‘â†“â†“â†’â†‘â†‘â†‘â†“â†“â†’â†“â†“â†“â†’â†“â†“â†“â†“â†“â†’â†’â†“â†“â†“â†“â†“â†“â†â†â†â†â†“â†â†â†â†â†“â†â†â†â†â†“policy
task
41234567891012345678910
learning
share
representations
value
functions
multi-task
environment
related
work
good
collection
methods
tackle
various
aspect
multi-task
reinforcement
learning
lazaric
2012
taylor
stone
2009a
approach
methods
try
learn
jointly
either
value
functions
poli-
cies
set
tasks
lazaric
ghavamzadeh
2010
dimitrakakis
rothkopf
2011
different
struc-
ture
environment
assumptions
recent
study
konidaris
al.
2012
also
employs
idea
shared
feature
space
learning
procedure
pro-
posed
way
transferring
tuned
knowledge
dif-
ferent
main
novel
idea
work
intro-
duces
modelling
explicitly
shared
abstraction
state-action
space
reï¬ned
throughout
learn-
ing
process
optimizing
value
functions
ability
change
representation
throughout
learning
process
model
improving
set
policies
crucial
way
option-like
features
could
emerges
already
incorporate
transition
model
good
policies
generalize
tasks
shown
previous
section
one
methods
investigated
ca-
landriello
al.
2014
study
sparsity
multi-task
closely
related
learning
procedure
work
seen
generalization
method
al-
though
focus
model
assumptions
quite
differ-
ent
perhaps
relevant
prior
work
shares
vision
modelling
assumption
ap-
proach
schaul
al.
2015
model
shared
state-
representation
goals
assume
linear
factor-
ization
state
embbeding
task/goal
embbe-
ding
conclusion
future
work
work
investigated
problem
representation
learning
multi-task/multi-goal
reinforcement
learning
introduced
multi-task
paradigm
showed
two
popular
classes
planning
algo-
rithms
ï¬tted
q-iteration
approximate
policy
iteration
extended
learn
multiple
tasks
jointly
focus-
ing
linear
parametrization
q-function
showed
least
two
ways
one
harness
power
well-established
multi-task
learning
transfer
algo-
rithms
developed
supervised
settings
apply
inferring
joint
structure
optimal
value
functions
implicitly
policies
argued
shown
preliminary
experiments
beneï¬t
lot
integrating
joint
treatment
goals
exploiting
com-
monality
tasks
ought
lead
ef-
ï¬cient
learning
better
generalization
although
encouraging
results
paradigm
need
investigation
assess
convergence
behaviour
scalability
complex
tasks
employing
multi-task
learn-
ing
representation
learning
procedures
hope
work
serve
staring
point
references
ando
rie
kubota
zhang
tong
framework
learning
predictive
structures
multiple
tasks
unlabeled
data
journal
machine
learning
re-
search
6:1817â€“1853
2005.
antos
andrÂ´as
szepesvÂ´ari
csaba
munos
rÂ´emi
learning
value-iteration
based
ï¬tted
policy
iteration
single
trajectory
approximate
dynamic
pro-
gramming
reinforcement
learning
2007.
adprl
2007.
ieee
international
symposium
330â€“337
ieee
2007.
argyriou
andreas
evgeniou
theodoros
pontil
mas-
similiano
convex
multi-task
feature
learning
machine
learning
:243â€“272
2008.
barto
andrew
mahadevan
sridhar
recent
ad-
vances
hierarchical
reinforcement
learning
discrete
event
dynamic
systems
:341â€“379
2003.
bengio
yoshua
learning
deep
architectures
foun-
dations
trends
cid:13
machine
learning
:1â€“127
2009.
calandriello
daniele
lazaric
alessandro
restelli
marcello
sparse
multi-task
reinforcement
learning
advances
neural
information
processing
systems
819â€“827
2014.
chen
jianhui
liu
jieping
learning
inco-
herent
sparse
low-rank
patterns
multiple
tasks
acm
transactions
knowledge
discovery
data
tkdd
:22
2012.
dietterich
thomas
hierarchical
reinforcement
learning
maxq
value
function
decomposition
artif
intell
res
jair
13:227â€“303
2000.
dimitrakakis
christos
rothkopf
constantin
bayesian
multitask
inverse
reinforcement
learning
recent
advances
reinforcement
learning
273â€“
284.
springer
2011.
ernst
damien
geurts
pierre
wehenkel
louis
tree-
based
batch
mode
reinforcement
learning
journal
machine
learning
research
503â€“556
2005.
hengst
bernhard
discovering
hierarchy
reinforcement
learning
hexq
icml
volume
243â€“250
2002.
learning
share
representations
value
functions
multi-task
learning
knowledge
unsupervised
sensorimotor
10th
international
conference
interaction
autonomous
agents
multiagent
systems-volume
761â€“768
international
foundation
autonomous
agents
multiagent
systems
2011.
taylor
matthew
stone
peter
transfer
learning
reinforcement
learning
domains
survey
journal
machine
learning
research
10:1633â€“1685
2009a
taylor
matthew
stone
peter
transfer
learning
reinforcement
learning
domains
survey
journal
machine
learning
research
10:1633â€“1685
2009b
zhou
jiayu
chen
jianhui
jieping
clustered
multi-task
learning
via
alternating
structure
optimiza-
tion
advances
neural
information
processing
sys-
tems
702â€“710
2011.
jalali
ali
sanghavi
sujay
ruan
chao
ravikumar
pradeep
dirty
model
multi-task
learning
advances
neural
information
processing
systems
964â€“972
2010.
konidaris
george
barto
andrew
building
portable
options
skill
transfer
reinforcement
learning
ij-
cai
volume
895â€“900
2007.
konidaris
george
scheidwasser
ilya
barto
an-
drew
transfer
reinforcement
learning
via
shared
features
journal
machine
learning
research
:1333â€“1371
2012.
lazaric
alessandro
transfer
reinforcement
learning
framework
survey
reinforcement
learning
143â€“173
springer
2012.
lazaric
alessandro
ghavamzadeh
mohammad
icml-
bayesian
multi-task
reinforcement
learning
27th
international
conference
machine
learning
599â€“606
omnipress
2010.
mnih
volodymyr
kavukcuoglu
koray
silver
david
rusu
andrei
veness
joel
bellemare
marc
graves
alex
riedmiller
martin
fidjeland
andreas
ostrovski
georg
human-level
control
deep
reinforcement
learning
nature
518
7540
:529â€“
533
2015.
schaul
tom
horgan
daniel
gregor
karol
silver
david
universal
value
function
approximators
pro-
ceedings
32nd
international
conference
ma-
chine
learning
icml-15
1312â€“1320
2015.
silver
david
huang
aja
maddison
chris
guez
arthur
sifre
laurent
van
den
driessche
george
schrittwieser
julian
antonoglou
ioannis
panneershel-
vam
veda
lanctot
marc
mastering
game
deep
neural
networks
tree
search
nature
529
7587
:484â€“489
2016.
stolle
martin
precup
doina
learning
options
re-
inforcement
learning
sara
212â€“223
springer
2002.
sutton
richard
barto
andrew
reinforcement
learning
introduction
volume
mit
press
cam-
bridge
1998.
sutton
richard
precup
doina
singh
satinder
be-
tween
mdps
semi-mdps
framework
tempo-
ral
abstraction
reinforcement
learning
artiï¬cial
intel-
ligence
112
:181â€“211
1999.
sutton
richard
modayil
joseph
delp
michael
de-
gris
thomas
pilarski
patrick
white
adam
precup
doina
horde
scalable
real-time
architecture
learning
share
representations
value
functions
multi-task
figure
learned
greedy
policies
indicated
arrows
value
functions
coloring
indicates
value
maxa
enabling
navigation
four
rooms
based
share
feature
subspace
discovered
multi-task
value
function
learning
goals
randomly
sampled
environment
value
functions
learnt
using
single-task
fqi
top
features
Ïˆaso
show
results
using
100
respectively
300
samples
option-deï¬ned
mdp
â†’â†’â†’â†’â†“â†“â†“â†’â†’â†“â†“â†“â†“â†“â†“â†“â†“â†â†‘â†“â†“â†“â†â†“â†“â†“â†“â†“â†“â†“â†“â†â†‘â†“â†“â†“â†â†“â†“â†â†â†â†â†“â†â†â†â†â†“â†â†“â†â†â†“1234567891012345678910â†“â†“â†’â†’â†“â†’â†“â†’â†’â†“â†“â†“â†“â†“â†“â†“â†“â†“â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†“â†“â†“â†â†’â†“â†“â†“â†“â†“â†“â†â†“â†â†â†“â†â†“â†â†â†“â†â†â†â†â†“1234567891012345678910â†“â†“â†’â†’â†“â†’â†“â†’â†’â†“â†“â†“â†“â†“â†“â†“â†“â†“â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†“â†“â†“â†â†’â†“â†“â†“â†“â†“â†“â†â†“â†â†â†“â†â†“â†â†â†“â†â†â†â†â†“1234567891012345678910â†’â†“â†’â†’â†“â†’â†“â†’â†’â†“â†“â†“â†“â†“â†“â†“â†“â†“â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†“â†“â†“â†â†’â†“â†“â†“â†“â†“â†“â†“â†“â†“â†â†“â†“â†“â†“â†â†“â†“â†â†â†â†“1234567891012345678910â†“â†“â†“â†“â†“â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†â†‘â†“â†“â†â†‘â†‘â†‘â†“â†“â†â†“â†“â†“â†â†â†â†â†â†“â†â†â†“â†â†â†â†â†â†â†“â†â†â†“â†â†â†â†â†â†â†“â†â†â†“â†â†â†â†â†1234567891012345678910â†“â†“â†“â†“â†“â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†â†’â†“â†“â†â†‘â†‘â†‘â†“â†“â†“â†“â†“â†“â†â†â†‘â†‘â†“â†“â†â†â†“â†“â†â†â†‘â†‘â†“â†“â†â†â†“â†“â†â†â†‘â†‘â†“â†“â†â†â†“â†â†â†â†‘â†‘1234567891012345678910â†“â†“â†“â†“â†“â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†â†‘â†“â†“â†â†‘â†‘â†‘â†“â†“â†â†“â†“â†“â†â†‘â†‘â†‘â†â†â†â†â†“â†â†â†â†‘â†â†â†“â†â†â†“â†â†â†â†â†‘â†â†â†â†â†“â†â†â†â†â†1234567891012345678910â†“â†“â†“â†“â†“â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†â†‘â†“â†“â†â†‘â†‘â†‘â†“â†“â†â†‘â†“â†“â†â†‘â†‘â†‘â†â†â†â†â†“â†â†â†â†â†â†â†â†â†â†“â†â†â†â†â†â†â†â†â†â†“â†â†â†â†‘â†1234567891012345678910â†“â†“â†“â†“â†“â†â†“â†“â†“â†“â†“â†“â†“â†“â†â†‘â†“â†“â†“â†â†“â†“â†â†“â†â†â†“â†“â†“â†â†â†“â†â†“â†â†â†“1234567891012345678910â†“â†“â†“â†“â†“â†â†“â†“â†“â†“â†“â†“â†“â†“â†â†‘â†“â†“â†“â†â†“â†“â†“â†“â†â†â†“â†“â†“â†â†â†“â†“â†“â†â†â†“1234567891012345678910â†“â†“â†“â†“â†“â†â†“â†“â†“â†“â†“â†“â†“â†“â†â†’â†“â†“â†“â†“â†“â†“â†“â†“â†â†â†“â†“â†“â†â†â†“â†“â†“â†â†â†“1234567891012345678910â†“â†“â†“â†“â†“â†â†“â†“â†“â†“â†“â†“â†“â†“â†â†’â†“â†“â†“â†“â†“â†“â†“â†“â†â†â†“â†â†“â†â†â†“â†“â†“â†â†â†“1234567891012345678910â†“â†“â†“â†“â†“â†“â†“â†“â†’â†“â†“â†“â†“â†’â†’â†’â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†“â†â†â†â†â†â†“â†â†â†â†â†â†“â†â†â†â†â†1234567891012345678910â†“â†“â†“â†“â†“â†“â†“â†“â†’â†“â†“â†“â†“â†’â†’â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†“â†â†‘â†‘â†‘â†‘â†“â†â†‘â†‘â†‘â†‘â†“â†â†‘â†‘â†‘â†‘1234567891012345678910â†“â†“â†“â†“â†“â†“â†“â†“â†â†“â†“â†“â†“â†’â†â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†“â†â†â†‘â†‘â†‘â†“â†â†â†‘â†‘â†‘â†“â†â†â†â†‘â†‘1234567891012345678910â†“â†“â†“â†“â†“â†“â†“â†“â†â†“â†“â†“â†“â†“â†â†‘â†‘â†‘â†“â†â†â†‘â†‘â†‘â†“â†â†â†‘â†‘â†‘â†“â†â†â†‘â†‘â†‘â†“â†â†â†‘â†‘â†‘1234567891012345678910â†“â†’â†’â†’â†’â†’â†“â†’â†’â†’â†’â†’â†‘â†‘â†‘â†‘â†‘â†‘â†“â†â†’â†‘â†‘â†‘â†“â†“â†“â†“â†“â†’â†“â†“â†“â†“â†“â†“â†“1234567891012345678910â†“â†’â†’â†’â†‘â†‘â†“â†’â†’â†’â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†“â†â†’â†‘â†‘â†‘â†“â†“â†“â†“â†“â†’â†“â†“â†“â†“â†“â†“â†“1234567891012345678910â†“â†’â†’â†’â†’â†’â†“â†’â†’â†’â†’â†’â†‘â†‘â†’â†‘â†‘â†‘â†“â†“â†’â†‘â†‘â†‘â†“â†“â†“â†“â†“â†’â†“â†“â†“â†“â†“â†“â†“1234567891012345678910â†“â†’â†’â†’â†‘â†’â†“â†’â†’â†’â†’â†’â†‘â†‘â†‘â†‘â†‘â†‘â†“â†â†’â†‘â†‘â†‘â†“â†“â†“â†“â†“â†’â†“â†“â†“â†“â†“â†“â†“1234567891012345678910â†’â†“â†“â†’â†“â†’â†“â†“â†’â†“â†“â†“â†“â†“â†“â†“â†“â†“â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“1234567891012345678910â†’â†’â†’â†’â†“â†’â†“â†“â†’â†“â†“â†“â†“â†“â†“â†“â†“â†â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“1234567891012345678910â†“â†“â†’â†’â†“â†’â†“â†’â†’â†“â†“â†“â†“â†“â†“â†“â†“â†“â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“1234567891012345678910â†“â†“â†’â†’â†“â†’â†“â†’â†’â†“â†“â†“â†“â†“â†“â†“â†“â†“â†â†“â†“â†“â†â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“1234567891012345678910
