accepted
version
doi
10.1109/tnnls.2016.2626341
network
unfolding
map
vertex-edge
dynamics
modeling
filipe
alves
neto
verri
paulo
roberto
urio
liang
zhao
senior
member
ieee
abstract—the
emergence
collective
dynamics
neural
networks
mechanism
animal
human
brain
information
processing
paper
develop
computational
technique
using
distributed
processing
elements
complex
net-
work
called
particles
solve
semi-supervised
learning
problems
three
actions
govern
particles
dynamics
gener-
ation
walking
absorption
labeled
vertices
generate
new
particles
compete
rival
particles
edge
domination
active
particles
randomly
walk
network
ab-
sorbed
either
rival
vertex
edge
currently
dominated
rival
particles
result
model
evolution
consists
sets
edges
arranged
label
dominance
set
tends
form
connected
subnetwork
represent
data
class
although
intrinsic
dynamics
model
stochastic
one
prove
exists
deterministic
version
largely
reduced
computational
complexity
speciﬁcally
linear
growth
furthermore
edge
domination
process
corresponds
unfolding
map
way
edges
stretch
shrink
according
vertex-
edge
dynamics
consequently
unfolding
effect
summarizes
relevant
relationships
vertices
uncovered
data
classes
proposed
model
captures
important
details
connectivity
patterns
vertex-edge
dynamics
evolution
contrast
previous
approaches
focused
vertex
edge
dynamics
computer
simulations
reveal
new
model
identify
nonlinear
features
real
artiﬁcial
data
including
boundaries
distinct
classes
overlapping
structures
data
index
terms—complex
networks
nonlinear
dynamical
sys-
tems
semi-supervised
learning
particle
competition
introduction
emi-supervised
learning
ssl
one
machine
learning
paradigms
lies
unsupervised
supervised
learning
paradigms
ssl
problems
unlabeled
labeled
data
taken
account
class
cluster
formation
prediction
processes
real-
world
applications
usually
partial
knowledge
given
dataset
example
certainly
know
every
material
future
media
cid:13
2018
ieee
personal
use
permitted
per-
mission
ieee
must
obtained
uses
including
reprinting/republishing
mate-
current
rial
advertising
promotional
purposes
creating
new
collective
works
resale
redistribution
servers
lists
reuse
copyrighted
component
work
works
published
ver-
sion
http
//ieeexplore.ieee.org/document/7762202/
doi
10.1109/tnnls.2016.2626341
research
supported
s˜ao
paulo
state
research
foundation
fapesp
coordination
improvement
higher
education
per-
sonnel
capes
brazilian
national
research
council
cnpq
f.a.n
verri
p.r
urio
institute
mathematical
computer
sciences
icmc
university
s˜ao
paulo
s˜ao
carlos
brazil
email
ﬁlipeneto
urio
usp.br
zhao
faculty
philosophy
sciences
letters
ribeir˜ao
preto
ffclrp
university
s˜ao
paulo
ribeir˜ao
preto
brazil
email
zhao
usp.br
movie
actor
except
famous
ones
large-scale
social
network
know
friends
biological
domain
far
away
completely
obtaining
ﬁgure
functions
genes
know
functions
sometimes
although
complete
almost
complete
knowledge
dataset
labeling
hand
lengthy
expensive
necessary
restrict
labeling
scope
reasons
partially
labeled
datasets
often
encoun-
tered
sense
supervised
unsupervised
learning
considered
extreme
special
cases
semi-supervised
learning
many
semi-supervised
learning
techniques
developed
including
generative
models
discriminative
models
clustering
labeling
techniques
multi-
training
low-density
separation
models
graph-
based
methods
among
approaches
listed
graph-based
ssl
triggered
much
attention
case
data
instance
represented
vertex
linked
vertices
according
predeﬁned
afﬁnity
rule
labels
propagated
whole
graph
using
particular
optimization
heuristic
complex
networks
large-scale
graphs
nontrivial
topology
networks
introduce
powerful
tool
describe
interplay
topology
structure
dynamics
complex
systems
therefore
provide
groundbreaking
mechanism
help
understand
behavior
many
real
systems
networks
also
turn
important
mechanism
data
representation
analysis
interpreting
data
sets
complex
networks
grant
access
inter-relational
nature
data
items
reason
consider
network-based
approach
ssl
work
however
above-mentioned
network-based
approach
focuses
optimization
label
propagation
result
pays
little
attention
detailed
dynamics
learning
process
hand
well-known
collective
neural
dynamics
generate
rich
information
redundant
processing
handles
adaptability
robustness
learning
process
moreover
traditional
graph-based
techniques
high
computational
complexity
usually
cubic
order
common
strategy
overcome
disadvantage
using
set
sparse
prototypes
derived
data
however
sampling
process
usually
loses
information
original
data
taking
account
facts
study
new
type
dynamical
competitive
learning
mechanism
complex
net-
work
called
particle
competition
consider
network
several
particles
walk
compete
occupy
many
vertices
possible
attempting
reject
rival
particles
particle
performs
combined
random
preferential
walk
accepted
version
doi
10.1109/tnnls.2016.2626341
choosing
neighbor
vertex
visit
finally
expected
particle
occupies
subset
vertices
called
community
network
way
community
detection
direct
result
particle
competition
particle
competition
model
originally
proposed
extended
data
clustering
task
later
applied
semi-
supervised
learning
particle
competition
formally
represented
nonlinear
stochastic
dynamical
sys-
tem
models
mentioned
authors
concern
vertex
dynamics—how
vertex
changes
state
level
dominance
particle
intuitively
vertex
dynamics
rough
modeling
network
vertex
several
edges
problem
data
analysis
approach
overlapping
nature
vertices
data
item
vertex
networked
form
belong
one
class
therefore
interesting
know
edge
changes
state
competition
process
acquire
detailed
knowledge
dynamical
system
paper
propose
transductive
semi-supervised
learning
model
employs
vertex–edge
dynamical
sys-
tem
complex
networks
dynamical
system
namely
labeled
component
unfolding
system
particles
compete
edges
network
subnetworks
generated
edges
grouped
class
dominance
call
subnetwork
unfolding
learning
model
employs
unfoldings
classify
unlabeled
data
proposed
model
offers
satisfactory
performance
semi-supervised
learning
problems
artiﬁcial
real
dataset
also
shown
suitable
detecting
overlapping
regions
data
points
simply
counting
edges
dominated
class
particles
moreover
low
computational
complexity
order
comparison
original
particle
competition
models
graph-based
semi-supervised
learning
techniques
proposed
one
presents
following
salient
features
particle
competition
dynamics
occurs
nodes
well
edges
inclusion
edge
domination
model
give
detailed
information
capture
connectivity
pattern
input
data
much
edges
vertices
even
sparse
network
consequently
proposed
model
beneﬁt
granting
essential
infor-
mation
concerning
overlapping
vertices
computer
simulations
show
proposed
technique
achieves
good
classiﬁcation
accuracy
suitable
situations
small
number
labeled
samples
proposed
model
particles
continuously
generated
removed
system
feature
con-
trasts
previous
particle
competition
models
incorporate
preferential
walking
mechanism
particles
tend
avoid
rival
particles
consequence
number
active
particles
system
varies
time
worth
noting
elimination
preferential
walking
mechanism
largely
simpliﬁes
dynamical
rules
particle
competition
model
new
model
characterized
competition
random
walking
particles
turn
permits
ﬁnd
equivalent
deterministic
version
original
particle
competition
model
intrinsically
stochastic
run
may
generate
different
result
consequently
high
computational
cost
work
ﬁnd
deter-
ministic
system
running
time
independent
number
particles
demonstrate
mathematically
equivalent
stochastic
model
moreover
deterministic
model
linear
time
order
ensures
stable
learning
words
model
generates
output
run
input
furthermore
system
simpler
easier
understood
implemented
thus
proposed
model
efﬁcient
original
particle
competition
model
explicit
objective
function
classical
graph-based
semi-supervised
learning
techniques
usually
objective
function
deﬁned
optimization
function
considers
label
information
also
semi-
supervised
assumptions
smoothness
cluster
manifold
particle
competition
models
need
deﬁne
objective
function
instead
dynamical
rules
govern
time
evolution
particles
vertices
edges
deﬁned
dynamical
rules
mimic
phenomena
observed
natural
social
systems
resource
competition
among
animals
territory
exploration
humans
animals
election
campaigns
etc
words
particle
competi-
tion
technique
typically
inspired
nature
kind
technique
focused
behavior
modeling
instead
objective
modeling
certain
objectives
achieved
corresponding
behavioral
rules
properly
deﬁned
way
may
classify
classical
graph-based
semi-supervised
learning
techniques
objective-based
design
particle
competition
technique
behavior-based
design
remainder
paper
organized
follows
proposed
particle
competition
system
studied
section
transductive
semi-supervised
learning
model
repre-
sented
section
iii
section
results
computer
sim-
ulations
shown
assess
proposed
model
performance
artiﬁcial
real-world
datasets
finally
section
concludes
paper
labeled
component
unfolding
system
section
give
introduction
labeled
component
unfolding
lcu
system—a
particle
competition
system
edge
domination—explaining
basic
design
whenever
pertinent
detail
clariﬁcation
overview
consider
complex
network
expressed
simple
unweighted
undirected
graph
set
vertices
set
edges
two
vertices
considered
similar
edge
connects
network
contains
|v|
vertices
either
labeled
unlabeled
data
points
set
contains
labeled
vertices
vertex
label
also
use
terms
label
class
synonymously—if
vertex
labeled
say
vertex
belongs
class
set
vl+1
vl+u
contains
unlabeled
vertices
suppose
cid:28
thus
network
represented
adjacency
matrix
aij
aij
aji
connected
denote
edge
vertices
accepted
version
doi
10.1109/tnnls.2016.2626341
practical
reasons
consider
connected
network
least
one
labeled
vertex
class
model
particles
objects
ﬂow
within
network
carrying
label
labeled
vertices
sources
particles
class
sinks
particles
classes
particle
released
randomly
walks
network
equal
probability
among
adjacent
vertices
chosen
next
vertex
visited
particle
consider
particle
decides
move
probability
aij
deg
deg
denoting
degree
step
moment
particle
decides
move
next
vertex
absorbed
removed
system
particle
absorbed
say
survived
remains
active
survives
continues
walking
otherwise
particle
absorbed
ceases
affect
system
absorption
depends
level
subordination
domination
class
classes
edges
determine
level
domination
subordination
class
edge
take
account
active
particles
system
current
directed
domination
˜nc
number
active
particles
belonging
class
decided
time
survived
similarly
move
current
relative
subordination
˜σc
fraction
active
particles
belong
class
successfully
passed
edge
regardless
direction
time
mathematically
deﬁne
latter
˜nc
˜nc
q=1
˜nq
˜nq
q=1
˜nq
˜nq
cid:80
1
˜σc
cid:80
otherwise
survival
particle
depends
current
relative
subordination
edge
destination
vertex
particle
decides
move
sink
absorbed
probability
destination
vertex
sink
survival
probability
λ˜σc
parameter
characterizing
compe-
tition
level
source
generates
particles
according
degree
current
number
active
particles
system
let
˜nc
number
active
particles
belonging
class
system
time
source
generates
new
particles
˜nc
˜nc
let
vi|vi
set
sources
particles
belong
class
number
newly
generated
particles
belonging
class
time
follows
˜nc
˜nc
˜nc
˜nc
otherwise
distribution
cid:40

cid:80
deg
vj∈gc
deg
otherwise
binomial
distribution
words
number
active
particles
fewer
initial
number
particles
˜nc
source
performs
˜nc
˜nc
trials
probability
generating
new
particle
therefore
expected
number
new
particles
belonging
cid:40
class
time
˜nc
˜nc
˜nc
˜nc
otherwise
interested
total
number
visits
particles
class
edge
thus
introduce
cumulative
domination
˜δc
total
number
particles
belong-
ing
class
passed
edge
time
mathematically
deﬁned
˜δc
cid:88
cid:12
cid:12
cid:12
cid:12
arg
max
cid:16
˜δq
cid:26
˜nc
using
cumulative
domination
group
edges
class
domination
class
subset
˜δq
cid:17
cid:27
deﬁne
subnetwork
unfolding
network
according
class
time
interpret
unfolding
subspace
relevant
relationships
given
class
use
available
information
subnetworks
study
overlapping
regions
semi-supervised
learning
illustrative
example
one
iteration
system
evolution
illustrated
figure
considered
system
contains
active
particles
time
time
iteration
particle
moves
neighbor
vertex
without
preference
movement
particle
indicated
arrow
interrupted
line
indicates
edge
coming
particle
absorbed
total
particles
absorbed
iteration
sources
generated
new
particles
time
example
one
red
particles
passing
edge
absorbed
due
current
edge
dom-
inance
0.5
edge
one
red
particle
one
green
particle
conversely
green
particles
moved
edge
remain
active
time
since
rival
particle
red
particle
passing
edge
updated
value
current
edge
dominance
green
red
classes
respectively
edge
one
red
two
green
particles
chose
pass
one
green
particle
absorbed
without
affecting
new
current
level
dominance
since
one
particle
class
successfully
passed
edge
new
current
level
dominance
edge
0.5.
occurs
edge
particles
passed
thus
current
level
dominance
set
equally
among
classes
accepted
version
doi
10.1109/tnnls.2016.2626341

let
˜gc
˜ac
respectively
number
parti-
cles
generated
absorbed
time
evolution
function
dynamical
system
cid:0
˜nc
cid:88
˜ac
˜gc
˜nc
cid:1
˜nc
˜nc
˜δc
˜δc
˜nc
intuitively
number
˜nc
active
particles
minus
vertex
total
number
particles
arriving
˜nc
absorbed
˜ac
number
particles
leaving
˜nc
number
generated
additionally
labeled
vertices
particles
˜gc
visits
particles
edge
simply
add
number
obtained
stochas-
time
values
˜nc
˜nc
tically
according
dynamics
walking
absorption
generation
moreover
calculate
total
number
˜δc
˜ac
˜gc
initial
state
system
given
arbitrary
number
˜nc
initial
active
particles
cid:40
˜nc
˜δc
achieve
desirable
network
unfolding
necessary
average
results
several
simulations
system
large
number
initial
particles
˜nc
thus
com-
putational
cost
simulation
high
conversely
provide
alternative
system
achieves
similar
results
deterministic
manner
details
follow
alternative
mathematical
modeling
consider
dynamical
system

cid:2
cid:3
cid:0
cid:1
cid:0
cid:1

row
vector
whose
elements
give
population
particles
label
vertex
time
values
associated
number
active
particles
system
elements
sparse
˜nc
matrices
related
current
directed
domination
˜nc
respec-
gives
number
particles
tively
words
class
moved
time
gives
total
number
time
cumulative
domination
˜δc
system
nonlinear
markovian
dynamical
system
deterministic
evolution
function
nc
diag
diag
square
matrix
elements
vector
main
diagonal
stands
vector-matrix
product
fig
illustration
one
iteration
system
evolution
network
consists
vertices
edges
color
represents
label
particle
source
ﬁrst
third
networks
depict
cumulative
domination
iteration
cumulative
domination
number
visits
particles
edge
since
initial
state
second
network
particles
depicted
small
circles
active
particles
time
depicted
dashed
borders
whereas
active
particles
time
full
borders
arrow
indicates
particle
movement
interrupted
line
indicates
particle
absorbed
trying
move
edge
particles
without
adjacent
arrow
generated
sources
time
edges
particles
tried
move
source
rival
particles
sinks
particles
absorbed
independently
current
level
dominance
edge-centric
system
measure
overlapping
nature
counting
edges
dominated
class
vertex-centric
approach
would
lost
information
mathematical
modeling
formally
deﬁne
labeled
component
unfolding
system
dynamical
system
state
system
˜nc
cid:2
˜nc
cid:3
˜∆c
cid:0
˜δc
cid:1
˜nc
vector
element
˜nc
active
particles
belonging
class
furthermore
˜∆c
matrix
whose
elements
˜δc
given
equation
number
time
accepted
version
doi
10.1109/tnnls.2016.2626341
function
system
time
gives
square
matrix
whose
elements
0
deg
aij
cid:0
λσc
1
cid:1
otherwise
cid:54
otherwise
cid:88
q=1
given
know
initial
state
system
function
system
time
returns
row
vector
i-th
element
max
row
vector
whose
elements
stands
inner
product
vectors
initial
state
system
given
arbitrary
population
size1
initial
active
particles
cid:40
initial
number
particles
vertex
system
proportional
initial
population
size
system
provide
evidence
unfolding
result
tends
systems—represented
equation
equa-
tion
respectively—
˜nc
|v|
mathematical
analysis
previous
subsections
modeled
two
possibly
equivalent
systems
section
present
mathematical
results
prove
equivalence
two
systems
certain
assumptions
theorem
systems
asymptotically
equivalent
following
conditions
hold
cid:3
cid:2
˜σc
cid:2
˜nc
cid:3
cid:2
˜nc
cid:3
cid:3
cid:3
cid:2
˜nq
q=1
cid:2
˜nq
cid:80
max
˜nc
˜nc
˜nc
˜gc
cid:3
cid:2
˜nc
cid:104
˜δc
cid:105
˜nc
constant
order
prove
theorem
study
following
mechanisms
particle
competition
system
particle
motion
absorption
proposed
system
particle
moves
independently
others
particle
movement
edge
affects
absorption
rival
par-
ticles
next
iteration
conditions
favorable
naturally
regard
system
evolution
terms
distribution
particles
network
next
present
formal
model
particle
movement
aij
cid:54
let
iij
discrete
random
variable
particle
time
moved
time
otherwise
since
particle
vertex
moves
independently
write
probability
terms
iij
particle
class
particle
belongs
class
time
ment
decision
particle
whether
absorbed
decision
formulation
dynamical
system
conditional
probability
given
˜σc
cid:3
affected
move-
cid:3
probability
cid:2
cid:12
cid:12
˜σc
cid:2
0
hence
probability
cid:2
cid:12
cid:12
˜σc
cid:3
f˜σc
cid:2
cid:90
cid:18
cid:90
cid:90
cid:0
cid:2
˜σc
particle
tries
move
sink
survival
probability
zero
otherwise
particle
reaches
chooses
move
vertex
absorbed
probability
density
function
random
cid:3
f˜σc
aij
cid:19
cid:3
cid:1
otherwise
let
f˜σc
variable
˜σc
f˜σc
cid:90
deg
deg
deg
ξf˜σc
aij
aij
cid:54
otherwise
zero
convex
ﬁxed
values
˜nq
˜nq
cid:54
thus
jensen
inequality
furthermore
˜σc
deg
cid:2
˜σc
cid:3
cid:3
cid:3
cid:2
˜nc
cid:2
˜nc
q=1
cid:2
˜nq
cid:3
cid:2
˜nq
cid:3
cid:80
particle
generation
dynamical
system
ex-
pected
number
particles
belonging
class
generated
time
˜gc
˜gc
|˜nc
˜nc
cid:88
η=0
conditional
expectation
˜gc
|˜nc
1in
system
vector
describes
quantity
particles
vertex
since
multiplicative
scaling
behavior
necessarily
composed
integer
values
values
discrete
distribution
particles
see
section
ii-e5
details
formulation
max
˜nc
accepted
version
doi
10.1109/tnnls.2016.2626341
thus
˜gc
cid:88
max
˜nc
˜nc
η=0
max
˜nc
˜nc
since
max
convex
according
jensen
inequality
˜gc
max
˜nc
˜nc
expected
edge
domination
beginning
sys-
tem
cid:104
˜δc
cid:105
cid:104
˜δc
˜nc
˜δc
cid:104
˜δc
cid:105
cid:2
˜nc
cid:105
cid:3
given
˜nc
known
since
particle
vertex
moves
independently
number
particles
successfully
reaches
time
particle
belongs
class
iij
k=1
˜nc
cid:88
expected
value
cid:2
˜nc
cid:3
cid:88
cid:3
˜nc
cid:88
cid:88
cid:88
cid:12
cid:12
˜nc
cid:2
˜nc
cid:88
cid:88
˜nc
˜nc
k=1
k=1
η=0
η=0
η=0
˜nc
finally
η=0
cid:2
˜nc
cid:3
˜nc
iij
cid:3
iij
cid:2
cid:3
cid:2
|v|
expected
number
particles
know
number
particles
beginning
system
˜nc
˜nc
expected
value
˜nc
cid:0
cid:2
˜nc
cid:3
cid:2
˜nc
cid:3
cid:1
˜nc
cid:88
˜gc
˜ac
however
expected
number
particles
ab-
sorbed
expected
number
particles
minus
expected
number
particles
survived
moving
away
thus
˜nc
written
cid:88
cid:0
cid:2
˜nc
cid:3
cid:2
˜nc
cid:3
cid:1
cid:3
e
˜nc
cid:88
cid:2
˜nc
cid:2
˜nc
cid:3
˜gc
cid:88
˜nc
˜gc
ﬁnally
˜nc
|v|
scale
invariance
unfolding
system
invariant
real
positive
multiplication
row
vector
order
prove
property
consider
following
lemma
lemma
system
positive
multiplicative
scaling
behavior
order
given
arbitrary
initial
state
system
means
κxt
κx0
proof
lemma
first
show
functions
invariant
parameter
scaling
given
arbitrary
system
state
κxt
0
aij
deg
cid:0
λσc
cid:1
cid:54
otherwise
since
term
either
cid:80
κxt
cid:80
κnc
κnc
q=1
κnq
q=1
κnq
consider
two
arbitrary
initial
states
κxt
κx0
nc
nc
κηi
κx0
κx0
κnc
κx0
κnc
accepted
version
doi
10.1109/tnnls.2016.2626341
κx0
κx0
κx0
cid:88
cid:88
cid:88
κnc
κx0
κx0
κx0
κnc
κδc
thus
relation
holds
true
assuming
relation
holds
true
time
show
relation
holds
true
κx0
κnc
κx0
κxt
κnc
κx0
κx0
κxt
κgc
κnc
cid:88
since
κxt
max
κx0
κx0
κx0
κδc
κnc
κδc
relation
indeed
holds
true
since
basis
inductive
step
performed
mathematical
induction
lemma
proved
natural
finally
using
studies
may
prove
theorem
proof
theorem
equations

˜nc
cid:88
cid:2
˜nc
cid:3
˜gc
cid:2
cid:3
cid:2
˜nc
cid:3
˜nc
cid:104
˜δc
cid:104
˜δc
cid:105
cid:3
cid:2
˜nc
cid:105
convergence
system
assuming
inequalities
tend
equality
large
number
particles
constant
scale
invariance
κ˜nc
property
remark
even
inequalities
true
another
property
possibly
makes
two
systems
equivalent
compensation
time
beginning
systems
equal
however
next
iteration
absorption
probability
generated
particles
underestimated
consequently
particles
survived
may
compensate
ones
gener-
ated
furthermore
lower
number
absorbed
particles
iteration
higher
absorption
probability
next
iteration
likewise
lower
number
generated
particles
iteration
higher
expected
number
new
particles
next
iteration
iii
semi-supervised
learning
labeled
component
unfolding
unfoldings
generated
lcu
system
incorpored
semi-supervised
learning
model
consider
two
sets
xlabeled
xunlabeled
xl+1
xl+u
data
point
xlabeled
associated
label
semi-supervised
learning
setting
goal
correctly
assign
existing
labels
unlabeled
data
xunlabeled
short
proposed
learning
model
three
steps
network
constructed
based
dataset
composed
feature
vectors
vertices
represent
data
points
edges
represent
similarity
relationship
lcu
system
applied
obtain
unfoldings
distinct
set
edges
class
dataset
infer
labels
every
data
point
xunlabeled
next
step
proposed
learning
model
presented
detail
model
algorithm
description
computational
complexity
analysis
also
presented
since
proposed
dynamical
system
takes
place
complex
network
original
dataset
needs
represented
network
structure
therefore
ﬁrst
step
learning
model
obtain
network
representation
data
point
associated
single
vertex
network
moreover
network
must
sparse
undirected
unweighted
labeled
vertices
correspond
set
points
xlabeled
unlabeled
vertices
set
points
xunlabeled
two
vertices
connected
edge
relationship
similarity
determined
metric
particular
problem
graph
construction
method
satisﬁes
conditions
may
used
step
nearest
neighbor
k-nn
graph
construction
method
one
second
step
run
system
deﬁned
equation
using
constructed
complex
network
input
two
conditions
satisﬁed
system
initialization
first
class
privileged
second
ﬁrst
iterations
particles
able
ﬂow
within
network
small
probability
absorption
thus
initial
conditions
system
deg
2|e|
since
always
particles
system
iteration
system
stopped
time
limit
reached
time
limit
parameter
controls
maximum
number
iterations
system
last
step
networks
used
vertex
classiﬁcation
assign
label
unlabeled
vertex
information
provided
networks
label
assigned
based
density
edges
neighborhood
formally
label
arg
max
...
accepted
version
doi
10.1109/tnnls.2016.2626341
algorithm
semi-supervised
learning
lcu
function
classifier
xlabeled
xunlabeled
buildnetwork
xlabeled
xunlabeled
subnetworks
unfold
return
classify
xunlabeled
subnetworks
end
function
cid:46
equation
end
algorithm
labeled
component
unfolding
system
function
unfold
end
function
cid:46
equation
cid:46
equation
diag
end
return
subnetworks
cid:46
equation
end
neighborhood
unfolding
denote
number
edges
neighborhood
algorithm
algorithm
summarizes
steps
learning
model
algorithm
accepts
labeled
dataset
xlabeled
unla-
beled
dataset
xunlabeled
user-deﬁned
parameters—the
competition
parameter
system
time
limit
parameter
moreover
necessary
choose
network
formation
technique
ﬁrst
step
learning
model
mapping
original
vector-formed
data
network
using
chosen
network
formation
technique
afterward
unfold
network
described
algorithm
algorithm
iterates
lcu
system
produce
one
subnetwork
class
steps
2–6
initialize
system
state
indicated
equation
steps
7–15
iterate
system
using
evolution
function
step
calculates
returns
unfoldings
class
back
algorithm
using
produced
unfoldings
unlabeled
data
classiﬁed
described
equation
computational
complexity
running
time
provide
computational
complexity
analysis
step
step
construction
complex
network
input
dataset
depends
chosen
method
since
|v|
|xlabeled|+
|xunlabeled|
number
data
samples
k-nn
method
example
complexity
order
|v|
log
|v|
using
multidimensional
binary
search
tree
time
complexity
common
graph-based
techniques
disregarding
graph
construction
step
table
algorithm
time
complexity
transductive
svm
local
global
consistency
large
scale
transductive
svm
dynamic
label
propagation
label
propagation
original
particle
competition
labeled
component
unfolding
minimum
tree
cut
|v|3
|v|3
|v|2
|v|2
|v|2
|v|
|e|
|v|
|e|
|v|
second
step
running
system
deﬁned
equa-
tion
using
sparse
matrices
system
initializa-
tion
steps
2–6
algorithm
complexity
order
|v|
|e|
system
iteration
calculates
times
evolution
function
represented
steps
8–14
time
complexity
part
system
evolution
presented
step
computation
matrix
matrix
|e|
non-zero
entries
necessary
calculate
non-zero
entry
hence
step
complexity
order
|e|
however
denominator
equation
values
step
computation
vector
vector
|l|
non-zero
entries
also
necessary
calculate
total
number
particles
system
calculation
time
complexity
order
|l|
|v|
step
computation
matrix
multiplica-
tion
diagonal
matrix
sparse
matrix
|e|
non-zero
entries
time
complexity
order
|e|
step
computation
vector
suppose
cid:104
cid:105
average
vertex
degree
input
network
follows
performed
|v|
cid:104
cid:105
|e|
summation
complexity
order
|e|
step
computation
matrix
sparse
matrix
system
evolution
unfolding
process
performs
|e|
operations
thus
total
time
complexity
order
system
simulation
|e|
|v|
however
value
ﬁxed
value
usually
small
vertex
labeling
step
last
step
learning
model
time
complexity
step
depends
calculation
number
edges
neighborhood
unlabeled
vertex
unfolding
efﬁ-
ciently
calculated
using
one
step
breadth-ﬁrst
search
hence
order
average-time
complexity
cid:0
|u|
cid:104
cid:105
cid:1
|e|
summary
considering
discussion
learning
model
runs
|v|
log
|v|
|e|
|v|
including
transformation
vector-based
dataset
network
ta-
ble
compares
time
complexity
common
graph-based
techniques
disregarding
graph
construction
step
proposed
lcu
method
minimum
tree
cut
linear
time
though
latter
must
either
receive
construct
spanning
tree
consequently
minimum
tree
cut
performance
similar
scalable
version
traditional
accepted
version
doi
10.1109/tnnls.2016.2626341
fig
running
time
seconds
iterations
system
random
networks
input
networks
400
000
edges
many
different
numbers
vertices
000
vertices
many
different
numbers
edges
algorithms
using
subsampling
practices
figure
depicts
running
time
single
iteration
system
varying
number
vertices
edges
respectively
independent
runs
measure
time
iterations
totalizing
300
samples
network
size
set
two
classes
labeled
vertices
experiments
run
intel
cid:13
coretm
cpu
860
2.80ghz
ram
memory
ddr3
1333
mhz
experiment
shows
system
runs
linear
time
function
number
vertices
edges
conforms
theoretical
analysis
computer
simulations
study
stochastic
system
deterministic
version
present
experimental
analyses
concern
equivalence
additionally
study
meaning
parameters
learning
model
evaluate
model
performance
using
artiﬁcial
synthetic
datasets
show
unfolding
process
learning
model
synthetic
data
finally
present
simulation
results
well-known
benchmark
dataset
real
application
human
activity
handwritten
digits
recognition
experimental
analysis
section
present
experiment
assesses
equivalence
unfolding
results
systems
increasing
initial
number
particles
system
networks
used
analysis
generated
following
model
complex
network
constructed
given
labeled
vector
number
edges
vertex
weight
controls
preferential
attachment
vertices
different
classes
resulting
network
contains
|y|
vertices
edges
randomly
preferential
connected
replacement
fig
proportionality
simulation
lines
correlation
measure
cumulative
domination
matrices
systems
varying
initial
number
active
particles
values
close
indicate
cumulative
domination
matrices
systems
tend
proportional
attachment
weight
otherwise
weight
parameter
proportional
overlap
classes
exists
positive
constant
˜δc
κδc
systems
generate
unfoldings
assess
proportionality
systems
simulated
different
networks
0.05
|y|
200
vertices
arranged
two
classes
system
parameter
discretized
0.5
varying
total
number
initial
particles
deg
set
˜nc
|v|
consider
correlation
cumulative
domi-
nation
matrices
systems
two
matrices
proportional
must
correlated
values
correla-
tion
close
indicate
cumulative
domination
matrices
proportional
figure
correlation
depicted
number
initial
particles
increases
correlation
approaches
result
suggests
systems
generate
unfolding
number
initial
particles
grows
inﬁnity
parameter
analysis
lcu
model
two
parameters
apart
network
construction
section
discuss
meaning
learning
model
applied
synthetic
datasets
whose
data
items
sampled
three
dimensional
knot
torus
parametric
curve
cos
sin
sin
cos
sampled
500
data
items
uniformly
along
possible
values
randomly
split
data
items
classes
samples
adjacent
belongs
class
also
added
sample
random
noise
dimension
distribution
0.25
0.35.
figure
depicts
example
dataset
classes
without
noise
since
dataset
complex
form
small
change
parameter
value
may
generate
different
results
therefore
suitable
study
sensitivity
parameters
0.25
0.5
0.75
500.
finally
unbiased
sets
run
lcu
model
parameters
accepted
version
doi
10.1109/tnnls.2016.2626341
fig
three
dimensional
knot
torus
dataset
500
samples
without
noise
left-hand
side
noise
right-hand
side
colors
classes
fig
average
error
proposed
model
different
numbers
classes
problem
colors
shapes
indicate
values
parameter
labeled
points
employed
k-nn
used
network
construction
discuss
parameter
model
discussion
network
construction
parameter
model
input
network
must
simple
pair
vertices
must
exist
one
edge
unweighted
undirected
connected
besides
requirements
two
vertices
must
connected
data
items
considered
similar
enough
particular
problem
experiments
use
k-nn
graph
euclidean
distance
since
proved
approximate
low-dimensional
manifold
point
set
smaller
value
better
results
discussion
system
parameter
lcu
sys-
tem
one
parameter
competition
parameter
parameter
deﬁnes
intensity
competition
particles
particles
randomly
walk
network
without
competition
approaches
particles
likely
compete
consequently
absorbed
figure
depicts
average
error
method
different
values
based
ﬁgure
model
sensitive
general
suggest
setting
better
consistent
classiﬁcation
values
discussion
system
iteration
stopping
parame-
ter
time
limit
parameter
controls
simulation
stop
must
least
large
diameter
network
way
guaranteed
every
edge
visited
particle
since
network
diameter
usually
small
value
simulation
stops
iterations
fig
unfoldings
generated
proposed
system
time
100
highleyman
dataset
edges
colored
according
dominating
class
time
light
gray
edges
stand
edges
presented
original
network
unfolding
vertex
position
imposed
original
data
points
blue
squares
represent
vertices
connected
unfoldings
vertex
position
imposed
original
data
points
color
vertices
result
classiﬁcation
simulations
artiﬁcial
datasets
better
understanding
details
lcu
system
subsection
illustrate
using
two
synthetic
datasets
dataset
different
class
distribution—banana
shape
highleyman
datasets
generated
using
prtools
framework
banana
shape
dataset
uni-
formly
distributed
along
speciﬁc
shape
superimposed
normal
distribution
standard
deviation
along
axes
highleyman
distribution
two
classes
deﬁned
bivariate
normal
distributions
different
parameters
datasets
network
representation
use
k-nn
graph
construction
method
transform
respective
network
form
constructed
network
vertex
represents
datum
connects
nearest
neighbors
determined
euclidean
distance
set
simulation
firstly
technique
tested
highleyman
dataset
class
300
samples
labeled
set
k-nn
algorithm
observe
labeled
data
points
green
class
form
barrier
samples
red
class
unfoldings
gred
100
ggreen
100
presented
figure
ﬁgure
blue
squares
represent
vertices
connected
edges
unfoldings
besides
labeled
data
green
class
forming
barrier
constructed
subnetworks
still
connected—there
single
component
connecting
vertices
subnetwork
better
visualized
figure
ﬁgure
unfoldings
presented
positions
vertices
imposed
original
data
furthermore
colors
vertices
ﬁgure
indicate
result
classiﬁcation
accepted
version
doi
10.1109/tnnls.2016.2626341
test
errors
standard
deviation
best
table
parameters
fig
system
evolution
banana-shaped
distribution
dataset
red
green
colors
represent
two
classes
unlabeled
points
black
ones
labeled
vertices
represented
larger
colored
points
edges
colored
according
dominating
class
current
iteration
light
gray
point
stands
vertex
dominated
yet
network
representation
dataset
beginning
system
system
iteration
time
respectively
result
dataset
classiﬁcation
overlapping
data
identiﬁed
vertices
belong
two
unfoldings
result
reveals
competition
system
edges
provide
information
competition
vertices
since
identify
overlapping
vertices
part
system
without
special
treatments
modiﬁcations
last
synthetic
dataset
600
samples
equally
split
two
classes
figure
initial
state
system
illustrated
dataset
represented
network
network
representation
obtained
setting
k-nn–graph
construction
stage
edges
dominated
classes
starting
state
labeled
vertices
sources
generate
particles
carry
label
sources
though
particles
shown
figures
snapshots
system
evolution—at
time
20—where
edge
colored
dominating
class
iteration
illustrations
solid
red
line
stands
+δgreen
edge
δred
dashed
green
δgreen
line
stands
opposite
δred
edge
drawn
solid
light
gray
line
expected
edges
close
sources
dominated
initially
farther
edges
progressively
dominated
time
figure
every
edge
dominated
edge
domination
change
anymore
figure
shows
dataset
classiﬁcation
following
system
result
example
points
labeled
set
technique
correctly
identify
pattern
formed
class
results
satisfactory
reinforcing
ability
technique
learning
arbitrary
class
distributions
δgreen
δgreen
δred
+δred
labeled
42.90
4.33
46.94
3.93
4.93
2.63
15.65
3.81
59.96
6.13
47.56
1.80
29.71
3.53
0.25
0.75
0.625
0.875
100
labeled
30.03
2.18
36.08
6.32
1.51
0.31
8.36
2.92
13.73
2.91
34.68
2.26
22.41
1.74
0.875
0.625
0.25
0.75
g241c
g241n
digit1
usps
coil
bci
text
test
errors
labeled
training
points
table
iii
1-nn
svm
mvu
1-nn
lem
1-nn
cmr
discrete
reg
tsvm
cluster–kernel
lds
laplacian
rls
lgc
lnp
original
particle
competition
labeled
component
unfolding
g241c
47.88
47.32
47.15
44.05
39.96
49.59
24.71
48.28
28.85
43.85
45.82
42.61
47.82
41.17
42.90
g241d
46.72
46.66
45.56
43.22
46.55
49.05
50.08
42.05
50.63
45.68
44.09
41.93
46.24
43.51
46.94
digit1
13.65
30.60
14.42
23.47
9.80
12.64
17.77
18.73
15.63
5.44
9.89
11.31
8.58
8.10
4.93
usps
16.66
20.03
23.34
19.82
13.61
16.07
25.20
19.41
17.57
18.99
9.03
14.83
17.87
15.69
15.65
coil
63.36
68.86
62.62
65.91
59.63
63.38
67.50
67.32
61.90
54.54
63.45
55.82
55.50
54.18
59.96
bci
49.00
49.85
47.95
48.74
50.36
49.51
49.15
48.31
49.27
48.97
47.09
46.37
47.65
48.00
47.56
text
38.12
45.37
45.32
39.44
40.79
40.37
31.21
42.72
27.15
33.68
46.83
49.53
41.06
34.84
29.71
avg
rank
9.3
13.0
9.3
9.1
6.9
10.4
10.0
10.1
8.0
5.9
6.9
5.1
7.1
4.0
4.9
test
errors
100
labeled
training
points
table
1-nn
svm
mvu
1-nn
lem
1-nn
cmr
discrete
reg
tsvm
cluster-kernel
lds
laplacian
rls
lgc
lnp
original
particle
competition
labeled
component
unfolding
g241c
43.93
23.11
43.01
40.28
22.05
43.65
18.46
13.49
18.04
24.36
41.64
30.39
44.13
21.41
30.03
g241d
42.45
24.64
38.20
37.49
28.20
41.65
22.42
4.95
23.74
26.46
40.08
29.22
38.30
25.85
36.08
digit1
3.89
5.53
2.83
6.12
3.15
2.77
6.15
3.79
3.46
2.92
2.72
3.05
3.27
3.11
1.51
usps
5.81
9.75
6.50
7.64
6.36
4.68
9.77
9.68
4.96
4.68
3.68
6.98
17.22
4.82
8.36
coil
17.35
22.93
28.71
23.27
10.03
9.61
25.80
21.99
13.72
11.92
45.55
11.14
11.01
10.94
13.73
bci
48.67
34.31
47.89
44.83
46.22
47.67
33.25
35.17
43.97
31.36
43.50
42.69
46.22
41.57
34.68
text
30.11
26.45
32.83
30.77
25.71
24.00
24.52
34.28
23.15
23.57
56.83
40.79
38.45
27.92
22.41
avg
rank
11.4
8.1
10.6
10.9
6.6
7.1
7.7
7.4
5.4
4.4
9.3
8.3
11.4
5.3
6.0
simulations
benchmark
datasets
compare
model
semi-supervised
techniques
tested
chapelle
benchmark
benchmark
formed
seven
datasets
1500
data
points
except
bci
400
points
datasets
described
dataset
distinct
unbiased
sets
splits
labeled
points
provided
within
benchmark
half
splits
formed
labeled
points
half
100
labeled
points
author
benchmark
ensured
split
contains
least
one
data
point
class
result
average
test
error—the
proportion
data
points
incorrectly
labeled—over
splits
compare
results
ones
obtained
following
techniques
1-nearest
neighbors
support
vector
machines
svm
maximum
variance
unfolding
mvu
1-nn
laplacian
eigenmaps
lem
quadratic
criterion
class
mass
regularization
cmr
discrete
regularization
discrete
reg
transductive
support
vector
machines
tsvm
cluster
kernels
cluster-
kernel
low-density
separation
lds
laplacian
regularized
least
squares
laplacian
rls
local
global
consistency
lgc
label
propagation
linear
neighborhood
propa-
accepted
version
doi
10.1109/tnnls.2016.2626341
gation
lnp
network-based
stochastic
semisupervised
learning
vertex
domination
simulation
results
collected
except
lgc
lnp
original
particle
competition
found
simulation
lcu
system
discretize
interval
parameter
0.125
also
vary
k-nn
parameter
tested
every
combination
moreover
1000.
table
present
results
standard
deviation
splits
along
best
combination
parameters
generated
best
accuracy
result
test
error
comparison
labeled
points
shown
table
iii
comparison
100
labeled
points
table
apart
dataset
last
column
average
performance
rank
technique
datasets
ranking
arranges
methods
comparison
test
error
rate
ascending
order
single
dataset
assign
rank
method
lowest
average
test
error
dataset
rank
method
second
lowest
test
error
average
ranking
average
value
rankings
method
datasets
smaller
ranking
score
better
method
performed
average
rank
column
lcu
technique
best
ranked
best
group
techniques
labeled
100
labeled
cases
statistically
compare
results
presented
tables
iii
tests
set
signiﬁcance
level
first
use
test
based
average
rank
method
evaluate
null
hypothesis
techniques
equivalent
friedman
test
statistically
signiﬁcant
difference
rank
techniques
since
friedman
test
result
reports
statistical
signiﬁcance
use
wilcoxon
signed-rank
test
pairwise
difference
test
test
null
hypothesis
ﬁrst
technique
greater
equal
error
results
second
rejected
signiﬁcance
level
say
ﬁrst
technique
superior
second
analyzing
results
100
labeled
points
together
conclude
technique
superior
1-nn
lem
1-nn
mvu
examining
separately
labeled
points
method
also
superior
discrete
regularization
cluster
kernel
svm
100
labeled
points
also
superior
lnp
lgc
whereas
laplacian
rls
superior
table
summarises
results
technique
provide
precision
recall
score
using
labeled
samples
average
results
inde-
pendent
labeled
set
conﬁguration
also
provide
original
results
using
svm
approximately
labeled
samples
technique
performs
well
svm
using
far
fewer
labeled
samples
using
suggested
parameter
set
feature
quite
attractive
may
represent
big
saving
money
efforts
involving
manually
data
labeling
semi-supervised
learning
simulations
mnist
dataset
mnist
dataset
comprises
70,000
examples
hand-
written
digits
digits
size-normalized
cen-
tered
ﬁxed-size
image
supervised
learning
setting
dataset
split
two
sets
60,000
examples
training
10,000
testing
adapt
dataset
semi-supervised
learning
problem
use
setting
similar
labeled
input
data
items
selected
training
set
unlabeled
ones
test
set
although
use
validation
set
use
additional
set
least
1,000
labeled
samples
parameter
tuning
network
representation
obtained
images
without
preprocessing
use
euclidean
distance
items
construct
k-nn
network
similarly
previous
experiment
value
smallest
value
generates
connected
network
parameters
ﬁxed
0.9
500.
table
compares
error
rate
method
semi-supervised
techniques
best
knowledge
could
ﬁnd
many
papers
experiments
semi-supervised
settings
mnist
dataset
due
available
results
literature
sought
carry
experiments
input
similar
possible
compared
results
however
single
sampling
less
1,000
labeled
samples
set
60,000
images
probably
results
biased
accuracy
result
opt
average
results
labeled
sets
parameter
setting
model
performs
well
even
without
preprocessing
validation
set
result
indicates
besides
simplicity
constructed
network
learning
system
obtain
enough
knowledge
data
simulations
human
activity
dataset
conclusion
human
activity
recognition
using
smartphones
dataset
comprises
10299
data
samples
sample
matches
561
features
extracted
motion
sensors
attached
person
time
window
person
performed
six
activities
target
labels
dataset—walking
walking
upstairs
walking
downstairs
sitting
standing
laying
use
k-nn
dataset
network
represen-
tation
smallest
value
generates
connected
network
parameters
ﬁxed
1000.
compare
results
ones
published
splitting
problem
six
binary
classiﬁcation
tasks
presented
transductive
semi-supervised
learn-
ing
technique
based
vertex-edge
dynamical
system
complex
networks
first
input
data
mapped
network
proposed
labeled
component
unfolding
lcu
system
runs
network
stage
particles
compete
edges
network
particle
passes
edge
increases
class
dominance
edge
decreasing
classes
dominance
three
dynamics—
walking
absorption
production—provide
biologically
inspired
scenario
competition
cooperation
labels
assigned
according
dominant
class
edges
system
unfolds
original
network
result
accepted
version
doi
10.1109/tnnls.2016.2626341
performance
comparison
human
activity
recognition
using
smartphones
dataset
table
labeled
component
unfolding
precision
.984
.013
.981
.009
.987
.017
.864
.034
.840
.024
.996
.002
labeled
recall
.941
.030
.935
.026
.901
.016
.698
.049
.842
.053
.999
.000
score
.962
.016
.957
.015
.942
.011
.770
.022
.839
.017
.998
.001
precision
.992
.004
.988
.008
.994
.008
.883
.015
.870
.023
.997
.001
labeled
recall
.985
.011
.961
.013
.918
.011
.743
.039
.844
.022
.999
.000
score
.989
.006
.974
.008
.955
.007
.806
.020
.856
.006
.998
.000
precision
.994
.002
.991
.003
.998
.001
.905
.014
.896
.013
.997
.001
labeled
recall
.997
.001
.981
.006
.945
.008
.814
.015
.872
.021
.999
.000
score
.995
.001
.986
.004
.971
.004
.857
.006
.884
.009
.998
.000
svm
≈70
labeled
precision
score
recall
.992
.958
.976
.880
.974
1.000
.974
.969
.982
.922
.936
1.000
.957
.980
.988
.969
.901
1.000
test
errors
mnist
dataset
table
method
lcu
tsvm*
100
labeled
10.62
1.91
1000
labeled
6.31
0.46
16.81
16.86
7.75
embed
nn*
embed
cnn*
comparison
biased
since
results
rely
single
unique
labeled
set
see
text
details
5.65
8.52
3.82
grouping
edges
dominated
class
finally
employ
unfoldings
classify
unlabeled
data
furthermore
rigorous
studies
done
novel
lcu
system
deterministic
system
implementation
brings
advantages
stochastic
counterpart
time
complexity
deterministic
one
depend
number
particles
beneﬁted
better
results
considering
continuously
varying
number
initial
particles
besides
lcu
system
allows
stable
transductive
semi-supervised
learning
technique
subquadratic
order
complexity
computer
simulations
show
proposed
technique
achieves
good
classiﬁcation
accuracy
suitable
situations
small
number
labeled
samples
available
another
interesting
feature
proposed
model
directly
provides
overlapping
information
vertex
subset
vertices
future
works
would
like
investigate
mathe-
matical
property
lcu
system
directed
weighted
networks
besides
interesting
improve
runtime
via
network
sampling
methods
estimation
methods
way
model
suitable
applied
process
large
enough
datasets
streaming
data
another
interesting
research
treat
labels
edges
instead
nodes
semi-supervised
learning
environment
references
chapelle
sch¨olkopf
zien
eds.
semi-supervised
learning
cambridge
mit
press
2006
zhu
goldberg
introduction
semi-supervised
learning
3rd
morgan
claypool
publishers
2009
nigam
mccallum
thrun
mitchell
text
clas-
siﬁcation
labeled
unlabeled
documents
using
machine
learning
vol
2-3
103–134
2000
loog
jensen
semi-supervised
nearest
mean
classiﬁca-
tion
constrained
log-likelihood
ieee
trans
neural
netw
learn
syst.
vol
995–1006
2015
wagstaff
cardie
rogers
schroedl
constrained
means
clustering
background
knowledge
proc
18th
international
conference
machine
learning
2001
577–584
z.-h.
zhou
tri-training
exploiting
unlabeled
data
using
three
classiﬁers
ieee
trans
knowl
data
eng.
vol
1529–1541
2005
vapnik
statitical
learning
theory
new
york
wiley
1998
silva
zhao
semi-supervised
learning
guided
modularity
measure
complex
networks
neurocomputing
vol
30–37
2012
cheng
pan
semi-supervised
domain
adaptation
manifolds
ieee
trans
neural
netw
learn
syst.
vol
2240–2249
2014
zhang
lan
kwok
vucetic
parvin
scaling
graph-based
semisupervised
learning
via
prototype
vector
machines
ieee
trans
neural
netw
learn
syst.
vol
444–457
2015
belkin
niyogi
sindhwani
manifold
regularization
geometric
framework
learning
labeled
unlabeled
examples
mach
learn
res.
vol
2399–2434
2006
newman
a.-l.
barab´asi
watts
structure
princeton
dynamics
networks
princeton
studies
complexity
university
press
2006
newman
networks
introduction
1st
new
york
oxford
university
press
2010
silva
zhao
machine
learning
complex
networks
1st
springer
2016
zhu
goldberg
khot
new
directions
graph-based
semi-supervised
learning
proc
eee
international
conference
multimedia
expo
1504–1507
2009
quiles
zhao
alonso
romero
particle
competition
complex
network
community
detection
chaos
vol
033107
2008
silva
zhao
network-based
stochastic
semisupervised
learning
ieee
trans
neural
netw
learn
syst.
vol
451–66
2012
network-based
stochastic
semisupervised
learning
ieee
trans
neural
netw
learn
syst.
vol
451–466
2012
detecting
preventing
error
propagation
via
competitive
learning
neural
networks
vol
70–84
2013
jensen
sur
les
fonctions
convexes
les
in´egalit´es
entre
les
valeurs
moyennes
acta
mathematica
vol
175–193
1906
bentley
multidimensional
binary
search
trees
used
associative
searching
commun
acm
vol
509–517
1975
zhou
bousquet
lal
weston
sch¨olkopf
learning
local
global
consistency
advances
neural
information
processing
systems
thrun
saul
sch¨olkopf
eds
mit
press
2004
321–328
collobert
sinz
weston
bottou
large
scale
transduc-
tive
svms
journal
machine
learning
research
vol
1687–1712
2006
wang
tsotsos
dynamic
label
propagation
semi-supervised
multi-class
multi-label
classiﬁcation
proc
ieee
international
conference
computer
vision
425–432
2013
zhu
ghahramani
learning
labeled
unlabeled
data
label
propagation
school
comput
sci
carnegie
mellon
univ
pittsburgh
tech
rep
vol
cmu-cald-02-107
1–19
2002
zhang
huang
geng
liu
mtc
fast
robust
graph-based
transductive
learning
method
ieee
trans
neural
netw
learn
syst
vol
1979–1991
2014
tenenbaum
silva
langford
global
geometric
framework
nonlinear
dimensionality
reduction
science
vol
290
5500
2319
2000.
accepted
version
doi
10.1109/tnnls.2016.2626341
duin
juszczak
pacl´ık
pekalska
ridder
tax
verzakov
pr-tools4.1
matlab
toolbox
pattern
recognition
2007
hollander
wolfe
nonparametric
statistical
methods
2nd
wiley-interscience
1999
anguita
ghio
oneto
parra
reyes-ortiz
public
domain
dataset
human
activity
recognition
using
smart-
phones
21th
european
symposium
artiﬁcial
neural
networks
computational
intelligence
machine
learning
esann
2013
2013
weston
ratle
mobahi
collobert
deep
learning
via
semi-supervised
embedding
neural
networks
tricks
trade
springer
2012
639–655
