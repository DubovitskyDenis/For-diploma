high-dimensional
black-box
optimization
via
divide
approximate
conquer
peng
yang1
tang1∗
xin
yao2
1ubri
school
computer
science
technology
university
science
technology
china
hefei
china
230027
2cercia
school
computer
science
university
birmingham
birmingham
b15
2tt
u.k.
emails
trevor
mail.ustc.edu.cn
ketang
ustc.edu.cn
x.yao
cs.bham.ac.uk
abstract
divide
conquer
conceptually
well
suited
high-dimensional
optimization
decom-
posing
problem
multiple
small-scale
sub-
problems
however
appealing
performance
seldom
observed
sub-problems
in-
terdependent
paper
suggests
major
difﬁculty
tackling
interdependent
sub-problems
lies
precise
evaluation
partial
solu-
tion
sub-problem
overwhelm-
ingly
costly
thus
makes
sub-problems
non-
trivial
conquer
thus
propose
approxi-
mation
approach
named
divide
approximate
conquer
dac
reduces
cost
partial
solution
evaluation
exponential
time
poly-
nomial
time
meanwhile
convergence
global
optimum
original
problem
still
guaranteed
effectiveness
dac
demon-
strated
empirically
two
sets
non-separable
high-dimensional
problems
arg
maxx∈x
introduction
developing
artiﬁcial
intelligence
applications
often
en-
counters
key
task
solving
challenging
optimization
prob-
lems
formally
stated
denotes
function
bounded
solu-
tion
space
denotes
global
optimum
consider
black-box
function
problem
information
completely
unknown
beforehand
function
value
directly
provided
ex-
plicitly
queried
therefore
derivative-free
approaches
brought
bear
however
often
suffer
curse
dimensionality
considerably
large
intuitive
idea
handle
high-dimensional
optimiza-
tion
problem
project
solution
space
onto
lower
di-
mensions
traditional
approaches
perform
well
kab´an
al.
2015
however
nontrivial
identify
ap-
propriate
projection
typical
approaches
category
e.g.
random
embedding
techniques
wang
al.
2013
qian
2016
consider
high-dimensional
problem
low
effective
dimensionality
random
pro-
jection
would
sufﬁce
ﬁnd
global
optimal
solution
high-dimensional
problem
low-dimensional
space
al-
though
algorithms
also
showed
appealing
performance
case
low
effective
dimensionality
assumption
mildly
violated
performance
may
satisfactory
wider
range
irreducible
problems
divide-and-conquer
another
general
idea
tack-
ling
large-scale
problems
case
high-dimensional
black-
box
optimization
implemented
dividing
original
problem
multiple
sub-problems
say
dimen-
sionality
...
yang
al.
2008a
i-th
sub-problem
optimized
along
di-
mensions
values
d−di
dimensions
ﬁxed
sub-problem
concerns
low-dimensional
subspace
original
solution
space
applying
ex-
isting
search
method
i-th
sub-problem
leads
di-
dimensional
partial
solution
original
high-dimensional
problem
solution
original
problem
obtained
combining
partial
solutions
achieved
sub-problems
given
appropriate
sub-problems
optimizer
above-
described
strategy
works
well
so-called
separable
problems
global
optimal
optimum
found
optimizing
one
dimension
time
regardless
val-
ues
taken
dimensions
chen
al.
2010
condition
hold
performance
heavily
relies
decomposition
method
omidvar
al.
2014
aims
divide
black-box
high-dimensional
problem
way
global
optimum
still
obtained
solving
sub-problems
fully
independent
manner
past
years
large
variety
decomposition
meth-
ods
proposed
mahdavi
al.
2015
despite
performance
enhancement
brought
none
methods
guaranteed
achieve
desired
sub-problems
meanwhile
practical
problem
interest
may
fully
non-
separable
ideal
decomposition
mentioned
even
exist
therefore
deal
conquer
interdependent
sub-problems
remains
challenge
context
high-dimensional
optimization
paper
suggest
major
challenge
tack-
ling
interdependent
sub-problems
lies
difﬁculty
eval-
uating
quality
partial
solution
sub-problem
dur-
ing
search
course
speciﬁc
quality
partial
solution
original
problem
depends
values
taken
dimensions
involved
sub-problems
precisely
evaluating
partial
solution
requires
overwhelming
compu-
tation
cost
increases
exponentially
number
interacting
variables
sub-problems
propose
approximation
approach
partial
solution
evaluation
yields
novel
framework
named
divide
approx-
imate
conquer
dac
dac
computational
cost
in-
creases
polynomially
number
solutions
generated
iteration
convergence
global
optimum
original
problem
still
guaranteed
major
difﬁculty
tackling
interdependent
sub-
problems
analyzed
section
proposed
dac
detailed
section
section
reports
empirical
studies
dac
ﬁve
synthetic
high-dimensional
optimization
prob-
lems
hyper-parameters
ﬁne-tuning
task
multi-
class
svm
section
concludes
work
major
challenge
dealing
interdependent
sub-problems
strategy
consists
three
steps
divide
decompose
problem
di-dimensional
sub-problems
...
conquer
search
best
partial
solution
sub-
problem
applying
existing
search
approach
merge
best
partial
solutions
obtained
sub-
problems
ﬁnal
output
restrict
discussions
conquer
phase
usually
derivative-free
search
process
sub-problem
guided
solutions
better
function
values
despite
exceptions
kirkpatrick
al.
1983
tang
al.
2016
di-dimensional
partial
solution
evaluated
must
complemented
d-dimensional
ﬁxing
values
variables
sub-problems
specially
call
vec-
tor
ﬁxed
values
complement
partial
solution
partial
solution
receive
different
function
val-
ues
different
complements
fortunately
indetermi-
nate
function
values
partial
solutions
inﬂuence
search
process
unless
rank
partial
solutions
changes
search
direction
actually
determined
rank
partial
solutions
may
change
sub-problems
interde-
pendent
deﬁned
follows
deﬁnition
interacting
variables
chen
al.
2010
given
d-dimensional
problem
i-th
j-th
variables
said
interacting
rank
two
partial
solutions
cid:48
i-th
dimension
may
change
different
complements
e.g.
cid:48
cid:48
...
...
...
...
cid:48
...
...
cid:48
...
...
cid:48
deﬁnition
interdependent
sub-problems
given
two
arbitrary
sub-problems
said
inter-
dependent
least
one
variable
sub-problem
inter-
acting
least
one
variable
sub-problem
intuitively
two
interacting
variables
2-d
schwefel
function
depicted
figure
rank
two
par-
tial
solutions
cid:48
ﬁrst
dimension
varies
ﬁxing
different
values
second
dimension
i.e.
cid:48
...
...
...
cid:48
...
j-th
variable
cid:48
figure
two
interacting
variables
2-d
schwefel
function
problem
separable
interdependency
ex-
ists
sub-problems
partial
solutions
comple-
mented
arbitrary
identical
values
without
perturbing
rank
however
many
non-separable
problems
sub-problems
interdependent
rank
partial
solutions
signiﬁcantly
relies
complements
re-
sult
search
direction
close
relation
choice
complements
hence
choice
complements
par-
tial
solutions
carefully
addressed
otherwise
search
process
sub-problem
run
risk
misled
eventually
resulting
ineffective
search
unfortunately
ﬁnd
accurately
complement
partial
solutions
difﬁcult
optimization
problem
lemma
difﬁculty
accurately
complementing
given
set
partial
solutions
di-dimensional
i-th
sub-problem
let
cid:99
set
variables
except
ones
i-th
sub-problem
cid:99
di|
cardinality
cid:99
cid:99
probability
ﬁxing
correct
value
j-th
variable
cid:99
arithmetic
mean
cid:99
...
probability
accurately
complementing
partial
solutions
cor-
rectly
ranked
denoted
cid:99
di|
cid:89
j=1
cid:99
cid:18
cid:80
cid:99
di|
j=1
cid:99
cid:99
di|
cid:19
cid:99
di|
d−di
proof
deﬁnition
learn
ﬁxing
correct
val-
ues
variables
independent
events
deﬁnition
know
variables
sub-problem
non-
interacting
thus
directly
cid:81
cid:99
di|
j=1
cid:99
notice
cid:99
cid:99
-th
variable
inter-
finally
according
am-gm
inequality
eq.
acting
variable
i-th
sub-problem
lemma
shows
computational
cost
accurately
complement-
ing
partial
solutions
increases
exponentially
number
variables
interacting
current
sub-problem
hence
interdependent
sub-problems
accurately
conquered
within
reasonable
time
budget
divide
approximate
conquer
3.1
accurate
complement
according
lemma
brute-force
method
appli-
cable
accurately
complement
partial
solutions
interde-
pendent
sub-problems
however
required
computational
costs
beyond
acceptable
hand
effective
way
derive
mathematical
formulation
problem
observing
corresponding
brute-force
method
method
usually
scan
whole
solution
space
thus
reﬂects
problem
characteristics
naturally
core
idea
brute-force
method
mathematically
described
follow
maximization
case
considered
∀xi
max
xr∈sr
max
xr∈sr
denotes
partial
solution
i-th
sub-problem
...
denotes
candidate
complement
...
...
denotes
sub-problems
except
i-th
one
denote
corresponding
sub-
space
solution
space
subjecting
|si|
|sr|
|s|
optimal
solution
states
fact
correct
rank
partial
solu-
tions
obtained
comparing
best
function
val-
ues
among
combinations
possible
complements
basis
mathematically
deﬁne
problem
accu-
rately
complementing
partial
solutions
follow
deﬁnition
accurate
complement
given
arbitrary
complement
said
accurate
complement
xr∈sr
arg
max
notice
every
partial
solution
accurate
complement
identify
accurate
complement
com-
binations
possible
complements
evaluated
among
complement
largest
function
value
chosen
3.2
dac
approximate
approach
fact
deﬁnition
allows
approximate
accurate
complements
partial
solutions
limited
set
candidate
complements
cid:48
sr.
arg
max
cid:23
cid:101
xr∈sr
means
cid:23
cid:101
cid:48
r∈s
cid:48
cid:48
arg
max
r⊆sr
accurate
cid:101
since
max
xr∈sr
max
r∈s
cid:48
r⊆sr
cid:48
cid:48
based
reasonable
assume
good
approx-
imate
complements
perturb
rank
partial
solutions
slightly
thus
gives
rise
proposed
divide
approximate
conquer
dac
shown
algorithm
dac
shares
framework
basic
difference
complementing
par-
tial
solution
dac
always
selects
complement
associated
largest
function
value
among
given
set
candidate
algorithm
dac
tmax
randomly
initialize
solutions
divide
sub-problems
tmax
cid:48
searchoperator
cid:101
cid:101
cid:48
cid:101
arg
max
xr∈x1
arg
max
xr∈x1
endfor
cid:48
cid:101
cid:48
cid:48
cid:101
endfor
endfor
output
best
solution
found
far
cid:101
cid:48
complements
speciﬁcally
dac
works
ﬁrst
randomly
ini-
tializing
solutions
step
problem
de-
composed
sub-problems
certain
decomposi-
tion
strategy
step
without
loss
generality
let
consider
i-th
sub-problem
new
partial
solutions
cid:48
generated
applying
search
operator
current
ones
i.e.
step
identify
approximate
complement
j-th
partial
solution
...
combinations
vectors
partial
solutions
sub-problems
evaluated
vec-
tor
associated
largest
function
value
chosen
step
strat-
j-th
new
partial
solution
cid:48
step
according
certain
selection
criterion
partial
solutions
re-
mained
next
iteration
step
notice
selec-
tion
partial
solution
conditioned
corresponding
approximate
complement
last
j-th
selected
par-
tial
solution
corresponding
partial
solutions
rest
sub-problems
replaced
approximate
com-
plement
optimizing
step
approximate
complement
cid:101
egy
used
obtain
approximate
complement
cid:101
cid:48
result
dac
consumes
function
evaluations
fes
iteration
signiﬁcant
reduction
brute
force
method
meanwhile
albeit
computational
time
cut
convergence
dac
still
guaranteed
lemma
convergence
dac
given
search
algorithm
converge
global
op-
timum
sub-problem
regarded
independent
prob-
lems
dac
approximately
converge
global
opti-
mum
original
problem
proof
loss
generality
let
consider
function
values
j-th
solution
t-th
t+1-th
iteration
i.e.
i-th
sub-problem
xt+1
max
r∈xt
max
r∈xt
xt+1
ﬁrst
indicates
procedure
identifying
approximate
complements
current
partial
solutions
step
algorithm
second
represents
procedures
generating
new
partial
solutions
step
al-
gorithm
identifying
approximate
complements
step
algorithm
selecting
better
ones
candidate
partial
solutions
conditioned
approximate
comple-
ments
step
algorithm
repeating
...
...
means
function
value
j-th
solution
monotonically
increases
iteration
index
...
xt+1
xt+1
note
equality
holds
two
cases
approximate
complement
partial
solution
hap-
pens
corresponding
partial
solutions
rest
sub-problems
i.e.
cid:101
search
algorithm
fails
produce
new
better
solu-
tions
i.e.
max
xr∈sr
max
xr∈sr
cid:48
ﬁrst
case
actually
explains
term
approximate
dac
happens
probability
least
hence
sub-problems
optimizer
dac
optimally
solve
sub-problem
separately
global
optimum
value
approximately
approached
dac
3.3
dac-hc
instantiation
dac
instantiation
dac
presented
illustrate
detail
steps
dac
algorithm
empirical
studies
instantiate
dac
decomposition
strategy
sub-problems
optimizer
speciﬁed
order
highlight
advantages
dac
framework
empirical
studies
improvement
performance
intro-
duced
two
speciﬁed
components
kept
minimal
basis
ﬁrst
decompose
problem
via
random
grouping
yang
al.
2008a
begin-
ning
iteration
equal-sized
sub-problems
ran-
domly
generated
sub-problems
optimizer
parallel
hill
climbing
phc
method
employed
thus
yields
dac-hill
climbing
dac-hc
dac-hc
conducts
rls
processes
sub-problem
speciﬁcally
iteration
i-th
sub-problem
j-th
rls
produces
one
new
partial
solution
cid:48
applying
gaussian
mutation
operator
current
partial
solution
using
cid:48
denotes
gaussian
random
variable
zero
mean
standard
deviation
identity
matrix
size
generally
value
represents
search
step-size
adaptively
varied
search
may
also
distinct
rlss
even
dimen-
sions
keep
simple
rlss
dac-hc
initially
set
search
step-size
i.e.
1.00.
search
step-size
adapted
every
iteration
terms
1/5
successful
rule
kern
al.
2004
using
exp
d+1
cid:101
cid:101
cid:48
randomly
divide
equal-sized
sub-problems
algorithm
dac-hc
tmax
randomly
initialize
solutions
tmax
cid:101
cid:48
arg
max
xr∈x1
i×exp
cid:101
cid:48
cid:101
d+1
cid:48
endfor
output
best
solution
found
far
endfor
endfor
cid:101
cid:101
indicator
function
returns
true
otherwise
selection
procedure
new
partial
solution
phc
competes
corresponding
old
one
survival
based
one-on-one
relation
re-
duce
fes
consumption
dac-hc
half
dac
i.e.
conducted
letting
two
competing
partial
so-
lutions
share
approximate
complement
reason
behind
adopting
rlss
small
search
step-
sizes
pairwise
partial
solutions
close
solution
space
case
respective
approximate
complements
may
also
similar
lastly
pseudo-code
dac-hc
given
algorithm
illustration
empirical
studies
dac
proposed
solving
non-separable
high-dimensional
optimization
problems
empirical
studies
concentrate
verify
effectiveness
dac-hc
purpose
two
sets
non-separable
high-dimensional
optimization
problems
employed
i=1
100
i=1
cid:80
2009
formulated
fsch
cid:80
fros
cid:80
d−1
4.1
varied
numbers
interacting
variables
tests
ﬁrst
set
problems
based
fully
non-separable
functions
i.e.
schwefel
1.2
rosenbrock
tang
al.
j=1
xi+1
two
functions
variables
interacting
meanwhile
also
observed
many
real-world
problems
parts
variables
interacting
friesen
domin-
gos
2015
hence
necessity
test
dac-hc
varied
numbers
interacting
variables
purpose
consider
three
problems
combine
schwefel
1.2
function
fully
separable
sphere
function
i.e.
different
formations
dimension-
ality
set
1000
problems
variables
ran-
domly
perturbed
avoid
potential
bias
problems
expected
minimized
global
optimal
value
0.00.
fsph
cid:80
i=1
table
formulations
tested
functions
shifted
solution
global
optimum
random
permutation
...
1000
50.
fsch
106
fsph
pm+1
cid:80
cid:80
cid:80
fsch
k=1
fsch
k−1
∗m+1
pk∗m
+fsph
k=1
fsch
k−1
∗m+1
pk∗m
k=1
fros
k−1
∗m+1
pk∗m
speciﬁcally
consists
group
interacting
vari-
ables
950
independent
variables
groups
interacting
variables
500
independent
variables
compose
groups
interacting
variables
involves
group
1000
interacting
variables
de-
tailed
formulations
listed
table
ﬁve
problems
two
groups
comparisons
conducted
different
purposes
advantages
dac-hc
existing
approaches
ﬁrst
group
comparison
dac-hc
compared
cma-es
hansen
ostermeier
2001
resoo
qian
2016
decc-i
omidvar
al.
2014
representatives
three
basic
ideas
high-dimensional
opti-
mization
straightforward
method
dimensionality
reduc-
tion
respectively
speciﬁcally
cma-es
widely
endorsed
powerful
global
optimizer
applied
many
aspects
basic
version
cma-es
uti-
lized
resoo
recently
proposed
approach
built
random
embedding
reducing
dimensionality
noted
decc-i
algorithm
black-box
op-
timization
ideal
approach
perfectly
decomposes
problems
using
priori
knowledge
functions
hence
sub-problems
decc-i
independent
case
dac-hc
besides
sub-problems
decc-i
optimized
variant
differential
evolution
yang
al.
2008b
empirically
observed
ad-
vanced
employed
rlss
tang
al.
2016
basis
dac-hc
outperforms
decc-i
reasonable
infer
proposed
dac
facilitates
non-separable
high-dimensional
optimization
problems
algorithms
repeated
runs
problem
diminish
noise
introduced
randomized
search
essence
time
budget
run
set
3e6
fes
cma-es
parameterless
parameter
needs
spec-
iﬁed
resoo
coarse-tuning
probability
set
1/3
restart
times
set
reduced
dimensionality
set
100.
decc-i
parameter
i.e.
population
size
set
100
omidvar
2014
suggested
dac-hc
two
parameters
speciﬁed
i.e.
number
rlss
number
sub-problems
recall
parameter
generally
inﬂuences
approximate
ability
dac
test
extreme
case
dac-hc
set
gain
relatively
fair
comparison
resoo
set
sub-problems
op-
timizer
faces
100-dimensional
problem
resoo
table
mean
standard
derivation
ﬁnal
outputs
function
mean
std
mean
std
mean
std
mean
std
mean
std
cma-es
1.35e+09
3.29e+08
0.00e+00
0.00e+00
0.00e+00
0.00e+00
2.87e+06
6.61e+05
3.36e+03
1.81e+03
resoo
2.52e+11
3.62e+10
1.28e+07
9.41e+05
3.62e+07
2.89e+06
7.80e+07
7.10e+06
1.41e+12
8.02e+09
decc-i
2.97e+01
8.59e+01
1.48e+03
4.28e+02
3.91e+04
2.75e+03
1.74e+06
9.54e+04
1.17e+03
9.66e+01
dac-hc
0.00e+00
0.00e+00
1.55e+00
1.25e+00
7.78e+02
7.12e+02
3.93e+05
2.52e+04
1.13e+03
2.32e+02
mean
standard
derivation
ﬁnal
outputs
runs
shown
table
gray
cell
indicates
algorithm
achieves
best
mean
value
problem
light
gray
cell
indicates
second
place
dac-hc
outperforms
compared
algorithms
though
slightly
inferior
cma-es
dac-hc
performs
signiﬁcantly
better
decc-i
resoo
since
dac-hc
dominates
decc-i
ﬁve
problems
effectiveness
dac
promoting
non-separable
high-dimensional
optimiza-
tion
problems
conﬁrmed
besides
lemma
veriﬁed
observing
solution
qualities
dac-hc
deteriorates
number
interacting
variables
increases
resoo
per-
forms
poorly
tested
problems
irreducible
empirical
support
convergence
dac
second
group
comparison
phc
rlss
compared
difference
phc
dac-hc
phc
complements
partial
solution
merely
corresponding
partial
solutions
rest
sub-problems
basis
phc
satisfy
con-
vergence
guaranteed
experimental
protocol
set
ﬁrst
group
comparison
convergence
rates
algorithms
shown
fig-
ure
x-axis
denotes
fes
y-axis
denotes
logarithm
function
values
seen
dac-hc
al-
ways
converges
faster
phc
specially
log-linear
conver-
gence
dac-hc
observed
ﬁrst
two
sphere
function
based
problems
noted
employed
rls
also
theoretically
proved
converge
log-linearly
sphere
function
jebalia
al.
2008
coincidence
actually
supports
convergence
dac
stated
lemma
comparatively
convergence
rates
phc
heavily
retarded
due
unﬁt
complements
partial
solutions
4.2
hyper-parameter
tuning
multi-class
svms
given
set
labelled
data
i=1
classiﬁcation
task
train
classiﬁer
terms
i=1
predict
labels
incoming
data
support
vector
ma-
chines
svms
vapnik
1998
often
considered
fam-
ily
powerful
tools
classiﬁcation
svm
linear
kernel
considered
let
svm
re-
quires
ﬁne-tune
three
parameters
solving
fol-
lowing
optimization
problem
minw
i=1
subject
notice
hyper-parameter
supplied
user
2wt
cid:80
table
testing
accuracies
tuned
multi-class
svm
usps
93.92
dataset
gridsearch
resoo
cma-es
dac-hc
94.60
±0.04
85.40
±0.11
85.72
±0.24
94.38
93.33
±0.37
±0.24
85.43
84.34
±0.21
±0.17
85.36
84.03
±0.12
±1.44
85.16
85.12
news20
letter
supposed
best
hyper-parameters
obtained
run
tested
testing
set
testing
accu-
racy
regarded
performance
algorithm
run
time
budget
training
phase
run
set
100
fes
resoo
probability
set
1/3
restart
times
set
reduced
dimensionality
set
1/5
original
dimensionality
i.e.
re-
spectively
grid
search
100
candidate
solutions
uniformly
selected
solution
space
dac-hc
parameters
set
respectively
table
lists
mean
standard
derivation
testing
accuracies
multi-class
svms
tuned
algorithm
seen
resoo
dac-hc
outperform
grid
search
although
improvement
marginal
hence
inferred
tuning
multiple
hyper-parameters
rather
one
shared
hyper-parameter
beneﬁcial
multi-
class
svms
dac-hc
outperforms
compared
algo-
rithms
usps
letter
datasets
although
shows
slightly
lower
accuracy
resoo
news20
dataset
stable
behavior
observed
standard
derivation
smaller
cma-es
inferior
grid
search
three
problems
phenomenon
suggests
tuning
multi-
ple
hyper-parameters
multi-class
svms
appropriate
optimization
approach
employed
otherwise
would
better
tune
one
shared
hyper-parameter
also
worthwhile
notice
cma-es
adopt
special
treatment
high-dimensional
optimization
conclusions
future
directions
work
investigated
divide
conquer
idea
high-
dimensional
black-box
optimization
problems
found
interdependent
sub-problems
decomposition
actu-
ally
accurately
conquered
instead
proposed
divide
approximate
conquer
dac
solve
sub-problem
approximately
convergence
dac
proved
empirically
supported
empirical
studies
simple
instantiation
dac
i.e.
dac-hc
also
pro-
posed
advantages
dac-hc
existing
represen-
tative
approaches
veriﬁed
two
sets
non-separable
high-dimensional
problems
future
work
interested
promoting
ability
dac
adopting
ad-
theoretically
analyzing
convergence
rate
dac
vanced
sub-problems
optimizers
references
chang
lin
2011
chih-chung
chang
chih-jen
lin
libsvm
library
support
vector
machines
figure
convergence
rates
dac-hc
phc
x-axis
denotes
fes
time
y-axis
denotes
log
penalizes
error
vector
dealing
multi-
class
classiﬁcation
problem
typical
idea
divide
multiple
binary
classiﬁcation
problems
adopting
one-
on-one
strategy
let
number
class
binary
classiﬁers
train
resulting
cid:1
k−1
cid:0
k−1
hyper-parameters
tune
course
simple
strategy
specifying
value
binary
classiﬁers
works
well
chang
lin
2011
also
intuition
varied
facilitate
multi-class
svm
better
basis
potentially
high-dimensional
optimization
problem
needs
solved
advanced
performance
recall
ﬁnal
output
multi-class
svm
based
majority
voting
due
overﬁtting
risk
binary
classiﬁer
votes
may
introduce
interde-
pendencies
sum
problem
hyper-
parameter
tuning
multi-class
svms
non-separable
high-dimensional
essence
thus
apply
proposed
dac-hc
deal
re-
soo
cma-es
included
compared
algo-
rithms
since
ideal
decomposition
longer
applicable
problem
decc-i
compared
dac-
grid
search
also
tested
assumption
binary
classiﬁers
share
value
grid
search
actually
solves
1-dimensional
problem
three
data
sets
i.e.
usps
hull
1994
news20
lang
1995
letter
hsu
lin
2002
used
comparison
contain
classes
yield
three
problems
190
325
dimensions
respectively
features
dataset
scaled
solution
space
binary
classiﬁer
bounded
10−3
102
algo-
rithms
repeated
runs
problem
except
deterministic
grid
search
run
hyper-parameters
tuned
training
set
5-fold
cross-validation
higher
accuracy
better
hyper-parameters
f1f3f4f5f2
tang
al.
2009
tang
xiaodong
suganthan
zhenyu
yang
thomas
weise
benchmark
functions
cec2010
special
session
competition
large-
scale
global
optimization
technical
report
nature
in-
spired
computation
applications
laboratory
2009
tang
al.
2016
tang
peng
yang
xin
yao
neg-
atively
correlated
search
ieee
journal
selected
areas
communications
:542–550
march
2016
vapnik
1998
vladimir
vapnik
statistical
learning
theory
volume
wiley
new
york
1998
wang
al.
2013
ziyu
wang
masrour
zoghi
frank
hut-
ter
david
matheson
nando
freitas
bayesian
optimization
high
dimensions
via
random
embeddings
proceedings
twenty-third
international
joint
conference
artiﬁcial
intelligence
ijcai
2013
pages
1778–1784
beijing
china
2013.
aaai
press
yang
al.
2008a
zhenyu
yang
tang
xin
yao
large
scale
evolutionary
optimization
using
cooperative
information
sciences
178
:2985–2999
coevolution
2008
yang
al.
2008b
zhenyu
yang
tang
xin
yao
self-adaptive
differential
evolution
neighborhood
ieee
congress
evolutionary
computa-
search
tion
2008
ieee
world
congress
computational
in-
telligence
pages
1110–1116
ieee
2008.
acm
transactions
intelligent
systems
technology
2:27:1–27:27
2011
chen
al.
2010
wenxiang
chen
thomas
weise
zhenyu
yang
tang
large-scale
global
optimization
using
cooperative
coevolution
variable
interaction
learning
parallel
problem
solving
nature
ppsn
pages
300–309
springer
2010
friesen
domingos
2015
abram
friesen
pedro
domingos
recursive
decomposition
nonconvex
opti-
mization
proceedings
24th
international
joint
conference
artiﬁcial
intelligence
2015
hansen
ostermeier
2001
nikolaus
hansen
an-
dreas
ostermeier
completely
derandomized
self-
adaptation
evolution
strategies
evolutionary
compu-
tation
:159–195
2001
hsu
lin
2002
chih-wei
hsu
chih-jen
lin
comparison
methods
multiclass
support
vector
ieee
transactions
neural
networks
machines
:415–425
2002
hull
1994
jonathan
hull
database
handwritten
text
recognition
research
ieee
transactions
pattern
anal-
ysis
machine
intelligence
:550–554
1994
jebalia
al.
2008
mohamed
jebalia
anne
auger
log-linear
convergence
optimal
artiﬁcial
evolution
pages
pierre
liardet
bounds
-es
207–218
springer
2008
kab´an
al.
2015
ata
kab´an
jakramate
bootkrajang
robert
john
durrant
toward
large-scale
continuous
eda
random
matrix
theory
perspective
evolutionary
com-
putation
2015
kern
al.
2004
stefan
kern
sibylle
m¨uller
nikolaus
hansen
dirk
b¨uche
jiri
ocenasek
petros
koumout-
sakos
learning
probability
distributions
continuous
evolutionary
algorithms–a
comparative
review
natural
computing
:77–112
2004
kirkpatrick
al.
1983
kirkpatrick
gelatt
vecchi
optimization
simulated
annealing
sci-
ence
220
4598
:671–680
1983
lang
1995
ken
lang
newsweeder
learning
ﬁlter
net-
news
proceedings
12th
international
conference
machine
learning
pages
331–339
1995.
mahdavi
moham-
mad
ebrahim
shiri
shahryar
rahnamayan
meta-
heuristics
large-scale
global
continues
optimization
survey
information
sciences
295:407–428
2015
mahdavi
al.
2015
sedigheh
omidvar
al.
2014
mohammad
nabi
omidvar
xi-
aodong
mei
xin
yao
cooperative
co-evolution
differential
grouping
large
scale
optimization
ieee
transactions
evolutionary
computation
:378–393
2014
qian
2016
hong
qian
yang
scaling
si-
multaneous
optimistic
optimization
high-dimensional
non-convex
functions
low
effective
dimensions
proceedings
30th
aaai
conference
artiﬁcial
in-
telligence
aaai
2016
phoenix
2016
