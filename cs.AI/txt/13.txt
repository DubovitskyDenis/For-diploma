basic
reasoning
tensor
product
representations
paul
smolensky*
department
cognitive
science
johns
hopkins
university
baltimore
21218
usa
smolensky
jhu.edu
moontae
lee
department
computer
science
cornell
university
ithaca
14850
usa
moontae
cs.cornell.edu
xiaodong
wen-tau
yih
jianfeng
gao
deng
microsoft
research
redmond
98052
usa
xiaohe
scottyih
jfgao
deng
microsoft.com
paper
present
initial
development
general
theory
mapping
inference
predicate
logic
computation
tensor
product
representations
tprs
smolensky
1990
smolensky
legendre
2006
initial
brief
synopsis
tprs
section
begin
particular
examples
inference
tprs
babi
question-answering
task
weston
2015
section
present
simplification
general
analysis
suffices
babi
task
section
finally
lay
general
treatment
inference
tprs
section
also
show
simplification
section
derives
inference
methods
described
lee
2016
shows
simple
methods
lee
2016
formally
extended
general
reasoning
tasks
brief
synopsis
tpr
present
purposes
tensor
order
taken
n-dimensional
array
real
numbers
written
tγ1…
two
types
tensor
operations
use
given
outer
tensor
product
order-increasing
contraction
order-
decreasing
combining
two
gives
inner
product
interpret
order-2
tensor
matrix
order-1
tensors
vectors/column-matrices
outer
product
matrix
algebra
corresponds
tensor
product
dot
product
utv
matrix-vector
product
correspond
tensor
inner
products
work
conducted
first
author
visiting
researcher
second
author
held
summer
internship
microsoft
research
redmond
tensor
operations
outer/tensor
product
n+m
contraction
•12
uβvβ
•23
mγβuβ
mγβuβ
inner
product
•jk
n+m−2
cjk
uγvγ
uvt
cjk
n−2
tγ1…
γnγ
′1…
uγ1…
γnvγ
′1…
tγ1…
γj−1γj+1…
γk−1γk+1…
tγ1…
γj−1βγj+1…
γk−1βγk+1…
tγ1…
γj−1γj+1…
γk−1γk+1…
γn+m
uγ1…
γj−1βγj+1…
γnvγn+1…
γk−1βγk+1…
γn+m
following
customary
practice
throughout
paper
except
explicitly
stated
otherwise
assume
implicit
summation
repeated
indices
single
factor
einstein
summation
convention
thus
explicit
summation
1b−c
would
omitted
left
implicit
particular
tpr
maps
space
symbolic
structures
vector
space
type
structure
determined
set
structural
roles
determines
filler/role
decomposition
token
structure
uniquely
characterized
set
filler/role
bindings
fk/rk
role
bound
particular
filler
illustration
one
type
filler/role
decomposition
positional
roles
let
set
strings
alphabet
symbols
let
role
kth
symbol
left
particular
string
acb
bpos
acb
a/r1
b/r3
c/r2
note
bindings
constitute
set
illustrate
type
filler/role
decomposition
contextual
roles
type
structure
strings
let
rx_y
denote
role
preceded
symbol
followed
symbol
new
decomposition
bcon
acb
one
binding
bcon
acb
c/ra−b
decomposition
string
characterized
trigrams
given
filler/role
decomposition
tpr
defined
encoding
filler
filler
tensor
role
role
tensor
role
tensors
principal
innovation
tpr
tpr
structure
bindings
fk/rk
tensor
thus
tpr
binding
done
via
tensor
product
positional-role
decomposition
bpos
tpr
acb
spos
use
type
positional-role
tpr
consider
contextual-role
decomposition
bcon
bcon
acb
c/ra−b
tpr
acb
scon
ra_b
need
tensor
encode
role
rx_y
role
structure
given
filler/role
decomposition
rx_y
binding
x/r—_y
giving
rise
encoding
tensor
rx_y
r—_y
role
tensor
r—_y
choose
filler
vector
rx_y
tpr
acb
scon
isomorphic
encoding
mnemonic
scon
thus
role
tensors
order
vector
encoding
string
tensor
order
primary
interest
vectorial
encodings
propositions
adopt
contextual
tpr
encoding
proposition
corresponding
tpr
encoding
set
propositions
space
order-4
tensors
vector
space
dimension
assuming
symbol
encoded
order-1
tensor
i.e.
symbol
components
tensor
encoding
assume
order-1
tensors
chosen
encode
symbols
form
orthonormal
set
δjk
else
assures
tprs
unbound
perfect
accuracy
via
inner
product
undoes
outer
product
binding
example
consider
set
propositions
one
form
find
unique
value
namely
tpr
encoding
computing
•15,26,37
i.e.
bπαβγ
every
value
except
either
comment
assume
convenience
throughout
paper
tensors
encoding
symbols
orthonormal
assumption
1-hot
vectors
presume
distributed
vectors
many
elements
non-zero
results
would
continue
hold
tensors
encoding
symbols
merely
linearly
independent
unbinding
would
done
unbinding
filler
replacing
filler
tensors
unbinding
example
previous
paragraph
tensors
δjk
would
become
•15,26,37
kth
row
vectors
must
exist
filler
tensors
linearly
independent
essentially
inverse
matrix
kth
column
invertible
linearly
independent
babi
example
consider
example
denotes
time
co-located
time
lee
2016
gloss
belongs
contained
information-question
operator
assuming
denotation
question
set
answers
℺x.p
denotes
case
english
question
apple
kitchen
unbinding
tensors
defined
assigned
logical
form
glossed
location
case
apple
time
apple
kitchen
time
immediately
following
example
type-3
question
babi
task
john
picked
apple
john
went
office
john
went
kitchen
john
dropped
apple
apple
kitchen
office
℺x.∃t
assume
given
surface
string
semantic
parser
generates
right
column
given
left
column
strive
separate
issues
commonsense
inference
per
issues
nlp
narrowly
construed
identifying
semantic
predicates
corresponding
english
words
referents
referring
expressions
antecedents
anaphoric
expressions
content
elided
material
thus
assume
given
nlp
procedures
performing
computations
focus
exclusively
problem
commonsense
reasoning
distributed
vectorial
representations
symbols
predicate
logic
analysis
encoded
tpr
vectors
order-1
tensors
thus
vector
encoding
symbol
real
components
general
symbol-encoding
vectors
distributed
sense
many
components
non-zero
general
1-hot
vectors
convenience
assume
vectors
orthonormal
set
actually
required
linearly
independent
time
reasoning
process
state
take
set
propositions
constituting
knowledge
base
facts
concerning
problem
situation
grows
monotonically
information
arrives
form
ith
sentence
denotes
proposition
time
immediately
precedes
time
illustrated
often
notate
times
ti+1
times
tprs
linearly
independent
linear
operator
satisfying
time-increment
operator
ti+1
tpr
time-ti
knowledge
base
tensor
fourth-order
tensor
sum
tprs
propositions
form
dummy
symbol
used
convenience
make
like
predicate
takes
arguments
propositions
given
contextual
tpr
tpr
propositions
four
indices
bπαβτ
thought
proposition-
first-argument-
second-argument
third-argument-indices
proposition
predicate
index
actor-
object-vector
index
location-
actor-vector
index
time-
vector
tpr
proposition
bπαβτ
t′τ
reasoning
example
requires
two
rules
inference
transitivity
axiom
persistence
axiom
vectorial
reasoning
system
develop
persistence
axiom
applied
every
time
deriving
positions
immediately
following
time
expressions
like
john
dropped
apple
interpreted
apple
john
time
tensor
encoding
proposition
negation
vector
encoding
apple
john
simply
cancel
vector
apple
john
generated
persistence
axiom
subsequent
times
longer
encoded
proposition
persistence
axiom
propagate
forward
reasoning
needed
expressed
apple
john
⇒persistence
apple
john
john
office
⇒transitivity
apple
office
⇒persistence
apple
john
john
kitchen
⇒transitivity
apple
kitchen
⇒persistence
apple
kitchen
apple
john
encodings
inference
rules
given
encodings
derived
general
procedure
illustrated
section
encoding
transitivity
axiom
multilinear
tensor
operation
encoding
inference
transitivity
axiom
π″αβ′τ″
tτ″
παβτ
πtτ
π′α′β′τ′
π′tτ′
δβα′
π″αβ′τ″
π″αβ′τ″
pπ″
παβτtτ
pπ′
π′ββ′τ
tτ″
iii
modified
penrose
diagram
one
t−1
ti−1
encoding
persistence
axiom
note
ti−1
iff
iff
t−1
ti−1
matrix
operating
b-tensors
encodes
inference
persistence
axiom
ti−1
πξητ
πξητ
π′ξ′η′τ′
ti−1
π′ξ′η′τ′
πξητ
π′ξ′η′τ′
δξξ′
δηη′
t−1
τ′τ″
tτ″
iii
penrose
diagram
t−1
throughout
paper
except
explicitly
stated
otherwise
deploy
einstein
summation
convention
according
repeated
indices
implicitly
summed
values
e.g.
implicit
sum
8d.ii
kronecker
δij
i=j
else
general
case
7d.ii
encoding
inference
using
transitivity
axiom
involves
sum
set
transitive
predicates
using
single
transitive
predicate
expressions
deal
predicate
persistent
well
transitive
modified
penrose
tensor
diagrams
7d.iii
8d.iii
box
denotes
tensor
nth
line
left
emanates
box
tensor
denotes
nth
index
lines
mth-order
tensor
two
lines
joined
values
two
indices
set
equal
sum
values
index
explicitly
shown
7d.ii
algebraic
expression
denoted
penrose
diagram
7d.iii
penrose
diagrams
enable
complex
tensor
equations
written
precisely
without
indices
thereby
making
structure
equations
transparent
correspondence
predicate
logic
expression
penrose
tensor
diagram
made
explicit
section
juxtaposition
7d.iii
penrose
diagram
corresponding
predicate
logic
expression
beneath
already
suggests
nature
correspondence
visually
colors
predicate
logic
symbols
matching
colors
corresponding
tensors
diagram
well
colors
corresponding
indices
algebraic
expressions
7d.ii
8d.i
identity
matrix
tensors
ensures
propositions
encoded
time
ti−1
propositions
problem
situation
times
ti−1
carried
also
encoded
time
example
use
transitive
inference
procedure
given
recall
tprs
symbols
form
orthonormal
set
example
transitivity
inference
using
let
result
transitive
inference
add
π″αβ′τ″
t2τ″
παβτ
t2τ
π′α′β′τ′
π′t2τ′δβα′
π″αβ′τ″
t2τ″
παβτ
π′α′β′τ′
πt2τ
π′t2τ′δβα′
t2τ″
παβτ
π′α′β′τ′
πt2τ
π′t2τ′δβα′
π″αβ′τ″
t2τ″
jα′δβα′
t1τt2τ
kβ′
t2τ′t2τ′
t2τ″
jα′δβα′
t2τt2τ
kβ′
t2τ′t2τ′
π″αβ′τ″
t2τ″aα
kβ′
π″αβ′τ″
π″αβ′τ″
π″αβ′τ″
i.e.
tpr
evaluation
tprs
symbols
orthogonal
terms
components
παβτ
π′α′β′τ′
annihilated
except
single
term
tpr
proposition
pair
inner
product
πt2τ
π′t2τ′δβα′
gives
pair
unless
e.g.
factor
red
brackets
t1τt2τ
orthogonal
factors
vµvµ
equal
tprs
symbols
normalized
length
consecutive
time
also
following
update
rule
immediate
temporal
precedence
relation
recall
definition
time-increment
operator
update
rule
symbolic
temporal
precedence
relation
tpr
ti−1
ti−1
ti−1
ti−1
ti−1
ti−1
ti−1
procedure
building
knowledge
base
incrementally
sentence
pertaining
time
processed
given
reasoning
algorithm
facts
story
inferred
facts
construct
loop
sentences
story
goal
knowledge
base
ti−1
given
inferences
persistence
axiom
ti−1
update
add
ti−1
ti−1
already
computed
ti−1
ti−1
tensors
persistence
matrix
ti−1
ti−1
time-update
matrix
ti−1
add
ith
sentence
repeat
change
e.g.
john
picked
apple
e.g
inferences
transitivity
axiom
p∈t
transitive
inference
multilinear
tensor
operation
algorithm
processing
example
produce
algorithm
processing
example
sentence
john
picked
apple
john
went
office
inferences
update
explanation
persistence
transitivity
john
went
kitchen
john
dropped
apple
apple
kitchen
office
℺x.∃t
contributes
cancels
inference
persistence
axiom
answer
query
construct
encoding†
query
apple
kitchen
℺x.∃t′
xβ2
bπ1α1β1γ
aα1
kβ1
bπ2α2β2γ
aα2
bπ3α3β3γ
≺π3
1β3
2α3
penrose
tensor
diagram
component-
wise
expression
tensor
equation
encoding
form
query
expression
given
alternative
replaces
one
factor
factor
encoding
alternative
form
query
℺x.∃ti
ti+1
either
form
query
slightly
simplified
additional
requirement
needed
persists
across
two
consecutive
times
encoding
yields
additional
factor
simple
post-processing
step
projects
onto
subspace
orthogonal
simplification
2.1
deriving
simplification
two-place
predicates
first
simplification
omit
vector
encoding
relation
written
shown
lee
2016
babi
problem
types
relation
needed
uni-relational
necessary
encode
explicitly
items
initial
tensor
factor
simplification
shown
third
column
table
matrix-algebra
expression
xyt
tensor
expression
define
exactly
elements
xyt
xjyk
simplification
sufficient
babi
task
implicit
predicate
implicit
time
stamps
symbolic
full
tpr
simplification
xy⊤
yz⊤
second
simplification
replace
time
stamps
slots
memory
table
slots
shown
vertical
queue
simplification
column
rather
explicitly
including
final
tensor
factor
encodes
explicit
time
stamp
locate
item
time
ith
position
queue
think
memory
positions
locations
vector
shown
simplification
column
however
recognize
vector
sum
two
vectors
tensor
product
cell
entry
unit
column
vector
spelled
simplification
column
indicated
simplification
column
reduces
full
tpr
representation
initial
tensor
factor
omitted
identify
analysis
obviously
trivially
extends
number
time
steps
two
simplifications
made
tensor
implementation
inference
transitivity
axiom
becomes
simplified
transitive
inference
operation
predicate
time
factors
made
explicit
full
form
π″αβ′γ″
pπ″
παβγtγ
pπ′
π′ββ′γ
tγ″
simplified
form
αβ′
ββ′
αβ′
i.e.
simple
matrix
multiplication
exactly
form
transitive
inference
takes
lee
2016
e.g.
example
type-2
question
discussed
fmt
mgt
football
mary
mary
garden
combined
matrix
multiplication
give
fgt
football
garden
2.2
deriving
simplification
three-place
predicates
analyses
lee
2016
questions
categories
involve
binding
entities
rather
example
representation
mary
travelled
garden
kitchen
let
relevant
entities
actors
objects
locations
etc
represented
unit
vectors
r0|e
r1|e
assume
generic
case
non-singular
êl0
suppose
assumptions
apply
implementation
discussed
lee
2016
assume
generic
case
linearly
independent
let
n-dimensional
subspace
span
let
restrictions
denoted
r2d
êl1
linearly
independent
sets
order
information-preserving
assume
two
sets
linearly
independent
i.e.
union
two
sets
also
linearly
independent
set
êlβ
0:1
possible
span
range
u|e
+r0
1|e
similarly
exists
yl0
since
êl0
get
+|s0
thus
non-singular
exist
inverses
span
êl1
linearly
independent
n-dimensional
subspaces
extension
respectively
defined
span
êl0
σlβ
ylβ
êlβ
+|s1
namely
+|s0
+|s1
inverting
binding
identified
contracted
tpr
follows
recall
matrix
product
kind
tensor
inner
product
contraction
tensor
outer
product
vjmkj
c13
vector
considered
order-1
tensor
matrix
considered
order-2
tensor
particular
c13
c13
thus
representation
travelled
contracted
tpr
travelled
contextual-binding
fillers
slots
c13
positional-binding
fillers
roles
tensors
representing
roles
alternatively
eliminating
displacement
travelled
c24
unbinding
actor
left-multiplying
gives
displacement
represented
•12
c24
c12,35
•12
c13
factor
•12
entity
vectors
normalized
considerably
less
entity
vectors
generically
distributed
considerably
larger
space
indeed
assuming
vectors
chosen
orthogonal
case
exactly
result
c13
contracted
tpr
pair
tells
represented
displacement
entities
extracted
via
inner
products
duals
role
tensors
standard
tprs
•13
lee
2016
questions
category
treated
operation
functions
identically
matrix
rather
analysis
given
∘/u
applies
show
*/v
method
amounts
contracted
tpr
equations
tensor
counterparts
r1k
r0g
r1k
•13
2.3
deriving
simplification
path
finding
babi
category
example
babi
category
problem
given
path-finding
example
problem
sentence
bedroom
south
hallway
bathroom
east
office
kitchen
west
garden
garden
south
office
office
south
bedroom
garden
bedroom
℺p.p
expression
denotes
path
consisting
west
south
accepts
argument
list
directions
general
denotes
path
consisting
followed
followed
followed
rules
inference
needed
solve
path-finding
problems
given
axioms/rules
inference
path-finding
problems
d′1
d′n′
full
tpr
model
d′n′
d′1
d′n′
d′1
rules
20a
express
inverse
semantics
within
pairs
rule
20b
states
example
one
block
north
manhattan-like
grid
locations
path
consisting
one-block
step
direction
north
goes
finally
20c
asserts
path
consisting
followed
followed
followed
leads
path
d′n′
d′1
consisting
d′1
followed
followed
d′n′
leads
d′n′
d′1
leads
rule
resembles
transitivity
rule
whereas
transitivity
involves
single
relation
path-finding
rule
involves
productive
combination
multiple
relations
sense
path
finding
multi-relational
problem
whereas
simpler
babi
problem
types
reducible
transitivity
uni-relational
main
point
lee
2016
reasoning
multi-relational
axioms
implemented
tpr
simpler
case
transitive
inference
third
column
table
shows
full
tpr
encoding
statement
example
problem
symbolic
knowledge
base
set
stated
inferred
propositions
nk⋯dk
tpr
direct
sum
tensors
forms
reply
query
test
possible
paths
see
whether
leads
babi
task
paths
length
need
considered
test
whether
take
inner
product
result
truth
value
simplification
full
analysis
relies
vectorial
model
axioms
sense
model
theory
mathematical
logic
set
linear-algebraic
objects
inter-related
ways
satisfy
axioms
final
column
table
shows
corresponding
representations
2dk
simpler
implementation
full
analysis
vectors
encoding
locations
directions
arbitrary
orthonormal
vectors
simpler
model
systematic
relations
encodings
locations
directions
specifically
directions
north
east
encoded
matrices
locations
encoded
vectors
rather
inference
relations
20a
implementing
inverse
relationships
among
directions
matrices
encoding
south
west
systematically
related
encoding
north
east
n−1
e−1
requires
non-
singular
rather
adding
arbitrary
fact-tensor
truth
encoded
relation
among
encoding
vectors
matrices
conditions
ensure
vectorial
encodings
positions
directions
provide
model
axioms
20a
encoded
entails
n−1x
encoding
set
possible
locations
given
problems
must
independent
sets
vectors
i.e.
need
following
condition
range
n|l
range
e|l
linearly
independent
2|l|
reminiscent
conditions
paths
let
encoding
dn⋯d1y
encodings
provide
model
composition
axiom
since
encodings
d′n′
d′1
dn⋯d1y
d′n′
d′1′
entail
dn⋯d1d′n′
d′1′
encoding
d′n′
d′1
also
base
case
expressed
axiom
20b
satisfied
since
encoding
simplified
approach
implemented
lee
2016
set
position
vectors
direction
matrices
encoding
statements
given
problem
generated
test
whether
given
path
determine
whether
answers
query
validity
equation
determined
d2d1
accord
2.4
performance
simplification
babi
dataset
simplification
full
tpr
reasoning
analysis
described
implemented
results
reported
lee
2016
briefly
summarized
performance
simplification
babi
tasks
100
question
categories
except
99.8
c16
99.5
present
analysis
performs
inference
programmed
vector
procedures
rather
learned
network
computations
performance
directly
compared
previous
work
addressing
babi
task
including
notably
peng
2015
neural
reasoner
achieved
66.4
/97.9
17.3
/87.0
tasks
1k/10k
training
examples
respectively
difficult
tasks
previous
best
performance
previous
best
performance
c5/c16
99.3
/100
strongly
supervised
memory
network
weston
chopra
bordes
2014
general
treatment
general
case
query
construction
λpk
eke
o´xkx
∃eke
xkx
ans
k∈1
bπkγ
=ck
j∈1
′′ie
′ie
′′ke
′ke
′ke
′ie
′′ke
′′ie
j∈1
k∈1
particular
example
query
follows
general
case
spelled
derivation
query
general
case
℺x.∃t′
℺x2
2.∃e3
1=e2
bπ2
2=e1
bπ3
bπ1
ans
δγ3
δγ3
xβ2
bπ1
bπ2
bπ3
aα1
kβ1
aα2
øγ3
δγ1β3
δγ2α3
analogous
methods
allow
general
instantiation
rules
inference
particular
forms
similarly
derived
references
lee
moontae
xiaodong
yih
wen-tau
gao
jianfeng
deng
smolensky
paul
reasoning
vector
space
exploratory
study
question
answering
review
iclr2016
2016.
smolensky
paul
tensor
product
variable
binding
representation
symbolic
structures
connectionist
systems
artificial
intelligence
1-2
1990.
smolensky
paul
legendre
géraldine
harmonic
mind
neural
computation
optimality-
theoretic
grammar
volume
cognitive
architecture
mit
press
2006.
weston
jason
chopra
sumit
bordes
antoine
memory
networks
corr
abs/1410.3916
2014.
url
http
//arxiv.org/abs/1410.3916
