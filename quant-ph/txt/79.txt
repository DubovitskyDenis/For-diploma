quantum
algorithms
nearest-neighbor
methods
supervised
unsupervised
learning
nathan
wiebe†,1
ashish
kapoor∗,1
krysta
svore†1
1†quantum
architectures
computation
group
microsoft
research
redmond
usa
∗adaptive
systems
interaction
group
microsoft
research
redmond
usa
present
quantum
algorithms
performing
nearest-neighbor
learning
k–means
clustering
core
algorithms
fast
coherent
quantum
methods
computing
euclidean
distance
directly
via
inner
product
couple
methods
performing
amplitude
estimation
require
measurement
prove
upper
bounds
number
queries
input
data
required
compute
distances
ﬁnd
nearest
vector
given
test
example
worst
case
quantum
algorithms
lead
polynomial
reductions
query
complexity
relative
monte
carlo
algorithms
also
study
performance
quantum
nearest-neighbor
algorithms
several
real-world
binary
classiﬁcation
tasks
ﬁnd
classiﬁcation
accuracy
competitive
classical
methods
quantum
speedups
long
known
problems
factoring
quantum
simulation
optimization
recently
quantum
algorithms
begun
emerge
promise
quantum
advantages
solving
problems
data
processing
machine
learning
classiﬁcation
practical
algorithms
problems
promise
provide
applications
outside
physical
sciences
quantum
computation
may
great
importance
work
provides
important
step
towards
goal
understanding
value
quantum
computing
machine
learning
rigorously
showing
quantum
speedups
achieved
nearest–neighbor
classiﬁcation
consider
task
faced
u.s.
postal
service
routing
150
billion
pieces
mail
annually
sheer
magnitude
problem
necessitates
use
software
automatically
recognize
handwritten
digits
letters
form
address
recipient
nearest-neighbor
algorithm
commonly
used
solve
tasks
handwriting
recognition
due
simplicity
high
performance
accuracy
nearest–neighbor
classiﬁcation
algorithms
also
found
extensive
use
detecting
distant
quasars
using
massive
data
sets
galactic
surveys
searches
computationally
intensive
often
require
analyzing
terabytes
worth
data
cases
hence
ﬁnding
distant
quasars
luminous
enough
detected
billions
light
years
away
requires
accurate
classiﬁers
frequently
make
false
positive
assignment
nearest–neighbor
methods
often
used
high
assignment
accuracy
needed
given
massive
data
sets
however
major
drawback
using
classical
nearest-neighbor
classiﬁcation
computationally
expensive
show
quantum
computation
used
polynomially
reduce
query
complexity
nearest–neighbor
classiﬁcation
via
fast
quantum
methods
computing
distances
vectors
use
amplitude
estimation
concert
grover
search
easy
task
methods
require
measurement
applied
subroutine
exploits
measurement
non–trivial
way
address
problem
providing
methods
removing
measurements
distance
calculations
amplitude
estimation
two
used
together
coherently
combining
techniques
achieve
near–quadratic
advantages
monte–
carlo
algorithms
improvements
help
mitigate
main
drawback
nearest–neighbor
classiﬁcation
computational
expense
relative
less
exact
methods
quantum
algorithms
also
reveal
new
possibilities
quantum
machine
learning
would
practical
classical
machine
learning
algorithms
algorithm
used
classify
data
output
quantum
simulator
eﬃciently
simulated
classically
similar
would
allow
fast
algorithms
constructed
rapidly
classify
chemical
using
large
database
chemicals
whose
features
computed
using
quantum
simulation
algorithms
unlike
classical
examples
features
i.e
results
simulated
experiments
molecule
used
classiﬁcation
need
ﬁxed
vary
instance
instance
form
classiﬁcation
practical
classical
setting
would
require
prohibitively
large
database
features
application
especially
compelling
unsupervised
machine
learning
wherein
algorithms
k–means
clustering
used
ﬁnd
appropriate
classiﬁcations
without
human
intervention
see
quantum
k–means
clustering
follows
direct
consequence
methods
develop
nearest–neighbor
classiﬁcation
paper
laid
follows
review
nearest-neighbor
classiﬁcation
section
discuss
existing
approaches
quantum
classiﬁcation
outline
quantum
algorithms
oracles
use
section
give
main
results
costs
performing
nearest-neighbor
classiﬁcation
using
inner
product
method
euclidean
method
section
iii
section
respectively
provide
numerical
results
section
show
algorithms
applied
real
world
data
sets
methods
substantial
errors
estimates
distance
tolerated
without
degrading
assignment
accuracy
compare
results
fig
example
handwritten
digits
digit
stored
256-pixel
greyscale
image
represented
unit
vector
256
features
fig
schematic
process
processing
raw
data
feature
vectors
distances
handwriting
recognition
monte–carlo
nearest–neighbor
classiﬁcation
section
apply
results
k–means
clustering
section
vii
concluding
proofs
theorems
given
appendix
nearest-neighbor
classification
idea
nearest–neighbor
classiﬁcation
classify
based
experience
classiﬁer
attempts
classify
piece
data
known
test
vector
comparing
set
training
data
already
classiﬁed
expert
usually
human
test
example
assigned
class
training
example
similar
features
features
often
expressed
real–valued
vectors
represent
nearly
parameters
data
user
wants
use
basis
classiﬁcation
similar
vector
calculated
training
vector
closest
test
vector
using
appropriate
distance
metric
euclidean
distance
test
vector
given
classiﬁcation
expert
gave
closest
training
vector
major
advantage
nearest–neighbor
classiﬁcation
tends
accurate
given
large
training
set
data
set
gets
larger
becomes
likely
test
vector
similar
training
vector
database
accuracy
simplicity
method
caused
nearest–neighbor
classiﬁcation
become
mainstay
classiﬁcation
machine
learning
three
main
drawbacks
nearest–
neighbor
approaches
statistical
outliers
lead
false
positives
sensitive
errors
training
labels
algorithm
become
prohibitively
expensive
classical
computers
cases
database
dimension
training
vectors
large
former
two
problems
dealt
using
k–nearest–neighbors
classiﬁcation
straight
forward
generalization
nearest–neighbor
classiﬁcation
classiﬁcation
generalized
function
k-nearest-neighbors
addressing
latter
problem
using
quantum
strategies
focus
paper
concrete
example
nearest–neighbor
classiﬁcation
consider
binary
classiﬁcation
task
determining
given
unlabeled
test
digit
even
odd
training
data
consists
handwritten
digits
expressed
multidimensional
feature
vectors
human-assigned
label
either
even
odd
entries
feature
vector
number
features
used
characterize
digit
pixel
values
comprise
image
figure
shows
example
digits
represented
256-dimensional
feature
vector
pixel
values
divide
training
set
two
sets
clusters
vectors
contains
odd
examples
contains
even
examples
goal
classify
label
given
unlabeled
test
point
generally
vectors
also
used
take
large
cost
algorithm
number
times
components
vectors
must
accessed
classiﬁcation
procedure
nearest-neighbor
algorithm
ﬁrst
computes
distance
test
vector
training
vector
assigns
cluster
contains
closest
vector
speciﬁcally
assigns
min
min
appropriate
distance
metric
directly
computing
quantities
using
classical
nearest-neighbor
algorithm
requires
accesses
components
vectors..
...
...
...
...
digitsfeature
vectors
pixel
values
distance
examples
quantum
nearest–neighbor
classification
quantum
computation
shows
promise
powerful
resource
accelerating
certain
classical
machine
learning
algorithms
8–11
natural
suspect
may
also
useful
quantum
nearest–neighbor
clasisﬁcation
however
major
challenge
facing
development
practical
quantum
machine
learning
algorithms
need
oracle
return
example
distances
elements
test
training
sets
lloyd
mohseni
rebentrost
recently
proposed
quantum
algorithm
addresses
problem
namely
algorithm
computes
representative
vector
set
referred
centroid
averaging
vectors
respectively
test
point
assigned
distance
centroid
written
mean
smallest
mean
mean
otherwise
note
algorithm
refer
nearest–centroid
classiﬁcation
form
nearest-neighbor
clas-
siﬁcation
nearest
centroid
used
determine
label
opposed
nearest
training
point
number
clusters
equals
number
points
reduces
nearest–neighbor
classiﬁcation
practice
nearest–centroid
classiﬁcation
perform
poorly
often
embedded
complicated
man-
ifold
mean
values
sets
within
manifold
contrast
nearest–neighbor
classiﬁcation
tends
work
well
practice
often
outperforms
centroid–based
classiﬁcation
prohibitively
expensive
classical
computers
therefore
present
quantum
nearest–neighbor
algorithm
assigns
point
either
cluster
probability
faulty
assignment
number
quantum
oracle
queries
minimized
consider
two
diﬀerent
ways
computing
distance
within
algorithm
inner
product
|u||v|
euclidean
distance
i=1
cid:113
cid:80
easy
verify
two
measures
equivalent
constants
proportionality
unit
vectors
ideally
algorithms
provide
exactly
classiﬁcation
applied
nearest–neighbor
classiﬁcation
quantum
algorithm
overcomes
main
drawback
nearest-centroid
approach
low
assignment
accuracy
many
real–world
problems
seen
section
appendix
throughout
test
point
set
training
set
consists
assume
following
input
vectors
d–sparse
i.e.
contain
non–zero
entries
large
chosen
much
larger
true
sparsity
vectors
performance
algorithms
suﬀer
quantum
oracles
provided
form
cid:105
cid:105
cid:105
cid:105
cid:105
|vji
cid:105
cid:105
cid:96
cid:105
cid:105
cid:96
cid:105
vji
ith
element
jth
vector
cid:96
gives
location
cid:96
non–zero
entry
user
knows
upper
bound
rmax
absolute
value
component
vji
vector
normalized
convenience
necessary
non–unit
vectors
accommodated
rescaling
inner
products
computed
using
inner
product
method
replacing
weighted
average
adding
ﬁctitious
vectors
make
run
time
algorithm
dominated
number
queries
made
oracles
main
results
ﬁnd
performing
nearest–neighbor
classiﬁcation
assumptions
number
queries
depends
dr2
max
rather
feature
dimension
sparsity
alone
thus
practical
applications
query
complexity
typically
independent
number
features
i.e.
rmax
1/√d
number
queries
scales
log
rather
nearest–neighbor
classiﬁcation
query
complexity
nearest–centroid
algorithm
uses
euclidean
method
independent
error
tolerance
dimension
number
training
vectors
sparsity
maximum
feature
value
rmax
failure
probability
training
vector
prepare
state
encodes
distance
amplitude
using
subroutine
appropriate
distance
metric
m−1/2
cid:80
cid:105
cid:105
cid:55
m−1/2
cid:80
cid:105
cid:112
|vj
u||0
cid:105
cid:112
|vj
u||1
cid:105
use
coherent
amplitude
ampliﬁcation
store
distance
estimate
qubit
string
without
measuring
state
resultant
state
m−1/2
cid:80
cid:105
||vj
cid:105
apply
d¨urr
høyer
minimization
algorithm
|vj
argmin
|vj
fig
high
level
description
structure
quantum
nearest–neighbor
algorithm
||vj
cid:105
denotes
quantum
state
holds
qubit
string
represents
distance
|vj
algorithm
tolerate
relatively
large
errors
distance
calculations
applied
real–world
classiﬁcation
problems
implementation
oracular
algorithms
require
instantiation
oracles
abstraction
many
ways
algorithms
interact
data
task
classify
chemicals
oracle
query
could
represent
call
eﬃcient
quantum
simulation
algorithm
yields
physical
features
chemicals
cases
oracle
query
could
represent
accesses
large
quantum
database
contains
classical
bit
strings
one
way
construct
database
use
quantum
random
access
memory
qram
however
alternate
implementations
possible
work
assume
oracles
provided
show
minimize
number
queries
oracle
three
phases
quantum
algorithm
laid
figure
given
pseudocode
appendix
ﬁrst
phase
use
oracles
prepare
states
form
cid:88
cid:18
cid:113
cid:88
cid:114
vji
cid:19
m−1/2
cid:105
j=1
i=1
ji/r2
max
cid:105
rmax
cid:105
approach
would
analogous
grover
rudolph
state
preparation
procedure
measure
last
qubit
avoid
measurements
would
disallow
subsequent
applications
amplitude
ampliﬁcation
states
used
circuit
whose
success
probability
ﬁxed
encodes
distance
test
vector
second
phase
uses
amplitude
estimation
estimate
success
probabilities
store
ancilla
register
directly
apply
amplitude
estimation
ﬁnd
distances
would
remove
possibility
using
grover
search
ﬁnd
closest
vector
use
form
amplitude
estimation
call
coherent
amplitude
estimation
forgo
measurement
price
preparing
logarithmically
large
number
copies
state
results
state
local
isometries
approximately
cid:88
j=1
m−1/2
cid:105
||vj
cid:105
ﬁnal
step
algorithm
use
grover
search
form
d¨urr
høyer
minimization
algortihm
search
closest
since
none
prior
steps
uses
measurement
algorithm
used
without
modiﬁcation
means
achieve
quadratic
reductions
scaling
algorithm
respect
error
tolerance
due
amplitude
estimation
also
respect
due
d¨urr
høyer
steps
common
approaches
nearest-neighbor
classiﬁcation
ﬁrst
discuss
inner
product
approach
discuss
alternative
approach
uses
circuit
originally
designed
implementing
linear
combinations
unitary
operations
directly
compute
euclidean
distance
euclidean
approach
conceptually
interesting
however
naturally
generalizes
nearest–neighbor
methods
nearest
centroid
methods
case
nearest–neighbor
classiﬁcation
methods
provide
exactly
classiﬁcation
provide
methods
number
queries
required
use
inner
product
method
sometimes
lower
required
euclidean
method
iii
inner
product
method
ﬁrst
describe
quantum
nearest–neighbor
algorithm
directly
computes
square
inner
product
two
vectors
compute
distance
show
somewhat
surprisingly
required
number
oracle
queries
explicitly
depend
number
features
rather
depends
implicitly
dr2
max
theorem
let
d–sparse
unit
vectors
maxj
|vji|
rmax
task
ﬁnding
maxj
cid:104
u|vj
cid:105
within
error
success
probability
least
requires
expected
number
combined
queries
bounded
cid:24
d2r4
max
cid:25

cid:16
81m
cid:17
8/π2
1/2

1080√m
0.5772
euler
constant
two
important
scaling
factors
theorem
emphasized
first
scaling
query
complexity
near–quadratically
better
classical
analog
second
rmax
1/√d
scaling
independent
expect
condition
occur
input
vectors
least
sparsity
note
swap
test
gives
square
inner
product
rather
inner
product
output
means
sign
inner
product
lost
calculation
cases
sign
inner
product
necessary
assignment
generalize
method
transforming
quantum
representation
training
vector
cid:96
cid:105
cid:96
cid:105
cid:55
cid:105
|0⊗
log2
cid:105
cid:105
cid:96
cid:105
using
states
theorem
allows
direct
estimation
cosine
distance
turn
inner
product
assume
simplicity
throughout
following
cid:96
cid:105
represents
quantum
state
directly
corresponds
cid:96
translating
results
cases
representation
used
straight
forward
prove
theorem
following
steps
see
appendix
details
assume
want
compute
inner
product
two
states
let
vji
rjieiφji
rji
positive
number
achieved
using
coherent
version
swap
test
states
cid:115
cid:32
cid:115
cid:105
d−1/2
cid:88
d−1/2
cid:88
vji
cid:54
v0i
cid:54
max
e−iφji
cid:105
vji
rmax
cid:105
|1
cid:105
cid:33
cid:105
cid:105
max
e−iφ0i
cid:105
v0i
rmax
cid:105
show
appendix
prepared
using
six
oracle
calls
two
single–qubit
rotations
swap
test
applied
states
probability
obtaining
outcome
denoted
found
cid:104
u|vj
cid:105
cid:104
v0|vj
cid:105
d2r4
max
statistical
sampling
requires
m/2
queries
achieve
desired
error
tolerance
expensive
small
values
required
reduce
scaling
1/
removing
measurement
swap
test
applying
amplitude
estimation
estimate
within
error
denoted
done
state
preparation
procedure
measurement–free
swap
test
invertible
register
dimension
used
inference
error
obeys
cid:24
d2r4
max
cid:25
choosing
large
enough
error
/2
yields
cid:112
inner
product
non–unit
vectors
easily
computed
output
amplitude
estimation
inner
product
two
non–normalized
vectors
computed
rescaling
inner
product
unit–vector
equivalents
done
eﬃciently
provided
oracle
yields
norm
vector
querying
times
scaling
also
quadratically
reduced
using
maximum/minimum
ﬁnding
algorithm
d¨urr
høyer
combines
grover
algorithm
exponential
search
ﬁnd
largest
smallest
element
list
order
apply
algorithm
need
make
step
reversible
call
form
coherent
amplitude
estimation
achieve
introducing
coherent
majority
voting
scheme
superposition
k–copies
output
|a|2
|y⊥
cid:105
bit–string
encodes
|y⊥
cid:105
outputs
state
form
a|y
cid:105
orthogonal
cid:105
median
bitstrings
computed
coherently
|x1
cid:105
···|xk
cid:105
cid:105
cid:55
|x1
cid:105
···|xk
cid:105
|¯x
cid:105
median
mode
could
also
used
application
guarantees
|a|2
8/π2
1/2
hoeﬀding
inequality
shows
overwhelming
probability
suﬃciently
large
cid:112
particular
straightforward
show
using
binomial
theorem
write
see
appendix
|a|2
|y⊥
cid:105
cid:105
|a|2
1−∆
cid:0
states
cid:105
cid:48
cid:105
computationally
irrelevant
use
coherent
majority
voting
construct
√2∆–approximate
oracle
maps
cid:105
cid:105
cid:55
cid:105
|¯y
cid:105
approximate
cid:112
|a|2
cid:48
cid:105
cid:1
cid:0
8/π2
a|ψ
cid:105
cid:105
a|y
cid:105
oracle
used
d¨urr
høyer
minimum
ﬁnding
algorithm
cid:1
make
pessimistic
assumption
use
approximate
oracle
leads
erroneous
outcome
minimum
ﬁnding
algorithm
even
whole
algorithm
fails
fortunately
since
number
repetitions
scales
log
1/∆
probability
made
vanishingly
small
low
cost
ﬁnal
cost
estimate
follows
multiplying
costs
state
preparation
number
iterations
used
d¨urr
høyer
algorithm
euclidean
method
describe
quantum
nearest-neighbor
algorithm
directly
computes
euclidean
distance
cluster
centroids
i.e.
mean
values
vectors
within
cluster
viewed
step
k–means
clustering
algorithm
refer
algorithm
nearest-centroid
algorithm
nearest–
centroid
algorithm
diﬀers
substantially
normalize
computed
distances
consider
generalization
cases
cluster
subdivided
cid:48
clusters
contain
cid:48
vectors
respectively
cid:48
algorithm
reduces
nearest–neighbor
classiﬁcation
diﬀerences
help
address
two
central
problems
centroid–based
classiﬁcation
first
imagine
cluster
dense
cluster
sparse
even
|v0
mean
|v0
mean
may
much
likely
test
vector
assigned
probability
large
deviation
centroid
much
greater
alternatively
rescaling
distances
also
useful
cases
faulty
assignment
one
class
less
desirable
faulty
assignment
class
normalizing
distance
width
cluster
help
address
issues
also
show
appendix
assignment
reduces
likelihood
ratio
test
certain
assumptions
second
non–convex
centroid
may
actually
point
segmenting
data
cid:48
smaller
clusters
help
address
issue
following
theorem
gives
query
complexity
quantum
nearest–centroid
algorithm
normalization
distance
cluster
width
easily
omitted
algorithm
desired
note
cid:48
occurs
nearest–centroid
classiﬁcation
reduces
nearest–neighbor
classiﬁcation
intra–cluster
variance
zero
normalization
step
make
sense
case
set
cases
order
euclidean
method
universally
applicable
nearest–neighbor
nearest–centroid
classiﬁcation
changing
normalization
constant
class–dependent
value
may
also
use
cases
want
bias
faulty
assignments
take
following
simplicity
theorem
let
satisfy
maxm
cid:48
d–sparse
unit
vectors
components
cid:107
otherwise
rmax
cid:80
task
ﬁnding
cid:80
p=1
cid:107
cid:32
cid:107
min
cid:33
j=1
cid:107
error
numerator
denominator
bounded
success
probability
least
requires
expected
number
combined
queries
bounded
cid:24
dr2
max
900√m
cid:48
cid:16
81m
cid:48
log
cid:48
cid:17
8/π2
1/2

cid:80
cid:25

log
cid:48
polylog
dr2
max
learning
problem
eﬃcient
motivates
use
centroid–based
classiﬁcation
supervised
learning
problems
partitioned
union
small
number
disjoint
training
sets
unimodal
convex
even
cid:48
required
query
complexity
method
comparable
inner–product–based
approach
proof
theorem
follows
similarly
theorem
use
coherent
amplitude
estimation
ﬁnd
numerator
distance
centroid
well
intra–cluster
variance
use
reversible
circuit
divide
distance
variance
use
d¨urr
høyer
algorithm
ﬁnd
minimum
relative
distance
cid:48
clusters
biggest
conceptual
diﬀerence
case
use
swap
test
instead
use
method
result
shows
unitary
transformation
mapping
cid:105
cid:105
cid:55
cid:105
|vj
cid:105
measurement
performed
success
probability
|vj0|2vj
choose
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:40
1√2
j=0
1√2mm
|vj0|
otherwise
cid:113
probability
success
gives
square
euclidean
distance
cluster
centroid
note
non–unit
vectors
accommodated
doubling
number
vectors
setting
αjvj
show
appendix
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
j≥1
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
4dr2
maxp
operator
implemented
eﬃciently
using
techniques
quantum
simulation
prepared
using
process
estimating
distance
therefore
eﬃcient
error
tolerance
ﬁxed
remainder
procedure
identical
inner–product
method
notable
technical
diﬀerence
phase
estimation
procedure
must
succeed
distance
intra–cluster
variance
calculations
results
success
probability
phase
estimation
dropping
least
8/π2
least
8/π2
2/3
thus
quantum
nearest–centroid
classiﬁcation
based
euclidean
method
requires
itera-
tions
quantum
nearest–neighbor
classiﬁcation
based
either
method
typical
cases
atypical
cases
method
direct
calculation
d2r4
monte–carlo
calculation
max
inner–product
method
log
d2r4
cid:48
log
cid:48
dr2
euclidean
method
max
cid:17
cid:16
d2r4
max
cid:16
log
d2r4
max
cid:16
cid:48
log
cid:48
dr2
max
max
cid:17
cid:17
table
query
complexities
performing
classiﬁcation
using
quantum
classical
methods
discussed
examples
training
vectors
dimension
used
cid:48
clusters
used
typical
cases
refer
examples
training
vectors
typical
random
vectors
implies
1/√n
needed
perform
classiﬁcation
accurately
atypical
cases
refer
examples
clear
dependence
cases
log
m/
qubits
needed
addition
required
arithmetic
operations
instantiation
oracles
fig
classiﬁcation
accuracy
digit
data
cases
half
dataset
used
training
fig
classiﬁcation
accuracy
digit
data
ﬁxed
noise
10−5
function
training
data
size
numerical
experiments
although
algorithms
clearly
exhibit
better
scaling
direct
classical
computation
nearest–neighbor
question
remains
whether
needs
prohibitively
small
order
ensure
quality
assignment
impacted
end
evaluate
performance
algorithms
several
real–world
tasks
simulating
noise
incurred
using
coherent
amplitude
estimation
example
consider
classifying
handwritten
digits
mnist
digits
database
wherein
problem
given
training
set
handwritten
digits
see
figure
labels
even
odd
assign
label
even
odd
unlabeled
test
digit
digit
represented
256-dimensional
feature
unit
vector
pixel
values
pixel
values
rescaled
zero
mean
unit
variance
data
set
normalizing
plots
error
bars
indicate
standard
deviation
accuracy
first
compare
accuracy
nearest–neighbor
algorithm
nearest–centroid
algorithm
cen-
troid
function
noise
distance
computation
distances
computed
inner
product
method
logically
equivalent
computed
using
euclidean
method
results
valid
either
algorithm
centroid
set
cid:48
figure
plots
versus
accuracy
blue
squares
centroid
red
circles
noise
drawn
independently
distance
computation
10−5
105
accuracy
averaged
across
classiﬁcation
100
random
test
examples
using
2000
training
points
low-noise
regime
signiﬁcantly
outperforms
centroid
roughly
0.1
1/√n
algorithms
exhibit
signiﬁcant
loss
accuracy
eventually
degrade
accuracy
centroid
tolerate
relatively
large
errors
without
sacriﬁcing
classiﬁcation
accuracy
tolerance
noise
size
1/√n
well–justiﬁed
high–dimensional
systems
concentration
measure
arguments
anticipate
1/√n
appropriate
problems
lack
underlying
symmetry
assignment
set
even/odd
classiﬁcation
second
study
eﬀect
training
data
size
performance
accuracy
two
algorithms
ﬁxed
noise
rate
0.1.
figure
plots
training
data
size
versus
performance
accuracy
blue
squares
centroid
red
circles
vary
training
set
size
taking
random
fractions
4000
points
fractions
0.1
0.2
0.9.
training
set
sizes
signiﬁcantly
outperforms
centroid
addition
exhibits
increasing
performance
accuracy
increases
contrast
centroid
accuracy
hovers
around
even
increases
−5050.40.50.60.70.80.91log
noiseaccuracydigit
data
odd
even
centroidnn00.20.40.60.810.750.80.850.90.9511.05fractioneofetrainingedataaccuracydigitedatae
oddevseeven
ecentroidnn
digit
classiﬁcation
task
estimate
accuracy
obtained
using
number
oracle
queries
scales
constant
success
probability
−5/4
log
contrast
number
queries
classical
nearest-neighbor
algorithm
scales
−5/2
centroid–based
algorithm
achieves
best
0.78.
addition
ﬁnd
dr2
max
2.8
problem
indicating
cost
state
preparation
likely
become
negligible
turn
outperforms
centroid
digit
classiﬁcation
task
ﬁnd
tasks
outlined
appendix
centroid
outperforms
however
tasks
centroid
performs
well
ﬁnd
methods
exhibit
low
classiﬁcation
accuracy
could
indicate
need
training
data
case
may
begin
outperform
centroid
amount
training
data
grows
problems
could
also
addressed
adding
clusters
nearest–centroid
method
i.e
take
cid:48
digit
classiﬁcation
case
makes
sense
take
cid:48
distance
calculated
digit
centroid
although
may
work
well
digit
classiﬁcation
ﬁnding
appropriate
clusters
data
high–dimensional
vector
spaces
costly
problem
np–hard
general
although
heuristic
algorithms
often
work
well
key
point
behind
neither
algorithm
viewed
innately
superior
simply
tools
appropriate
use
circumstances
others
see
results
nearest–neighbor
classiﬁcation
quite
robust
error
problem
handwriting
recognition
whereas
nearest
centroid
assignment
perform
well
provide
numerical
experiments
test
cases
related
diagnosing
medical
conditions
appendix
results
suggest
quantum
nearest–neighbor
classiﬁcation
algorithm
likely
adversely
aﬀected
errors
coherent
amplitude
estimation
subroutine
problems
relatively
small
expect
errors
much
signiﬁcant
though
cases
large
occurs
frequently
text
classiﬁcation
problems
argued
appendix
begs
question
whether
direct
calculation
may
actually
provide
advantages
cases
owing
fact
dependence
address
issue
along
question
algorithms
compare
monte–carlo
methods
comparison
monte–carlo
approaches
although
natural
analog
quantum
nearest–neighbor
methods
direct
calculation
important
consider
well
algorithm
performs
compared
monte–carlo
algorithms
ﬁnding
distance
core
idea
behind
monte–carlo
approaches
seldom
necessary
query
oracle
ﬁnd
components
compute
distance
substantially
reduce
scaling
cost
distance
calculations
cases
tend
particularly
useful
cases
training
data
tightly
clustered
high–dimensional
spaces
monte–carlo
approximation
inner
product
two
d–sparse
vectors
found
via
following
approach
first
samples
individual
components
taken
assume
locations
mutually
non–zero
known
apriori
imagine
vector
dimension
max
let
denote
sequence
indexes
component
d–dimensional
vector
drawn
uniform
probability
i.e.
1/d
union
set
vectors
support
unbiased
estimator
inner
product
given
particular
shown
cid:32
cid:88
i=1
cid:33
itb2
cid:18
d2r4
max
cid:19
cid:80
chebyshev
inequality
therefore
implies
ﬁxed
vectors
d2r4
max−2
suﬃcient
guarantee
correct
estimate
within
distance
high
probability
also
random
unit
vectors
high
probability
typically
cost
estimate
simply
1/2
max/2
logarithmic
factors
quadratically
cost
nearest–neighbor
classiﬁcation
d2r4
worse
nearest–neighbor
algorithm
cases
dr2
i=1
itb2
similar
calculation
implies
estimate
components
mean
vector
within
error
/nc
guarantees
overall
error
estimate
need
variance
component
max
cid:88
t=1
aitbit
vector
let
set
d–sparse
unit
vectors
cid:88
m=1
thus
wish
estimate
samples
vector
component
i.e.
obeys
within
error
/nc
high
probability
suﬃces
take
number
4r2
max
cid:19
cid:18
d6r14
cid:18
cid:19
maxn
cid:18
maxn
cid:19
cost
max
since
diﬀerent
components
total
cost
ncns
implies
since
b|2
2at
follows
euclidean
distance
centroid
computed
using
number
queries
scales
simple
argument
suggests
number
queries
needed
classical
oracle
estimate
distance
centroid
also
eﬃcient
ﬁxed
using
classical
sampling
algorithm
cost
practice
prohibitively
high
exact
error
analysis
would
needed
ﬁnd
better
estimate
true
complexity
classical
centroid-based
classiﬁcation
important
question
remains
scaling
actually
better
direct
calculation
see
appendix
almost
unit
vectors
lie
within
band
width
equator
unit-hypersphere
means
vectors
classes
evenly
distributed
1/√n
needed
order
ensure
nearest–neighbor
methods
correctly
assign
test
vector
cases
cost
monte–carlo
calculation
nearest–neighbor
cost
cid:0
d2r4
max
cid:1
asymptotically
equivalent
cost
direct
calculation
dr2
max
therefore
regard
random
unit
vectors
typical
monte–carlo
methods
typically
oﬀer
asymptotic
improvements
direct
calculation
furthermore
see
near–quadratic
improvement
scaling
aﬀorded
use
oblivious
amplitude
ampliﬁcation
needed
order
provide
superior
scaling
cases
worth
noting
however
training
data
atypical
haar–random
unit
vectors
monte–carlo
methods
turn
centroid–based
classiﬁcation
may
yield
advantages
direct
calculation
vii
application
k–means
clustering
techniques
also
used
accelerate
single
step
k–means
unsupervised
learning
algorithm
clusters
training
examples
k–clusters
based
distances
cluster
centroids
unlike
supervised
classiﬁcation
nearest–neighbor
classiﬁcation
k–means
clustering
require
human
input
form
pre–classiﬁed
training
data
instead
seeks
learn
useful
criteria
classifying
data
may
necessarily
apparent
raw
feature
data
clusters
uniquely
speciﬁed
centroid
goal
algorithm
assign
training
vectors
clusters
intra–cluster
variance
minimized
algorithm
k–means
clustering
also
known
lloyd
algorithm
follows
choose
initial
values
centroids
training
vector
compute
distance
cluster
centroids
assign
training
vector
cluster
whose
centroid
closest
recompute
location
centroids
cluster
repeat
steps
clusters
converge
maximum
number
iterations
reached
problem
optimally
clustering
data
known
np–hard
ﬁnding
optimal
clustering
using
algorithm
computationally
expensive
also
mentioned
quantum
computing
leveraged
accelerate
task
query
complexity
performing
single
iteration
k–means
i.e
single
repetition
steps
given
following
corollary
quantum
algorithm
clustering
deviates
subtlely
standard
algorithm
clustering
algorithm
easily
output
cluster
centroids
cluster
labels
assigned
training
vector
centroids
need
inferred
using
process
quantum
state
tomography
particular
compressed
sensing
used
number
samples
needed
learn
centroid
within
ﬁxed
accuracy
scales
log
prohibitively
expensive
given
k–means
algorithm
classically
following
corollary
gives
query
complexity
performing
iteration
k–means
using
complete
set
labels
members
cluster
specify
centroids
vector
representation
centroids
computed
using
techniques
theorem
proof
given
appendix
corollary
let
d–sparse
unit
vectors
components
satisfy
maxm
rmax
average
number
queries
made
involved
computing
cluster
assignments
training
vector
iteration
k–means
clustering
using
distance
calculations
error
success
probability
cid:24
dr2
max
cid:25

log
cid:16
81k
log
cid:17
8/π2
1/2

360m√k
shows
step
k–means
performed
using
number
queries
scales
m√k
log
substantially
better
scaling
direct
classical
method
cid:29
viii
conclusions
presented
quantum
algorithms
performing
nearest-neighbor
classiﬁcation
k–means
clustering
promise
signiﬁcant
reductions
query
complexity
relative
classical
counterparts
algorithms
enable
classiﬁcation
clustering
datasets
high-dimensional
feature
space
well
large
number
training
examples
computation
distances
extremely
common
machine
learning
algorithms
devel-
oped
two
fast
methods
computing
distance
vectors
quantum
computer
implemented
coherently
finally
shown
algorithms
robust
noise
arises
coherent
amplitude
esti-
mation
perform
well
applied
typical
real–world
tasks
asymptotically
outperform
monte–carlo
methods
nearest–neighbor
classiﬁcation
ﬁnd
quantum
algorithms
machine
learning
provide
algorithmic
improvements
classical
machine
learning
techniques
algorithms
step
toward
blending
fast
quantum
methods
proven
machine
learning
techniques
work
needed
provide
complete
cost
assessment
terms
number
elemen-
tary
gate
operations
logical
qubits
needed
practically
achieve
speedups
using
fault
tolerant
quantum
computer
possibilities
future
work
include
examining
width/depth
tradeoﬀs
arise
executing
algorithms
quantum
computers
well
algorithm
performs
cases
oracle
represents
database
also
would
interesting
see
whether
methods
directly
adapted
practical
cases
characterizing
data
yielded
eﬃcient
quantum
circuit
remains
open
question
whether
exponential
speedups
obtained
quantum
algorithm
supervised
unsupervised
semi-supervised
machine
learning
tasks
beyond
computational
speedups
ﬁnal
interesting
question
whether
quantum
computation
allows
new
classes
learning
algorithms
natural
classical
analogs
search
inherently
quantum
machine
learning
algorithms
may
reveal
potentially
useful
methods
also
shed
light
deeper
questions
means
learn
whether
quantum
physics
places
inherent
limitations
system
ability
learn
data
thank
matt
hastings
martin
roeteller
valuable
comments
feedback
acknowledgments
http
//about.usps.com/who-we-are/postal-facts/size-scope.htm/
2014
thomas
cover
peter
hart
nearest
neighbor
pattern
classiﬁcation
information
theory
ieee
transactions
:21–27
1967
nicholas
ball
robert
brunner
adam
myers
natalie
strand
stacey
alberts
david
tcheng
robust
machine
learning
applied
astronomical
data
sets
iii
probabilistic
photometric
redshifts
galaxies
quasars
sdss
galex
astrophysical
journal
683
:12
2008
nicholas
ball
robert
brunner
data
mining
machine
learning
astronomy
international
journal
modern
physics
:1049–1106
2010
fabian
gieseke
kai
lars
polsterer
andreas
thom
peter
zinn
dominik
bomanns
r-j
dettmar
oliver
kramer
jan
vahrenhold
detecting
quasars
large-scale
astronomical
surveys
machine
learning
applications
icmla
2010
ninth
international
conference
pages
352–357
ieee
2010
nathan
wiebe
christopher
granade
christopher
ferrie
cory
hamiltonian
learning
certiﬁcation
using
quantum
resources
physical
review
letters
112
:190501
2014
keinosuke
fukunaga
patrenahalli
narendra
branch
bound
algorithm
computing
k-nearest
neighbors
computers
ieee
transactions
100
:750–753
1975
esma
a¨ımeur
gilles
brassard
s´ebastien
gambs
machine
learning
quantum
world
advances
artiﬁcial
intelligence
pages
431–442
springer
2006
daoyi
dong
chunlin
chen
hanxiong
tzyh-jong
tarn
quantum
reinforcement
learning
systems
man
cybernetics
part
cybernetics
ieee
transactions
:1207–1220
2008
seth
lloyd
masoud
mohseni
patrick
rebentrost
quantum
algorithms
supervised
unsupervised
machine
learning
arxiv
preprint
arxiv:1307.0411
2013
patrick
rebentrost
masoud
mohseni
seth
lloyd
quantum
support
vector
machine
big
feature
big
data
classiﬁcation
arxiv
preprint
arxiv:1307.0471
2013
ilya
levner
feature
selection
nearest
centroid
classiﬁcation
protein
mass
spectrometry
bmc
bioinformatics
:68
2005
nuanwan
soonthornphisaj
kanokwan
chaikulseriwat
piyanan
tang-on
anti-spam
ﬁltering
centroid-based
clas-
siﬁcation
approach
signal
processing
2002
6th
international
conference
volume
pages
1096–1099
ieee
2002
james
whitﬁeld
jacob
biamonte
al´an
aspuru-guzik
simulation
electronic
structure
hamiltonians
using
quantum
computers
molecular
physics
109
:735–750
2011
dave
wecker
bela
bauer
bryan
clark
matthew
hastings
matthias
troyer
quantum
chemistry
performed
small
quantum
computer
arxiv
preprint
arxiv:1312.1695
2013
vittorio
giovannetti
seth
lloyd
lorenzo
maccone
quantum
random
access
memory
physical
review
letters
100
:160501
2008
lov
grover
terry
rudolph
creating
superpositions
correspond
eﬃciently
integrable
probability
distributions
arxiv
preprint
quant-ph/0208112
2002
gilles
brassard
peter
hoyer
michele
mosca
alain
tapp
quantum
amplitude
ampliﬁcation
estimation
arxiv
preprint
quant-ph/0005055
2000
christoph
d¨urr
peter
høyer
quantum
algorithm
ﬁnding
minimum
arxiv
preprint
quant-ph/9607014
1996
harry
buhrman
richard
cleve
john
watrous
ronald
wolf
quantum
ﬁngerprinting
physical
review
letters
:167902
2001
stuart
lloyd
least
squares
quantization
pcm
information
theory
ieee
transactions
:129–137
1982
robert
tibshirani
trevor
hastie
balasubramanian
narasimhan
gilbert
chu
diagnosis
multiple
cancer
types
shrunken
centroids
gene
expression
proceedings
national
academy
sciences
:6567–6572
2002
andrew
childs
nathan
wiebe
hamiltonian
simulation
using
linear
combinations
unitary
operations
quantum
information
computation
11-12
:901–924
2012
yann
lecun
corinna
cortes
mnist
database
handwritten
digits
1998
michel
ledoux
concentration
measure
phenomenon
volume
89.
ams
bookstore
2005
sylvester
eriksson-bique
mary
solbrig
michael
stefanelli
sarah
warkentin
ralph
abbey
ilse
ipsen
importance
sampling
monte
carlo
matrix
multiplication
algorithm
application
information
retrieval
siam
journal
scientiﬁc
computing
:1689–1706
2011
macqueen
methods
classiﬁcation
analysis
multivariate
observations
1967
steven
flammia
david
gross
yi-kai
liu
jens
eisert
quantum
tomography
via
compressed
sensing
error
bounds
sample
complexity
eﬃcient
estimators
new
journal
physics
:095022
2012
anil
jain
data
clustering
years
beyond
k-means
pattern
recognition
letters
:651–666
2010
bache
lichman
uci
machine
learning
repository
2013
olvi
mangasarian
nick
street
william
wolberg
breast
cancer
diagnosis
prognosis
via
linear
programming
operations
research
:570–577
1995
john
ross
quinlan
compton
horn
lazarus
inductive
knowledge
acquisition
case
study
proceedings
second
australian
conference
applications
expert
systems
pages
137–156
addison-wesley
longman
publishing
co.
inc.
1987
yoshua
bengio
learning
deep
architectures
foundations
trends
machine
learning
:1–127
2009.
also
published
book
publishers
2009
dorit
aharonov
amnon
ta-shma
adiabatic
quantum
state
generation
statistical
zero
knowledge
proceedings
thirty-ﬁfth
annual
acm
symposium
theory
computing
pages
20–29
acm
2003
andrew
childs
richard
cleve
enrico
deotto
edward
farhi
sam
gutmann
daniel
spielman
exponential
algorithmic
speedup
quantum
walk
proceedings
thirty-ﬁfth
annual
acm
symposium
theory
computing
pages
59–68
acm
2003
fig
half–moon
data
set
vectors
unnormalized
two
clusters
red
blue
vectors
correspond
two
classes
used
assignment
set
red
blue
stars
give
centroids
corresponding
cluster
nathan
wiebe
dominic
berry
peter
høyer
barry
sanders
simulating
quantum
dynamics
quantum
computer
journal
physics
mathematical
theoretical
:445308
2011
ashwin
nayak
felix
quantum
query
complexity
approximating
median
related
statistics
proceedings
thirty-ﬁrst
annual
acm
symposium
theory
computing
pages
384–393
acm
1999
d´enes
petz
j´ulia
r´eﬀy
asymptotics
large
haar
distributed
unitary
matrices
periodica
mathematica
hungarica
:103–117
2004.
appendix
additional
numerical
experiments
evaluate
performance
nearest–neighbor
nearest–centroid
centroid
algorithms
several
additional
machine
learning
tasks
list
datasets
respective
training
set
sizes
feature
dimensions
listed
algorithm
task
mapped
binary
classiﬁcation
problem
two
classes
data
sets
general
contain
equal
number
training
vectors
per
class
denote
number
training
vectors
classes
respectively
noise
induced
inaccurate
estimation
distances
modeled
introducing
gaussian
random
noise
zero
mean
variance
clipping
result
interval
distributions
uniformly
distributed
noise
gave
qualitatively
similar
results
features
used
data
set
take
dramatically
diﬀerent
value
types
example
diabetes
data
set
contains
features
patient
age
blood
pressure
tasks
scale
feature
zero
mean
unit
variance
given
data
set
scale
vectors
unit
length
length
vector
important
classiﬁcation
since
points
one
class
likely
nearly
co-linear
another
class
low–dimensional
spaces
non–unit
vectors
easily
accommodated
algorithms
multiplying
norms
vectors
inner–product
based
approach
increasing
number
vectors
used
centroid
approach
also
means
|vj|
typically
order
suggests
data
sets
consider
|vj|
unreasonable
hence
refer
regime
low–noise
regime
high–noise
regime
ﬁrst
evaluate
algorithms
standard
machine
learning
benchmark
commonly
referred
half
moon
dataset
consists
two
synthetically
generated
crescent-shaped
clusters
points
shown
figure
dataset
challenges
classiﬁcation
algorithms
since
convex
hulls
two
moons
overlap
mean
value
cluster
denoted
star
sits
region
covered
points
data
set
hard
classify
centroid–based
methods
using
one
cluster
14.3
data
closer
centroid
opposite
set
centroid
means
accuracy
centroid–based
assignment
85.7
contrast
expect
nearest–neighbor
classiﬁcation
work
well
typical
euclidean
distance
points
roughly
0.03
whereas
two
classes
separated
distance
approximately
0.5.
means
succeed
near
100
probability
except
cases
training
set
size
small
figure
plot
accuracies
nearest–neighbor
algorithm
blue
squares
nearest–centroid
−2−1.5−1−0.500.511.52−2−1.5−1−0.500.511.52assigns
blueassigns
red
fig
accuracy
function
noise
distance
computation
half–moon
data
data
used
train
classiﬁer
remaining
used
test
fig
accuracy
function
training
data
size
half–moon
data
noise
10−5
used
algorithm
centroid
red
circles
functions
noise
distance
computation
signiﬁcantly
outperforms
centroid
low–noise
regime
exhibiting
accuracy
near
100
versus
centroid
accuracy
noise
level
increases
accuracy
algorithms
decays
however
low
noise
regimes
outperforms
centroid
statistical
signiﬁcance
high
noise
levels
algorithms
decay
accuracy
expected
figure
shows
accuracy
function
training
data
size
training
data
size
taken
fraction
2000
vectors
set
remaining
fraction
2000
vectors
used
test
accuracy
assignments
almost
always
successful
classifying
vectors
whereas
centroid
achieves
accuracies
84–88
neither
algorithm
exhibits
signiﬁcant
improvements
learning
training
set
size
increased
behavior
indicates
diﬃculty
classiﬁcation
task
centroid
course
methods
employed
order
boost
success
probability
centroid–based
classiﬁcation
simplest
cluster
data
using
k–means
clustering
algorithm
subdivide
half
moons
two
clusters
semi–supervised
approach
often
works
well
expensive
certain
representations
data
next
tasks
consider
consist
determining
whether
given
disease
present
based
patient
data
diseases
considered
include
breast
cancer
heart
disease
thyroid
conditions
diabetes
data
taken
ucl
machine
learning
repository
details
features
data
size
given
algorithm
number
features
number
points
year
half
moon
breast
cancer
heart
disease
statlog
data
set
thyroid
diabetes
pima
2000
683
270
215
532
1000
1000
239
444
1992
120
150
1993
150
1987
177
355
1990
table
evaluation
datasets
sizes
data
set
conditions
examined
figure
cases
modiﬁed
data
slightly
breast
cancer
data
thyroid
data
pima
diabetes
study
contained
instances
missing
data
case
removed
vector
missing
value
also
removed
boolean
features
thyroid
pima
diabetes
data
sets
left
column
figure
shows
accuracy
blue
squares
centroid
red
circles
function
noise
distance
computations
ﬁrst
row
shows
accuracies
breast
cancer
data
algorithms
exhibit
similarly
high
accuracies
low–noise
regime
outperforming
centroid
signiﬁcance
extreme
noise
regime
performs
slightly
better
random
expected
second
last
rows
accuracies
heart
disease
diabetes
data
shown
tasks
ﬁnd
low–noise
regime
centroid
slightly
outperforms
without
statistical
signiﬁcance
except
presence
high
amounts
noise
methods
exhibit
learning
however
cases
learning
limited
around
third
row
accuracy
thyroid
data
shown
exhibits
signiﬁcantly
better
accuracy
compared
less
centroid
case
centroid–based
algorithm
performed
worse
random
guessing
poor
accuracy
caused
part
decision
divide
distance
standard
deviation
−5050.40.50.60.70.80.911.1logd10dnoiseaccuracyhalfdmoonddatadcentroidnn00.20.40.60.810.50.60.70.80.91fractiondofdtrainingddataaccuracyhalfdmoonddatadcentroidnn
fig
left
column
accuracy
function
noise
distance
computation
right
column
accuracy
function
training
set
size
breast
cancer
ﬁrst
row
heart
disease
second
row
thyroid
third
row
diabetes
fourth
row
data
data
used
training
remainder
testing
data
left
column
10−5
taken
data
right
column
distances
seen
figure
10.
found
variance
hypothyroid
cases
high
enough
mean
training
vectors
tested
negative
thyroid
conditions
within
one
standard
deviation
−5050.40.50.60.70.80.91logp10pnoiseaccuracybreastpcancercentroidnn−5050.40.50.60.70.80.9logp10pnoiseaccuracyheartcentroidnn−5050.40.50.60.70.80.91logp10pnoiseaccuracythyroidcentroidnn−5050.450.50.550.60.650.70.750.8logp10pnoiseaccuracypimacentroidnn00.20.40.60.810.910.920.930.940.950.960.970.98fractionpofptrainingpdataaccuracybreastpcancercentroidnn00.20.40.60.810.650.70.750.80.850.9fractionpofptrainingpdataaccuracyheartcentroidnn00.20.40.60.810.20.40.60.811.2fractionpofptrainingpdataaccuracythyroidcentroidnn00.20.40.60.810.650.70.750.8fractionpofptrainingpdataaccuracypimacentroidnn
fig
10.
accuracy
function
noise
distance
computation
fraction
total
data
used
training
thyroid
data
set
normalization
step
distances
omitted
data
used
training
remainder
testing
left
plot
10−5
taken
data
right
plot
cid:112
cid:112
ev∈xa
mean
/σa
4.4.
thus
test
particular
incorrectly
assign
vectors
high
probability
correctly
assign
vectors
high
probability
therefore
expect
accuracy
roughly
since
probability
drawing
vector
roughly
65/215
close
observed
accuracy
/σb
0.49
ev∈xb
mean
data
figure
forgoes
normalizing
computed
distances
centroid
devoid
problems
low
noise
centroid
succeeds
roughly
time
falls
within
statistical
error
data
also
observe
assignment
accuracy
increases
methods
training
data
used
stark
contrast
data
figure
however
imply
centroid–based
method
actually
performing
well
assign
data
class
every
time
regardless
distance
would
succeed
probability
centroid
used
accuracy
increases
roughly
also
since
two
clusters
strongly
overlap
distance
centroid
trustworthy
statistic
base
classiﬁcation
reasons
use
centroid
diagnose
thyroid
conditions
either
without
normalization
inferior
using
methods
right
column
figure
shows
accuracy
two
algorithms
function
training
set
size
breast
cancer
task
ﬁrst
row
see
centroid
exhibit
little
variation
accuracy
amount
training
data
increases
similarly
heart
disease
diabetes
tasks
second
last
rows
increase
training
data
size
imply
signiﬁcant
increases
accuracy
however
thyroid
task
see
diﬀerences
learning
centroid
training
data
size
increases
accuracy
improves
centroid
accuracy
decreases
slightly
hard
determine
general
centroid
sometimes
outperforms
outliers
data
frequently
one
reason
outliers
cause
problems
becomes
increasingly
likely
training
data
included
outlier
point
close
given
element
thus
increasing
training
size
actually
harmful
certain
nearest–neighbor
classiﬁcation
problems
centroid
less
sensitive
problems
averaging
data
set
reduces
signiﬁcance
outliers
problems
addressed
case
using
k–nearest–neighbor
classiﬁers
instead
nearest–neighbor
classiﬁcation
quantum
algorithms
trivially
modiﬁed
output
classes
closest
vectors
see
appendix
alternatively
problems
also
addressed
using
alternative
machine
learning
strategies
deep
learning
summary
numerical
results
indicate
classiﬁcation
accuracy
turn
best
choice
algorithm
highly
dependent
particular
task
dataset
nearest–neighbor
classiﬁcation
appears
preferred
algorithm
tasks
presented
practice
highly
non-linear
combination
classiﬁcation
algorithms
commonly
used
however
classical
approaches
computationally
expensive
particular
classiﬁcation
large
dataset
required
quantum
algorithms
classiﬁcation
oﬀer
advantage
fast
classiﬁcation
conjunction
high
performance
accuracy
may
enable
accurate
classiﬁcation
datasets
otherwise
classically
would
possible
−5−4−3−2−10123450.40.50.60.70.80.91logp10pnoiseaccuracythyroidpcentroidnn00.10.20.30.40.50.60.70.80.910.780.80.820.840.860.880.90.920.940.960.98percentagepofptrainingpdataaccuracythyroidpcentroidnn
appendix
proofs
main
results
present
proofs
theorem
theorem
way
number
propositions
independently
veriﬁed
begin
preliminary
results
show
state
preparations
used
algorithms
eﬃcient
review
known
results
performance
quantum
minimum
ﬁnding
algorithm
amplitude
estima-
tion
present
coherent
majority
voting
scheme
variant
swap
test
provide
intermediate
results
needed
apply
d¨urr
høyer
algorithm
amplitude
estimation
coherently
use
results
prove
theorem
finally
turn
attention
proving
theorem
uses
many
techniques
used
prove
theorem
addition
requires
introduction
new
methods
computing
distances
cluster
centroids
intra–cluster
variance
begin
introducing
method
implement
operator
needed
nearest–centroid
classiﬁ-
preliminary
results
cation
algorithm
lemma
unitary
cid:40
1√2
1√2m
|vj0|
otherwise
eﬃciently
synthesized
within
error
quantum
computer
equipped
hadamard
π/8
cnot
gates
proof
since
unitary
hermitian
straightforward
exercise
taylor
theorem
show
thus
choose
e−iht
ﬁxed
value
e−ih⊗mt
cos
ih⊗m
sin
cid:18
sin
cid:105
sin
cid:105
cos
value
found
setting
1/m
yields
finally
made
sparse
eﬃciently
sin−1
cid:19
cid:32
cid:114
cid:20
cid:33
cid:88
cid:105
cid:21
eiπ/4
e−iπ/4
hence
h⊗n
transformed
one–sparse
matrix
applying
basis
transformation
qubit
one–
sparse
matrices
eﬃciently
simulated
34–36
using
gates
cnot
gates
within
error
completing
cid:117
cid:116
proof
lemma
also
use
amplitude
estimation
result
brassard
estimate
amplitude
squared
marked
component
quantum
state
denote
algorithm
works
applying
phase
estimation
algorithm
operator
performs
iteration
grover
algorithm
wish
estimate
amplitude
marked
state
provide
circuit
amplitude
estimation
figure
11.
following
theorem
shows
amplitude
estimation
learn
resultant
probabilities
quadratically
faster
statistical
sampling
theorem
brassard
høyer
mosca
tapp
positive
integers
amplitude
estimation
algorithm
outputs
cid:112
cid:18
cid:19
|˜a
2πk
probability
least
8/π2
probability
greater
uses
exactly
iterations
grover
algorithm
certainty
even
certainty
cid:105
cid:105
fig
11.
quantum
circuit
amplitude
estimation
l–dimensional
fourier
transform
controlled
operator
applies
grover
iterations
target
state
top
register
cid:105
|ip
cid:105
|mp
cid:105
cid:80
fig
12.
circuit
performing
cmp
illustrated
single
qubit
inputs
|ip
cid:105
|mp
cid:105
repeating
circuit
times
lower
register
contain
cid:105
lemma
state
1√m
j=1
cid:105
prepared
eﬃciently
deterministically
using
quantum
computer
proof
proof
proceeds
ﬁrst
showing
non–deterministic
protocol
preparing
state
question
showing
amplitude
ampliﬁcation
used
make
state
preparation
process
deterministic
let
cid:100
log2
cid:101
let
cid:105
computational
basis
state
stores
binary
string
proof
follows
fact
following
circuit
h⊗m
cmp
|0⊗m
cid:105
cid:105
cid:105
cid:105
prepares
desired
state
given
measurement
outcome
occurs
probability
cmp
obeys
operation
cid:40
cmp|i
cid:105
cid:105
cid:105
cid:105
cid:105
cid:105
cid:105
cid:105
cid:105
cid:16
cid:113
cid:17
cmp
implemented
using
circuit
figure
12.
hence
state
prepared
eﬃciently
high
probability
measurements
used
also
note
quantum
control
value
replaced
classical
control
cases
quantum
superposition
diﬀerent
values
needed
since
success
probability
known
success
probability
boosted
certainty
amplitude
applications
cmp
according
theorem
means
ampliﬁcation
requires
measurement
removed
state
preparation
step
without
sacriﬁcing
eﬃciency
algorithm
cid:117
cid:116
another
important
result
method
d¨urr
høyer
given
following
lemma
lemma
d¨urr
høyer
expected
number
grover
iterations
needed
learn
min
bounded
nayak
show
algorithm
also
near–optimal
providing
matching
lower
bound
minimum
ﬁnding
proven
using
polynomial
method
proof
theorem
order
ﬁnd
minimum
value
set
diﬀerent
quantum
amplitudes
using
lemma
need
able
perform
iterations
grover
algorithm
using
result
theorem
done
directly
high
probability
traditional
approach
amplitude
estimation
reversible
provide
reversible
algorithm
uses
coherent
form
majority
voting
obtain
reversible
analog
algorithms
like
amplitude
estimation
cid:112
lemma
let
unitary
operation
maps
|0⊗n
cid:105
cid:55
√a|y
cid:105
|a||y⊥
cid:105
1/2
|a0|
|a|
using
queries
exists
deterministic
algorithm
exists
integer
state
cid:105
cid:39
produced
obeys
cid:107
cid:105
|0⊗nk
cid:105
cid:105
cid:107
√2∆
using
number
queries
bounded
cid:38
cid:0
1/∆
|a0|
cid:1
proof
basic
idea
behind
algorithm
prepare
copies
state
√a|y
cid:105
+√1
a|y⊥
cid:105
coherently
compute
median
via
reversible
circuit
uncompute
resource
states
used
ﬁnd
median
values
first
let
circuit
performs
|y1
cid:105
···|yk
cid:105
cid:105
cid:55
|y1
cid:105
···|yk
cid:105
|¯y
cid:105
use
¯yk
denote
median
transformation
performed
implementing
sort
algorithm
using
log
operations
hence
eﬃcient
initial
state
part
protocol
form
therefore
partition
k–fold
tensor
product
sum
two
disjoint
sets
sum
states
median
another
sum
states
median
equal
denote
two
sums
cid:105
cid:105
respectively
equivalent
expressing
cid:112
√a|y
cid:105
|a||y⊥
cid:105
a|ψ
cid:105
cid:112
cid:112
cid:112
|a||y⊥
cid:105
a|ψ
cid:105
cid:105
|a|2
cid:105
√a|y
cid:105
|a||y⊥
cid:105
cid:105
represents
subspace
median
see
let
imagine
measuring
ﬁrst
register
computational
basis
probability
obtaining
value
direct
consequence
exists
possibly
entangled
state
cid:105
a|ψ
cid:105
cid:105
compliment
goal
show
|a|2
suﬃciently
large
results
resulting
bit
strings
given
binomial
theorem
|a|2
cid:105
√a|y
cid:105
|a|2
cid:105
√a|y
cid:105
cid:112
|a||y⊥
cid:105
cid:112
cid:112
cid:18
cid:19
|a|p|1
|a||k−p
compute
probability
measurement
last
register
yield
observing
fact
sequence
measurements
contains
k/2
y–outcomes
median
must
k/2
therefore
probability
computed
value
median
probability
measured
results
contain
k/2
outcomes
given
cid:19
cid:18
cid:98
k/2
cid:99
cid:88
cid:33
cid:0
k|a|
cid:32
cid:1
p=0
exp
|a|p|1
|a||k−p
cid:18
cid:32
exp
−2k
|a0|
cid:19
cid:33
b10
b11
using
hoeﬀding
inequality
b10
|a|
|a0|
1/2
ﬁnd
b11
therefore
implies
cid:1
b12
cid:17
cid:112
next
applying
a†⊗k
ﬁrst
register
obtain
cid:1
cid:17
a†⊗k
cid:16
note
cid:10
y|y⊥
cid:11
hence
cid:105
orthogonal
cid:105
|y⊥
cid:105
taken
2–norm
b13
gives
a†⊗k
cid:16
cid:112
|a|2
cid:0
cid:1
cid:17
cid:105
cid:105
cid:105
|a|2
cid:105
|a|2
cid:105
cid:105
a|ψ
cid:105
cid:105
b13
cid:17
cid:105
cid:105
cid:105
cid:112
|a|2
√2∆
b14
cid:12
cid:12
cid:12
a†⊗k
cid:16
a|ψ
cid:105
cid:105
cid:0
cid:1
cid:0
|a0|
a†⊗k
cid:16
cid:112
|0⊗nk
cid:105
cid:105
a†⊗k
cid:16
cid:112
cid:17
cid:112
|a|2
cid:105
a|ψ
cid:105
cid:105
|a|2
cid:0
cid:12
cid:12
cid:12
|0⊗nk
cid:105
cid:105
since
|a|2
chosen
per
b12
result
follows
noting
must
chosen
cid:117
cid:116
integer
total
number
queries
made
prepare
state
2qk
lemma
shows
coherent
majority
voting
used
remove
measurements
used
algorithms
amplitude
estimation
price
introducing
small
amount
error
resultant
state
use
protocol
d¨urr
høyer
algorithm
ﬁnd
minimum
value
possible
outputs
algorithm
shown
following
corollary
|a||y⊥j
cid:105
corollary
assume
unitary
transformation
cid:105
|0⊗n
cid:105
cid:55
cid:105
1/2
|a0|
|a|
performed
using
queries
expected
number
queries
made
ﬁnd
minj
failure
probabilty
bounded
cid:16
√a|yj
cid:105
cid:112
cid:17

cid:16
81m
cid:0
cid:1
|a0|
cid:17

90√m
proof
lemma
states
45√m
applications
grover
search
required
requires
45√m
queries
approximate
oracle
prepares
since
two
queries
required
per
grover
iteration
two
required
perform
reﬂection
initial
state
none
required
reﬂect
space
orthogonal
marked
state
lemma
therefore
says
cost
performing
portion
algorithm
nqueries
90√m
cid:38
cid:0
1/∆
|a0|
cid:39
cid:1
b15
next
need
ﬁnd
value
make
failure
probability
approximate
oracle
let
assume
worst
case
scenario
measurement
fails
output
desired
value
even
entire
algorithm
fails
upper
bound
probability
failure
summing
probability
failure
steps
search
assuming
algorithm
searching
element
rank
least
sense
d¨urr
høyer
number
calls
oracle
yielding
means
amplitude
erroneous
component
state
using
subadditivity
quantum
errors
worst
case
scenario
algorithm
must
search
entries
extremely
unlikely
large
average
complexity
means
probability
least
one
failed
observation
occuring
cid:88
r=2
81∆m
81m
hm−1
81m
b16
cid:114
cid:114
hm−1
1th
harmonic
number
euler
constant
therefore
want
total
probability
error
suﬃces
choose
combining
b15
b17
gives
average
query
complexity
obeys
81m

cid:16
81m
cid:17
|a0|
1/2

nqueries
90√m
b17
b18
cid:117
cid:116
note
want
maximize
value
sin2
πyj/r
yielded
amplitude
estimation
algorithm
maximization
equivalent
minimizing
|r/2
yj|
given
returned
coherently
de–randomized
amplitude
estimation
circuit
measurement–free
circuit
used
computes
|r/2
yj|
input
requires
oracle
calls
hence
corollary
applies
circumstances
modiﬁcation
results
theorem
theorem
follow
directly
corollary
substitution
appropriate
values
|a0|
remaining
work
focuses
devising
appropriate
state
preparation
algorithm
used
theorem
lemma
10.
let
d–sparse
assume
quantum
computer
access
unitary
transformation
exists
implemented
eﬃciently
using
oracle
calls
maps
cid:115
cid:88
i=1
cid:105
cid:105
cid:55
cid:105
cid:105
proof
begin
preparing
state
eiφjf
cid:105
max
e−iφjf
cid:105
rjf
eiφjf
max
cid:88
i=1
cid:105
cid:105
cid:105
cid:105
prepared
reversibly
eﬃciently
applying
lemma
next
step
apply
oracle
result
performs
cid:105
cid:105
cid:105
cid:105
cid:55
querying
implements
cid:105
cid:105
cid:105
cid:105
cid:55
cid:88
i=1
cid:88
i=1
cid:88
cid:88
i=1
i=1
cid:105
cid:105
cid:105
cid:105
cid:105
cid:105
|vjf
cid:105
cid:105
applying
sin−1
rjf
/rj
max
ﬁnal
qubit
b21
obtain
cid:105
cid:105
|vjf
cid:105
cid:105
cid:55
cid:105
cid:105
|vjf
cid:105
max
cid:105
cid:88
i=1
cid:115
cid:88
i=1
b19
b20
b21
b22
rjf
max
cid:105
result
follows
applying
2φjf
last
qubit
b22
using
clean
ancilla
register
cid:117
cid:116
containing
|vjf
cid:105
three
queries
used
process
next
use
swap
test
provide
method
compute
inner
product
two
vectors
test
implemented
circuit
figure
arbitrary
states
cid:105
cid:105
resultant
state
measurement
cid:105
cid:105
cid:105
cid:105
cid:105
cid:105
cid:105
cid:105
cid:105
cid:105
probability
measuring
ﬁrst
qubit
1/2−|
cid:104
φ|ψ
cid:105
|2/2
ignore
measurement
since
want
use
swap
test
within
grover
iterations
used
theorem
cid:38
cid:115
cid:115
cid:88
cid:114
cid:105
cid:105
cid:105
fig
13.
swap
test
probability
measuring
top
qubit
zero
1/2
cid:104
φ|ψ
cid:105
|2/2
allows
statistical
testing
used
eﬃciently
discriminate
states
cid:112
lemma
11.
ﬁxed
pair
d–sparse
unit
vectors
state
form
|a||φ
cid:105
eﬃciently
prepared
encodes
cid:104
u|vj
cid:105
within
error
|a|
8/π2
using
number
queries
bounded
|a||ψ
cid:105
cid:105
cid:112
d2r2
maxr2
max
cid:39
cid:104
i|vj
cid:105
max
proof
state
cid:105
cid:105
prepared
using
lemma
cnot
gates
cid:105
cid:105
cid:105
cid:105
cid:105
|0⊗n
cid:48
cid:105
max
e−iφjf
cid:105
vjf
max
cid:105
cid:105
|0⊗n
cid:48
cid:105
cid:105
max
e−iφ0f
cid:105
v0f
max
cid:105
b23
b24
|1
cid:105
important
note
qubit
encodes
component
vjf
v0f
diﬀers
cid:105
cid:105
done
order
ensure
inner
product
two
vectors
corresponds
true
inner
product
vectors
speciﬁcally
cid:104
φ|ψ
cid:105
vjiv∗0i
maxr0
max
cid:104
v0|vj
cid:105
drj
maxr0
max
b25
note
directly
apply
swap
test
two
states
undesirable
contribution
inner
product
form
max
e−iφjf
max
eiφ0f
would
arise
remove
possibility
terms
appearing
adding
ancilla
qubit
b23
b24
selects
component
gives
information
inner
product
probability
measuring
swap
test
|a|
maxr2
2|a|
d2r2
cid:104
φ|ψ
cid:105
implies
max
cid:104
u|vj
cid:105
b26
point
could
learn
sampling
distribution
given
swap
test
eﬃcient
use
amplitude
estimation
amplitude
estimation
eﬀect
uses
controlled
grover
search
oracle
case
grover
oracle
requires
implement
reﬂection
initial
state
also
reﬂect
space
orthogonal
target
state
refer
reﬂection
unlike
grover
problem
reﬂection
target
state
trivial
since
target
state
obtained
swap
test
yields
means
oracle
calls
needed
implement
reﬂection
ﬁnal
state
reﬂection
initial
state
form
as0a†
algorithm
maps
|02n
cid:48
+2n+4
cid:105
cid:105
cid:105
form
cid:54
cid:105
cid:40
b27
cid:105
−|x
cid:105
implemented
using
multi–controlled
gate
hence
eﬃcient
prior
steps
show
implemented
using
oracle
calls
lemma
implies
three
queries
needed
preparation
cid:105
three
needed
preparation
cid:105
implies
step
grover
algorithm
given
as0a†sχ
implemented
using
oracle
queries
cid:88
cid:88
i=1
i=1
cid:114
amplitude
estimation
requires
applying
iteration
grover
algorithm
controlled
fashion
times
hilbert–space
dimension
register
contains
output
amplitude
estimation
algo-
rithm
controlled
version
as0a†sχ
requires
additional
oracle
calls
as0a†
made
control-
lable
introducing
controlled
version
using
controlled
rotation
implementation
given
lemma
furthermore
require
oracle
calls
hence
made
controllable
additional
cost
error
resultant
estimate
applying
amplitude
estimation
probability
least
8/π2
means
˜a|
b28
b29
given
want
total
error
probability
least
8/π2
/2
factor
1/2
due
fact
calculation
sin−1
rji/rj
max
inexact
b26
gives
choosing
4d2r2
maxr2
max
b30
suﬃcient
b30
gives
suﬃces
take
number
steps
grover
algorithm
obeys
d2r2
maxr2
max
b31
cid:38
cid:38
cid:39
cid:39
since
grover
iteration
requires
applications
oracles
query
complexity
algorithm
12r
d2r2
maxr2
max
b32
claimed
lastly
need
show
error
resultant
probabilities
inexactly
evaluating
sin−1
rji/rj
max
made
less
/2
polynomial
cost
shown
previously
cid:48
log
1/
cid:105
cid:105
approximate
versions
two
inner
products
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:68
cid:69
cid:12
cid:12
cid:12
φ|ψ
cid:69
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:68
˜φ|
means
since
sin−1
eﬃcient
computed
easily
using
taylor
series
expansion
approximation
cid:117
cid:116
chebyshev
polynomials
cost
making
numerical
error
suﬃciently
small
polynomial
proof
theorem
trivially
follows
proof
theorem
proof
follows
immediate
consequence
corollary
lemma
using
|a0|
8/π2
cid:117
cid:116
max
rmax
proof
theorem
structure
proof
theorem
similar
theorem
biggest
diﬀerence
swap
test
used
compute
euclidean
distance
case
use
method
childs
wiebe
perform
state
preparation
task
see
method
works
let
assume
access
oracle
cid:105
cid:105
cid:105
|vj
cid:105
b33
unitary
clog2
m×log2
following
circuit
|0⊗
log2
cid:105
|0⊗n
cid:105
cid:80
j=0
|vj,0|2vj
cid:107
thus
euclidean
distance
property
probability
measurement
yielding
cid:107
computed
approach
setting
|v0
cid:105
−|u
cid:105
choosing
unitary
appropriately
employ
variant
following
lemma
explicitly
shows
construct
required
states
lemma
12.
ﬁxed
quantities
state
form
cid:112
cid:80
reversibly
prepared
encodes
cid:107
cid:105
cid:24
dr2
|a||ψ
cid:105
cid:105
cid:48
cid:105
within
error
cid:48
encodes
cid:25
within
error
|a|
64/π4
2/3
using
number
queries
bounded
|a||φbad
cid:105
eﬃciently
cid:107
|vp
cid:105
|vj
cid:105
cid:107
|vj
cid:105
cid:107
cid:80
cid:80
cid:112
max
rmax
maxj
max
proof
first
let
deﬁne
oracle
cid:105
cid:105
cid:105
cid:105
cid:105
query
implemented
via
cid:40
cid:105
cid:105
cid:105
|vp
cid:105
|v0
cid:105
cid:105
cid:105
cid:105
|vji
cid:105
|vpi
cid:105
otherwise
b34
cid:105
cid:105
cid:105
cid:105
cid:105
see
following
transformation
eﬃcient
performed
using
one
query
applying
ﬁrst
register
applying
lemma
second
third
registers
|0⊗n
cid:105
|0⊗m
cid:105
|0⊗m
cid:105
|0⊗n
cid:48
cid:105
cid:105
cid:55
vj0
cid:105
cid:105
cid:105
|0⊗n
cid:48
cid:105
cid:105
b35
cid:88
cid:88
cid:88
j=1
p=1
i=1
take
furthermore
use
convention
|v0
cid:105
cid:105
−vp
also
take
cid:100
log2
cid:101
cid:48
number
bits
needed
store
components
following
state
implemented
eﬃciently
within
error
/2
using
oracle
calls
applying
lemma
modiﬁcation
used
place
applying
cid:88
cid:88
cid:88
j=0
i=1
p=1
†qjvj0
cid:105
cid:105
cid:105

cid:118
cid:117
cid:117
cid:117
cid:116
rmax
2
e−iφ
cid:105
rmax
eiφ
cid:105
|θ
cid:105
b36
cid:105
computational
basis
state
stores
ancilla
qubits
prepared
qubit
register
containing
cid:105
need
cleaned
since
qubits
aﬀect
trace
use
deﬁnition
lemma
probability
measuring
ﬁrst
register
state
|0⊗n
cid:105
b37
b38
b39
second–last
register
state
cid:105
drop
state
cid:105
aﬀect
trace
thus
mean
square
distance
centroid
max

max
dr2
max
cid:48
cid:48
cid:48
cid:48
j≥1
|vj
cid:105
cid:48
cid:69
cid:68
cid:88
|vp
cid:105
cid:69
vj0v
†0jv
cid:48
†∗0j
cid:48
cid:68
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
2
i|v
cid:88
cid:88
cid:88
vj0v
†0jv
cid:48
†∗0j
cid:48
vjp
iv∗j
cid:48
cid:48
cid:105
cid:104
cid:48
cid:105
cid:104
cid:48
cid:88
cid:88
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
−|vp
cid:105
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
−|vp
cid:105
cid:88
cid:88
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
maxp
maxp
4dr2
4dr2
|vj
cid:105
|vj
cid:105
j≥1
j≥1
three
queries
needed
draw
sample
distribution
distance
computed
similarly
except
queried
directly
instead
–register
eliminated
since
need
average
diﬀerent
distances
saves
one
additional
query
hence
straightforward
verify
relationship
distance
squared
probability
success
two
queries
needed
draw
sample
distribution
cid:112
cid:112
similar
proof
lemma
use
amplitude
estimation
estimate
cases
following
arguments
used
b28
b32
coherent
used
prepare
state
probability
form
|a||ψgood
cid:105
cid:48
cid:105
|a||ψbad
cid:48
cid:105
|a|
8/π2
using
number
queries
bounded
similarly
cost
preparing
state
form
mean
b40
|a||φ
cid:105
|a|
8/π2
cid:105
encodes
therefore
combining
b40
b41
see
state
form
constructed
|a|
8/π2
2/3
using
number
oracle
calls
bounded
|a||ψ
cid:105
cid:105
cid:48
cid:105
max
cid:24
dr2
cid:112
cid:24
dr2
|a||ψ
cid:105
cid:105
cid:25
cid:112
cid:25
max
cid:24
dr2
cid:25
max
cid:112
cid:112
b41
|a||φbad
cid:105
b42
cid:117
cid:116
theorem
follows
trivially
results
proof
result
given
proof
theorem
proof
follows
trivial
consequence
taking
cid:48
corollary
applying
lemma
observing
dividing
calculated
distance
eﬃcient
irrespective
whether
cid:48
cid:48
note
upper
bound
tight
cases
cid:48
need
computed
cases
nonetheless
removing
cost
reduces
expected
query
complexity
constant
factor
change
theorem
statement
simplicity
also
using
non–constant
value
cid:48
change
problem
since
state
preparation
method
lemma
takes
input
state
set
cid:48
cid:117
cid:116
coherently
results
hand
prove
corollary
gives
query
complexity
performing
iteration
k–means
proof
corollary
ﬁrst
fact
notice
unlike
theorem
algorithm
k–means
require
normalize
distance
reduces
cost
algorithm
factor
4/10
using
b40
b41
also
algorithm
succeeds
probability
π/8
rather
π/8
costs
reduced
factor
π/8
1/2
π/8
1/2
thus
see
observations
total
cost
algorithm
cid:24
dr2
max
cid:25

log
cid:16
81k
log
cid:17
8/π2
1/2

360m√k
cid:117
cid:116
appendix
justiﬁcation
normalizing
distance
important
question
remains
normalized
distance
cluster
centroid
useful
statistic
machine
learning
task
course
mentioned
earlier
statistic
always
optimal
cases
training
data
points
live
complicated
manifold
many
points
close
cluster
centroid
yet
cluster
even
circumstances
normalized
distance
leads
upper
bound
probability
cluster
concreteness
let
assume
mean
mean
deﬁne
intra–cluster
variances
states
regardless
underlying
distributions
clusters
point
clusters
chebyshev
inequality
mean
ξa|x
mean
ξb|x
tells
normalized
distance
large
probability
point
corresponding
cluster
small
unfortunately
necessarily
provide
enough
information
merit
use
decision
problem
guarantee
inequalities
tight
chebyshev
inequality
tight
basing
decision
normalized
distance
equivalent
likelihood–ratio
test
widely
used
hypothesis
testing
theorem
13.
assume
exist
positive
numbers
min
mean
χ|x
mean
χ|x
either
using
normalized
distance
cluster
centroid
decide
whether
equivalent
using
likelihood
ratio
test
proof
likelihood
ratio
test
concludes
assigned
mean
χ|x
mean
χ|x
assumptions
show
implied
cid:32
cid:18
cid:19
cid:18
cid:19
cid:33
normalized
distance
used
classiﬁcation
decision
assigned
therefore
two
tests
make
assignment
likelihood
ratio
test
similarly
assigns
cid:18
cid:16
cid:17
cid:16
cid:17
cid:19
mean
χ|x
mean
χ|x
cid:18
cid:16
cid:17
cid:16
cid:17
cid:19
similar
implied
equivalent
distance–based
assignment
cid:117
cid:116
theorem
shows
validity
distance–based
assignment
depends
strongly
tightness
chebyshev
bound
however
necessarily
clear
priori
whether
lower
bounds
mean
χ|x
mean
χ|x
exist
values
non–zero
bounds
clearly
exist
example
drawn
gaussian
distributions
follows
mean
χ|x
upper–
lower–bounded
function
distance
appropriate
values
extracted
covariance
matrix
since
distributions
case
normalized
distance
well
motivated
drawn
two
gaussian
distributions
whose
centroids
suﬃciently
distant
appendix
sensitivity
decision
problem
although
quantum
algorithms
computing
inner
product
euclidean
distance
provide
better
scaling
dimension
vectors
number
vectors
training
set
classical
analogs
quantum
algorithms
introduce
1/
scaling
noise
tolerance
error
tolerance
distance
computation
typical
distances
assignment
set
shrink
1/n
positive
integer
possible
savings
provided
using
quantum
computer
could
negated
−1
cases
show
typical
cases
vectors
uniformly
distributed
unit
sphere
1/√n
suﬃce
high
probability
since
nearest–neighbor
algorithms
scale
euclidean
case
corresponds
cid:48
implies
algorithms
cost
scales
scales
quadratically
better
classical
analog
monte–carlo
sampling
result
similar
concentration
measure
arguments
hypersphere
show
almost
unit
vectors
concentrated
band
width
1/√n
equator
hypersphere
concentrated
band
vectors
origin
curse
dimensionality
implies
almost
random
unit
vectors
within
euclidean
distance
1/√n
ﬁxed
unit
vector
means
underlying
distribution
distances
nearest–neighbor
learning
tends
ﬂatten
implying
accurate
estimates
distances
may
needed
high–dimensional
spaces
haar–random
vectors
shown
probability
distribution
magnitude
component
vector
within
negligible
error
limit
large
independently
distributed
probability
density
|vjk|
n−2
substitution
gives
√n|vjk|
cid:19
n−2
cid:18
2u√n
e−u2
2rn
e−n
chebyshev
inequality
used
show
high
probability
1/√n
component
shows
distribution
varies
smoothly
hence
highly
probable
diﬀerences
two
components
random
vectors
1/√n
since
haar
measure
invariant
unitary
transformation
take
basis
vector
without
loss
generality
deﬁne
two
closest
vectors
see
high
probability
limit
w|2
cid:88
j=2
|w1|2
1/n
1/√n
last
line
follows
observation
high
probability
|w1|
1/√n
repeating
argument
see
z|2
1/√n
hence
fact
distribution
distances
smooth
components
independent
see
suggests
1/√n
case
members
training
set
haar–random
vectors
demonstrate
point
numerically
z|2
w|2
1/√n
fig
14.
mean
diﬀerence
euclidean
distance
two
closest
vectors
vectors
randomly
chosen
according
haar
measure
plot
computed
100
using
100
trials
per
value
blue
crosses
show
mean
values
distances
dashed
lines
give
conﬁdence
interval
distance
green
line
gives
best
powerlaw
data
generate
data
figure
generating
large
set
random
vectors
chosen
uniformly
respect
haar
measure
take
cid:105
cid:105
examples
without
loss
generality
measure
rotationally
invariant
compute
wj|2
sort
distances
randomly
drawn
vectors
finally
compute
distance
gap
diﬀerence
two
smallest
distances
repeat
100
times
order
get
reliable
statistics
diﬀerences
figure
shows
diﬀerence
two
distances
tends
order
1/√n
anticipated
concentration
measure
arguments
easy
see
taylor
theorem
diﬀerences
square
distances
also
1/√n
hence
taking
1/√n
suﬃce
high–probability
since
error
scale
smaller
suﬃcient
aﬀect
decision
identity
nearest
vector
contrast
scaling
much
less
interesting
volume
expands
exponentially
hence
takes
large
number
points
densely
cover
hypersphere
reason
focus
scaling
rather
however
problems
small
large
discernable
boundaries
issue
could
potentially
problematic
estimate
regime
quantum
algorithms
deﬁnite
advantage
brute–
force
classical
computation
assume
1/√n
0.5
dr2
max
cid:48
numerically
compute
points
upper
bounds
query
complexity
theorem
theorem
equal
cost
10010110210310−310−210−1100distance
gap
two
closest
vectorsn
brute–force
classical
computation
use
points
estimate
regime
quantum
algorithms
cost–advantageous
classical
brute–force
classiﬁcation
seen
figure
quantum
algorithms
exhibit
superior
time
complexities
large
range
values
trade–oﬀ
point
euclidean
method
occurs
1016n−1.07
1014n−1.08
inner–product
method
important
note
upper
bounds
query
complexity
expected
tight
means
say
conﬁdence
quantum
algorithms
beneﬁcial
upper
bounds
less
tighter
bounds
query
complexity
algorithm
may
needed
order
give
better
estimate
performance
algorithm
typical
applications
fig
15.
estimated
regions
quantum
algorithms
cost-advantageous
brute–force
classical
calculation
shaded
regions
represent
parameter
space
upper
bounds
theorem
theorem
greater
brute–force
classical
cost
appendix
k–nearest–neighbor
classiﬁcation
practice
nearest–neighbor
classiﬁcation
runs
diﬃculties
faced
errors
labels
training
set
faced
data
sets
signiﬁcant
number
outliers
one
way
combat
issues
use
k–nearest–neighbor
classiﬁcation
class
label
chosen
mode
labels
closest
training
vectors
test
vector
makes
assignment
much
robust
errors
averaging
labels
closest
vectors
test
vector
cost
generalizing
nearest–neighbor
classiﬁcation
algorithm
k–nearest–neighbor
algorithm
given
lemma
14.
assume
query
complexity
classifying
test
vector
using
either
nearest–neighbor
euclidean
method
inner–product
method
query
complexity
performing
either
algorithm
k–nearest–
neighbor
algorithm
proof
proof
proceeds
inductively
claim
clear
case
demonstrates
base
case
assume
know
cid:48
nearest–neighbors
test
vector
wish
ﬁnd
cid:48
vector
done
applying
steps
used
either
classiﬁer
removing
ﬁrst
cid:48
vectors
set
complexity
performing
d¨urr
høyer
optimization
distances
computed
reduced
set
vectors
given
lemma
cid:48
implies
cost
step
algorithm
needs
change
search
outermost
loop
either
algorithms
hence
total
cost
cid:48
queries
thus
remove
vectors
consideration
follows
induction
cost
nearest–neighbor
classiﬁcation
queries
vectors
need
directly
removed
consideration
rather
grover
oracle
used
d¨urr
høyer
algorithm
modiﬁed
ignore
cid:48
closest
vectors
consideration
let
marked
denote
marked
1021041061081041061081010101210141016mn8both8q.8alg8superiorinner8product8q.8alg8superiorunknown
set
grover
oracle
straightforward
construct
circuit
marks
set
marked
cid:48
assume
without
loss
generality
cid:48
cid:48
closest
vectors
since
vectors
known
classically
need
queries
remove
vectors
consideration
modiﬁed
grover
oracle
thus
assume
ﬁrst
cid:48
vectors
eﬀectively
removed
training
set
without
changing
cid:117
cid:116
number
queries
completes
proof
algorithms
computing
nearest–neighbors
exist
neighbors
individually
estimated
using
methods
provides
log
loglog
algorithm
approximating
smallest
element
although
directly
eﬃcient
since
values
must
found
better
suited
parallel
quantum
computation
nearest–neighbors
found
independently
another
heuristic
approach
would
use
d¨urr
høyer
algorithm
focus
subset
points
sample
points
ﬁnd
k–nearest
vectors
appendix
pseudocode
clarity
provide
pseudocode
explaining
perform
several
important
steps
quantum
algorithm
including
coherent
amplitude
estimation
calculation
distances
using
inner
product
euclidean
methods
particular
explicitly
show
compute
distances
explicitly
using
inner
product
method
non–positive
vectors
1algorithm1
quantumalgorithmforminimumdistance.input
numberofvectorsm.input
dimensionofvectorsn.input
numberofvotesusedinmajorityvotingschemek.input
errortolerance.input
quantumalgorithm
|ji|0i7→|ji|vjiforanyj
quantumalgorithmasuchthatforanyj
0|v0−vj|canbeinferredbysamplingfromtheoutputofa.output
estimateofminj|v0−vj|thatisaccuratewithinerror
where|·|isthedistancecomputedbya.functionqmindist
applythed¨urr
høyerminimizationalgorithmtoﬁndtheminimumvalueofmedian
...
outputbyqdist
.returnestimateofminj|v0−vj|yieldedbyminimizationalgorithm.endfunctionalgorithm2
quantumalgorithmfordistanceestimate.input
dimensionofvectorsn.input
numberofvotesusedinmajorityvotingschemek.input
errortolerance.input
indexofvectortobecomparedwithv0inputasquantumstate|ji.input
quantumalgorithm
|ji|0i7→|ji|vjiforanyj
quantumalgorithmasuchthatforanyj
0|v0−vj|canbeinferredbysamplingfromtheoutputofa
.output
quantumstatethatcontainsan–closeapproximationto|v0−vj|
where|·|isthedistancecomputedbya.functionqdist
fori∈1→kdoapplyamplitudeestimationona
|ji
usingaregisterofsizer∝1/andwithoutmeasuringtheresult.storeresultasquantumstate|ψki=a|yki+p1−|a|2|y⊥ki..eachykstores|v0−vjkwith|a|2≈81
.|y⊥kireferstoanincorrectanswerthatisyieldedbytheamplitudeestimationalgorithm.endfor|median
...
i←median
|ψ1i
...
|ψki
.undoallstepsprevioustothemediancalculationstepabove.return|median
...
i.endfunctionalgorithm3
quantumalgorithmforinnerproductmethodfornon–positivevectors.input
dimensionofvectorsn.input
vectorlabelnumberjinputasquantumstate|ji.input
upperboundonmagnitudesofthecomponentsofthevectorsrmax.input
sparsityofvectorsd.output
quantumstatesuchthattheconditionalprobabilityofmeasuringtheprobabilityofmeasuringthecontrolqubittobe0isproportionalto:1+|
1+hvj|v0i
/2|22.algorithmassumesthatvjandv0containnegativenumbers.functionqinnerproduct
|ji
rmax
useoandftopreparethestates|ψi←1√2
|0i|0⊗log2ni|1i|1i+d−1/2pi
vji6=0|ii
r1−r2jir2maxe−iφji|0i+vjirmax|1i
|1i
|φi←1√2
|0i|0⊗log2ni|1i|1i+d−1/2pi
vji6=0|ii|1i
r1−r2jir2maxe−iφji|0i+vjirmax|1i
|resulti←swaptest
|0i|ψi|φi
..applyswaptest
forgoingmeasurement
on|φiand|ψiwhereleftqubitisthecontrol.return|resultiendfunction1algorithm1
quantumalgorithmforminimumdistance.input
numberofvectorsm.input
dimensionofvectorsn.input
numberofvotesusedinmajorityvotingschemek.input
errortolerance.input
quantumalgorithm
|ji|0i7→|ji|vjiforanyj
quantumalgorithmasuchthatforanyj
0|v0−vj|canbeinferredbysamplingfromtheoutputofa.output
estimateofminj|v0−vj|thatisaccuratewithinerror
where|·|isthedistancecomputedbya.functionqmindist
applythed¨urr
høyerminimizationalgorithmtoﬁndtheminimumvalueofmedian
...
outputbyqdist
.returnestimateofminj|v0−vj|yieldedbyminimizationalgorithm.endfunctionalgorithm2
quantumalgorithmfordistanceestimate.input
dimensionofvectorsn.input
numberofvotesusedinmajorityvotingschemek.input
errortolerance.input
indexofvectortobecomparedwithv0inputasquantumstate|ji.input
quantumalgorithm
|ji|0i7→|ji|vjiforanyj
quantumalgorithmasuchthatforanyj
0|v0−vj|canbeinferredbysamplingfromtheoutputofa
.output
quantumstatethatcontainsan–closeapproximationto|v0−vj|
where|·|isthedistancecomputedbya.functionqdist
fori∈1→kdoapplyamplitudeestimationona
|ji
usingaregisterofsizer∝1/andwithoutmeasuringtheresult.storeresultasquantumstate|ψki=a|yki+p1−|a|2|y⊥ki..eachykstores|v0−vjkwith|a|2≈81
.|y⊥kireferstoanincorrectanswerthatisyieldedbytheamplitudeestimationalgorithm.endfor|median
...
i←median
|ψ1i
...
|ψki
.undoallstepsprevioustothemediancalculationstepabove.return|median
...
i.endfunctionalgorithm3
quantumalgorithmforinnerproductmethodfornon–positivevectors.input
dimensionofvectorsn.input
vectorlabelnumberjinputasquantumstate|ji.input
upperboundonmagnitudesofthecomponentsofthevectorsrmax.input
sparsityofvectorsd.output
quantumstatesuchthattheconditionalprobabilityofmeasuringtheprobabilityofmeasuringthecontrolqubittobe0isproportionalto:1+|
1+hvj|v0i
/2|22.algorithmassumesthatvjandv0containnegativenumbers.functionqinnerproduct
|ji
rmax
useoandftopreparethestates|ψi←1√2
|0i|0⊗log2ni|1i|1i+d−1/2pi
vji6=0|ii
r1−r2jir2maxe−iφji|0i+vjirmax|1i
|1i
|φi←1√2
|0i|0⊗log2ni|1i|1i+d−1/2pi
vji6=0|ii|1i
r1−r2jir2maxe−iφji|0i+vjirmax|1i
|resulti←swaptest
|0i|ψi|φi
..applyswaptest
forgoingmeasurement
on|φiand|ψiwhereleftqubitisthecontrol.return|resultiendfunction
1algorithm1
quantumalgorithmforminimumdistance.input
numberofvectorsm.input
dimensionofvectorsn.input
numberofvotesusedinmajorityvotingschemek.input
errortolerance.input
quantumalgorithm
|ji|0i7→|ji|vjiforanyj
quantumalgorithmasuchthatforanyj
0|v0−vj|canbeinferredbysamplingfromtheoutputofa.output
estimateofminj|v0−vj|thatisaccuratewithinerror
where|·|isthedistancecomputedbya.functionqmindist
applythed¨urr
høyerminimizationalgorithmtoﬁndtheminimumvalueofmedian
...
outputbyqdist
.returnestimateofminj|v0−vj|yieldedbyminimizationalgorithm.endfunctionalgorithm2
quantumalgorithmfordistanceestimate.input
dimensionofvectorsn.input
numberofvotesusedinmajorityvotingschemek.input
errortolerance.input
indexofvectortobecomparedwithv0inputasquantumstate|ji.input
quantumalgorithm
|ji|0i7→|ji|vjiforanyj
quantumalgorithmasuchthatforanyj
0|v0−vj|canbeinferredbysamplingfromtheoutputofa
.output
quantumstatethatcontainsan–closeapproximationto|v0−vj|
where|·|isthedistancecomputedbya.functionqdist
fori∈1→kdoapplyamplitudeestimationona
|ji
usingaregisterofsizer∝1/andwithoutmeasuringtheresult.storeresultasquantumstate|ψki=a|yki+p1−|a|2|y⊥ki..eachykstores|v0−vjkwith|a|2≈81
.|y⊥kireferstoanincorrectanswerthatisyieldedbytheamplitudeestimationalgorithm.endfor|median
...
i←median
|ψ1i
...
|ψki
.undoallstepsprevioustothemediancalculationstepabove.return|median
...
i.endfunctionalgorithm3
quantumalgorithmforinnerproductmethodfornon–positivevectors.input
dimensionofvectorsn.input
vectorlabelnumberjinputasquantumstate|ji.input
upperboundonmagnitudesofthecomponentsofthevectorsrmax.input
sparsityofvectorsd.output
quantumstatesuchthattheconditionalprobabilityofmeasuringtheprobabilityofmeasuringthecontrolqubittobe0isproportionalto:1+|
1+hvj|v0i
/2|22.algorithmassumesthatvjandv0containnegativenumbers.functionqinnerproduct
|ji
rmax
useoandftopreparethestates|ψi←1√2
|0i|0⊗log2ni|1i|1i+d−1/2pi
vji6=0|ii
r1−r2jir2maxe−iφji|0i+vjirmax|1i
|1i
|φi←1√2
|0i|0⊗log2ni|1i|1i+d−1/2pi
vji6=0|ii|1i
r1−r2jir2maxe−iφji|0i+vjirmax|1i
|resulti←swaptest
|0i|ψi|φi
..applyswaptest
forgoingmeasurement
on|φiand|ψiwhereleftqubitisthecontrol.return|resultiendfunction1algorithm4
quantumalgorithmfordistancetocentroidofaclusterofvectors.input
dimensionofvectorsn.input
numberofvectorsm.input
numberofclustersm0.input
clusternumberpinputasquantumstate|pi.input
quantumalgorithm
|pi|ji|0i7→|pi|ji|v
whereeach|v
jiisoneofthevectorsinclusterpand|v
0i=|v0iforallp.output
quantumstatesuchthattheprobabilityofmeasuringtheﬁrstregister
theoneofdimensionm
tobe0ispropor-tionaltothesquareoftheeuclideandistance
|v0−m0mpm/m0j=1v
j|.functionqcentdist
|pi
|ψi←v|0i
=1√2|pi|0i|0i+qm02mpmj=1|pi|ji|0i.usestatepreparationprocedure
ptotransformstateas|ψi←p|ψi=−1√2|pi|0i|v0i+qm02mpmj=1|pi|ji|vji..thiscreatestherightsuperpositionofstatestogivetheaverageofthevectorsandsubtractoﬀthevector|v0i.multiplyinganyofthetermsaboveby−1
ormoregenerallyacomplexphase
doesnotchangethealgorithm
sfunction|ψi←v†|ψi..v†isinverseofvoperationreturn|ψiendfunctionalgorithm5
quantumalgorithmforcomputingmeansquareofdistancesbetweenpointswithinaclusterandtheclustercentroid.input
dimensionofvectorsn.input
numberofvectorsm.input
numberofclustersm0.input
clusternumberpinputasquantumstate|pi.input
quantumalgorithmp
|qi|pi|ji|0i7→|qi|pi|ji|v
jiforj
whereeach|v
jiisoneofthevectorsinclusterpand|v
0i=|v0iforallpandp
|qi|pi|ji|0i7→|qi|pi|ji|v
qiforj=0.output
quantumstateincmnsuchthattheprobabilityofmeasuringtheﬁrstregister
theoneofdimensionm
tobe0isproportionalto
m0mpm/m0q=1||v
qi−m0mpm/m0j=1|v
ji|2.functionqmnsqdist
|pi
|ψi←v|0i
=1√2pm/m0q=1
cid:18
|qi|pi|0i|0i+1√2m/m0pm/m0j=1|qi|pi|ji|0i
cid:19
.usestatepreparationprocedure
ptotransformstateas|ψi←p|ψi..thiscreatestherightsuperpositionofstatestogivetheaverageofthevectorsandsubtractoﬀthevector|v0i|ψi←v†|ψi..v†isinverseofvoperationreturn|ψiendfunction1algorithm4
quantumalgorithmfordistancetocentroidofaclusterofvectors.input
dimensionofvectorsn.input
numberofvectorsm.input
numberofclustersm0.input
clusternumberpinputasquantumstate|pi.input
quantumalgorithm
|pi|ji|0i7→|pi|ji|v
whereeach|v
jiisoneofthevectorsinclusterpand|v
0i=|v0iforallp.output
quantumstatesuchthattheprobabilityofmeasuringtheﬁrstregister
theoneofdimensionm
tobe0ispropor-tionaltothesquareoftheeuclideandistance
|v0−m0mpm/m0j=1v
j|.functionqcentdist
|pi
|ψi←v|0i
=1√2|pi|0i|0i+qm02mpmj=1|pi|ji|0i.usestatepreparationprocedure
ptotransformstateas|ψi←p|ψi=−1√2|pi|0i|v0i+qm02mpmj=1|pi|ji|vji..thiscreatestherightsuperpositionofstatestogivetheaverageofthevectorsandsubtractoﬀthevector|v0i.multiplyinganyofthetermsaboveby−1
ormoregenerallyacomplexphase
doesnotchangethealgorithm
sfunction|ψi←v†|ψi..v†isinverseofvoperationreturn|ψiendfunctionalgorithm5
quantumalgorithmforcomputingmeansquareofdistancesbetweenpointswithinaclusterandtheclustercentroid.input
dimensionofvectorsn.input
numberofvectorsm.input
numberofclustersm0.input
clusternumberpinputasquantumstate|pi.input
quantumalgorithmp
|qi|pi|ji|0i7→|qi|pi|ji|v
jiforj
whereeach|v
jiisoneofthevectorsinclusterpand|v
0i=|v0iforallpandp
|qi|pi|ji|0i7→|qi|pi|ji|v
qiforj=0.output
quantumstateincmnsuchthattheprobabilityofmeasuringtheﬁrstregister
theoneofdimensionm
tobe0isproportionalto
m0mpm/m0q=1||v
qi−m0mpm/m0j=1|v
ji|2.functionqmnsqdist
|pi
|ψi←v|0i
=1√2pm/m0q=1
cid:18
|qi|pi|0i|0i+1√2m/m0pm/m0j=1|qi|pi|ji|0i
cid:19
.usestatepreparationprocedure
ptotransformstateas|ψi←p|ψi..thiscreatestherightsuperpositionofstatestogivetheaverageofthevectorsandsubtractoﬀthevector|v0i|ψi←v†|ψi..v†isinverseofvoperationreturn|ψiendfunction
