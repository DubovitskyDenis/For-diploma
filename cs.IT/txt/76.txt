lower
bounds
interactive
function
computation
via
wyner
common
information
shijin
rajakrishnan1
iit
madras
chennai
email
ee12b128
ee.iitm.ac.in
sundara
rajan
iit
madras
chennai
email
ee11b130
ee.iitm.ac.in
vinod
prabhakaran
tifr
mumbai
email
vinodmp
tifr.res.in
abstractâ€”the
question
much
communication
re-
quired
collaborating
parties
compute
function
data
fundamental
importance
ï¬elds
theoretical
computer
science
information
theory
work
focus
coming
lower
bounds
information
cost
protocol
amount
information
protocol
reveals
alice
bob
others
inputs
information
complexity
function
inï¬mum
information
costs
valid
protocols
amortized
case
known
optimal
rate
computation
equal
information
complexity
exactly
computing
information
complexity
straight
forward
however
work
lower
bound
information
complexity
independent
inputs
terms
wyner
common
information
certain
pair
random
variables
show
structural
property
optimal
auxiliary
random
variable
wyner
common
information
exploit
exactly
compute
wyner
common
information
certain
cases
lower
bound
obtained
technique
shown
tight
non-trivial
example
equality
ternary
alphabet
also
give
example
show
lower
bound
may
general
tight
introduction
amount
communication
required
two
parties
compute
function
data
central
question
the-
oretical
computer
science
also
information
theory
since
seminal
work
yao
much
progress
made
understanding
communication
complexity
computer
science
literature
early
progress
based
combinatorial
techniques
recently
advances
area
centered
around
notion
information
complexity
measures
amount
information
learned
parties
inputs
protocol
transcript
rather
number
bits
protocol
transcript
compute
function
somewhat
correctly
speciï¬cally
inputs
parties
come
distribution
information
cost
protocol
computing
whose
transcript
denoted
deï¬ned
m|y
m|x
information
complexity
inï¬mum
information
costs
valid
protocols
i.e.
protocols
allow
parties
compute
within
desired
error
performance
denoted
icxy
computation
function
quantity
close
connection
problem
interactive
source
coding
interactive
function
computation
studied
information
theory
literature
particular
works
1authors
contributed
equally
kaspi
ishwar
show
information
complexity
zero-error
precisely
rate
communica-
tion
required
compute
asymptotically
vanishing
error
parties
allowed
code
long
blocks
independent
identically
distributed
inputs
general
computing
information
complexity
straightforward
known
exactly
interesting
examples
algorithm
albeit
run-time
exponential
alphabet
size
approximating
proposed
goal
better
understanding
information
complexity
monotonicity
property
interactive
protocols
leveraged
obtain
lower
bounds
information
complexity
monotonicity
property
tension
region
views
two
users
tension
region
pair
random
variables
introduced
measure
dependence
captured
using
common
random
variable
question
well
correlation
captured
random
variable
may
formulated
terms
common
information.
two
different
notions
common
information
developed
cigk
gÂ´acs-
kÂ¨orner
ciwyn
wyner
cigk
max
pq|a
qâˆ’aâˆ’b
qâˆ’bâˆ’a
ciwyn
min
pq|a
aâˆ’qâˆ’b
one
deï¬ne
corresponding
notions
tension
gap
mutual
information
accounts
corre-
lation
may
correspond
common
random
variable
common
information
precisely
one
deï¬ne
non-negative
tension
quantities
tgk
cigk
twyn
ciwyn
notions
tension
identiï¬ed
special
cases
uniï¬ed
3-dimensional
notion
tension
region
tension
region
pair
random
variables
deï¬ned
following
upward
closed
region
deï¬nition
pair
random
variables
tension
region
deï¬ned
jointly
distr
s.t
q|a
q|b
b|q
shown
without
loss
generality
may
assume
cardinality
bound
|q|
|a||b|
alphabet
deï¬nition
alphabets
respectively
operational
meaning
also
obtained
tension
region
terms
generalization
common
information
problem
gÂ´acs
kÂ¨orner
tension
region
proved
useful
deriving
converse
results
secure
computation
speciï¬cally
used
strictly
improve
upon
upper
bound
ahlswede
csiszÂ´ar
oblivious
transfer
capacity
channels
suppose
inputs
outputs
parties
protocol
let
denote
transcript
protocol
let
denote
views
parties
end
protocol
key
monotonicity
property
use
proposition
theorem
5.4
consequence
following
result
theorem
icxy
twyn
twyn
z|y
z|x
see
general
result
implies
lower
bound
case
independent
inputs
twyn
term
goes
zero
give
proof
theorem
case
independent
inputs
appendix
bound
always
tight2
present
non-trivial
example
bound
turns
give
tight
result
worth
noting
technique
easily
yield
result
example
ternary
let
independent
uniformly
distributed
goal
compute
indicator
event
theorem
gives
lower
bound
cid:1
log2
shown
tight
cid:0
equality
function
determines
whether
two
parties
inputs
studied
extensively
best
knowledge
lower
bound
information
complexity
available
trivial
icxy
z|y
z|x
best
available
upper
bound
4.5
k-ary
computation
probability
distribution
inputs
paper
obtain
lower
bounds
upper
bounds
information
complexity
function
uniformly
distributed
inputs
evaluate
lower
bound
theorem
need
compute
wyner
common
information
equivalent
quantity
given
note
computing
wyner
common
information
general
straightforward
using
standard
techniques
based
carathÂ´eodory
theorem
upper
bound
|q|
|a|Ã—|b|+2
auxiliary
random
variable
available
show
enough
consider
potentially
smaller
cardinality
depends
number
maximal
cliques
bipartite
characteristic
graph
bipartite
graph
aÃ—b
edge
conditioned
element
characteristic
graph
b|q=q
distinct
clique
theorem
allows
compute
wyner
common
information
exactly
certain
examples
interest
section
iii
particular
resulting
lower
bound
turns
tight
ternary
example
also
give
randomized
protocol
4-ary
problem
performs
better
deterministic
protocols
terms
information
cost
lower
bound
meet
upper
bound
given
protocol
problem
formulation
alice
bob
get
inputs
respectively
joint
distribution
pxy
common
objective
computation
function
connected
channel
makes
errors
transmission
protocol
compute
function
proceeds
sequential
manner
follows
initially
alice
bob
sends
message
link
say
bob
waits
message
reach
sends
message
link
procedure
iterates
long
enough
alice
bob
compute
function
fig
model
two-party
computation
let
...
denote
transcript
link
till
ith
stage
...
denote
ï¬nal
transcript
end
protocol
easy
see
following
two
conditions
satisï¬ed
protocol
beginning
alice
end
iâˆ’1
iâˆ’1
odd
even
entropy
ï¬nal
transcript
lower
bound
average
number
bits
needed
protocol
information
complexity
lower
bound
prove
ï¬rst
prove
following
inequality
let
odd
iâˆ’1
iâˆ’1
|xm
iâˆ’1
iâˆ’1
|xm
due
|xm
iâˆ’1
|xm
iâˆ’1
inequality
obtained
case
even
similar
argument
consequence
2an
example
bound
turns
tight
computing
two
independent
uniform
bits
information
complexity
known
alice
bob
ğ‘€ğ‘–+1
m|x
m|x
m|y
m|y
m|y
m|x
icxy
due
amortized
case
consider
block
independent
identically
distributed
inputs
length
sequence
schemes
one
block
length
following
theorem
proved
gives
minimum
rate
communication
needed
compute
function
vanishing
probability
block
error
rate
scheme
deï¬ned
total
number
bits
exchanged
divided
block
length
rate
said
achievable
sequence
schemes
whose
probability
error
goes
optimal
rate
inï¬mum
achievable
rates
theorem
optimal
amortized
rate
computing
function
inf
m|y
m|x
icxy
inï¬mum
...
satisfying
markov
chain
conditions
z|y
z|xm
lower
bounding
information
complexity
via
wyner
com-
mon
information
wyner
tension
deï¬ned
section
written
twyn
inf
pq|u
uâˆ’qâˆ’v
q|v
q|u
let
independent
theorem
write
icxy
twyn
z|y
z|x
rewriting
form
suitable
computation
problem
compute
supremum
term
auxiliary
random
variable
given
random
variables
independent
given
conditionally
independent
edges
characteristic
graph
necessarily
form
bipartite
clique
shown
fig
ï¬rst
classify
possible
elements
various
classes
based
characteristic
graph
formed
group
elements
underlying
bipartite
clique
class
since
bipartite
graph
ï¬nite
number
vertices
vertex
set
ï¬nitely
many
bipartite
cliques
ï¬nite
number
classes
combine
several
classes
one
looking
maximal
bipartite
cliques
since
non
maximal
clique
special
case
maximal
clique
probability
values
zero
thus
classes
given
distribution
correspond
one
maximal
bipartite
clique
characteristic
graph
fig
gives
example
classes
particular
distribution
narrow
search
space
alphabet
qopt
optimal
auxiliary
r.v
qopt
leads
maximum
value
u|q
theorem
theorem
given
ï¬nd
corresponding
qopt
sufï¬cient
consider
alphabets
two
elements
class
proof
consider
figure
maximal
bipartite
clique
characteristic
graph
left-degree
right-degree
assume
two
elements
namely
cid:48
class
shown
figure
probability
term
refers
pqu
example
figure
probability
pqu
fig
characteristic
graph
class
random
variables
whose
alphabets
two
elements
class
construct
qnew
u|q
u|qnew
|qnew
adding
weights
merging
corresponding
edges
cid:48
elements
remaining
unchanged
shown
figure
prove
ï¬rst
prove
u|q
u|qnew
cid:88
cid:88
u|q
cid:32
pl+1
p2l
...
kâˆ’1
l+1
pkl
need
prove
u|q
cid:48
u|q
cid:48
pqnew
qnew
u|qnew
qnew
qnew
easy
see
cid:80
since
terms
summation
i=1
cid:33
icxy
x|y
u|q
sup
pq|u
uâˆ’qâˆ’v
u|q
edges
edges
ğ‘ğ‘˜ğ‘™
ğ‘ğ‘˜ğ‘™âˆ’1
fig
two
elements
class
merged
form
new
qnew
cid:48
i=1
cid:48
cid:48
cid:48
cid:48
cid:80
therefore
pqnew
qnew
cid:80
cid:19
cid:18
cid:48
cid:18
hence
equivalent
...
p0h
cid:18
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
cid:19
cid:48
cid:48
cid:48
cid:48
...
...
i=1
cid:19
cid:48
cid:48
cid:48
iâˆ’1
l+1
iâˆ’1
l+2
...
pil
likewise
cid:48
iâˆ’1
l+1+p
cid:48
cid:48
cid:48
log
log-sum
inequality
ai+a
cid:48
log
cid:48
cid:48
cid:20
cid:88
cid:48
iâˆ’1
l+2+
...
cid:48
cid:21
log
ai+a
cid:48
p0+p
cid:48
cid:20
cid:21
cid:48
log
cid:48
cid:48
i=1
log
cid:48
log
cid:48
cid:48
cid:88
i=1
thus
u|q
u|qnew
follows
|qnew
proved
equivalent
argument
iii
lower
bounds
information
complexity
via
wyner
common
information
restrict
attention
inputs
independent
uniformly
distributed
ternary
computation
alice
bob
inputs
independent
come
uniformly
distribution
ternary
alphabets
say
function
want
compute
1x=y
function
ternary
alphabet
theorem
restrict
cardinality
different
classes
shown
figure
uniform
input
distribution
pxz
leads
constraints
cid:80
pxz
p11
p12
p10
u|q
cid:88
cid:88
i=1
cid:20
p2iâˆ’1
cid:21
i=1
used
fact
set
equations
get
icxy
log
consider
following
protocol
upper
bound
amortized
case
repeated
block
inputs
2.5033.
protocol
ternary
computation
alice
sends
symbol
ternary
alphabet
indicating
input
bob
bob
locally
computes
1x=y
sends
resultant
bit
alice
information
cost
protocol
2.5033.
thus
see
log
lower
bound
developed
tight
example
protocol
could
represented
qopt
figure
follows
two
bit
computation
9âˆ€i
0âˆ€i
9âˆ€i
alice
bob
communicate
order
compute
function
two
bits
bits
i.i.d
theorem
sufï¬cient
look
s.t
|q|
18.
classes
case
consists
types
maximal
bipartite
cliques
one
edges
edges
similar
analysis
along
lines
elucidated
case
ternary
would
result
u|q
1.5
upper
bound
attained
distribution
edge
classes
uniform
i.e
probability
metric
associated
edge
4-edge
class
equal
u|q
1.5
implies
sup
pq|u
uâˆ’qâˆ’v
hence
icxy
2.5.
derive
upper
bound
icxy
giving
randomized
protocol
ğ‘„=ğ‘0
ğ‘„=ğ‘â€²0
ğ‘„ğ‘›ğ‘’ğ‘¤=ğ‘ğ‘›ğ‘’ğ‘¤
fig
characteristic
graph
classes
ternary
computation
deï¬nitions
let
alice
uniform
uniform
deï¬ne
sets
input
input
bob
protocol
two
bit
computation
randomized
alice
uniformly
picks
sends
bob
bob
sends
else
sends
equal
probability
bob
message
protocol
terminates
protocol
proceeds
step
alice
reveals
input
bob
computes
sends
result
alice
occurs
probability
parties
learn
bits
cid:54
happens
probability
bob
sends
thus
proceed
step
bob
sends
probability
given
bob
sends
bob
input
probability
hence
protocol
goes
step
alice
uncertainty
bob
input
1.5
end
protocol
stops
step
alice
bob
would
learnt
bit
therefore
information
cost
2.75
wyner
tension
arbitrary
sized
input
alpha-
bet
uniformly
independently
distributed
alphabet
cardinality
want
compute
function
takes
value
inputs
equal
maximal
bipartite
cliques
new
setting
characteristic
graph
functions
kci
maximal
bipartite
cliques
nodes
side
nodes
side
total
number
classes
would
cid:80
kâˆ’1
i=1
kci
ï¬nal
classes
case
containing
one
edge
cliques
refer
maximal
bipartite
clique
nodes
set
belonging
class
given
edge
connecting
enumerate
number
classes
contains
edge
edge
cid:54
occurs
classes
classes
kâˆ’2c1
times
general
occurs
kâˆ’2ciâˆ’1
times
classes
earlier
cases
edges
probability
associated
leads
set
constraints
cid:88
addition
constraints
u|q
cid:88
cid:88
u|q
log
log
cid:88
Â·Â·Â·
log
cid:88
lkâˆ’1
case
even
using
fact
non-
negative
integers
constant
get
log
maximum
value
log
log
using
get
log
u|q
cid:88
cid:88
lkâˆ’1
log
log
ğ’‘ğ‘¼ğ‘½ğ‘¸
ğ’‘ğ‘¼ğ‘½ğ‘¸
ğ’‘ğ‘¼ğ‘½ğ‘¸
ğ’‘ğ‘¼ğ‘½ğ‘¸
ğ’‘ğ‘¼ğ‘½ğ‘¸
ğ’‘ğ‘¼ğ‘½ğ‘¸
ğ’‘ğŸğŸ
ğ’‘ğŸğŸ
ğ’‘ğŸğŸ
ğ’‘ğ‘¼ğ‘½ğ‘¸
ğ’‘ğ‘¼ğ‘½ğ‘¸
ğ’‘ğ‘¼ğ‘½ğ‘¸
characteristic
graph
ğ’‘ğŸğŸ‘
ğ’‘ğŸğŸ’
ğ’‘ğŸğŸ“
cid:88
lkâˆ’1
log
cid:18
log
kâˆ’2c
kâˆ’2
cid:19
consider
distribution
cid:1
cid:1
cid:0
cid:0
cid:16
edges
classes
edges
set
course
edges
satisfy
constraints
easy
need
verify
distribution
ensures
hence
valid
choice
distribution
value
u|q
cid:19
cid:17
log
u|q
cid:0
icxy
log
cid:0
cid:21
cid:18
cid:20
case
odd
like
previous
case
one
see
u|q
sup
pq|u
uâˆ’qâˆ’v
cid:1
log
get
kâˆ’2c
kâˆ’2
log
cid:18
log
log
consider
distribution
edges
classes
k+1
cid:20
cid:18
u|q
log
u|q
sup
pq|u
uâˆ’qâˆ’v
get
icxy
log
cid:2
log
cid:0
kâˆ’1
k+1
conclusion
log
cid:1
log
cid:19
cid:19
cid:18
cid:19
cid:19
cid:21
cid:18
cid:16
k2âˆ’1
cid:17
cid:105
cid:0
cid:1
cid:1
cid:3
cid:0
kâˆ’2c
kâˆ’1
cid:1
log
cid:104
paper
demonstrated
method
obtaining
lower
bounds
information
complexity
functions
independent
input
distributions
via
computing
wyner
common
information
showed
tightness
lower
bound
ternary
function
2-bit
function
lower
bound
works
2.5
obtained
upper
bound
2.75
giving
randomized
protocol
k-ary
function
lower
bound
converges
repeated
use
2-bit
computation
protocol
gives
upper
bound
3.667
appendix
proof
theorem
consider
case
independent
hence
using
valid
protocol
transcript
z|m
z|m
z|xy
z|m
z|m
z|xy
hence
z|m
markov
chain
m|y
m|x
terms
four
z|y
z|x
z|y
z|x
m|y
m|xz
z|y
z|x
twyn
follows
fact
z|m
z|m
true
m|y
m|y
z|m
m|y
result
relaxation
implies
infor-
mation
complexity
setting
icxy
z|y
z|x
twyn
thus
proving
theorem
independent
inputs
acknowledgments
vinod
prabhakaran
would
like
acknowledge
useful
dis-
cussions
prakash
narayan
shun
watanabe
problem
section
iii-b
due
shun
watanabe
presented
authors
would
like
thank
visiting
students
research
programme
vsrp
tata
institute
fundamental
research
tifr
mumbai
facilitating
shijin
rajakrish-
nan
sundara
rajan
summer
internship
tifr
vinod
prabhakaran
research
funded
part
ramanujan
fellowship
department
science
technology
government
india
references
c.-c.
yao
complexity
questions
related
distributive
com-
puting
preliminary
report
stoc
1979
209â€“213
kushilevitz
nisan
communication
complexity
cambridge
kaspi
two-way
source
coding
ï¬delity
criterion
infor-
mation
theory
ieee
transactions
vol
735â€“740
1985
ishwar
inï¬nite-message
limit
two-terminal
interactive
source
coding
information
theory
ieee
transactions
vol
4071â€“4094
2013
braverman
schneider
information
complexity
com-
putable
arxiv:1502.02971
2015
prabhakaran
prabhakaran
tension
bounds
information
complexity
arxiv:1408.6285
2014
prabhakaran
prabhakaran
assisted
common
in-
formation
application
secure
two-party
sampling
ieee
transactions
information
theory
vol
3413â€“3434
2014
gÂ´acs
kÂ¨orner
common
information
far
less
mutual
information
problems
control
information
theory
vol
149â€“162
1973
wyner
common
information
two
dependent
random
variables
ieee
transactions
information
theory
vol
163â€“179
1975
ahlswede
csiszÂ´ar
oblivious
transfer
capacity
infor-
mation
theory
combinatorics
search
theory
springer
2013
145â€“166
rao
prabhakaran
new
upperbound
oblivious
transfer
capacity
discrete
memoryless
channels
information
theory
workshop
itw
2014
ieee
ieee
2014
35â€“39
witsenhausen
values
bounds
common
information
two
discrete
random
variables
siam
journal
applied
mathematics
vol
313â€“333
1976
gamal
y.-h.
kim
network
information
theory
cambridge
university
press
2011
1997
braverman
interactive
information
complexity
proceedings
forty-fourth
annual
acm
symposium
theory
computing
505-524
2012
narayan
interactive
multi-terminal
communication
information
theory
workshop
itw
jerusalem
2015
