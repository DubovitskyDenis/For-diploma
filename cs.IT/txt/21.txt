learning
data
triage
linear
decoding
works
compressive
mri
yen-huan
volkan
cevher
laboratory
information
inference
systems
´ecole
polytechnique
f´ed´erale
lausanne
abstract
standard
approach
compressive
sampling
considers
re-
covering
unknown
deterministic
signal
certain
known
structure
designing
sub-sampling
pattern
recov-
ery
algorithm
based
known
structure
approach
requires
looking
good
representation
reveals
sig-
nal
structure
solving
non-smooth
convex
minimization
problem
e.g.
basis
pursuit
paper
another
approach
considered
learn
good
sub-sampling
pattern
based
available
training
signals
without
knowing
signal
struc-
ture
advance
reconstruct
accordingly
sub-sampled
signal
computationally
much
cheaper
linear
reconstruc-
tion
provide
theoretical
guarantee
recovery
error
show
via
experiments
real-world
mri
data
effec-
tiveness
proposed
compressive
mri
scheme
index
terms—
compressive
sampling
magnetic
reso-
nance
imaging
mri
learning
least
squares
estimation
sub-
modular
minimization
introduction
standard
theory
compressive
sampling
consid-
ers
recovering
unknown
deterministic
signal
cer-
tain
known
structure
designing
sampling
recovery
schemes
based
known
structure
example
unknown
signal
known
sparse
one
measure
sub-sampling
matrix
satisfying
restricted
isometry
property
rip
apply
basis
pursuit
obtain
estimate
signal
similar
ideas
extended
low-rank
matrix
recovery
general
signal
recovery
problems
signal
structures
encoded
atomic
norms
convex
functions
despite
success
many
applications
note
undesired
features
standard
theory
signal
structure
must
known
advance
usually
requires
seeking
good
signal
represen-
tation
reveal
signal
structure
non-trivial
task
called
dictionary
learning
work
supported
part
european
commission
un-
der
grant
mirg-268398
erc
future
proof
snf
200021-132548
snf
200021-146750
snf
crsii2-147633
recovery
scheme
computationally
expensive
typical
examples
basis
pursuit
lasso
non-smooth
convex
optimization
problems
features
seem
necessary
according
existing
literature
applications
real-world
setting
deviate
standard
setting
cre-
ates
opportunity
getting
rid
undesired
features
focus
one
important
observation
stan-
dard
theory
take
consideration—we
usually
training
signals
i.e.
signals
given
similar
unknown
signal
sense
fact
practitioners
indeed
applying
learning-
based
approach
na¨ıve
way
example
exam-
ining
large
amount
real-world
images
discovered
sparsity
sophisticated
structures
proper
repre-
sentations
although
na¨ıve
learning
procedure
made
rigorous
automated
dictionary
learning
training
signals
still
required
paper
propose
alternative
compressive
sam-
pling
apply
compressive
magnetic
resonance
imag-
ing
mri
proposed
scheme
automatically
adapts
given
training
signals
without
priori
knowledge
signal
structure
highlight
following
contributions
propose
novel
statistical
learning
view
point
compressive
sampling
problem
allows
study
effect
training
signals
compressive
mri
scheme
computationally
ef-
ﬁcient
learning
procedure
cast
com-
binatorial
optimization
program
exactly
solved
efﬁcient
algorithm
recovery
algorithm
consider
simply
least-squares
reconstruc-
tion
contrast
standard
approach
using
random
sub-
sampling
patterns
sub-sampling
scheme
ﬁxed
given
training
signals
hence
simpler
implementation
provide
theoretical
guarantee
reconstruc-
tion
error
characterize
dependence
num-
ber
training
signals
show
via
experiments
real
mri
images
reconstruction
error
performance
proposed
scheme
comparable
performance
using
ﬁnely-tuned
sub-sampling
pattern
given
review
existing
approaches
compressive
mri
essentially
linear
inverse
problem
goal
recover
unknown
signal
given
sub-
sampling
pattern
|ω|
outcome
compressive
sampling
pωf
fourier
transform
matrix
linear
operator
keeps
entries
indexed
practice
usually
object
replaced
corresponding
multidimensional
fourier
transform
existing
approaches
compressive
mri
brieﬂy
summarized
follows
find
wavelet
transform
matrix
ψ−1z♮
possesses
certain
structure
example
sparsity
smoothness
exploited
tree
sparsity
con-
sidered
multi-level
sparsity
considered
choose
random
sub-sampling
pattern
sample
accordingly
probability
distribution
might
dependent
knowledge
structure
finally
apply
non-linear
decoding
algorithms
recon-
struct
standard
basis
pursuit
estimator
con-
sidered
basis
pursuit
like
estimator
minimiz-
ing
linear
combination
ℓ1-norm
total
variation
semi-norm
proposed
closely-
related
lasso
like
estimator
ℓ1-norm
to-
tal
variation
semi-norm
penalization
considered
similar
lasso
like
estimator
one
additional
penalization
term
tree
sparsity
introduced
note
existing
approaches
essentially
follow
standard
theory
compressive
sampling
hence
inherit
two
undesired
features
mentioned
intro-
duction
learning
data
triage
standard
approach
compressive
mri
models
deterministic
unknown
signal
adopt
another
mod-
eling
philosophy
assume
random
vector
fol-
lowing
unknown
probability
distribution
access
training
signals
in-
dependent
identically
distributed
random
vectors
also
fol-
lowing
independent
note
different
bayesian
compressive
sampling
unknown
model
consider
reconstruction
given
sub-
sampling
pattern
estimator
explicit
form
ˆxω
arg
min
nky
pωf
xk2
rpo
reconstruction
scheme
ﬁxed
issue
choose
optimizes
resulting
estimation
performance
show
section
8.1
given
expected
normalized
reconstruction
error
satisﬁes
cid:13
cid:13
ˆxω
cid:13
cid:13
kx♮k
expectations
respect
respectively
deﬁne
kpωf
kxk2
convenience
implies
optimal
sub-sampling
pattern
denoted
ωopt.
given
solution
following
combinatorial
optimization
program
ωopt
arg
max
|ω|
however
since
assumed
unknown
optimization
pro-
gram
tractable
motivated
idea
empirical
risk
minimization
statistical
learning
theory
make
use
training
signals
approximate
ωopt
via
solution
opti-
mization
program
arg
max
nˆem
|ω|
ˆem
denotes
expectation
respect
empiri-
cal
measure
i.e.
ˆem
xi=1
kpωf
xik
kxik
optimization
program
tractable
need
solve
realization
training
signals
note
ˆem
depends
random
overall
systems
summarized
follows
find
sub-sampling
pattern
sub-sample
using
obtain
measurement
outcome
recover
pωm
computing
optimization
program
modu-
lar
hence
exactly
solved
simple
greedy
algo-
i-th
row
compute
values
rithm
let
xi=1
cid:0
cid:1
set
set
indices
corresponding
largest
computational
complexity
dominated
com-
putation
behaves
mp2
general
log
suitably
structured
fourier
hadamard
matrices
performance
analysis
analyze
reconstruction
error
proposed
learning-
based
compressive
sampling
system
could
solve
optimization
program
esti-
mation
performance
would
given
cid:13
cid:13
ˆxωopt
cid:13
cid:13
kx♮k
deﬁne
max
|ω|
note
constant
given
independent
training
signals
since
optimization
program
replaced
empirical
version
reasonable
guess
estimation
performance
would
behave
cid:13
cid:13
ˆxωm
cid:13
cid:13
kx♮k
high
probability
respect
training
signals
converge
following
proposition
veriﬁes
guess
proposition
4.1.
cid:20
log
cid:18
cid:19
log
cid:21
probability
least
means
size
training
signals
order
log
sufﬁces
small
enough
high
probability
note
worst-case
guarantee
distribution-
independent
practice
much
smaller
6.25
sampling
12.5
sampling
sampling
fig
first
row
subsampling
maps
tuned
random
variable
sampling
scheme
second
row
maps
given
learning-based
approach
numerical
results
use
3-dimensional
dataset
raw
knee-images
data
given
k-space.1
ﬁrst
take
inverse
fourier
transform
along
z-axis
eliminate
low
energy
z−slices
close
boundary
datacube
noise-
like
slices
exhibit
knee
feature
close
skin
patient
investigate
subsam-
pling
schemes
320
320
fourier
plane
corresponds
compressive
sampling
z-slice
pick
ﬁrst
patients
given
dataset
training
test
learned
subsampling
maps
remain-
ing
patients
compare
learning
based
approach
variable
density
function
proposed
parametrized
radius
fully
sampled
region
polynomial
degree
tune
values
yield
highest
average
psnr
training
data
figure
illustrates
best
performing
randomized
in-
dices
learned
set
indices
plane
k-space
variable
density
approach
learning-based
approach
concentrates
sampling
budget
low
frequencies
however
latter
endowed
capability
adapt
frequency
selection
frequency
content
training
signals
instead
assuming
circu-
larly
symmetric
selection
table
average
psnr
test
data
indices
best-n
approx
lustig
work
sampling
rate
12.50
6.25
25.29
26.36
28.35
24.51
25.11
26.05
24.66
25.18
26.12
1available
http
//mridata.org/fullysampled
acknowledgement
authors
would
like
thank
baran
g¨ozc¨u
luca
bal-
dassarre
providing
numerical
results
jonathan
scar-
lett
complexity
discussion
proofs
8.1.
proof
fact
equality
holds
deterministically
kˆxωk
cid:13
cid:13
ˆxω
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
pωf
cid:13
cid:13
cid:10
ˆxω
cid:11
cid:13
cid:13
cid:13
cid:13
pωf
cid:11
cid:13
cid:13
cid:13
cid:13
cid:10
pωf
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
pωf
cid:13
cid:13
third
equality
used
fact
aa†a
matrix
moore-penrose
generalized
inverse
setting
pωf
8.2.
proof
proposition
4.1
sufﬁces
choose
probability
least
fωopt
fωm
note
cid:16
fωopt
ˆem
fωopt
cid:17
cid:16
ˆem
fωopt
ˆem
fωm
cid:17
cid:16
ˆem
fωm
fωm
cid:17
second
summand
right-hand
side
posi-
tive
deﬁnition
max
cid:12
cid:12
cid:12
ˆem
cid:12
cid:12
cid:12
|ω|
random
variables
bounded
use
hoeffding
inequality
union
bound
obtain
upper
bound
holds
high
probability
b.3
references
j.-y
audibert
bousquet
combining
pac-
bayesian
generic
chaining
bounds
mach
learn
res.
vol
863–889
2007
bach
learning
submodular
functions
con-
vex
optimization
perspective
found
trends
mach
learn.
vol
2–3
145–373
2013.
learning-based
lustig
12.5
6.25
fig
mri
reconstructions
schemes
different
sub-
sampling
rates
knee
slice
patient
whose
fully
sampled
reconstruction
shown
top
left
table
shows
performance
approaches
test
data
addition
error
lower-bounds
obtained
best
n-sample
approximations
respect
fourier
basis
appears
learning
based
approach
slightly
out-
performs
randomized
variable
density
based
approach
however
slight
numerical
improvements
actually
accentuated
look
details
reconstructions
shown
figure
test
patient
13.
clear
learning-based
reconstructions
provide
details
espe-
cially
6.25
12.5
discussions
essential
idea
learning-based
approach
sum-
marized
follows
fix
decoder
ﬁnd
optimal
sub-
sampling
pattern
minimizes
corresponding
expected
recovery
error
approximated
empirical
risk
minimization
performance
essentially
determined
distribution
signal
ensemble
paper
consider
linear
decoder
compu-
tational
efﬁciency
works
well
ensemble
mri
images
signal
ensembles
possible
better
recovery
error
performance
non-linear
decoder
basis
pursuit
lasso
realize
trade-off
be-
tween
computational
complexity
recovery
performance
note
idea
learning-based
approach
still
applies
empirical
risk
minimization
formulation
choos-
ing
sub-sampling
pattern
modiﬁed
accordingly
given
decoder
currently
working
research
direction
baraniuk
cevher
duarte
hegde
model-based
compressive
sensing
ieee
trans
inf
theory
vol
1982–2001
apr
2010
vapnik
overview
statistical
learning
the-
ory
ieee
trans
inf
theory
vol
988–
999
sep.
1999
yang
zhang
yin
fast
alternating
di-
rection
method
tvl1-l2
signal
reconstruction
partial
fourier
data
ieee
sel
topics
signal
pro-
cess.
vol
288–297
2010
cand
recht
exact
matrix
completion
via
convex
optimization
found
comput
math.
vol
717–772
2009
cand
restricted
isometry
property
implications
compressed
sensing
acad
sci
paris
ser
vol
346
589–592
2008
cand
romberg
tao
robust
un-
certainty
principles
exact
signal
reconstruction
highly
incomplete
frequency
information
ieee
trans
inf
theory
vol
489–509
feb.
2006
cevher
learning
compressible
priors
adv
neural
information
processing
systems
2009
chandrasekaran
recht
parrilo
willsky
convex
geometry
linear
inverse
prob-
lems
found
comput
math.
vol
805–849
2012
chen
huang
compressive
sensing
mri
wavelet
tree
sparsity
adv
neural
information
pro-
cessing
systems
2012
halabi
cevher
totally
unimodular
view
structured
sparsity
18th
int
conf
artiﬁcial
intel-
ligence
statistics
2015
foucart
rauhut
mathematical
introduction
compressive
sensing
basel
birkh¨auser
2013
fujishige
submodular
functions
optimization
2nd
amsterdam
elsevier
2005
xue
carin
bayesian
compressive
sens-
ing
ieee
trans
sig
process.
vol
2346–
2356
2008
lustig
donoho
pauly
sparse
mri
application
compressed
sensing
rapid
imaging
magn
reson
med.
vol
1182–1195
2007
mallat
wavelet
tour
signal
processing
sparse
way
3rd
burlington
academic
press
2009
roman
adcock
asymptotic
structure
compressed
sensing
2014
arxiv:1406.4178v2
math.fa
hansen
toˇsi´c
frossard
dictionary
learning
ieee
sig
process
mag.
vol
27–38
2011
