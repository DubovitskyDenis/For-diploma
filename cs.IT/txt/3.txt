walsh
sampling
incomplete
noisy
signals
janet
national
research
center
fundamental
software
beijing
p.r.china
department
informatics
university
bergen
bergen
norway
email
yi.janet.lu
gmail.com
june
2018
abstract
advent
massive
data
outputs
regular
rate
admittedly
signal
processing
technology
plays
increasingly
key
role
nowadays
signals
merely
restricted
physical
sources
extended
digital
sources
well
general
assumption
discrete
statistical
signal
sources
propose
practical
problem
sampling
incomplete
noisy
signals
know
priori
sampling
size
bounded
approach
sampling
problem
shannon
channel
coding
theorem
main
results
demonstrate
large
walsh
coeﬃcient
characterize
discrete
statistical
signals
regardless
signal
sources
connection
shannon
theorem
establish
necessary
suﬃcient
condition
generic
sampling
problem
ﬁrst
time
generic
sampling
results
ﬁnd
practical
powerful
applications
statistical
cryptanalysis
software
system
performance
optimization
keywords
walsh
transform
shannon
channel
coding
theorem
channel
capacity
classical
distinguisher
statistical
cryptanalysis
generic
sampling
digital
signal
processing
introduction
advent
massive
data
outputs
regularly
confronted
challenge
big
data
processing
analysis
admittedly
signal
processing
become
increasingly
key
technology
open
question
sampling
problem
signals
assume
know
priori
due
reasons
practical
consideration
sampling
aﬀected
possibly
strong
noise
and/or
limited
measurement
precision
assuming
signal
source
restricted
particular
application
domain
concerned
practical
generic
problem
sample
noisy
signals
motivation
arises
following
problem
modern
applied
statistics
assume
discrete
statistical
signals
general
setting
follows
samples
generated
arbitrary
possibly
noise-corrupted
source
2n-valued
ﬁxed
known
hypothesis
testing
problem
test
presence
signals
traditionally
deterministic
function
small
medium
input
size
computationally
easy
collect
complete
precise
dis-
tribution
based
notion
kullback-leibler
distance
conventional
approach
aka
classic
distinguisher
solves
sampling
problem
given
distribution
priori
see
nevertheless
reality
might
function
complete
description
might
large
input
size
maybe
non-deterministic
function
thus
infeasible
collect
complete
precise
distribution
gives
rise
new
generic
statistical
sampling
problem
discrete
incomplete
noisy
signals
using
bounded
number
samples
work
show
solve
generic
sampling
problem
reliable
possible
with-
knowing
signals
priori
novel
translations
shannon
channel
coding
theorem
solve
generic
sampling
problem
general
assumption
statistical
signal
sources
specif-
ically
necessary
suﬃcient
condition
given
ﬁrst
time
sample
incomplete
noisy
signals
bounded
sampling
size
signal
detection
interesting
observe
classical
signal
processing
tool
walsh
transform
2,9
essential
regardless
signal
sources
large
walsh
coeﬃcient
characterize
discrete
statistical
signals
put
way
sampling
incomplete
noisy
signals
source
multiple
times
one
expect
see
repeatedly
large
walsh
coeﬃcient
magnitude
ﬁxed
frequency
position
note
known
application
domains
images
voices
results
show
strong
connection
shannon
theorem
walsh
transform
key
innovative
technologies
digital
signal
processing
generic
sampling
results
ﬁnd
practical
useful
applications
statistical
cryptanalysis
expected
become
powerful
universal
an-
alytical
tool
core
building
blocks
symmetric
cryptography
performance
analysis
heterogeneous
acceleration
latter
seems
one
main
bottlenecks
large-scale
systems
era
revolutionary
development
memory
technologies
rest
paper
organized
follows
section
review
basics
walsh
transforms
application
multi-variable
tests
statistics
section
shannon
famous
channel
coding
theorem
also
known
shannon
second
theorem
reviewed
section
present
main
sampling
results
put
forward
two
sampling
problems
namely
classical
generic
versions
also
conjecture
quantitative
relation
renyi
divergence
degree
1/2
shannon
channel
capacity
section
give
illustrative
applications
experimental
results
finally
give
conclusions
future
work
section
cid:98
def=
j∈gf
cid:88
cid:88
walsh
transforms
statistics
given
real-valued
function
deﬁned
n-tuple
binary
vector
input
walsh
transform
denoted
cid:98
another
real-valued
function
deﬁned
cid:104
cid:105
denotes
inner
product
two
n-tuple
binary
vectors
later
convenience
give
alternative
deﬁnition
given
input
array
x2n−1
reals
time
domain
walsh
transform
cid:98
y2n−1
deﬁned
def=
j∈gf
cid:104
cid:105
n-tuple
binary
vector
call
resp
time-domain
component
resp
transform-
domain
coeﬃcient
signal
dimension
properties
references
walsh
trans-
forms
refer
let
probability
distribution
n-bit
random
variable
xn−1
cid:98
bias
boolean
variable
cid:104
cid:105
ﬁxed
n-bit
vector
often
called
output
pattern
mask
note
pattern
nonzero
recall
boolean
random
variable
bias
deﬁned
def=
hence
always
uniformly
distributed
bias
walsh
transforms
used
statistics
ﬁnd
dependencies
within
multi-variable
data
set
multi-variable
tests
indicates
presence
absence
represented
particular
feature
pattern
recognition
experiment
fast
walsh
transform
fwt
used
obtain
coeﬃcients
cid:98
one
shot
checking
walsh
coeﬃcients
one
one
identifying
large
ones
able
tell
dependencies
among
instance
histogram
pdf
probability
density
function
triples
depicted
fig
obtained
experiment
trying
total
160
times
walsh
spectrum
histogram
i.e.
array
calculated
shown
fig
largest
nontrivial
walsh
coeﬃcient
found
located
index
position
implies
correlation
observed
strongest
variables
review
shannon
channel
coding
theorem
brieﬂy
review
shannon
famous
channel
coding
theorem2
first
recall
basic
deﬁnitions
shannon
entropy
entropy
discrete
random
variable
alphabet
probability
mass
function
deﬁned
def=
cid:88
x∈x
log2
1throughout
paper
refer
large
transform-domain
coeﬃcient
one
large
absolute
value
2sometimes
called
shannon
second
theorem
def=
cid:88
cid:88
def=
x1···
joint
entropy
collection
discrete
random
variables
joint
distribution
deﬁned
log2
deﬁne
conditional
entropy
random
variable
given
mutual
information
two
random
variables
equal
always
equals
x|y
communication
channel
system
output
depends
probabilistically
input
characterized
probability
transition
matrix
determines
conditional
distribution
output
given
input
theorem
shannon
channel
coding
theorem
given
channel
denote
input
output
respectively
send
information
maximum
rate
bits
per
transmission
arbitrarily
low
probability
error
channel
capacity
deﬁned
max
maximum
taken
possible
input
distributions
binary
symmetric
channel
bsc
crossover
probability
input
symbols
complemented
probability
transition
matrix
form
express
cid:18
cid:19
bits/transmission
refer
bsc
crossover
probability
small
i.e.
|d|
cid:28
extremal
bsc
note
bsc
becomes
useful
studying
correlation
attacks
see
stream
ciphers
put
simply
observed
keystream
bit
found
correlated
one
bit
linear
relation
internal
state
case
probability
equality
holds
denoted
crossover
probability
bsc
using
cid:16
cid:17
cid:16
cid:17
cid:16
cid:17
log
···
cid:124
cid:123
cid:122
cid:125
d10
d2/
log
show
show
following
result
channel
capacity
extremal
bsc
corollary
extremal
bsc
given
bsc
channel
crossover
probability
small
i.e.
|d|
cid:28
constant
log
therefore
send
one
bit
arbitrarily
low
probability
error
minimum
number
transmissions
1/c
log
/d2
i.e.
1/d2
interestingly
communication
theory
extremal
bsc
rare
typically
deal
|d|
cid:29
see
sampling
theorems
incomplete
signals
section
put
forward
two
sampling
problems
namely
classical
generic
versions
without
loss
generality
assume
discrete
statistical
signals
restricted
particular
application
domain
signals
2n-valued
ﬁxed
speciﬁcally
give
mathematical
model
signal
represented
arbitrary
necessarily
deterministic
function
follows
let
n-bit
output
sample
assuming
input
random
uniformly
distributed
denote
output
distribution
note
assumption
general
setting
discrete
statistical
signals
described
assumption
arbitrary
yet
ﬁxed
function
n-bit
output
classical
sampling
problem
formally
stated
follows
theorem
classical
sampling
problem
assume
largest
walsh
coeﬃcient
cid:98
nonzero
n-bit
vector
detect
signals
represented
arbitrarily
low
probability
error
using
minimum
number
log
/d2
samples
i.e.
1/d2
note
interpreted
classical
distinguisher
used
often
statistical
cryptanalysis
though
problem
statement
classical
distinguisher
slightly
diﬀerent
uses
slightly
diﬀerent
classical
sampling
problem
assumes
together
characteristics
i.e.
largest
walsh
coeﬃcient
known
priori
next
present
main
sampling
results
practical
widely
applicable
sampling
as-
suming
infeasible
know
signal
represented
priori
want
use
bounded
number
samples
detect
signals
arbitrarily
low
probability
error
note
sampled
signal
often
incomplete3
associated
distribution
precise
call
3that
possible
outputs
generated
sampling
analogy
classical
distin-
problem
generic
sampling
incomplete
noisy
signals
guisher
result
interpreted
generalized
distinguisher4
context
statistical
cryptanalysis
give
main
result
theorem
generic
sampling
problem
assume
sampling
size
upper-
bounded
regardless
input
size
order
detect
signal
arbitrarily
low
probability
error
necessary
suﬃcient
following
condition
satisﬁed
i.e.
nontrivial
walsh
coeﬃcient
|d|
constant
log
assume
satisfy
following
conditions
cardinality
support
power
two
i.e.
small
3/2n
present
generalized
result
incorporates
theorem
special
case
proposition
generic
sampling
problem
assume
sampling
size
upper-bounded
regardless
input
size
order
detect
signal
arbitrarily
low
probability
error
necessary
suﬃcient
following
condition
satisﬁed
i.e.
cid:88
cid:98
log
cid:54
states
cid:80
note
suﬃcient
condition
also
proved
based
results
classic
distin-
guisher
i.e.
squared
euclidean
imbalance
uses
notion
kullback-leibler
distance
cid:54
cid:98
log
required
high
probability
success
secondly
discrete
statistical
signals
characterized
large
walsh
coeﬃcients
asso-
ciated
distribution
thus
signiﬁcant
transform-domain
signals
largest
coeﬃcients
generalized
model
4.1
proof
theorem
ﬁrst
prove
following
hypothesis
testing
result
shannon
channel
coding
theorem
theorem
assume
boolean
random
variable
bias
small
given
sequence
random
samples
i.i.d
following
distribution
either
uniform
distribution
tell
sample
source
arbitrarily
low
probability
error
using
minimum
number
samples
log
/d2
i.e.
1/d2
cid:18
cid:19
1/2
y|x
proof
propose
novel
non-symmetric
binary
channel
assume
channel
following
transition
matrix
small
matrix
entry
xth
row
yth
column
denotes
conditional
probability
received
sent
input
bit
transmitted
channel
error
probability
i.e.
received
sequence
bias
input
symbols
input
bit
transmitted
error
probability
1/2
i.e.
received
sequence
bias
input
symbols
shannon
channel
coding
theorem
minimum
number
1/2
4with
appears
informal
result
symmetric
cryptanalysis
used
black-box
analysis
tool
several
crypto-systems
1/c
transmissions
reliably
i.e.
arbitrarily
low
probability
error
detect
signal
source
i.e.
determine
whether
input
compute
channel
capacity
i.e.
ﬁnd
maximum
deﬁned
closed-form
solu-
tion
exists
general
nonlinear
optimization
algorithms
see
known
ﬁnd
numerical
solution
propose
simple
method
give
closed-form
estimate
extremal
binary
channel
ﬁrst
compute
denotes
short
next
compute
cid:17
cid:16
cid:88
cid:16
cid:17
cid:17
p0h
cid:17
cid:16
cid:17
cid:16
0d4
combining
cid:16
p0pe
p0d
small
apply
appendix
0d2
log
note
last
term
approach
maximum
0d4
right
side
ignorable
thus
estimated
1−d
d2/
log
d2/
log
d2/
log
consequently
estimate
channel
capacity
d2/
log
d2/
log
d2/
log
cid:16
cid:17
d2/
log
proceed
prove
theorem
nontrivial
walsh
coeﬃcient
cid:98
suppose
|d|
bias
first
show
contradiction
necessary
condition
identify
arbitrarily
low
probability
error
must
|d|
otherwise
following
proof
theorem
know
error
probability
bounded
away
zero
consequence
shannon
channel
coding
theorem
contradictory
thus
shown
condition
necessary
condition
next
show
also
suﬃcient
condition
|d|
identify
arbitrarily
low
probability
error
follows
directly
theorem
complete
proof
4.2
proof
proposition
assume
channel
transition
matrix
y|x
let
y|x
denote
distribution
let
y|x
uniform
distribution
denote
channel
capacity
convenience
−1/2n
2n−1
/2n
let
+1/2n
2n−1
note
cid:80
taylor
series
log
log
+···
cid:17
cid:16
log
2ui
+ui
deduce
small
calculate
log
2·h
know
ui/
cid:88
assuming
|2n−1ui|
small
cid:88
log
log
2ui
log
2n−1ui
cid:88
cid:88
cid:88
2n−1
2n−1ui
2n−1ui
cid:17
cid:16
cid:88
+2n−1
cid:80
cid:17
cid:16
cid:88
cid:98
cid:88
cid:16
cid:88
cid:80
cid:54
cid:98
cid:17
log
2n−1ui
continue
log
2·h
log
sect.2
know
cid:88
shown
important
result
follows
log
meanwhile
property
walsh
transform
assuming
cardinality
support
power
two
i.e.
small
3/2n
next
order
calculate
ﬁrst
compute
p0h
cid:80
cid:54
cid:98
log
denote
short
denote
distribution
p0f
/2n
p0ui
1/2n
apply
get
log
cid:80
cid:54
cid:100
cid:80
cid:54
cid:98
cid:80
cid:54
cid:98
cid:17
cid:16
log
log
1/2
maximum
equals
cid:16
cid:98
cid:17
consequently
1/c
i.e.
cid:80
cid:54
cid:98
log
necessary
log
cid:80
cid:54
log
cid:80
suﬃcient
condition
following
shannon
theorem
4.3
generalized
results
consider
case
far
uniform
distribution
based
weaker
assumption
small
1/2n
i.e.
−1/2n
show
general
result
log
following
similar
computations
2n−1ui
obtain
following
general
result
log
log
2n−1ui
p0ui
2n−1p0ui
cid:88
cid:16
max
log
p0ui
2n−1p0ui
p0ui
2n−1ui
recall
2/2n
approximated
achieved
1/2
speciﬁcally
2/2n
approximation
addend
expressed
follows
use
1+v
p0ui
p0ui
2n−1p0ui
1+v
|v|
2n−1u2
2n−1ui
note
|v|
1+v
1/v
1/v2
show
2n−1ui
addend
achieve
maximum
1/k
max
p0ui
2n−1p0ui
p0ui
2n−1ui
2n−1
cid:17
cid:88
cid:88
cid:88
hand
right-hand
side
equals
meanwhile
2n−1ui
p0ui
k2/2n−1
much
larger
max
max
2n−1p0ui
2n−1
p0ui
2n−1ui
2n−1
1/4
4.4
discussions
make
conjecture5
even
based
renyi
information
measures
general
form
channel
capacity
recall
renyi
information
divergence
see
order
1/2
distribution
another
distribution
ﬁnite
set
deﬁned
cid:107
def=
log
d1/2
cid:107
def=
log
cid:88
cid:0
cid:1
q1−α
cid:0
cid:1
cid:88
cid:112
x∈x
x∈x
1/2
conjecture
let
non-uniform
distribution
uniform
distribution
support
cardinality
let
matrix
consist
two
rows
columns
following
relation
renyi
divergence
degree
1/2
generalized
channel
capacity
degree
1/2
i.e.
standard
shannon
channel
capacity
d1/2
cid:107
c1/2
finally
assume
2n-valued
potentially
large
input
space
collected
normalized
distribution
cid:29
output
samples
time-domain
approximately
ﬁts
noisy
model
4,11
additive
gaussian
noise
i.i.d
zero
mean
variance
case
noisy
sparse
wht
used
generic
sampling
problem
recovering
nonzero
walsh
coeﬃcients
cid:98
index
positions
obtain
evaluation
time
queries
processing
time
roughly
order
provided
cid:54
cid:88
cid:98
log
cid:17
cid:16
cid:98
cid:88
cid:54
log
cid:88
cid:54
snr
cid:17
cid:16
cid:98
5see
recent
results
conjecture
applications
experimental
results
ﬁrst
demonstrate
cryptographic
sampling
application
famous
block
cipher
gost
28147-
balanced
feistel
network
rounds
64-bit
block
size
key
length
256
bits
let
32-bit
denote
left
right
half
round
denote
plaintext
subkey
round
denoted
purpose
multi-round
analysis
target
function
ri−1
ri−1
ri−1
ri−1
round
function
turns
largest
three
walsh
coeﬃcients
2−6
2−6.2
2−6.3
choose
240.
respectively
new
weakness
leads
various
severe
attacks
gost
similarly
cryptographic
sampling
technique
applicable
threatens
security
snow
2.0
cipher
general
snr
log
regardless
choose
appropriate
cid:96
roughly
cid:96
processing
time
needed
would
become
powerful
universal
analytical
tool6
security
evaluation
core
building
blocks
symmetric
cryptography
another
notable
practical
application
software
performance
optimization
modern
large-
scale
systems
hybrid
nature
usually
partial
information
architecture
well
component
units
known
revolutionary
change
physical
components
e.g.
main
memory
storage
inevitably
demand
system
take
full
advantages
new
hardware
characteristics
expect
sampling
techniques
transform-
domain
analysis
running
time
would
help
performance
analysis
optimization
whole
heterogeneous
system
conclusion
paper
model
general
discrete
statistical
signals
output
samples
unknown
arbitrary
yet
ﬁxed
function
signal
source
translate
shannon
channel
coding
theorem
solve
hypothesis
testing
problem
translated
result
allows
solve
generic
sam-
pling
problem
know
nothing
signal
source
priori
aﬀord
bounded
sampling
measurements
main
results
demonstrate
classical
signal
process-
ing
tool
walsh
transform
essential
large
walsh
coeﬃcient
characterize
discrete
statistical
signals
regardless
signal
sources
shannon
theorem
establish
necessary
suﬃcient
condition
generic
sampling
problem
general
assumption
statistical
signal
sources
future
direction
work
interesting
investigate
noisy
sparse
wht
generic
sampling
problem
general
setting
cid:28
large
case
noise
modelled
gaussian
references
arimoto
algorithm
computing
capacity
arbitrary
discrete
memoryless
chan-
nels
ieee
trans
inform
theory
it-18
14-20
1972
6note
current
computing
technology
aﬀord
exascale
wht
i.e.
order
260
within
years
uses
215
modern
pcs
blackledge
digital
signal
processing
mathematical
computational
methods
software
development
applications
horwood
publishing
england
second
edition
2006
blahut
computation
channel
capacity
rate
distortion
functions
ieee
trans
inform
theory
it-18
460-473
1972
chen
guo
robust
sublinear
complexity
walsh-hadamard
transform
arbitrary
sparse
support
ieee
int
symp
information
theory
2573
2577
2015
cover
thomas
elements
information
theory
john
wiley
sons
second
edition
2006
csisz´ar
generalized
cutoﬀ
rates
r´enyi
information
measures
ieee
trans
inform
theory
vol.41
no.1
jan.
1995
itai
dinur
orr
dunkelman
nathan
keller
adi
shamir
memory-eﬃcient
algorithms
finding
needles
haystacks
crypto
2016
part
lncs
9815
185
206
2016
gray
davisson
introduction
statistical
signal
processing
cambridge
university
press
2004.
http
//www-ee.stanford.edu/~gray/sp.pdf
horadam
hadamard
matrices
applications
princeton
university
press
2007
joux
algorithmic
cryptanalysis
chapman
hall/crc
cryptography
network
se-
curity
2009
bradley
pawar
ramchandran
spright
algorithm
robust
sparse
hadamard
transforms
ieee
int
symp
information
theory
1857
1861
2014
desmedt
walsh
transforms
cryptographic
applications
bias
computing
cryptography
communications
vol.8
no.3
435
453
springer
2016
new
linear
attacks
block
cipher
gost
iacr
eprint
http
//eprint.iacr.org/
2017/487
2017
new
results
dmc
capacity
renyi
divergence
arxiv:1708.00979
2017
meier
staﬀelbach
fast
correlation
attacks
certain
stream
ciphers
journal
cryptology
vol.1
no.3
159
176
springer
1989
reed
dongarra
exascale
computing
big
data
next
frontier
communi-
cations
acm
vol.58
no.7
2015
scheibler
haghighatshoar
vetterli
fast
hadamard
transform
signals
sublinear
sparsity
transform
domain
ieee
transactions
information
theory
vol.61
no.4
2115
2132
2015
shokrollahi
personal
communication
2006
vaudenay
experiment
des
statistical
cryptanalysis
third
acm
conference
computer
security
139
147
1996
vaudenay
classical
introduction
modern
cryptography
applications
communi-
cations
security
springer
new
york
2006
vaudenay
direct
product
theorem
submitted
zhang
meier
fast
correlation
attacks
extension
ﬁelds
large-unit
linear
approximation
cryptanalysis
snow
2.0
crypto
2015
lncs
vol
9215
643
662
springer
2015
zhang
feng
practical
cryptanalysis
bluetooth
encryption
condition
masking
journal
cryptology
https
//doi.org/10.1007/s00145-017-9260-1
springer
2017
