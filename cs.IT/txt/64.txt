generalised
differential
framework
measuring
signal
sparsity
anastasios
maronidis
elisavet
chatzilari
spiros
nikolopoulos
ioannis
kompatsiaris
abstract—the
notion
signal
sparsity
gaining
increasing
interest
information
theory
signal
processing
communities
recent
advances
ﬁelds
like
signal
compression
sampling
analysis
accentuated
crucial
role
sparse
representations
signals
consequence
strong
need
measure
sparsity
towards
end
plethora
metrics
presented
literature
appropriateness
metrics
typically
evaluated
set
objective
criteria
proposed
assessing
credibility
sparsity
metric
paper
propose
generalised
differential
sparsity
gds
framework
generating
novel
sparsity
metrics
whose
functionality
based
concept
sparsity
encoded
differences
among
signal
coefﬁcients
rigorously
prove
every
metric
generated
using
gds
satisﬁes
aforementioned
criteria
provide
computationally
efﬁcient
formula
makes
gds
suitable
high-dimensional
signals
great
advantage
gds
ﬂexibility
offer
sparsity
metrics
well-tailored
certain
requirements
stemming
nature
data
problem
solved
contrast
current
state-of-
the-art
sparsity
metrics
like
gini
index
actually
proven
speciﬁc
instance
gds
demonstrating
generalisation
power
framework
verifying
claims
incorporated
gds
stochastic
signal
recovery
algorithm
experimentally
investigated
efﬁcacy
recon-
structing
randomly
projected
sparse
signals
result
proven
gds
comparison
loosens
bounds
assumed
sparsity
original
signals
reduces
minimum
number
projected
dimensions
required
guarantee
almost
perfect
reconstruction
heavily
compressed
signals
superiority
gds
conjunction
fact
latter
considered
standard
numerous
scientiﬁc
domains
prove
great
potential
gds
general
purpose
framework
measuring
sparsity
index
terms—signal
sparsity
signal
compressibility
differen-
tial
sparsity
signal
reconstruction
introduction
parse
representation
signals
celebrated
premise
permits
solution
problems
previously
unsolvable
paving
way
unprecedented
possibilities
ﬁelds
like
signal
compression
reconstruction
roughly
speaking
sparsity
measures
extent
informa-
tion
signal
distributed
coefﬁcients
speciﬁ-
cally
highly
sparse
signals
information
concentrated
small
portion
coefﬁcients
non-sparse
signals
information
uniformly
distributed
across
coefﬁcients
context
sparsity
desirable
property
allows
succinct
representations
large
pieces
information
recall
occam
razor
dictates
among
set
representations
compact
always
preferred
many
paradigms
stemming
diverse
research
domains
advocating
importance
sparsity
compressive
sampling
comprises
vivid
example
role
sparsity
demonstrated
process
compressing
reconstructing
signal
speciﬁcally
introduction
null
space
property
nsp
restricted
isometry
property
rip
proven
assumption
data
sparsity
possible
solve
underdetermined
linear
system
equations
important
result
allows
perfect
reconstruction
signal
compressed
using
random
projections
original
sparse
signal
towards
end
variety
optimisation
algorithms
incorporate
notion
sparsity
proposed
reconstructing
compressed
signal
instance
dantzig
selector
solves
l1-regularisation
problem
attempt
estimate
ground
truth
sparse
signal
noisy
projections
signal
similar
vein
sparsity
also
utilised
lasso
algorithm
recovering
sparse
representations
high-
dimensional
signals
apart
aforementioned
applications
notion
sparsity
also
incorporated
already
existing
methods
various
ﬁelds
instance
bayesian
methods
providing
sparse
solutions
regression
classiﬁcation
problems
attracted
renewed
interest
moreover
support
vector
machines
svm
optimal
guarantees
sparsity
support
vector
set
encoding
boundary
two
classes
also
investigated
sparsity
appears
play
key
role
boosting
techniques
well
leading
sparse
combinations
number
weak
classiﬁers
additionally
unsupervised
conﬁguration
sparse
principal
component
analysis
s-pca
introduced
framework
trades
redundancy
minimisation
sparsity
maximisation
basis
signals
comprise
indicative
examples
endless
catalogue
diverse
scientiﬁc
domains
signal
sparsity
ﬁnds
application
nevertheless
fairly
enough
demonstrate
notion
sparsity
occupies
dominant
position
signal
processing
information
theory
commu-
nities
given
importance
sparsity
essential
ﬁnd
effective
way
measure
apparently
way
sparsity
deﬁned
measured
dictated
speciﬁc
purpose
designed
serve
paper
context
particularly
concerned
role
sparsity
reconstruction
signals
heavily
compressed
using
random
projections
signal
reconstruction
covers
large
portion
problems
concern
sparsity
hence
conclusions
drawn
analysis
expected
impact
case
studies
well
formally
core
idea
sparsity
originally
introduced
count
integer
number
non-
zero
coefﬁcients
signal
measured
help
norm
practice
though
proves
strict
deﬁnition
rarely
real-world
problems
signals
contain
exact
zeros
consequence
research
community
resorted
new
relaxed
measures
sparsity
whose
actual
objective
estimate
approximation
number
non-zero
coefﬁcients
allowing
sparsity
take
decimal
values
along
lines
notion
sparsity
usually
referred
signal
compressibility
instead
though
work
consistently
use
term
sparsity
even
cases
actually
refer
signal
compressibility
proposed
variety
sparsity
metrics
literature
among
gini
index
offers
state-of-the-art
solution
led
impressive
results
reconstructing
compressed
signals
validity
majority
sparsity
metrics
often
relies
merely
intuitive
criteria
overcome
drawback
number
objective
criteria
proposed
origin
criteria
stems
ﬁnancial
science
notion
sparsity
analogous
inequity
wealth
distribution
human
society
aforementioned
criteria
provide
degree
credibility
sparsity
metric
enabling
comparison
different
metrics
already
existing
sparsity
metrics
e.g.
norm
use
magnitude
vector
coefﬁcients
encode
sparsity
drawback
though
way
relativity
among
coefﬁcients
completely
deﬁed
paper
claim
process
measuring
sparsity
vector
absolute
value
coefﬁcients
important
relative
differences
rationale
claim
relies
upon
observation
small
large
value
depends
reference
value
compared
therefore
calculating
relative
differences
actually
compare
coefﬁcients
following
example
elaborates
relativity
coefﬁcients
might
prove
crucial
considering
10−10
10−10
10−10
10−10
10−10
two
questions
naturally
emerge
close
coefﬁcients
zero
thereby
sparse
answer
ﬁrst
question
equally
close
zero
regardless
actual
distance
sense
coefﬁcients
contain
equal
percentage
signal
energy
means
equally
important
representation
signal
hence
none
discarded
consequence
although
ﬁrst
glance
may
look
counter-intuitive
answer
second
question
clearly
totally
non-sparse
motivated
previous
analysis
main
contribution
paper
propose
generalised
differential
spar-
sity
gds
framework
whose
functionality
based
differences
among
signal
coefﬁcients
advantageous
feature
framework
customisable
certain
data
types
problem
requirements
due
adjustable
parameter
call
order
gds
different
values
order
generate
novel
metric-instances
varying
strictness
estimating
signal
sparsity
part
analysis
rigorously
prove
gds
metric-
instances
satisfy
objective
criteria
sparsity
metrics
moreover
prove
gds
ﬁrst
order
col-
lapses
encapsulation
within
gds
emphasises
generalisation
power
latter
unifying
already
existing
metrics
apart
generating
novel
ones
addition
although
computation
gds
using
original
formula
tractable
even
large
values
order
proves
cumbersome
high-dimensional
data
dealing
shortcoming
provide
equivalent
formula
gds
allows
efﬁcient
calculation
number
dimensions
high
drawback
though
latter
formula
contrast
original
one
costly
big
values
order
gds
consequently
formulas
prove
useful
used
interchangeably
according
given
circumstances
order
gds
determines
tendency
cor-
responding
metric-instance
qualify
arbitrary
signal
sparse
proves
great
advantage
since
offers
gds
ﬂexibility
adjust
certain
requirements
arising
nature
data
problem
solved
order
verify
claim
used
gds
reconstruct
sparse
signals
heavily
compressed
via
random
projections
purpose
employed
reconstruction
approach
presented
combined
returned
excellent
results
reconstruction
performed
incorporating
sparsity
metric
stochas-
tic
approximation
method
solves
dedicated
sparsity
maximisation
problem
speciﬁcally
given
compressed
signal
based
prior
assumption
original
signal
compression
sparse
idea
ﬁnd
original
space
signal
highest
sparsity
gives
smallest
reconstruction
error
experimental
study
similar
one
presented
prove
incorporating
gds
previous
re-
construction
approach
comparison
top
performing
loosens
assumptions
underlying
sparsity
original
signal
required
number
projected
dimensions
words
gds
offers
compression
capacity
lowly
sparse
signals
simultaneously
allows
using
smaller
number
projected
dimensions
without
increasing
reconstruction
error
along
lines
proven
optimal
order
gds
strongly
dependent
type
sparsity
original
data
well
desired
compression
level
ﬁnding
justiﬁes
rationale
behind
using
different
values
order
gds
provides
useful
rule
thumb
deciding
order
gds
appropriate
certain
problem
parameters
remainder
paper
organised
follows
section
number
works
related
measuring
signal
sparsity
reviewed
section
iii
present
set
desirable
criteria
sparsity
metric
must
obey
give
intuitive
interpretations
section
propose
novel
gds
framework
rigorously
prove
satisﬁes
aforementioned
criteria
section
prove
encapsulated
within
generalised
framework
section
provide
computationally
efﬁcient
formula
calculating
gds
signal
normalised
version
gds
based
statistics
data
also
presented
section
vii
subsequently
series
experiments
section
viii
study
gds
signal
reconstruction
performance
function
type
sparsity
original
signals
number
projected
dimensions
gds
order
conclude
paper
section
finally
appendix
containing
lengthy
mathematical
proofs
provided
end
paper
related
work
variety
methods
already
proposed
bibliography
measuring
inherent
sparsity
signal
straightforward
way
measure
sparsity
using
l0-norm
although
led
impressive
theoretical
results
sparse
representation
practice
l0-norm
suffers
several
disadvantages
instance
changing
non-zero
coefﬁcients
affect
even
inﬁnitesimal
amount
noise
zero
coefﬁcients
may
dramatically
distort
overcoming
disadvantages
approximations
l0-norm
proposed
sparsity
optimisation
problems
presence
noise
thresholding
techniques
also
employed
how-
ever
selection
reasonable
threshold
may
prove
problematic
due
disadvantages
l0-norm
often
re-
placed
l1-norm
offers
plausible
alternative
metric
surpassing
shortcomings
accompanying
former
towards
direction
comprises
milestone
work
authors
prove
classical
error
correcting
problem
certain
conditions
translated
l1-optimisation
problem
latter
trivially
solved
linear
programming
conﬁguration
using
existing
dedicated
methods
similar
vein
authors
employ
homotopy
method
solve
underdetermined
system
linear
equations
l1-minimisation
prob-
lem
authors
propose
methodology
sparse
signal
recovery
often
outperforms
l1-minimisation
problem
reducing
number
measurements
required
perfect
reconstruction
compressed
signal
prob-
lem
decomposed
sequence
l1-minimisation
sub-
problems
weights
updated
iteration
based
previous
solution
norms
higher
order
also
used
measuring
signal
sparsity
moreover
combinations
norms
proposed
well
instance
hoyer
sparsity
metric
based
relationship
norm
utilised
sparsity
constrained
non-negative
matrix
factorisation
nmf
setting
ﬁnding
linear
representations
non-negative
data
apart
norms
mathematical
functions
also
used
measuring
signal
sparsity
example
kurtosis
proposed
data
following
unimodal
symmetric
distribution
form
vein
adoption
tanh
functions
approximate
solution
norms
furthermore
introduce
metric
based
order
statistics
contrast
norms
similarly
work
functionality
methods
based
distribution
form
signal
coefﬁcients
rather
magnitudes
main
drawback
though
handle
signals
authors
suggest
whose
coefﬁcients
contain
unique
dominant
mode
zero
thus
must
avoided
dealing
signals
containing
multiple
modes
constrains
scope
applications
connection
sparsity
entropy
clearly
demonstrated
entropy
expresses
complexity
signal
sparsity
expresses
compressibility
appropriate
basis
along
lines
authors
argue
sparsity
entropy
follow
similar
intuitive
crite-
ria
towards
end
propose
novel
sparsity
novel
entropy
metric
satisfy
criteria
functionality
metrics
based
calculation
similarity
signal
theoretically
totally
non-sparse
one
using
inner
product
relying
connection
entropy
diversity
metrics
also
used
measure
sparsity
instance
shannon
gaussian
entropy
presented
constitute
plausible
measures
sparsity
incorporated
sparsity
minimisation
problems
may
lead
sparse
solutions
best
basis
selection
problem
among
prevalent
sparsity
metrics
gini
index
offers
state-of-the-art
solution
variety
applications
couple
examples
effectively
used
maximising
sparsity
wavelet
representations
via
parameterised
lifting
well
ﬁnding
sparse
representation
speech
signals
moreover
relation
work
shown
top
performance
recovering
randomly
projected
signals
therefore
comparison
mandatory
prove
potential
framework
context
worth
noticing
framework
build
common
incentives
speciﬁcally
methods
important
relativity
among
coefﬁcients
rather
absolute
magnitudes
actually
see
later
paper
framework
shares
advantages
accompanying
plus
extra
advantage
contains
adjustable
parameter
makes
method
sparsity
metric
generator
instead
single
sparsity
metric
sparsity
metrics
mentioned
section
along
others
like
instance
log
measure
collectively
reviewed
compared
although
advantages
number
objective
criteria
proposed
literature
assessing
performance
set
criteria
serves
benchmark
allows
comparing
different
metrics
authors
summarise
sparsity
criteria
satisﬁed
sparsity
metrics
proposed
literature
interestingly
metric
satisﬁes
criteria
paper
prove
proposed
framework
well
following
section
provide
full
set
objective
criteria
acquired
literature
iii
sparsity
metric
objective
criteria
remainder
paper
borrow
term
vector
linear
algebra
order
refer
signal
let
-length
vector
whose
sparsity
would
like
measure
sparsity
metric
function
given
returns
real
number
comprises
estimation
sparsity
onwards
implicitly
assume
sparsity
measured
using
magnitudes
coefﬁcients
algebraic
values
i.e.
coefﬁcient
signs
neglected
therefore
simplicity
assume
rising
tide
says
adding
scalar
vector
coefﬁcients
reduces
vector-sparsity
signiﬁcance
property
becomes
obvious
examining
limit
behaviour
vector
operation
indeed
adding
increasing
amount
coefﬁcients
relative
difference
among
coefﬁcients
becomes
negligible
therefore
sparsity
asymptotically
become
zero
cloning
following
give
mathematically
rigorous
deﬁnitions
above-mentioned
sparsity
criteria
along
intuitive
interpretations
continuity
property
requires
small
changes
coefﬁ-
cients
lead
dramatic
change
sparsity
permutation
invariance
permutation
property
postulates
permuting
coefﬁcients
vector
affect
sparsity
provided
property
holds
metric
usually
case
convenience
without
loss
generality
given
arbitrary
vector
since
position
coefﬁcients
matter
consider
sorted
ascending
order
i.e
robin
hood
let
robin
hood
property
says
subtracting
speciﬁc
amount
large
coefﬁcient
adding
amount
smaller
coefﬁcient
decreases
vector-sparsity
energy
vector
spreads
along
coefﬁcients
constraint
used
avoid
scaling
scaling
property
requires
multiplying
vector
coefﬁcients
scalar
must
affect
vector-
sparsity
rising
tide
except
case
ckc
ckck
denotes
concatenation
cloning
requires
concatenating
number
vectors
comprise
exact
copies
original
one
must
affect
vector-sparsity
also
quite
reasonable
take
account
relative
difference
vector
coefﬁcients
sorting
resulting
vector
kept
intact
bill
gates
increasing
value
vector
coefﬁcient
maintaining
remaining
coefﬁcients
sparsity
in-
creases
vector
energy
concentrated
mere
coefﬁcient
babies
ck0
adding
extra
zero
original
vector
vector-
sparsity
increases
action
similar
effect
since
adding
zero
energy
concentrated
fewer
coefﬁcients
saturation
lim
n→+∞
||1
−1||1
saturation
says
concatenating
extra
zeros
vector
change
sparsity
asymptotically
becomes
negligible
p10
lower
bound
smallest
possible
sparsity
encoded
vector
consisting
ones
p11
upper
bound
−1||1
largest
possible
sparsity
encoded
vector
consisting
one
zeros
generalised
differential
sparsity
already
mentioned
section
central
idea
sparsity
based
number
zero
coefﬁcients
vector
consequence
already
existing
sparsity
metrics
e.g.
lp-norm
use
magnitude
vector
coefﬁcients
encode
sparsity
contrast
section
propose
novel
generalised
differential
sparsity
gds
metric
framework
takes
account
differences
among
coefﬁcients
vector
rather
magnitudes
per
measuring
sparsity
way
gds
achieves
measure
sparsity
vector
examining
extent
energy
distributed
coefﬁcients
proof
lower
upper
bound
properties
contained
theorem
proof
due
extensive
length
appended
end
paper
hereunder
provide
proofs
properties
theorem
gds
satisﬁes
scaling
i.e
deﬁnition
gds
order
non-zero
vector
deﬁned
proof
acj
aci
i=1
npn
−1xi=1
nxj=i+1
coefﬁcients
sorted
ascending
order
using
deﬁnition
prove
following
theorem
provides
lower
upper
bounds
possible
values
gds
regardless
order
theorem
proof
−1xi=1
nxj=i+1
npn
i=1
coefﬁcients
sorted
therefore
becomes
since
recall
−1xi=1
nxj=i+1
α
−1xi=1
nxj=i+1
−1xi=1
nxj=i+1
i
αhcp
−1i
2cp
2cp
nxi=1
finally
obvious
since
sum
always
therefore
hence
furthermore
easily
shown
means
minimum
maximum
sparsity
value
respectively
following
provide
rigorous
proofs
proposed
gds
metric
satisﬁes
eleven
basic
properties
presented
section
iii
continuity
permutation
in-
variance
properties
trivially
proven
deﬁnition
i=1
aci
cid:17
cid:16
i=1
cid:17
apn
cid:16
−1xi=1
nxj=i+1
−1xi=1
nxj=i+1
theorem
gds
satisﬁes
rising
tide
i.e
except
case
proof
first
clearly
otherwise
npn
i=1
−1xi=1
npn
i=1
nxj=i+1
−1xi=1
nxj=i+1
−1xi=1
nxj=i+1
nxi=1
i=1
npn
nxi=1
inequality
follows
therefore
theorem
gds
satisﬁes
cloning
i.e
ckc
ckck
proof
−times
ckck
kc
sp
npn
npn
i=1
−1xi=1
nxj=i+1
−1xi=1
nxj=i+1
i=1
cni
theorem
gds
satisﬁes
bill
gates
babies
i.e
∃βi
ck0
proof
proof
straightforward
combining
theorems
appendix
theorems
2.1
2.2
presented
former
states
robin
hood
scaling
satisﬁed
bill
gates
also
satisﬁed
latter
states
robin
hood
scaling
cloning
satisﬁed
babies
also
satisﬁed
theorem
gds
satisﬁes
saturation
i.e
lim
n→+∞
||1
−1||1
proof
trivially
shown
||1
therefore
||1
−1||1
n→+∞
−−−−−→
proven
gds
fulﬁls
objective
criteria
explore
order
affects
estimation
sparsity
provide
insights
appropriate
order
gds
certain
circumstances
towards
end
following
important
theorem
shows
increases
vectors
difﬁcultly
qualiﬁed
sparse
gds
p-th
order
theorem
given
vector
proof
proving
theorem
assume
∀i∈
otherwise
since
theorem
multiply
coefﬁcient
value
obtain
αci
∀i∈
j=i+1
i=1
−1xi=1
nxj=i+1
nxj=i+1
cid:18
nxj=i+1
cid:18
cid:19
cid:19
−1xi=1
−1xi=1
i=1
−1xi=1
nxj=i+1
j=i+1
inequality
holds
−ci
moreover
since
assumed
also
combined
inequality
straightforwardly
implies
npn
i=1
j=i+1
j=i+1
equivalent
completing
proof
npn
i=1
direction
also
next
result
examines
limit
behaviour
tends
inﬁnity
theorem
arbitrary
vector
limp→+∞
proof
every
j=i+1
i=1
npn
i=1
j=i+1
npn
i=1
−1xi=1
nxj=i+1
nxj=i+1
cid:18
cid:19
lim
−1xi=1
p→+∞
cid:18
cid:19
cid:19
nxj=i+1
cid:18
−1xi=1
lim
p→+∞
since
implies
hence
applying
squeeze
theorem
inequality
limp→+∞
words
theorem
states
order
gds
tends
inﬁnity
vectors
regardless
values
coefﬁcients
considered
totally
non-sparse
ﬁndings
theorems
actually
show
order
determines
strictness
gds
qualifying
arbitrary
vector
sparse
speciﬁcally
higher
order
means
strict
gds
feature
offers
appropriate
granularity
gds
allows
adjust
certain
circum-
stances
stemming
nature
data
speciﬁc
problem
solved
instance
anticipated
data
containing
zeros
supposed
inherently
non-sparse
large
order
might
needed
discriminate
among
different
levels
sparsity
contrary
data
plenty
zeros
smaller
orders
i.e.
less
strict
metrics
might
prove
optimal
therefore
see
section
viii
ﬁnding
adopting
appropriate
gds
metric
sparsity
maximisation
problem
might
lead
improved
reconstruction
results
connection
gds
section
prove
gds
ﬁrst
order
collapses
ﬁrst
let
provide
deﬁnition
deﬁnition
gini
index
vector
deﬁned
even
formula
nxi=1
kck1
cid:18
cid:19
theorem
gds
order
equivalent
proof
kck1
−1xi=1
nxj=i+1
kck1
−1xi=1
nxj=i+1
kck1
nxi=1
kck1
kck1
nxi=1
nxi=1
nxi=1
kck1
nxi=1
kck1
nxi=1
cid:18
nxi=1
ci
−1xi=1
nxj=i+1
nxi=1
ici
nxi=1
nxi=1
nxi=1
nxi=1
nxi=1
cid:19
ici
ici
encapsulation
within
gds
conjunction
fact
former
proven
state-of-the-art
metric
sparsity
literature
demonstrate
power
gds
generalised
framework
unifying
already
existing
metrics
well
potential
framework
develop
novel
state-of-the-art
metrics
sparsity
computationally
efficient
formula
gds
high
dimensional
data
although
formula
used
deﬁnition
gds
metric
simple
easy
comprehend
certain
cases
i.e.
number
vector
dimensions
large
difﬁcult
compute
tractable
computationally
efﬁcient
formula
gds
order
integer
presented
section
moreover
computational
analysis
also
provided
order
compare
two
formulas
due
computational
reasons
alternative
formula
different
even
odd
values
purpose
two
cases
separately
presented
rigorous
derivation
formulas
original
one
provided
appendix
kck2k
k−1xω=1
ωkck2k−ω
2k−ω
cid:18
cid:19
kckω
cid:18
cid:19
kck2k
s2k+1
kck2k+1
2k+1
s2k
odd
formula
cid:18
kxω=0
cid:19
nxi=1
cid:0
c2k+1−ω
ixj=1
f2k+1−ω
cid:1
computational
analysis
easily
proven
original
formula
calcu-
lating
sparsity
-length
vector
using
see
requires
multiplications
additions
total
order
computational
load
many
practical
reasons
inefﬁcient
large
corresponding
load
using
even
formula
s2k
see
consists
multiplications
2kn
additions
using
odd
formula
s2k+1
see
4k2n
6kn
multiplications
additions
even
odd
formulas
computational
complexity
total
k2n
clearly
efﬁcient
approximately
p/4
however
opposite
case
original
formula
tractable
indicatively
10000-dimensional
vector
even
formula
needs
around
5000
times
less
calculations
original
one
100
times
less
calculations
100.
corresponding
numbers
odd
formula
1700
respectively
summarising
practical
reasons
new
formula
proves
useful
even
odd
cases
however
notice
situation
reversed
small
relation
example
100
even
odd
formulas
need
respectively
around
times
calculations
original
formula
therefore
original
alternative
formulas
prove
important
may
preferred
according
speciﬁc
conditions
vii
differential
sparsity
normalised
data
analysis
implicitly
assumed
vector
coefﬁcients
commensurate
sense
measured
scale
way
normalised
comparable
actually
assumption
indispensable
proposed
differential
metric
make
sense
however
practice
assumption
often
hold
handling
cases
propose
normalisation
coefﬁcients
prior
application
sparsity
metric
normalisation
accomplished
centralising
data
zero
mean
unit
standard
deviation
key
difference
approach
main
approach
presented
previous
sections
need
dataset
vectors
ideally
underlying
distributions
order
model
mean
value
standard
deviation
vector
coefﬁcients
let
x1,1
...
xn,1
...
...

=
nxk=1
nxk=1
dataset
consisting
-dimensional
column-vectors
let
also
mean
value
standard
deviation
i-th
coefﬁcient
along
dataset
centralised
data
denote
centralised
sparsity
bxi
bsp
bx1
bx2
bxn
data
normalisation
scheme
permits
applica-
tion
gds
type
data
regardless
underlying
distributions
representation
coefﬁcients
viii
experiments
section
investigate
reconstruction
error
randomly
projected
sparse
vectors
function
sparsity
type
original
data
order
gds
number
reduced
dimensions
speciﬁcally
given
original
vector
projection
matrix
projected
ax0
subsequently
reconstructed
initial
space
work
entries
generated
using
i.i.d
random
variables
zero
mean
unit
standard
deviation
gaussian
distribution
reconstruction
accomplished
solving
following
constrained
optimisation
problem
argmax
subject
x∈rn
estimate
original
vector
sparsity
essentially
prior
information
original
vector
compression
sparse
aim
ﬁnd
sparsest
solution
pool
feasible
solutions
satisfying
constraint
solving
problem
employed
iterative
simultaneous
perturbation
stochastic
approximation
spsa
algorithm
actually
adopted
implementation
presented
combined
shown
impressive
performance
signal
reconstruction
case
parameters
involved
spsa
selected
based
previous
research
results
two
main
reasons
opted
use
spsa
first
make
direct
reference
gradient
objective
function
instead
approximates
gradient
using
two
calculations
objective
function
per
iteration
regardless
signal
dimensionality
fea-
ture
renders
spsa
computationally
efﬁcient
high-
dimensional
problems
second
proven
spsa
general
conditions
converges
global
optima
experiment
generated
random
vector
size
100
non-zero
coefﬁcients
varied
range
non-zero
coefﬁcients
generated
using
four
different
distributions
binomial
uniform
normal
exponential
worth
noting
smaller
sparser
vector
sparsity
inverse
quantities
several
values
order
range
exhaustively
varied
number
projected
dimensions
reconstructed
employing
spsa/gds
using
finally
setting
calculated
mean
square
error
mse
recovered
original
vector
ensuring
statistical
signiﬁcance
repeated
whole
approach
100
times
calculated
average
mse
triple
values
reconstruction
errors
obtained
using
settings
pooled
fig
four
rows
correspond
binomial
normal
uniform
exponential
data
respectively
subﬁgures
row
correspond
different
values
i.e.
sparsity
level
subﬁgure
horizontal
axis
depicts
order
gds
vertical
axis
number
projected
dimensions
mse
corresponds
every
pair
indicated
colourbar
right
fig
performing
row-wise
i.e.
sparsity
oriented
comparison
interesting
observe
regardless
data
type
greater
i.e.
less
sparsity
original
vector
larger
mse
becomes
general
behaviour
clearly
veriﬁes
importance
sparsity
signal
reconstruction
similarly
performing
column-wise
i.e.
data
type
oriented
comparison
clear
regardless
reconstruction
error
decreases
move
top
binomial
data
bottom
exponential
data
attributed
fact
speciﬁc
although
four
cases
use
equal
number
non-zeros
fact
gds
tends
consider
sparse
vectors
whose
sorted
absolute
coefﬁcients
larger
differences
fig
illustrates
general
form
arbitrary
vector
generated
using
either
four
distributions
case
indicatively
ﬁrst
order
gds
sparsities
prototypic
vectors
approximately
0.60
0.73
0.76
0.79
respectively
apparently
terms
gds
exponential
distribution
gives
sparsest
vectors
binomial
uniform
normal
exponential
k=10
k=20
k=30
k=40
k=50
k=60
0.1
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
fig
reconstruction
error
using
various
values
order
gds
number
reduced
dimensions
different
values
number
non-zero
coefﬁcients
original
data
horizontal
axis
vertical
axis
colourbar
mse
thus
eligible
reconstruction
using
adopted
methodology
order
perform
almost
perfect
reconstruction
sparse
signals
closer
inspection
subﬁgure
observe
regardless
data
type
reconstruction
error
similar
form
speciﬁcally
clear
almost
cases
except
binomial
uniform
values
range
provide
best
results
becomes
evident
small
values
direction
next
concern
quantify
optimal
varies
function
fig
serve
exactly
purpose
fig
horizontal
axis
depicts
vertical
axis
contains
mean
optimal
calculated
across
different
values
ﬁgure
clear
small
approximately
best
reconstruction
obtained
setting
moreover
worth
noticing
small
reduction
mean
optimal
intermediate
values
i.e.
interval
superiority
high
orders
becomes
intensely
evident
large
values
i.e.
however
must
pointed
large
difference
reconstruction
error
among
several
values
becomes
negligible
mse
becomes
almost
zero
finally
worth
noticing
recall
obtained
never
optimal
choice
reconstruction
justifying
use
higher
orders
proving
superiority
gds
summary
ﬁndings
explicitly
demonstrate
gds
high
orders
reduces
least
number
projected
dimensions
required
similar
case
investigate
optimal
function
number
non-zero
coefﬁcients
fig
horizontal
axis
depicts
vertical
axis
mean
optimal
calculated
time
across
different
values
interesting
observe
never
offers
best
reconstruction
performance
instead
every
orders
gds
larger
needed
particular
sparsity
data
decreases
larger
values
order
required
better
reconstructing
signal
average
provides
best
results
outcome
attributed
strictness
provides
gds
theorems
section
explicitly
demonstrates
gds
loosens
bounds
assumed
sparsity
original
data
offering
capacity
reconstructing
lowly
sparse
signals
summarising
results
gds
undoubtedly
substitute
sparse
signal
reconstruction
inference
conjunction
proven
prevalence
among
top
performing
sparsity
metrics
induces
superiority
gds
state-of-the-art
closing
section
important
stress
previous
experimental
study
offers
rule
thumb
deciding
optimal
order
gds
certain
compression-
reconstruction
scenario
sense
fig
actually
serves
look-up
table
indicating
appropriate
depending
type
sparsity
original
data
well
target
compression
level
following
comprises
concrete
exam-
binomial
uniform
normal
exponential
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
100
100
100
100
fig
general
form
absolute
coefﬁcients
vectors
generated
using
binomial
uniform
normal
exponential
distributions
non-zeros
sorted
ascending
order
binomial
uniform
normal
exponential
100
100
100
100
number
projected
dimensions
fig
optimal
order
gds
function
number
projected
dimensions
binomial
uniform
normal
exponential
number
non−zeros
fig
optimal
order
gds
function
number
non-zero
coefﬁcients
ple
demonstrating
rule
thumb
could
work
consider
100-dimensional
signal
containing
zeros
whose
non-zero
coefﬁcients
generated
either
different
normal
distributions
note
latter
case
coefﬁcients
normalised
using
approach
presented
section
vii
also
consider
would
like
compress
signal
dimensions
based
settings
refer
subﬁgure
lying
third
row
i.e.
normal
second
column
i.e.
fig
subﬁgure
taking
horizontal
cross
section
value
y-axis
i.e.
target
reduced
dimension
ﬁnd
least
reconstruction
error
using
colourbar
right
obviously
error
corresponds
optimal
settings
speciﬁc
example
approximately
conclusions
−ci
proof
let
sorted
coefﬁcients
obtained
robin
hood
operation
main
contribution
paper
generalised
differ-
ential
sparsity
gds
framework
generating
novel
sparsity
metrics
proposed
framework
accumulates
number
advantages
first
characterised
ﬂexibility
generate
metrics
well-tailored
speciﬁc
problem
data
requirements
second
shown
contrast
sparsity
metrics
gds
satisﬁes
set
benchmark
crite-
ria
proving
credibility
effective
metric
measuring
signal
sparsity
third
proven
constitutes
speciﬁc
case
gds
demonstrating
generalisation
power
latter
unify
already
existing
metrics
fourth
calculated
using
alternative
formulas
complementary
computational
advantages
therefore
allowing
efﬁcient
calculation
different
settings
features
offer
gds
great
potential
general
purpose
framework
regardless
domain
used
matter
fact
paper
potential
demonstrated
within
context
compressive
sampling
process
reconstructing
signals
heavily
com-
pressed
using
random
projections
along
lines
extensive
experimental
study
synthetic
data
whose
coefﬁcients
generated
using
binomial
uniform
normal
exponential
distributions
gds
proven
effective
measuring
sparsity
terms
signal
reconstruc-
tion
capability
speciﬁcally
gds
comparison
loosens
assumptions
least
number
projected
dimensions
inherent
sparsity
original
data
required
order
almost
perfectly
reconstruct
compressed
signal
superiority
gds
conjunction
fact
latter
categorically
outperformed
state-of-the-art
sparsity
metrics
signal
reconstruction
places
gds
pole
position
sparsity
metric
literature
near
future
plan
replicate
experimental
study
presented
paper
real
data
instead
synthetic
extend
impact
gds
types
data
since
often
coefﬁcients
real
data
approximated
modelled
four
distributions
used
paper
envisage
results
similar
ones
presented
moreover
occupies
leading
position
reconstruction
signals
contaminated
noise
also
intend
investigate
effectiveness
gds
problem
towards
direction
anticipate
privilege
gds
contain
adjusting
parameter
potential
offer
appropriate
robustness
view
noise
finally
although
signal
reconstruction
extended
ﬁeld
adapting
gds
case
studies
well
reinforce
potential
end
based
proven
contribution
sparsity
optimisation
problems
plan
investigate
could
gds
applied
multi-objective
optimisation
emerging
ﬁeld
many
applications
appendix
proof
criterion
theorem
10.
gds
satisﬁes
robin
hood
i.e
ck+1
ck−1

clarity
see
also
table
clearly
proving
equivalent
proving
∂sp
expanding
eliminating
terms
contain
inequality
reduces
di+m
di+m
i+m−1xk=1
j−n−1xk=1
i+mxk=1
nxk=j−n
k6=i
k6=j
nxk=i+m+1
nxk=j−n+1
j−n−1xk=i+m+1
i+mxk=1
k6=i
dj−n
dj−n
k6=j
nxk=j−n
j−n−1xk=i+m+1
i+mxk=1
j−n−1xk=i+m+1
nxk=j−n
k6=i
k6=j
hence
table
correspondence
indices
robin
hood
operation
di−1
di+m−1
di+m
di+m+1
dj−n−1
dj−n
dj−n+1
dj+1
ci−1
ci+1
ci+m
ci+m+1
cj−n−1
cj−n
cj−1
cj+1
k6=i
p−1
p−1
i+mxk=1
j−n−1xk=i+m+1
p−1
p−1
p−1
p−1
nxk=j−n
k6=j
p−1
computationally
efﬁcient
gds
formula
even
values
theorem
11.
s2k
kck2k
k−1xω=1
cid:18
cid:19
kckω
cid:18
ωkck2k−ω
2k−ω
cid:19
kck2k
proof
deﬁnition
gds
see
s2k
kck2k
p−1
p−1
i+mxk=1
k6=i
similarly
p−1−
p−1
nxk=j−n
k6=j
moreover
positive
implies
j−n−1xk=i+m+1
p−1+
p−1
finally
obviously
−4p
p−1
hence
terms
completes
proof
negative
therefore
c2k
c2k
c2k−ω
c2k−ω
c2k−ω
c2k−ω
c2k−ω
c2k
−1xi=1
nxj=i+1
−1xi=1
nxj=i+1
cid:18
cid:19
2kxω=0
−1xi=1
nxj=i+1
nxj=i+1
c2k
cid:18
cid:19
k−1xω=1
−1xi=1
cid:18
cid:18
cid:19
cid:19
2k−1xω=k+1
−1xi=1
−1xi=1
nxj=i+1
nxj=i+1
k−1xω=1
cid:18
cid:19
nxj=i+1
cid:18
cid:19
−1xi=1
k−1xω=1
cid:18
cid:19
nxi=1
nxi=1
nxi=1
nxi=1
nxi=1
k−1xω=1
cid:18
cid:19
nxi=1
nxi=1
−1xi=1
nxi=1
nxi=1
nxj=1
cid:19
nxi=1
cid:18
nxi=1
−1xi=1
nxj=i+1
cid:19
cid:18
nxj=i+1
nxj=1
c2k
c2k
c2k−ω
c2k−ω
c2k−ω
c2k
c2k
c2k
c2k
c2k
c2k
c2k
c2k
c2k−ω
nxj=1
nxi=1
cid:18
k−1xω=1
cid:19
nxi=1
cid:18
cid:18
cid:19
k−1xω=1
cid:18
cid:19
kckω
k−1xω=1
cid:19
nxi=1
nxj=1
cid:19
nxi=1
cid:18
cid:18
ωkck2k−ω
2k−ω+
2k+
c2k
cid:19
kck2k
kck2k
since
notice
proof
made
use
identity
cid:0
cid:1
cid:0
s2k
cid:18
cid:19
cid:18
cid:19
k−1xω=1
cid:18
cid:19
kxω=0
2k−ω
cid:1
therefore
k−1xω=1
cid:18
cid:19
kckω
cid:18
cid:19
kck2k
kck2k
kck2k
cid:18
cid:19
ωkck2k−ω
2k−ω
2k+1
−1xi=1
nxj=i+1
cid:18
cid:19
c2k+1−ω
2k+1xω=0
−1xi=1
nxj=i+1
cid:18
cid:19
c2k
nxj=i+1hc2k+1
−1xi=1
cid:18
cid:19
cid:18
cid:19
cjc2k
−1xi=1
nxj=i+1
cid:18
cid:19
−1xi=1
nxj=i+1
cid:18
cid:19
−1xi=1
nxj=i+1
cjc2k
c2k+1
c2k+1
c2k+1
ck+1
c2k
c2k+1
ck+1
kck2k+1
2k+1
c2k+1
nxi=1
cid:18
cid:19
nxi=1
cid:0
c2k
cid:18
cid:19
nxi=1
cid:0
ck+1
nxi=1
ixj=1
nxi=1
nxi=1
nxi=1
f2k+1
c2k+1
c2k+1
nxi=1
cif2k
cid:1
fk+1
cid:1
c2k+1
ic2k+1
computationally
efﬁcient
gds
formula
odd
values
theorem
12.
s2k+1
kck2k+1
2k+1
cid:18
kxω=0
cid:19
nxi=1
cid:0
c2k+1−ω
ixj=1
f2k+1−ω
cid:1
proof
deﬁnition
gds
see
s2k+1
kck2k+1
2k+1
kck2k+1
2k+1
therefore
becomes
nxi=1
f2k+1
ic2k+1
nxi=1
c2k+1
c2k+1−ω
f2k+1
nxi=1
nxi=1
cid:18
cid:19
nxi=1
kxω=0
nxi=1
nxi=1
kxω=0
cid:18
cid:19
nxi=1
cid:0
c2k+1−ω
kxω=0
cid:18
cid:19
nxi=1
cid:0
c2k+1−ω
c2k+1
ic2k+1
nxi=1
f2k+1−ω
ic2k+1
f2k+1−ω
cid:1
f2k+1−ω
cid:1
kreutz-delgado
rao
measures
algorithms
best
basis
selection
acoustics
speech
signal
processing
1998.
proceedings
1998
ieee
international
conference
vol
ieee
1998
1881–1884
hurley
rickard
curran
drakakis
maximizing
sparsity
wavelet
representations
via
parameterized
lifting
digital
signal
processing
2007
15th
international
conference
ieee
2007
631–634
rickard
fallon
gini
index
speech
proceedings
38th
conference
information
science
systems
ciss04
2004
spall
stochastic
optimization
simultaneous
perturbation
method
proceedings
31st
conference
winter
simulation
simulation—a
bridge
future-volume
acm
1999
101–109
sadegh
spall
optimal
random
perturbations
stochastic
approximation
using
simultaneous
perturbation
gradient
approxima-
tion
automatic
control
ieee
transactions
vol
1480–1484
1998
maryak
chin
global
random
optimization
simul-
taneous
perturbation
stochastic
approximation
american
control
conference
2001.
proceedings
2001
vol
ieee
2001
756–762
references
hamilton
discussions
philosophy
literature
education
university
reform
harper
1855
cand
al.
compressive
sampling
proceedings
international
congress
mathematicians
vol
madrid
spain
2006
1433–1452
baraniuk
davenport
duarte
hegde
introduction
compressive
sensing
connexions
e-textbook
2011
cand
wakin
introduction
compressive
sam-
pling
signal
processing
magazine
ieee
vol
21–30
2008
candes
tao
dantzig
selector
statistical
estimation
much
larger
annals
statistics
2313–2351
2007
meinshausen
lasso-type
recovery
sparse
representa-
tions
high-dimensional
data
annals
statistics
246–270
2009
tipping
sparse
bayesian
learning
relevance
vector
machine
journal
machine
learning
research
vol
211–
244
2001
cotter
shalev-shwartz
srebro
learning
optimally
sparse
30th
international
support
vector
machines
proceedings
conference
machine
learning
icml-13
2013
266–274
xiang
ramadge
schapire
speed
sparsity
regularized
boosting
international
conference
artiﬁcial
intelligence
statistics
2009
615–622
chennubhotla
jepson
sparse
pca
extracting
multi-scale
structure
data
computer
vision
2001.
iccv
2001.
proceed-
ings
eighth
ieee
international
conference
vol
ieee
2001
641–647
hurley
rickard
comparing
measures
sparsity
informa-
tion
theory
ieee
transactions
vol
4723–4741
2009
gini
measurement
inequality
incomes
economic
jour-
nal
124–126
1921
zonoobi
kassim
venkatesh
al.
gini
index
sparsity
measure
signal
reconstruction
compressive
samples
selected
topics
signal
processing
ieee
journal
vol
927–932
2011
pastor
mora-jim´enez
j¨antti
caamano
sparsity-
based
criteria
entropy
measures
wireless
communication
systems
iswcs
2013
proceedings
tenth
international
symposium
vde
2013
1–5
dalton
measurement
inequality
incomes
economic
journal
348–361
1920
karvanen
cichocki
measuring
sparseness
noisy
signals
4th
international
symposium
independent
component
analysis
blind
signal
separation
2003
125–130
elad
sparse
redundant
applications
theory
mathematics
http
//opac.inria.fr/record=b1133206
signal
new
york
springer
2010.
image
representations
ser
online
available
processing
fuchs
recovery
exact
sparse
representations
presence
bounded
noise
information
theory
ieee
transactions
vol
3601–3608
2005
donoho
elad
temlyakov
stable
recovery
sparse
overcomplete
representations
presence
noise
information
theory
ieee
transactions
vol
6–18
2006
rath
guillemot
j.-j
fuchs
sparse
approximations
joint
source-channel
coding
multimedia
signal
processing
2008
ieee
10th
workshop
ieee
2008
481–485
candes
tao
decoding
linear
programming
information
theory
ieee
transactions
vol
4203–4215
2005
donoho
tsaig
fast
solution
of-norm
minimization
problems
solution
may
sparse
information
theory
ieee
transactions
vol
4789–4812
2008
candes
wakin
boyd
enhancing
sparsity
reweighted
minimization
journal
fourier
analysis
applications
vol
5-6
877–905
2008
hoyer
non-negative
matrix
factorization
sparseness
con-
straints
journal
machine
learning
research
vol
1457–
1469
2004
olshausen
field
sparse
coding
sensory
inputs
current
opinion
neurobiology
vol
481–487
2004
rao
kreutz-delgado
afﬁne
scaling
methodology
best
basis
selection
signal
processing
ieee
transactions
vol
187–200
1999
