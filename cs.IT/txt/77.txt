cartesian
line
sampling
anisotropic
total
variation
regularization
clarice
poon
october
2018
abstract
paper
considers
use
anisotropic
total
variation
seminorm
recover
two
dimensional
vector
cn×n
partial
fourier
coeﬃcients
sampled
along
cartesian
lines
prove
xk−1
nonzero
coeﬃcients
column
j−1
nonzero
coeﬃcients
row
multi-
plication
log
factors
one
exactly
recover
sampling
along
horizontal
lines
fourier
coeﬃcients
along
vertical
lines
fourier
coeﬃcients
finally
unlike
standard
compressed
sensing
estimates
log
factors
involved
dependent
sep-
aration
distance
nonzero
entries
row/column
gradient
ambient
dimension
introduction
research
compressed
sensing
resulted
several
examples
one
recover
s-sparse
vector
length
log
randomly
chosen
linear
measurements
one
ﬁrst
examples
cand
romberg
tao
recovery
gradient
sparse
vector
samples
fourier
transform
means
solving
total
variation
regularization
problem
perhaps
one
well
known
inﬂuential
results
compressed
sensing
links
applications
particular
result
motivated
use
total
variation
regularization
reduce
sampling
cardinality
many
imaging
applications
electron
microscopy
magnetic
resonance
imaging
mri
optical
deﬂectometric
tomography
phase-contrast
tomography
radio
interferometry
however
studies
uniformly
random
sampling
provide
insight
total
variation
regularization
allow
one
subsample
fourier
transform
two
aspects
one
consider
dense
sampling
low
frequencies
sparsity
structure
observed
one
obtain
far
superior
results
via
variable
density
sampling
one
samples
densely
low
frequencies
eﬀect
demonstrated
figure
compare
reconstruction
boat
test
image
12.3
fourier
coeﬃcients
via
diﬀerent
sampling
maps
theoretical
side
one
particular
type
variable
density
sampling
ﬁrst
studied
krahmer
ward
later
analysis
showed
compared
sampling
uniformly
random
one
advantages
oﬀered
sampling
densely
low
frequencies
improved
robustness
inexact
sparsity
noise
however
important
reason
eﬀectiveness
sampling
densely
low
frequencies
although
∗ceremade
universit´e
paris-dauphine
†email
cmhsp2
cam.ac.uk
sampling
cardinality
log
optimal
recovery
s-sparse
vectors
one
reduce
sampling
cardinality
placing
structure
assumption
vector
recovered
observation
made
cand
fernandez-granda
context
recovering
superposition
diracs
super-resolution
case
total
variation
regularization
one
dimensional
signals
exploiting
results
following
result
proved
theorem
1.1.
let
let
let
n/4
10.
let
discrete
fourier
transform
deﬁned
section
1.3
let
cardinality
suppose
min
j∈∆
cid:54
let
cid:48
cid:48
consist
indices
chosen
uniformly
random
cid:38
max
log2
log
cid:26
cid:18
cid:19
cid:17
log
cid:16
cid:18
cid:19
cid:27
probability
exceeding
given
pωax
cid:107
cid:107
solution
satisﬁes
cid:107
cid:107
subject
cid:107
pωax
cid:107
cid:1
cid:0
cid:107
p∆cdx
cid:107
min
x∈cn
cid:107
cid:107
cid:46
1.1
error
bound
holds
probability
s-gradient
sparse
minimum
separation
2/s
exactly
recovered
fourier
coeﬃcients
however
random
sampling
guarantees
recovery
log
samples
thus
one
reduce
number
samples
required
choosing
samples
accordance
underlying
sparsity
structure
need
understand
realistic
sampling
patterns
sampling
applications
mri
constrained
sampling
along
smooth
trajectories
radial
lines
spirals
cartesian
lines
hand
majority
results
compressed
sensing
describe
eﬀects
pointwise
sampling
knowledge
theoretical
result
direction
boyer
consider
use
wavelet
regularization
sampling
along
horizontal
vertical
cartesian
lines
fourier
domain
1.1
paper
contribution
purpose
paper
present
two
dimensional
version
theorem
1.1
consider
one
eﬃciently
sample
fourier
transform
along
cartesian
lines
taking
account
sparsity
structure
gradient
underlying
vector
cartesian
sampling
pattern
studied
paper
one
sampling
patterns
empirically
studied
application
compressed
sensing
mri
thus
result
paper
provides
justiﬁcation
insight
use
compressed
sensing
mri
main
result
paper
presented
discussed
section
proof
presented
section
rel
err
28.9
rel
err
6.6
rel
err
7.6
figure
top
row
shows
three
diﬀerent
sampling
maps
covering
12.3
fourier
coeﬃcients
note
zeroth
fourier
frequency
corresponds
centre
map
bottom
row
shows
corresponding
reconstructions
1.2
related
works
wavelet
regularization
link
success
dense
sampling
low
frequencies
correspondence
sampling
patterns
underlying
sparsity
structure
previously
investigated
context
orthogonal
wavelet
regularization
fourier
sampling
adcock
context
wavelet
regularization
relevant
sparsity
structure
sparsity
underlying
wavelet
coeﬃcients
within
wavelet
scale
results
provide
link
distribution
fourier
samples
wavelet
sparsity
scale
result
demonstrate
one
recover
ﬁrst
wavelet
coeﬃcients
lowest
scales
fourier
coeﬃcients
lowest
frequencies
cid:96
wavelet
regularization
thus
notion
cid:96
regularization
allow
sampling
rates
without
log
factor
relevant
greater
generality
boyer
investigated
structure
dependency
case
wavelet
regularization
cartesian
line
sampling
fourier
domain
particular
proved
one
guarantee
stable
error
bounds
provided
number
horizontal
lines
within
block
fourier
coeﬃcients
proportional
log
factors
sparsity
column
wavelet
transform
within
corresponding
wavelet
scale
1.3
notation
let
let
cid:100
n/2
cid:101
cid:98
n/2
cid:99
let
discrete
fourier
transform
zje−2πikj/n
j=1
cid:88
j=1
let
ﬁnite
diﬀerences
operator
deﬁned
j=1
given
unif
means
consists
zj−1
elements
drawn
uniformly
random
without
replacement
given
operator
column
vector
whenever
given
cn×n
let
col
denote
jth
column
let
let
cn×n
cn×n
column
vector
row
vector
whenever
row
vector
row
denote
kth
row
cid:32
cid:88
cid:88
cid:33
˜az
zk1
k2e−2πi
k1n1+k2n2
let
˜d1
cn×n
cn×n
˜d2
cn×n
cn×n
k1=1
k2=1
n2∈
˜d1z
zk−1
j=1
˜d2z
j−1
j=1
cid:13
cid:13
cid:13
˜dz
cid:13
cid:13
cid:13
cid:114
cid:13
cid:13
cid:13
˜d1z
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜dz
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d1z
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d2z
cid:13
cid:13
cid:13
let
˜dz
˜d1z
˜d2z
given
let
˜pω
cn×n
cn×n
cid:13
cid:13
cid:13
˜d2z
cid:13
cid:13
cid:13
cid:40
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d2z
cid:13
cid:13
cid:13
˜p∆2
˜pωz
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d1z
cid:13
cid:13
cid:13
˜p∆1
˜d1z
cid:13
cid:13
cid:13
cid:54
cn×n
cid:13
cid:13
cid:13
˜d2z
cn×n
let
cid:107
cid:107
denote
anisotropic
total
variation
norm
cid:107
cid:107
given
let
cid:107
cid:107
given
write
cid:46
exists
constant
independent
variables
consideration
key
concepts
theorem
1.1
sparsity
structure
considered
separation
discontinuities
underlying
signal
considering
recovery
vector
cn×n
sampling
along
cartesian
lines
fourier
transform
main
result
demonstrate
one
subsample
depends
sparsity
minimum
separation
distance
within
column
˜d1x
row
˜d2x
ﬁrst
present
three
deﬁnitions
main
result
depend
deﬁnition
2.1
sparsity
let
column
cardinality
row
cardinality
max
j=1
max
k=1
deﬁnition
2.2
minimum
separation
distance
let
let
minimum
separation
distance
rows
deﬁned
νrow
cid:54
minimum
separation
distance
columns
deﬁned
νcol
cid:54
deﬁnition
2.3.
say
distinct
column
supports
min
min
n=1
min
n=1
cid:26
cid:26
cid:12
cid:12
cid:8
k=1
cid:9
cid:12
cid:12
j=1
cid:9
cid:12
cid:12
cid:12
cid:12
cid:8
min
say
distinct
row
supports
cid:27
cid:27
main
theorem
let
cn×n
let
suppose
˜p∆1
sgn
˜d1x
distinct
column
supports
minimum
separation
2/m1
along
columns
˜p∆2sgn
˜d2x
distinct
supports
minimum
separation
2/m2
along
rows
suppose
column
cardinality
row
cardinality
assume
also
log
tisi/
log
timi/
theorem
3.1.
let
let
let
|ω|
unif
cid:38
log
t1s1/
log
t1m1/
let
pωx
cid:107
cid:107
unif
cid:38
log
t2s2/
log
t2m2/
suppose
minimizer
cid:107
cid:107
subject
min
z∈cn×n
cid:13
cid:13
cid:13
˜pω
˜az
cid:13
cid:13
cid:13
3.1
probability
exceeding
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:16
m0n
−1/2√
cid:107
cid:107
cid:107
cid:107
cid:46
max
min
min
3.2
3.3
hold
probability
one
m/m0
1/2√
cid:107
cid:107
cid:46
cid:16
cid:17
cid:17
3.2
3.3
samp
map
recov
rel
err
samp
map
recov
rel
err
49.78
figure
test
image
500×500
perfectly
recovered
1.2
fourier
coeﬃcients
indexed
cartesian
lines
passing
low
frequencies
reconstruction
obtained
sampling
1.2
fourier
coeﬃcients
uniformly
random
shown
3.1
remarks
main
result
removal
log
factor
suppose
cn×n
support
col
consists
lines
minimum
separation
1/m1
support
row
consists
lines
minimum
separation
1/m2
one
perfectly
recover
sampling
fourier
transform
along
horizontal
lines
vertical
lines
hand
case
sparsity
one
guaranteed
exact
recovery
sampling
uniformly
random
one
observes
log
samples
figure
illustrates
eﬀect
showing
recovery
image
1.2
fourier
coeﬃcients
test
image
perfect
reconstructed
sampling
low
frequency
cartesian
lines
hand
sampling
1.2
fourier
coeﬃcients
uniformly
random
yields
poor
reconstruction
note
image
fact
recovered
fourier
coeﬃcients
drawn
uniformly
random
simply
one
sample
less
considering
sparsity
structure
test
image
importance
structure
dependency
note
theorem
3.1
range
one
sample
dependent
minimum
separation
discontinuities
corresponding
direction
number
samples
one
draw
log
factors
dependent
maximum
sparsity
row
column
corresponding
direction
important
consider
structure
dependency
devising
sampling
scheme
see
figure
proof
standard
compressed
sensing
proof
theorem
3.1
consists
showing
existence
dual
certiﬁcate
following
arguments
one
show
given
cn×n
supp
˜d1x
supp
˜d2x
unique
solution
3.1
following
two
conditions
hold
exists
dual
certiﬁcate
ran
˜a∗
˜pω
cid:107
cid:107
˜p∆iρi
sgn
˜dix
˜pω
˜p∆i
injective
samp
map
recov
rel
err
samp
map
recov
rel
err
44.8
figure
ﬁgure
shows
two
sampling
maps
corresponding
reconstructions
256×256
sampling
maps
cover
5.4
fourier
coeﬃcients
sampling
map
constructed
sampling
along
lines
uniformly
random
horizontal
direction
along
lowest
frequency
lines
vertical
direction
sampling
map
ob-
tained
manner
sampled
opposite
orientations
random
lines
vertical
direction
lowest
frequency
lines
horizontal
direction
however
instead
directly
showing
existence
one
dual
certiﬁcate
deﬁned
cn×n
exploit
fact
given
fourier
samples
along
cartesian
lines
apply
proposition
4.2
show
suﬃces
prove
existence
sequence
one
dimensional
certiﬁcates
deﬁned
lemma
4.1.
let
let
cn×n
cid:13
cid:13
cid:13
˜az
cid:13
cid:13
cid:13
pωaz
row
cid:13
cid:13
cid:13
cid:88
k=1
cid:13
cid:13
cid:13
cid:33
cid:13
cid:13
cid:13
˜pω×
cid:13
cid:13
cid:13
˜az
cid:88
k=1
proof
ﬁrst
note
cid:13
cid:13
cid:13
pωaz
col
cid:13
cid:13
cid:13
cid:32
cid:88
cid:88
cid:88
k2=1
k1=1
˜az
zk1
e−2πik1n1/n
e−2πik2n2/n
col
n1e−2πik2n2/n
k2=1
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜pω×
cid:0
col
cid:1
cid:12
cid:12
cid:12
˜az
cid:12
cid:12
cid:12
cid:88
cid:88
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
cid:88
cid:13
cid:13
cid:13
pωaz
col
cid:13
cid:13
cid:13
cid:88
n2∈
n1∈ω
n1∈ω
˜az
n1∈ω
k=1
k=1
using
identity
cid:88
cid:88
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
n1∈ω
n2∈
cid:12
cid:12
cid:12
cid:88
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
col
k=1
n1∈ω
cid:12
cid:12
cid:12
4.1
applied
second
line
fact
n−1/2a
unitary
finally
symmetric
argument
cid:13
cid:13
cid:13
˜az
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
pωaz
row
cid:13
cid:13
cid:13
cid:88
k=1
proposition
4.2
dual
certiﬁcates
let
cn×n
let
let
let
|ω1|
|ω2|
let
let
|ω|
let
pωx
cid:107
cid:107
cid:107
cid:107
subject
min
z∈cn×n
cid:13
cid:13
cid:13
˜pω
˜az
cid:13
cid:13
cid:13
suppose
minimizer
4.2
let
let
maxn
following
conditions
hold
j=1
|∆n
assume
−1/2
inf
supp
=∆1
cid:107
cid:107
2=1
cid:107
pω1ax
cid:107
−1/2
inf
supp
=∆2
cid:107
cid:107
2=1
cid:107
pω2ax
cid:107
iii
exists
p∆1
p∆1
sgn
˜d1x
col
exists
p∆2
p∆2
sgn
˜d2x
col
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
−1/2
a∗pω1
cid:88
cid:107
cid:107
j=1
−1/2
a∗pω2
cid:88
cid:107
cid:107
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:107
cid:107
cid:46
j=1
cid:46
+c2
cid:107
cid:107
cid:114
cid:107
cid:107
max
min
m0n
−1/2m1/2
c−1
c−1
−1n−1/2
c−1
similarly
follows
assumption
cn×n
proof
proposition
4.2.
first
suppose
cn×n
applying
assumption
lemma
4.1
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜p∆1z
cid:88
m1n
cid:13
cid:13
cid:13
˜pω1×
cid:18
m1n
j=1
max
cid:54
∈∆1
note
thus
follows
j=1
˜az
˜aej
m−1
˜p∆1z
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜pω1×
cid:13
cid:13
cid:13
p∆1
col
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
pω1ap∆1
col
cid:13
cid:13
cid:13
cid:88
cid:13
cid:13
cid:13
˜pω1×
m1n
cid:16
cid:13
cid:13
cid:13
˜pω1×
cid:13
cid:13
cid:13
cid:19
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜pω1×
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜pω1×
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜pω1×
m1n
−1/2
max
cid:54
∈∆1
˜ax
˜aˆx
˜aej
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
×ω2
cid:13
cid:13
cid:13
˜az
cid:17
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜az
4.3
4.4
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜p∆1z
cid:13
cid:13
cid:13
˜p∆2z
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜pω1×
cid:13
cid:13
cid:13
˜pω×
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d2h
˜d1h
˜ah
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜p∆1
cid:13
cid:13
cid:13
˜p∆2
cid:13
cid:13
cid:13
p∆1
cid:13
cid:13
cid:13
˜d1
cid:104
p∆1
cid:13
cid:13
cid:13
˜pω1×
cid:13
cid:13
cid:13
˜pω1×
cid:13
cid:13
cid:13
˜d1
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
p∆1
˜d1x
let
observe
since
satisfy
constraint
4.2
note
also
˜d1h
e−2πik/n
˜ah
therefore
˜d1h
apply
bounds
4.3
4.4
obtain
cid:13
cid:13
cid:13
˜d2h
e−2πij/n
˜ah
similarly
˜d2h
cid:13
cid:13
cid:13
˜ah
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
×ω2
4.5
proceed
derive
upper
bounds
˜d1h
h¨older
in-
equality
triangle
inequality
˜d1h
˜d2h
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d2h
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d1x
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d1
˜d1h
sgn
˜d1x
cid:105
˜d1h
proceed
bound
existence
following
bound
rearranging
terms
yields
cid:13
cid:13
cid:13
similarly
cid:13
cid:13
cid:13
since
minimizer
4.2
4.6
˜d2x
˜d1x
˜d2h
˜d1h
˜d2h
˜d1h
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:107
cid:107
cid:104
p∆1
col
sgn
col
cid:105
−1/2
a∗pω1wj
assumption
iii
˜d1h
sgn
˜d1x
cid:105
cid:12
cid:12
cid:12
˜d2h
sgn
˜d2x
cid:105
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:104
p∆1
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d1x
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d1
cid:13
cid:13
cid:13
cid:12
cid:12
cid:12
cid:104
p∆2
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d2x
cid:13
cid:13
cid:13
˜d2
cid:13
cid:13
cid:13
˜d2x
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d1x
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d2
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d1
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜d2h
sgn
˜d2x
cid:105
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:104
p∆2
˜d1h
sgn
˜d1x
cid:105
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:104
p∆1
˜d1h
sgn
˜d1x
cid:105
cid:12
cid:12
cid:12
let
˜d1x
let
˜d1h
using
cid:12
cid:12
cid:12
cid:104
p∆1
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:104
˜p∆1z
sgn
cid:105
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:13
cid:13
cid:13
col
cid:13
cid:13
cid:13
cid:88
cid:118
cid:117
cid:117
cid:116
cid:88
cid:118
cid:117
cid:117
cid:116
cid:88
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
pω1az
col
cid:13
cid:13
cid:118
cid:117
cid:117
cid:116
cid:88
m1n
−1/2
cid:13
cid:13
cid:13
˜pω1×
cid:13
cid:13
pω1az
col
cid:13
cid:13
cid:13
cid:13
cid:13
m1n
−1/2
cid:13
cid:13
cid:13
˜pω1×
cid:12
cid:12
cid:12
cid:104
˜p∆1
˜d1h
sgn
˜d1x
cid:105
cid:12
cid:12
cid:12
m1n
−1/2m1/2δ
cid:13
cid:13
cid:13
cid:12
cid:12
cid:12
cid:104
˜p∆2
˜d2h
sgn
˜d2x
cid:105
cid:12
cid:12
cid:12
m2n
−1/2m1/2δ
cid:13
cid:13
cid:13
cid:104
p∆1
col
sgn
col
cid:105
cid:104
col
cid:105
cid:104
m1n
−1/2m1/2δ
pω1az
col
cid:105
cid:13
cid:13
cid:13
col
cid:105
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:107
cid:107
−1/2
m−1
m−1
˜adh
˜d1h
˜d2h
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:104
˜az
j=1
j=1
j=1
j=1
j=1
j=1
j=1
applied
cauchy-schwarz
inequality
obtain
last
line
recall
lemma
4.1
hence
follows
similar
argument
also
yields
plugging
two
estimates
back
4.6
rearranging
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:16
˜d1h
cid:13
cid:13
cid:13
˜d2h
4ln−1/2m1/2
−1/2
4.7
combining
estimate
4.5
yields
required
bound
derive
bound
cid:107
cid:107
ﬁrst
let
˜d1
ˆx−
note
cauchy-schwarz
inequality
j=1
−1/2
cid:88
cid:107
cid:107
cid:17
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
p∆1
col
cid:13
cid:13
cid:13
cid:16
cid:13
cid:13
cid:13
pω1
col
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
pω1
cid:13
cid:13
cid:13
cid:88
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
max
max
cid:54
∈∆1
j=1
cid:107
pω1
ael
cid:107
cid:13
cid:13
cid:13
j=1
col
cid:13
cid:13
cid:13
cid:17
col
cid:13
cid:13
cid:13
applying
condition
upper
bounded
j=1
cid:13
cid:13
cid:13
p∆1
col
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜p∆1z
cid:88
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜p∆1
cid:13
cid:13
cid:13
pω1ap∆1
col
cid:13
cid:13
cid:13
cid:88
cid:118
cid:117
cid:117
cid:116
cid:88
cid:13
cid:13
pω1
col
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜pω1×
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
˜az
j=1
j=1
cid:88
j=1
symmetric
argument
cid:13
cid:13
cid:13
˜p∆2z
therefore
combining
bound
4.7
cid:107
cid:107
cid:46
sm√
cid:19
cid:18
cid:107
cid:107
−1n−1/2
finally
recall
max
min
c−1
due
poincar´e
inequality
zero
mean
image
cn×n
satisﬁes
cid:12
cid:12
cid:12
cid:80
cid:12
cid:12
cid:12
cid:107
cid:107
cid:107
cid:107
since
4.8
triangle
inequality
yields
letting
cid:80
4.8
hence
conclusion
follows
cid:107
cid:107
cid:107
cid:107
proof
theorem
3.1.
prove
theorem
simply
need
show
conditions
proposition
4.2
hold
high
probability
note
conditions
studied
recall
lemmas
4.25
given
minimum
separation
distance
1/m
consists
indices
chosen
uniformly
random
cid:38
max
cid:8
log2
m/
|∆|
log
|∆|
log
m/
cid:9
following
hold
probability
exceeding
cid:107
p∆x
cid:107
m−1/2
cid:107
pωap∆x
cid:107
exists
m−1/2a∗pωw
cid:107
cid:107
cid:46
cid:112
|∆|
cid:13
cid:13
cid:13
cid:13
0.99993
0.92
max
p∆ρ
cid:26
cid:27
furthermore
two
conditions
hold
probability
5/3
cid:46
iii
proposition
4.2
c−1
exceeding
t1
provided
chosen
uniformly
random
therefore
applying
fact
times
applying
union
bound
conditions
hold
probability
|ω1|
cid:38
max
cid:8
log2
m1/
log
s1/
log
m1/
cid:9
4.9
hold
probability
similarly
conditions
proposition
hold
probability
exceeding
t2
4.2
c−1
chosen
uniformly
random
5/3
cid:46
|ω2|
cid:38
max
cid:8
log2
m2/
log
s2/
log
m2/
cid:9
5/3
max
cid:46
max
cid:8
4.10
cid:9
hold
probability
applying
union
bound
conditions
c−1
satisﬁed
probability
exceeding
t1
t2
provided
chosen
uniformly
random
4.9
4.10
hold
conclusion
paper
derived
recovery
guarantees
total
variation
regularized
solutions
given
partial
measurements
fourier
transform
taken
along
cartesian
lines
particular
established
link
sparsity
structure
sampling
pattern
proving
number
cartesian
lines
required
accurate
recovery
dependent
gradient
sparsity
underlying
vector
also
separation
distance
discontinuities
acknowledgements
author
acknowledges
support
fondation
science
mathematique
paris
references
adcock
hansen
poon
roman
breaking
coherence
barrier
new
theory
compressed
sensing
arxiv
preprint
arxiv:1302.0561
2013
boyer
bigot
weiss
compressed
sensing
structured
sparsity
struc-
tured
acquisition
arxiv
preprint
arxiv:1505.01619
2015
cand
fernandez-granda
towards
mathematical
theory
super-resolution
communications
pure
applied
mathematics
:906–956
2014
candes
plan
probabilistic
ripless
theory
compressed
sensing
infor-
mation
theory
ieee
transactions
:7235–7254
2011
cand
romberg
tao
robust
uncertainty
principles
exact
signal
re-
information
theory
ieee
construction
highly
incomplete
frequency
information
transactions
:489–509
2006
cong
yang
wang
diﬀerential
phase-contrast
interior
tomography
physics
medicine
biology
:2905
2012
gonzalez
jacques
vleeschouwer
antoine
compressive
optical
deﬂecto-
metric
tomography
constrained
total-variation
minimization
approach
inverse
problems
imaging
2014
krahmer
ward
stable
robust
sampling
strategies
compressive
imaging
image
processing
ieee
transactions
:612–622
2014
leary
saghi
midgley
holland
compressed
sensing
electron
tomog-
raphy
ultramicroscopy
131:70–91
2013
lustig
donoho
pauly
sparse
mri
application
compressed
sensing
rapid
imaging
magnetic
resonance
medicine
:1182–1195
2007
lustig
donoho
santos
pauly
compressed
sensing
mri
ieee
signal
process
mag.
:72–82
march
2008
poon
consistent
stable
approach
generalized
sampling
journal
fourier
analysis
applications
pages
1–35
2014
poon
role
total
variation
compressed
sensing
siam
journal
imaging
sciences
:682–720
2015
tang
bhaskar
shah
recht
compressive
sensing
grid
communication
control
computing
allerton
2012
50th
annual
allerton
conference
pages
778–785
ieee
2012
wang
liang
ying
pseudo
random
sampling
compressed
sensing
mri
engineering
medicine
biology
society
2009.
embc
2009.
annual
international
conference
ieee
pages
2672–2675
ieee
2009
wiaux
jacques
puy
scaife
vandergheynst
compressed
sensing
imaging
techniques
radio
interferometry
monthly
notices
royal
astronomical
society
395
:1733–1742
2009
