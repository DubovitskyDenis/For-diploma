noise
robustness
simultaneous
orthogonal
matching
pursuit
jean-fran¸cois
determe
j´erˆome
louveaux
laurent
jacques
fran¸cois
horlin
april
2018
ieee
copyright
notice
published
paper
determe
louveaux
jacques
horlin
noise
robustness
simultaneous
orthogonal
matching
pursuit
ieee
transactions
signal
processing
vol
864-875
feb.
2017.
doi
10.1109/tsp.2016.2626244
http
//ieeexplore.ieee.org/document/
7738592/
abstract
paper
joint
support
recovery
several
sparse
signals
whose
supports
exhibit
similarities
examined
sparse
signal
acquired
using
noisy
linear
measure-
ment
process
returns
fewer
observations
dimension
sparse
signals
measurement
noise
assumed
additive
gaussian
admits
diﬀerent
variances
sparse
signal
measured
using
theory
compressed
sensing
perfor-
mance
simultaneous
orthogonal
matching
pursuit
somp
analyzed
envisioned
signal
model
cornerstone
paper
novel
analysis
method
upper
bounding
probability
somp
recovers
least
one
incorrect
entry
joint
support
prescribed
number
iterations
furthermore
probability
somp
failing
investi-
gated
whenever
number
sparse
signals
recovered
simultaneously
increases
tends
inﬁnity
particular
convincing
observations
theoretical
results
suggest
somp
committing
mistake
noiseless
case
guarantee
absence
error
noisy
case
whenever
number
acquired
sparse
signals
scales
inﬁnity
finally
simulation
results
conﬁrm
validity
theoretical
results
introduction
recovery
sparse
signals
i.e.
signals
exhibiting
low
number
non-zero
entries
acquired
noisy
linear
measurements
central
problem
digital
signal
processing
task
involved
number
measurements
lower
dimension
signal
recovered
recently
researchers
paid
special
attention
class
problems
due
emergence
compressed
sensing
ﬁeld
research
aims
providing
reliable
recovery
methods
sparse
signals
number
measurements
low
describing
problem
wish
introduce
key
notions
cardinality
set
denoted
|a|
notation
denotes
set
refers
j-th
jean-fran¸cois
determe
fran¸cois
horlin
opera
wireless
communications
group
univer-
sit´e
libre
bruxelles
1050
brussels
belgium
e-mail
jdeterme
ulb.ac.be
fhorlin
ulb.ac.be
jean-fran¸cois
determe
funded
belgian
national
science
foundation
f.r.s.-fnrs
laurent
jacques
j´erˆome
louveaux
jean-fran¸cois
determe
icteam
departement
uni-
laurent.jacques
uclouvain.be
jerome.louveaux
uclouvain.be
laurent
versit´e
catholique
louvain
e-mail
jacques
funded
belgian
national
science
foundation
f.r.s.-fnrs
deﬁne
s-sparse
signals
vectors
whose
supports
exhibit
cardinality
equal
less
entry
support
vector
deﬁned
supp
cid:54
loosely
speaking
signal
said
sparse
whenever
support
signiﬁcantly
smaller
dimension
space
i.e.
|supp
cid:28
finally
notation
cid:38
means
1.1
objective
signal
model
focus
scenario
objective
recovering
joint
support
sparse
signals
i.e.
supp
∪k∈
supp
observed
means
common
measurement
matrix
rm×n
resulting
measurement
vectors
gather
measurements
sparse
signal
φxk
m×m
cid:54
additive
noise
term
assumed
distributed
ek1
ek2
statistically
independent
vector
noise
standard
deviations
denoted
sake
simplicity
equation
aggregates
equations
φxk
single
relationship
cid:0
cid:1
rm×k
cid:0
cid:1
rn×k
cid:0
cid:1
rm×k
note
signal
models
incorporating
one
sparse
vector
called
single
measurement
vector
smv
models
sparse
signals
measured
referred
multiple
measurement
vector
mmv
models
mmv
signal
model
applies
several
scenarios
example
section
iv.b
source
localization
problem
studied
using
measurements
diﬀerent
time
instants
signal
model
describing
problem
equivalent
equation
measurement
vector
corresponds
one
time
instant
section
authors
describe
jointly
sparse
signal
models
comparable
typically
occur
networks
sensors
possibly
large
number
sensing
nodes
exchange
measurements
object
reconstruct
case
individual
measurement
vector
within
generated
one
sensor
applications
described
survey
paper
atoms
understood
columns
i.e.
denotes
j-th
column
although
notion
atom
typically
used
dealing
dictionaries
use
simpliﬁes
discussions
note
results
presented
paper
assume
atoms
unit
cid:96
norm
1.2
detailed
contribution
main
contribution
method
analyze
somp
noisy
setting
principal
techni-
cal
contribution
upper
bound
probability
somp
identiﬁes
incorrect
entries
i.e.
entries
belonging
prescribed
number
iterations
using
quantities
describing
reliability
somp
noiseless
case
show
probability
incorrect
identiﬁcation
decreases
exponentially
number
sparse
signals
condition
amplitudes
sparse
signals
suﬃciently
high
compared
noise
variances
also
establish
guarantee
somp
correctly
identiﬁes
entries
joint
support
absence
noise
adequate
noisy
scenarios
tends
development
presented
paper
sheds
light
phenomenon
providing
two
convincing
explanations
existence
discussed
end
paper
interesting
corollary
upper
bound
asymptotic
exact
recovery
condition
aerc
somp
i.e.
condition
ensuring
probability
somp
failing
falls
condition
derive
actually
also
guarantees
arbitrarily
small
probability
error
suﬃciently
high
value
stronger
result
aerc
particular
ﬁnal
result
involves
four
fundamental
quantities
channels
available
joint
recovery
quantity
minj∈s
number
sparse
signals
deﬁnes
many
independent
measurement
tudes
coeﬃcients
associated
atom
indexed
support
cid:80
quantity
1/k
cid:80
quantity
1/√k
cid:107
cid:107
cid:107
cid:107
1/√k
quantiﬁes
close
sparse
particular
1-sparse
1/√k
practice
prefer
working
upper
bound
k=1
|xj
sets
minimal
averaged
ampli-
average
noise
power
sparse
signals
measurement
channels
vector
entries
identical
max1≤k
belonging
k=1
cid:18
given
four
quantities
minimum
signal-to-mean-noise
ratio
snrmin
deﬁned
shown
probability
somp
committing
least
one
error
min1≤k
ﬁrst
iterations
perr
cid:38
snrmin
βωσ
log
log
2e|s|
log
perr
condition
valid
snrmin
βωσ
αsnrmin
βωσ
cid:39
αsnrmin
clariﬁed
later
term
upper
bounded
2/π
quantiﬁes
sensing
properties
matrix
well
reliability
somp
decisions
noiseless
case
note
condition
snrmin
βωσ
testiﬁes
impossibility
performing
correct
decisions
unless
snr
certain
minimal
ﬂoor
level
cid:112
particular
snrmin
cid:29
βωσ
cid:19
finally
numerical
simulations
validate
methodology
showing
expression
appearing
equation
accurate
numerical
adjustments
aiming
tightening
large
theoretical
constants
1.3
related
work
smv
setting
full
support
recovery
guarantees
omp
bounded
noise
signals
well
gaussian
noises
proposed
work
also
provides
criteria
stopping
criteria
guarantee
omp
terminates
picked
correct
atoms
contribution
slightly
reﬁned
provide
conditions
independent
particular
support
recovered
gribonval
investigated
performance
somp
problem
resembling
contribution
provide
lower
bound
probability
correct
full
support
recovery
signal
estimated
sparse
non-zero
entries
statistically
independent
mean-zero
gaussian
random
variables
1.4
outline
first
section
describes
somp
related
quantities
technical
prerequisites
delivered
reader
section
section
present
results
somp
noiseless
case
used
afterwards
section
provides
upper
bounds
probability
somp
fails
identify
correct
atom
ﬁxed
iteration
results
ﬁnally
exploited
section
deliver
usable
easily
interpretable
upper
bounds
probability
somp
includes
least
one
incorrect
entry
estimated
support
prescribed
number
iterations
numerical
results
presented
section
conﬁrm
validity
results
conclusion
follows
technicalities
reported
appendix
simplify
presentation
core
paper
1.5
conventions
cid:80
cid:107
cid:107
cid:80
ﬁnd
useful
introduce
main
notations
used
paper
j=1
|xj|p
1/p
cid:107
cid:107
maxj∈
|xj|
rm×n
deﬁne
cid:107
cid:107
p→q
equation
a.8
cid:107
cid:107
p→q
sup
cid:107
cid:107
p=1
cid:107
cid:107
particular
rn×k
cid:107
cid:107
∞→∞
equal
maxj∈
k=1
|aj
lemma
a.5
unless
otherwise
speciﬁed
every
vector
understood
column
vector
also
denotes
vector
formed
entries
indexed
within
likewise
fashion
rm×n
deﬁne
matrix
formed
columns
whose
indexes
belong
notation
refers
relative
complement
respect
moore-penrose
pseudoinverse
matrix
given
transpose
denoted
range
matrix
i.e.
space
spanned
columns
denoted
inner
product
two
vectors
written
cid:104
cid:105
given
xty
minimum
maximum
eigenvalues
matrix
denoted
λmin
λmax
respectively
probability
measure
given
mathematical
expectation
denoted
simultaneous
orthogonal
matching
pursuit
many
algorithms
proposed
solve
joint
support
recovery
problem
associated
equation
smv
setting
canonical
algorithms
include
cid:96
1-minimization
matching
pursuit
orthogonal
matching
pursuit
omp
algo-
rithms
relying
cid:96
1-minimization
probably
among
reliable
algorithms
designed
compressed
sensing
problems
often
exhibit
higher
computational
complexity
greedy
counterparts
omp
greedy
algorithms
thus
suited
real
time
applications
well-known
algorithms
described
smv
problems
admit
several
extensions
within
mmv
framework
speciﬁcally
one
natural
general-
ization
omp
somp
described
algorithm
described
algorithm
somp
updates
support
iteration
including
current
estimated
support
single
atom
φjt
whose
index
denoted
maximizes
somp
metric
cid:88
k=1
cid:104
cid:105
cid:107
tφj
cid:107
algorithm
simultaneous
orthogonal
matching
pursuit
somp
require
rm×k
rm×n
initialization
determine
atom
included
support
argmaxj∈
cid:107
tφj
cid:107
update
support
st+1
projection
measurement
vector
onto
φst+1
t+1
φst+1φ+st+1
projection
measurement
vector
onto
φst+1
t+1
t+1
end
return
support
last
step
description
somp
steps
denotes
k-th
column
residual
matrix
iteration
steps
measurement
vector
projected
onto
orthogonal
complement
φst+1
denoted
φst+1
way
atom
picked
twice
since
included
support
projection
onto
φst+1
ensures
cid:104
t+1
cid:105
st.
algorithm
ﬁnishes
prescribed
number
it-
erations
s+1
attained
turn
description
useful
quantities
related
somp
cid:1
orthogonal
projectors
incorrect
atoms
indexed
set
contains
cid:0
|s|
atoms
indexed
joint
support
referred
correct
atoms
φstφ+
|st|
deﬁned
zero
matrix
loosely
speaking
set
possible
orthogonal
projectors
iteration
assuming
atoms
belonging
picked
previously
enumerating
possible
orthogonal
projection
matrices
iteration
means
necessary
later
since
knowing
sequence
atoms
picked
somp
beforehand
requires
consider
possible
projectors
iteration
also
deﬁne
cid:104
φxk
cid:105
belongs
value
consequently
inner
product
j-th
atom
k-th
column
residual
matrix
whenever
orthogonal
projector
iteration
noise
present
deﬁne
without
noise
similarly
highest
somp
metric
obtained
correct
atoms
iteration
counterpart
quantity
incorrect
atoms
instead
max
j∈s
max
j∈s
also
convenient
deﬁne
two
quantities
identifying
best
correct
atom
incorrect
atom
noiseless
case
iteration
orthogonal
projection
matrix
cid:88
k=1
cid:88
k=1
cid:88
k=1
cid:88
k=1
arg
max
j∈s
arg
max
j∈s
subscripts
refer
correct
incorrect
atoms
respectively
technical
prerequisites
notations
wish
settle
key
theoretical
notions
used
later
actual
analysis
somp
3.1
restricted
isometry
property
related
concepts
rm×n
exhibits
restricted
isometry
property
rip
order
exists
constant
cid:107
cid:107
cid:107
cid:107
cid:107
cid:107
s-sparse
vectors
smallest
equation
holds
rectricted
isometry
constant
ric
order
ric
order
theoretically
given
max
maxs⊆
|s|=s
λmax
φts
mins⊆
|s|=s
λmin
φts
ric
therefore
provides
upper
bound
alteration
cid:96
norm
sparse
vectors
multiplication
interestingly
enough
implies
full
column
rank
every
support
size
|s|
since
mins⊆
|s|=s
λmin
φts
3.2
lipschitz
functions
function
cid:55
called
lipschitz
function
regard
cid:96
2-norm
cid:107
cid:107
holds
constant
referred
lipschitz
constant
interesting
property
lipschitz
functions
following
concentration
inequality
standard
gaussian
random
vectors
theorem
8.40
cid:18
cid:19
exp
−ε2
2l2
result
shows
k×k
concentrates
around
expectation
concentration
improves
decreases
similar
inequality
exp
−ε2/
2l2
results
observation
lipschitz
3.3
folded
normal
distribution
recurring
distribution
paper
folded
normal
distribution
absolute
value
normal
random
variable
|x|
associated
folded
normal
random
variable
often
refer
underlying
normal
variable
similarly
underlying
mean
variances
underlying
normal
variable
note
expectation
folded
random
variable
always
higher
underlying
normal
random
variable
folding
probability
density
function
pdf
occurring
values
lower
results
somp
without
noise
discussed
later
reliability
somp
noiseless
case
determines
noise
levels
unlikely
make
somp
detect
incorrect
atoms
section
thus
provide
lower
bound
measure
noiseless
reliability
given
i.e.
quantity
ﬁxes
minimum
gap
somp
metrics
correct
incorrect
atoms
noiseless
case
assume
existence
three
quantities
ﬁrst
quantity
denoted
quantiﬁes
minimum
relative
reliability
somp
decisions
metric
best
correct
atom
ψτx
cid:2
cid:3
cid:2
cid:3
remaining
quantities
entwined
provide
lower
bound
somp
quantity
δ|s|
|s|
typically
increases
δ|s|
|s|
dwindle
property
conveys
idea
small
support
size
|s|
measurement
matrix
endowed
good
properties
tend
increase
value
finally
related
intrinsic
energy
sparse
signals
prior
projection
therefore
depends
combining
yields
desired
lower
bound
absolute
reliability
somp
noiseless
case
lemma
every
cid:2
cid:3
cid:18
cid:19
ψτx
deﬁned
equations
following
subsections
aim
providing
evidence
existence
particular
valid
expressions
provided
parameters
light
previous
works
literature
4.1
lower
bound
somp
relative
reliability
section
brieﬂy
discuss
existence
lower
bounds
i.e.
possible
expressions
quantifying
reliably
somp
distinguishes
correct
incorrect
atoms
noiseless
case
proposed
several
expressions
use
ric
sometimes
restricted
orthogonality
constant
roc
thereby
requiring
know
residual
support
size
equivalently
current
iteration
number
example
shown
|s|
δ|s|
similarly
bound
using
support
obtained
straightforward
adaptation
proof
theorem
4.5
i.e.
full
column
rank
note
papers
bounds
hold
correct
decisions
made
iterations
preceding
get
details
matters
objective
provide
evidence
existence
1−δ|s|
√|s|−1
cid:107
φ+s
cid:107
1→1
δ|s||s|
4.2
lower
bound
present
convenient
lower
bound
lemma
adapted
satisﬁes
rip
|s|-th
ric
δ|s|
associated
expressions
δ|s|
δ|s|
min
j∈s
|xj
cid:112
|s|δ|s|
cid:88
k=1
deﬁned
section
lemma
shows
minj∈s
cid:80
cid:112
atom
also
inﬂuences
|s|δ|s|
δ|s|
δ|s|
depends
properties
ric
δ|s|
k=1
|xj
indicates
sum
absolute
coeﬃcients
associated
k=1
|xj
lemma
thus
provides
minj∈s
cid:80
notice
without
restrictions
sparse
signal
matrix
nothing
said
non-maximum
somp
metrics
atoms
belonging
correct
support
i.e.
might
happen
somp
metrics
correct
atoms
zero
except
highest
one
among
result
focus
correct
atom
index
associated
noiseless
somp
metric
short
example
available
section
prove
statement
upper
bounds
probability
somp
failing
iteration
section
provides
upper
bound
probability
somp
picks
incorrect
atom
iteration
given
ﬁxed
orthogonal
projector
time
derived
results
include
noise
first
examine
statistical
distribution
somp
metric
single
atom
section
5.1.
desired
upper
bound
derived
5.1
distribution
cid:107
tφj
cid:107
quantity
cid:107
tφj
cid:107
cid:80
iteration
therefore
interesting
determine
statistical
distribution
cid:105
ultimately
deﬁnes
atom
picked
cid:88
k=1
k=1
cid:104
cid:88
cid:88
cid:105
k=1
k=1
cid:104
cid:104
φxk
cid:105
cid:104
φxk
cid:105
cid:104
cid:105
prove
cid:10
cid:11
distributed
let
consider
ﬁxed
projection
matrix
easy
provided
cid:107
cid:107
.we
deﬁne
related
noise
standard
deviation
vectors
cid:107
cid:107
2σk
cid:107
cid:107
atom
thereby
possible
replace
term
cid:104
moreover
cid:104
sum
folded
normal
random
variables
since
cid:104
φxk
cid:105
gk|
distributed
cid:104
cid:88
distribution
φxk
cid:105
cid:105
gk|
hence
cid:107
tφj
cid:107
admits
φxk
cid:105
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:55
k=1
5.2
probability
somp
picking
incorrect
atom
iteration
section
provide
general
upper
bound
probability
somp
picks
incorrect
atom
iteration
ﬁxed
orthogonal
projector
idea
proof
notice
metrics
associated
every
incorrect
atom
lower
real
positive
number
metric
associated
one
correct
atom
higher
correct
decision
necessarily
made
approach
pessimistic
sense
one
speciﬁc
correct
atoms
could
picked
note
among
correct
atoms
best
atom
noiseless
case
considered
i.e.
atom
indexed
proof
theorem
available
appendix
theorem
ﬁxed
iteration
let
i.e.
one
residuals
could
generated
somp
basis
iteration
assuming
correct
atoms
identiﬁed
k×k
probability
somp
picking
incorrect
atom
running
one
iteration
upper
bounded
cid:20
cid:21
cid:104
cid:88
j∈s
cid:105
theorem
assume
generated
basis
past
iterations
somp
i.e.
upper
bound
derive
independent
way
obtained
ignoring
precaution
would
imply
able
upper
bound
conditional
probability
given
event
somp
succeeded
previous
iterations
would
involved
theorem
establishes
cid:3
cid:2
cid:3
intervene
appropriate
ﬁnd
upper
bounds
probabilities
use
produce
easily
interpretable
result
i.e.
theorem
idea
proof
theorem
set
convex
combination
probabilities
form
cid:2
cid:3
max
cid:2
i.e.
express
theorem
relatively
cid:2
j∈s
cid:3
maxj∈s
use
concentration
inequalities
section
3.2
conjunction
fact
lipschitz
see
lemma
2.1
chosen
set
0.5
simplify
ﬁnal
result
visual
interpretation
theorem
depicted
figure
sake
simplicity
ﬁgure
considers
identical
noise
levels
atom
remember
cid:107
cid:107
noise
may
exhibit
diﬀerent
powers
atom
general
case
full
proofs
available
appendix
lemma
2.1.
function
lipschitz
constant
cid:107
cid:107
deﬁned
equation
lipschitz
function
whose
best
theorem
let
k×k
ﬁxed
iteration
let
i.e.
one
residuals
could
generated
somp
basis
iteration
assuming
correct
atoms
identiﬁed
far
let
cid:20
cid:21
cid:104
cid:105
max
j∈s
assume
cid:107
cid:107
probability
somp
making
incorrect
decision
executing
one
iteration
upper
bounded
cid:20
cid:21
|s|
exp
cid:107
cid:107
cid:3
j-th
atom
vertical
lines
represent
mean
figure
explanation
theorem
probability
density
function
k×k
folded
normal
distribution
associated
atom
i.e
cid:2
normal
distribution
virtually
equal
suﬃciently
high
values
cid:80
guarantee
cid:2
position
obtained
0.8
two
atoms
belonging
including
best
one
two
atoms
belonging
including
best
one
represented
sake
clarity
figure
observed
mean
folded
normal
distribution
underlying
dis-
crepancy
observed
low
values
second
incorrect
atom
figure
explained
section
3.3
taking
absolute
value
normal
distribution
yields
folding
pdf
thereby
increases
mean
regard
underlying
normal
distribution
implies
two
atom
indexes
two
explanations
phenomenon
denotes
value
somp
metric
j-th
atom
provide
cid:3
cid:2
cid:80
cid:3
k=1
k=1
valueofthepdfotherincorrectatom
bestincorrectatomothercorrectatom
bestcorrectatomαλ00.51
noise
vectors
may
exhibit
diﬀerent
cid:96
norms
atom
thus
even
possibly
higher
noise
level
j2-th
atom
suﬃciently
small
gap
note
phenomenon
occur
iteration
noise
variances
necessarily
equal
case
since
might
yield
even
though
possibly
identical
noise
vectors
way
distributed
plays
signiﬁcant
role
example
one
quantities
overwhelmingly
greater
others
limited
folding
occur
entry
signiﬁcant
folding
present
ones
conversely
entries
uniformly
distributed
overall
increase
expectation
due
folding
decreased
thus
depending
way
terms
distributed
might
hold
despite
cid:3
replaced
cid:2
cid:3
deﬁ-
note
reason
maxj∈s
cid:2
nition
similar
two
explanations
stated
abstract
two
points
provide
convincing
explanations
i.e.
success
noiseless
case
guarantee
correct
recovery
indeed
random
variable
1/k
cid:2
cid:3
concentrates
arbitrarily
well
around
expectation
suﬃciently
high
value
therefore
expected
asymptotic
correct
recovery
iteration
orthogonal
projector
i.e.
recovery
ensured
condition
actually
neither
necessary
suﬃcient
correct
asymptotic
recovery
phenomena
discussed
might
beneﬁt
correct
atoms
i.e.
condition
necessary
incorrect
atoms
i.e.
condition
suﬃcient
however
analyses
follow
rely
conservative
assumption
incorrect
atoms
beneﬁt
increase
expectation
due
folding
upper
bound
probability
somp
failing
ﬁrst
iterations
ﬁnal
problem
addressed
derive
upper
bound
probability
somp
picking
incorrect
atoms
ﬁrst
iterations
particular
bound
require
know
sequence
orthogonal
projectors
intervening
iteration
design
lower
bound
theorem
independent
orthogonal
projector
conveys
simple
manner
impact
somp
performance
sensing
properties
matrix
absolute
reliability
somp
noiseless
case
i.e.
asso-
ciated
lower
bound
lemma
noise
variances
bound
obtained
show
theorem
yields
upper
bound
probability
failure
somp
iteration
iteration
|s|
included
i.e.
upper
bound
probability
somp
picks
least
one
incorrect
atom
ﬁrst
iterations
6.1
deriving
lower
bound
section
propose
lower
bound
upper
bound
maxj∈s
valid
lower
bound
obtained
excep-
tionally
proof
presented
core
paper
gives
reader
better
understanding
derivation
method
regarding
cid:21
cid:105
cid:20
cid:12
cid:12
cid:12
cid:12
cid:21
cid:12
cid:12
cid:12
cid:12
cid:20
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
k=1
cid:88
cid:88
cid:88
k=1
k=1
cid:104
cid:12
cid:12
cid:12
cid:35
|gk|
k=1
cid:34
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:80
k=1
k=1
cid:88
k=1
cid:112
2/π
cid:80
inequality
results
jensen
inequality
convex
functions
unless
bound
sharp
discards
gain
expectation
resulting
folding
moreover
triangle
inequality
provides
|gk|
inequality
sharp
assumes
maximum
folding
i.e.
folding
obtained
underlying
mean
terms
non-zero
also
inequality
holds
see
section
5.1
conclusion
obtain
k=1
gk|
general
case
least
one
cid:112
expression
quantifying
absolute
reliability
decisions
noiseless
case
intervenes
i.e.
well
penalty
depending
noise
standard
deviations
channels
i.e.
2/π
cid:107
cid:107
using
lemma
yields
cid:107
cid:107
ψτx
cid:114
cid:107
cid:107
cid:18
cid:19
cid:114
theoretical
expressions
discussed
section
note
result
holds
whenever
theoretical
expression
chosen
next
step
combine
derived
lower
bound
theorem
6.2
upper
bound
probability
failure
somp
ﬁrst
iterations
main
diﬃculty
derive
desired
upper
bound
knowing
sequence
supports
chosen
ﬁrst
s+1
iterations
circumvent
issue
possible
orthogonal
projection
matrices
tested
one
intervenes
practice
sub-optimal
approach
linked
fact
statistically
dependent
random
variables
noise
contributes
determining
atom
picked
thus
inﬂuences
orthogonal
projection
matrices
statistical
link
variables
appears
diﬃcult
capture
consequently
chose
use
workaround
trick
used
similar
theoretical
analyses
best
authors
knowledge
better
solution
yet
found
literature
proof
available
appendix
theorem
using
quantities
deﬁned
equation
let
consider
cid:18
cid:19
cid:114
ψτx
cid:107
cid:107
assume
cid:107
cid:107
probability
somp
picking
least
one
incorrect
atom
iteration
iteration
included
upper
bounded
cid:21
cid:20
cid:107
cid:107
cid:80
t=0
cid:0
|s|
cid:1
s≥1
ncs
exp
cid:16
|s|+s−1
cid:17
expressions
appearing
theorem
deciphered
following
manner
decisions
prior
addition
noise
explained
section
term
1/γ
ψτx
conveys
impact
reliability
terms
cid:112
2/π
cid:107
cid:107
cid:107
cid:107
translates
spread
pdf
convey
negative
inﬂuence
noise
somp
performance
phenomena
account
however
diﬀerent
nature
cid:112
quantity
cid:107
cid:107
characterized
spread
gaussian-like
functions
figure
hand
expression
2/π
cid:107
cid:107
describes
negative
impact
noise
existing
reliability
equation
commentary
theorem
noiseless
case
i.e.
thoroughly
discusses
matters
next
section
deals
understanding
somp
performance
whenever
increases
particular
aerc
mentioned
introduction
derived
6.3
probability
failure
increasing
values
max1≤k
1/√k
cid:107
cid:107
cid:107
cid:107
snrmin
min1≤k
implies
cid:107
cid:107
cid:80
gain
insight
theorem
implies
increases
using
quan-
tities
deﬁned
introduction
i.e.
minj∈s
k=1
|xj
k=1
deﬁnition
cid:114
thus
using
expression
provided
lemma
obtain
cid:114
cid:107
cid:107
cid:107
cid:107
cid:107
cid:107
cid:80
cid:32
cid:18
cid:32
cid:18
cid:19
cid:19
cid:33
cid:33
ψsnrmin
hence
ensuring
yields
cid:107
cid:107
cid:18
cid:19
kξ2
ψsnrmin
cid:114
consequence
condition
probability
somp
picks
least
one
incorrect
atom
ﬁrst
iterations
upper
bounded
cid:20
cid:21
ncs
exp
kξ2
indicates
condition
suﬃcient
asymptotic
recovery
i.e.
valid
aerc
precisely
expression
shows
snrmin
suﬃciently
high
guarantee
asymptotic
recovery
also
worth
noticing
snrmin
result
equivalent
exact
recovery
criterion
erc
1/γ
noiseless
case
conclude
technical
discussions
show
long
always
ex-
ists
value
ensuring
arbitrary
maximum
probability
failure
somp
fixing
maximum
probability
failure
perr
elementary
algebraic
operations
show
satisfying
kmin
perr
yields
probability
failure
inferior
perr
cid:19
cid:18
ncs
perr
kmin
perr
log
quantity
increases
snrmin
number
sparse
signals
needed
achieve
prescribed
maximum
probability
failure
decreases
whenever
snr
improves
finally
according
theorem
log
cid:46
log
|s|+s−1
cid:18
log
2e|s|
equation
yields
kmin
perr
cid:46
log
log
2e|s|
log
perr
cid:17
cid:46
cid:16
|s|+s−1
cid:17
cid:16
|s|+s−1
cid:19
suggests
kmin
scale
logarithmically
1/perr
number
atoms
linearly
number
iterations
performed
numerical
results
7.1
objective
methodology
numerical
simulations
wish
validate
theoretical
developments
properly
describe
somp
performance
show
performing
adjustments
equa-
tion
enables
accurately
predict
minimum
number
sparse
signals
needed
achieve
prescribed
probability
error
perr
using
somp
perform
full
support
recovery
precisely
consider
three
parameters
identiﬁed
equation
rewrites
parameter
1/γ
related
relative
reliability
somp
noiseless
2/π
however
theoretical
development
provides
case
theory
quantity
equal
kmin
perr
snrmin
ωσβ
log
perr
cid:112
values
sharp
general
case
see
discussion
section
6.1
possibly
even
less
constrained
signal
models
finally
parameter
log
nc|s|−1
conveys
impact
number
atoms
number
iterations
i.e.
|s|
case
practice
|s|
incorrect
atoms
non-negligible
probability
picked
replaced
cid:28
|s|
also
discussed
c|s|−1
suboptimal
term
results
fact
considered
possible
correct
supports
iteration
deduce
probability
error
practice
one
support
numerous
possibilities
matters
identiﬁcation
procedure
uses
cost
function
cid:88
cid:104
cid:112
perr∈iperr
snrmin∈isnr
cid:112
cid:105
kmin
αsnrmin
βωσ
log
perr
kmin
obtained
means
simulations
iperr
0.05
0.5
0.9
isnr
speciﬁed
afterwards
cost
function
evaluated
3-tuple
set
consists
500
uniformly
distributed
points
intervals
0.1
1.4
2/π
respectively
3-tuple
minimizing
cost
function
chosen
cid:112
7.2
simulations
signal
model
describe
signal
model
consider
run
simulations
simple
general
model
described
section
1.1
snrmin
easily
computed
first
noise
standard
deviation
vector
odd-indexed
even-indexed
entries
identical
i.e.
σodd
σeven
σodd
σeven
deﬁne
σeven/σodd
easy
show
even
and/or
furthermore
assume
kµx
ﬁxed
statistically
independent
rademacher
variables
i.e.
random
variables
returning
either
probability
0.5
outcomes
assumption
every
cid:112
7.3
simulation
setup
sensing
matrix
chosen
size
250
1000
i.e.
250
1000
columns
realizations
statistically
independent
vectors
uniformly
distributed
unit
hypersphere
sm−1
cid:107
cid:107
sensing
matrix
identical
simulations
conducted
simulations
reason
always
possible
recast
signal
model
cid:54
another
one
satisfying
maintaining
value
snrmin
multiplying
1/ζ
multiplication
aﬀect
somp
decisions
inner
products
intervening
somp
decision
metric
equally
aﬀected
see
step
algorithm
note
results
matlab
software
available
7.4
results
analysis
wish
determine
whether
equation
accurately
predicts
somp
performance
provided
parameters
properly
identiﬁed
begin
monte-carlo
simulations
homoscedastic
signal
model
i.e.
two
diﬀerent
support
car-
dinalities
i.e.
|s|
|s|
20.
support
cardinality
identify
3-tuple
parameters
explained
section
7.1.
predictions
based
identiﬁed
pa-
rameters
compared
another
set
monte-carlo
simulations
varies
monte-carlo
simulation
support
chosen
random
random
variables
statistically
independent
figure
plots
results
obtained
homoscedastic
signal
model
conﬁguration
|s|
identiﬁed
parameters
isnr
1.25
1.5
1.75
see
section
7.1
1.0535
0.54045
2.0741
conﬁguration
|s|
identiﬁed
parameters
isnr
1.5
1.75
see
section
7.1
1.0535
0.58682
2.5451.
figure
probability
somp
comitting
least
one
error
performing
joint
full
support
recovery
continuous
curves
plot
values
kmin
predicted
equation
several
values
perr
using
identiﬁed
parameters
dashed
curves
correspond
numerical
level
sets
ﬁxed
probability
failure
perr
corresponding
associated
continuous
curve
note
color
curve
particular
meaning
purpose
constrast
enhancement
number
monte-carlo
cases
equal
2000
2-tuple
snrmin
perr=0.9|empiricalperr=0.5|empiricalperr=0.05|empiricalperr=0.9|predictedperr=0.5|predictedperr=0.05|predictedprobabilityoferrorofsompnumberofsparsesignals
snrmin123400.20.40.60.8110203040506070
perr=0.9|empiricalperr=0.5|empiricalperr=0.05|empiricalperr=0.9|predictedperr=0.5|predictedperr=0.05|predictedprobabilityoferrorofsompnumberofsparsesignals
snrmin123400.20.40.60.8110203040506070
cid:112
observed
figure
ﬁtting
equation
numerical
results
satisfactory
full
range
simulations
identiﬁcation
procedure
yielded
values
slightly
higher
incoherent
theory
value
obtained
whenever
δ|s|
also
values
similar
cardinalities
lower
2/π
cid:39
0.7979
predicted
theory
values
remain
high
constant
equation
might
sharp
particular
replaced
8/θ
equation
provides
identical
values
kmin
provided
becomes
α/√θ
re-
placed
β/√θ
values
obtained
equal
2.0741
2.5451
indicates
nc|s|−1
replaced
exp
2.0741
cid:39
7.9574
exp
2.5451
cid:39
12.7445
respectively
higher
value
|s|
coherent
theory
c|s|−1
increases
|s|
values
obtained
nc|s|−1
also
suggest
expression
nc|s|−1
sharp
figure
plots
results
obtained
|s|
increases
theoretical
developments
appropriately
predict
value
kmin
decreases
noise
standard
deviation
vector
gets
sparser
i.e.
increases
model
pessimistic
regard
amplitude
improvement
kmin
hypothesize
inequalities
used
section
6.1
sharp
enough
accurately
predict
kmin
basis
identiﬁed
parameters
might
also
optimal
figure
levels
sets
probability
somp
comitting
least
one
error
performing
joint
full
support
recovery
|s|
continuous
curves
plot
values
kmin
pre-
dicted
equation
several
2-tuples
perr
snrmin
using
parameters
identiﬁed
figure
dashed
curves
correspond
numerical
level
sets
corresponding
associated
continuous
curve
number
monte-carlo
cases
equal
2000
2-tuple
even
values
simulated
perr=0.9–snrm=2|empiricalperr=0.9–snrm=1.75|empiricalperr=0.05–snrm=2|empiricalperr=0.05–snrm=1.75|empiricalperr=0.9–snrm=2|predictedperr=0.9–snrm=1.75|predictedperr=0.05–snrm=2|predictedperr=0.05–snrm=1.75|predictedrσnumberofsparsesignals
12345010203040
conclusion
paper
theoretical
analysis
somp
operating
presence
gaussian
additive
noise
presented
shown
signal
recovered
suﬃciently
large
compared
mean
noise
power
measurement
channels
succeed
support
recovery
assuming
condition
met
minimum
number
sparse
signals
gathered
achieve
prescribed
probability
failure
derived
interesting
corollary
aforementioned
results
ensuring
snr
values
threshold
allows
asymptotic
recovery
i.e.
probability
error
tends
tends
inﬁnity
finally
numerical
results
conﬁrmed
validity
theoretical
developments
technical
proofs
lemma
example
section
4.2
let
|s|
let
consider
maxj∈s\
cid:104
φsxs
cid:105
cid:107
φts\
φsxs
cid:107
maximal
value
somp
metric
correct
atoms
except
φsxs
φs\
xs\
φηxη
easily
show
φts\
φs\
xs\
−φts\
φηxη
implies
thus
enforcing
possible
setting
xs\
φts\
φs\
−1φts\
φηxη
−φ+s\
φηxη
non-zero
vector
except
pathological
cases
result
iteration
noiseless
somp
metric
might
non-zero
single
correct
atom
i.e.
particular
example
proof
proof
theorem
simplify
shorten
notations
abbreviate
consider
correct
atom
whose
index
chance
picked
correct
atoms
could
picked
conservative
analysis
present
take
account
possibility
suﬃcient
condition
pick
atom
belonging
iteration
given
projection
matrix
thus
given
maxj∈s
diﬀerent
condition
maxj∈s
denoting
succ
projection
matrix
succ
maxj∈s
event
occurring
somp
picks
correct
atom
iteration
given
correct
atoms
could
picked
implied
maxj∈s
maxj∈s
j∈s
max
cid:20
cid:21
let
event
new
lower
bound
given
second
line
obtained
considering
complementary
event
previous
line
using
union
bound
yields
succ
max
j∈s
max
cid:18
j∈s
cid:21
cid:20
cid:20
cid:21
cid:19
succ
max
j∈s
since
event
maxj∈s
time
yields
cid:2
max
equal
∪j∈s
j∈s
cid:3
cid:2
cid:3
using
union
bound
second
cid:88
cid:2
cid:3
j∈s
noticing
probability
somp
failing
iteration
equal
succ
concludes
proof
proof
lemma
2.1
consider
two
arbitrary
vectors
using
sequentially
triangle
inequality
reverse
triangle
inequality
cauchyschwarz
inequality
yields
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:16
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:88
k=1
k=1
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:68
cid:12
cid:12
cid:12
cid:17
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:69
k=1
cid:107
|xk
yk|
cid:107
cid:107
|xk
yk|
cid:107
cid:107
|xk
yk|
cid:107
cid:107
cid:107
quantity
cid:107
1.5
cid:107
thus
valid
lipschitz
constant
also
best
one
since
setting
0.5
saturates
inequality
proof
theorem
better
readability
omit
dependence
abbreviate
section
3.2
yield
lipschitz
lipschitz
constant
equal
cid:107
cid:18
cid:19
cid:17
since
cid:107
concentration
inequalities
cid:16
exp
−ε2
2l2
rewrites
cid:17
cid:19
cid:16
2l2
exp
cid:16
cid:16
similarly
using
concentration
inequality
obtained
cid:16
cid:17
cid:19
exp
2l2
cid:18
cid:17
provided
yields
cid:18
cid:17
suggested
figure
value
chosen
maxj∈s
cid:2
cid:3
cid:2
cid:3
assumption
guarantees
latter
strictly
higher
former
consequently
corresponds
best
correct
atom
second
concentration
inequality
used
since
conversely
ﬁrst
concentration
inequality
used
incorrect
atoms
consider
convex
combination
thus
obtain
cid:18
obtain
cid:16
cid:2
cid:3
cid:2
cid:3

cid:19
cid:17

cid:17
cid:107
cid:16
j∈s
j∈s
exp
cid:107
cid:2
cid:3
max
cid:2
cid:3
max
−
cid:18
cid:19
−
−
cid:16
−
cid:16
cid:32
cid:107
exp
exp
exp
cid:17
cid:107
exp
cid:107
cid:107
cid:17
cid:107
cid:107
max˜j∈s
cid:33
cid:17
cid:16
cid:107
cid:107

second
inequality
holds
max˜j∈s
every
less
sharp
upper
bound
obtained
inequalities
remembering
cid:107
cid:107
cid:107
cid:107
setting
0.5
combining
theorem
derived
inequalities
conclude
proof
proof
theorem
event
succ
occurs
whenever
somp
picks
correct
atom
iteration
given
projection
matrix
considering
possible
orthogonal
projectors
iteration
succeeds
choosing
correct
atom
possible
orthogonal
projectors
iteration
know
correct
decisions
occur
iteration
thus
deﬁning
succ
event
occurring
whenever
somp
successful
ﬁrst
iterations
iteration
included
cid:80
cid:92
cid:1
possible
orthogonal
projectors
somp
cid:0
|s|
cid:91
cid:92
succ
cid:91
succ
t=0
fail
t=0
p∈p
t=0
p∈p
cid:91
cid:80
cid:80
cid:105
t=0
t=0
fail
cid:104
event
intervening
r.h.s
complementary
event
preceding
expression
particular
event
occurring
somp
picks
incorrect
atom
iteration
given
orthogonal
projector
union
bound
yields
fail
cid:91
fail
p∈p
cid:88
cid:88
t=0
p∈p
fail
cid:21
cid:20
therefore
fail
succ
fail
probability
failure
somp
ﬁrst
iterations
p∈p
fail
theorem
yields
|s|
exp
cid:19
cid:18
cid:107
cid:107
lower
bound
derived
section
6.1
provides
cid:114
independent
remembering
cid:0
|s|
cid:1
conclude
ﬁrst
part
proof
using
inequality
|s|
noticing
upper
bound
cid:1
well
cid:0
|s|+s−1
regarding
inequality
appendix
cid:80
cid:0
|s|
cid:1
cid:18
|s|
cid:0
|s|
cid:19
thus
easily
obtain
fail
become
cid:16
e|s|
cid:107
cid:107
cid:88
ψτx
cid:17
cid:19
cid:18
cid:1
t=1
t=1
|s|
references
baron
wakin
duarte
sarvotham
baraniuk
distributed
compressed
sensing
rice
university
technical
report
tree-0612
nov.
2006
cai
wang
orthogonal
matching
pursuit
sparse
signal
recovery
noise
information
theory
ieee
transactions
vol
4680–4688
2011
cai
wang
shifting
inequality
recovery
sparse
signals
signal
processing
ieee
transactions
vol
1300–1308
2010
cand
restricted
isometry
property
implications
compressed
sens-
ing
comptes
rendus
mathematique
vol
346
589–592
2008
candes
romberg
tao
stable
signal
recovery
incomplete
inaccurate
measurements
communications
pure
applied
mathematics
vol
1207–1223
2006
candes
tao
near-optimal
signal
recovery
random
projections
universal
encoding
strategies
information
theory
ieee
transactions
vol
5406–5425
2006
chen
huo
theoretical
results
sparse
representations
multiple-
measurement
vectors
ieee
transactions
signal
processing
vol
4634–
4643
2006
dan
wang
robustness
orthogonal
matching
pursuit
restricted
isom-
etry
property
science
china
mathematics
vol
627–634
2014
davenport
wakin
al.
analysis
orthogonal
matching
pursuit
using
restricted
isometry
property
information
theory
ieee
transactions
vol
4395–4401
2010
davis
mallat
avellaneda
adaptive
greedy
approximations
constructive
approximation
vol
57–98
1997
j.-f.
determe
somp
simulation
framework
v0.5.3.
online
available
//homepages.ulb.ac.be/∼jdeterme/software.html
http
j.-f.
determe
louveaux
jacques
horlin
improving
correlation
lower
bound
simultaneous
orthogonal
matching
pursuit
signal
processing
letters
ieee
press
exact
recovery
condition
simultaneous
orthogonal
matching
pursuit
signal
processing
letters
ieee
vol
164–168
2016
ding
chen
robustness
orthogonal
matching
pursuit
multiple
mea-
surement
vectors
noisy
scenario
acoustics
speech
signal
processing
icassp
2012
ieee
international
conference
ieee
2012
3813–3816
donoho
compressed
sensing
information
theory
ieee
transactions
vol
1289–1306
2006
donoho
elad
optimally
sparse
representation
general
nonorthogonal
dictionaries
via
minimization
proceedings
national
academy
sciences
vol
100
2197–2202
2003
eldar
mishali
robust
recovery
signals
structured
union
sub-
spaces
information
theory
ieee
transactions
vol
5302–5316
2009
eldar
rauhut
average
case
analysis
multichannel
sparse
recovery
using
convex
relaxation
information
theory
ieee
transactions
vol
505–519
2010
fletcher
rangan
orthogonal
matching
pursuit
brownian
motion
analy-
sis
signal
processing
ieee
transactions
vol
1010–1021
2012
foucart
rauhut
mathematical
introduction
compressive
sensing
springer
2013
gribonval
rauhut
schnass
vandergheynst
atoms
channels
unite
average
case
analysis
multi-channel
sparse
recovery
using
greedy
algorithms
journal
fourier
analysis
applications
vol
5-6
655–687
2008
jacques
laska
boufounos
baraniuk
robust
1-bit
compres-
sive
sensing
via
binary
stable
embeddings
sparse
vectors
information
theory
ieee
transactions
vol
2082–2102
2013
liu
temlyakov
orthogonal
super
greedy
algorithm
applications
compressed
sensing
information
theory
ieee
transactions
vol
2040–2047
2012
malioutov
etin
willsky
sparse
signal
reconstruction
perspective
source
localization
sensor
arrays
ieee
transactions
signal
processing
vol
3010–3022
2005
mallat
zhang
matching
pursuits
time-frequency
dictionaries
signal
processing
ieee
transactions
vol
3397–3415
1993
pati
rezaiifar
krishnaprasad
orthogonal
matching
pursuit
recursive
function
approximation
applications
wavelet
decomposition
signals
systems
computers
1993
1993
conference
record
twenty-seventh
asilomar
confer-
ence
ieee
1993
40–44
rakotomamonjy
surveying
comparing
simultaneous
sparse
approximation
group-lasso
algorithms
signal
processing
vol
1505–1526
2011
rangan
fletcher
orthogonal
matching
pursuit
noisy
random
measure-
ments
new
analysis
advances
neural
information
processing
systems
2009
540–548
tropp
relax
convex
programming
methods
identifying
sparse
signals
noise
information
theory
ieee
transactions
vol
1030–1051
2006
tropp
gilbert
signal
recovery
random
measurements
via
orthogonal
matching
pursuit
information
theory
ieee
transactions
vol
4655–
4666
2007
tropp
gilbert
strauss
algorithms
simultaneous
sparse
ap-
proximation
part
greedy
pursuit
signal
processing
vol
572–588
2006
huang
d.-r.
chen
exact
support
recovery
sparse
signals
noise
via
orthogonal
matching
pursuit
signal
processing
letters
ieee
vol
403–406
2013
tian
wang
lin
perturbation
analysis
simultaneous
orthogonal
matching
pursuit
signal
processing
vol
116
91–100
2015
zhang
sparse
recovery
orthogonal
matching
pursuit
rip
information
theory
ieee
transactions
vol
6215–6221
2011
