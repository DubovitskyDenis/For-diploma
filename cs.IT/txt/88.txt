strengthening
entropy
power
inequality
thomas
courtade
department
electrical
engineering
computer
sciences
university
california
berkeley
abstract
tighten
entropy
power
inequality
epi
one
random
summands
gaussian
strengthening
closely
connected
concept
strong
data
processing
gaussian
channels
generalizes
vector
extension
costa
epi
leads
new
reverse
entropy
power
inequality
corollary
sharpens
stam
inequality
relating
entropy
power
fisher
information
applications
network
information
theory
given
including
short
self-contained
proof
rate
region
two-encoder
quadratic
gaussian
source
coding
problem
argument
based
weak
convergence
technique
employed
geng
nair
establishing
gaussian
optimality
via
rotational-invariance
traces
roots
doubling
trick
successfully
used
study
functional
inequalities
entropy
power
inequality
costa
epi
stam
inequality
strong
data
processing
gaussian
source
coding
index
terms
random
variable
density
differential
entropy
deﬁned
introduction
main
result
log
similarly
deﬁned
differential
entropy
random
vector
density
celebrated
entropy
power
inequality
epi
put
forth
shannon
rigorously
established
stam
blachman
asserts
independent
22h
x+w
22h
22h
x+w
x+w
22h
assumption
gaussian
prove
following
strengthening
theorem
let
let
independent
satisfying
notation
theorem
follows
usual
convention
indicating
random
variables
form
markov
chain
order
throughout
write
denote
random
variables
joint
distribution
factoring
pxy
pxqpy
|xqpv
form
markov
chain
conditioned
integral
exist
density
adopt
convention
case
inequality
trivial
consequence
data
processing
inequality
classical
epi
theorem
informative
density
exists
conditional
version
epi
often
useful
applications
theorem
easily
generalizes
along
lines
indeed
due
joint
convexity
log
obtain
following
corollary
theorem
work
supported
nsf
grants
ccf-1528132
ccf-0939370
center
science
information
email
courtade
berkeley.edu
corollary
suppose
conditionally
independent
given
moreover
conditionally
gaussian
given
satisfying
x+w|q
x|q
x+w
22h
w|q
interesting
note
conditional
version
classical
epi
assumes
form
symmetric
particular
holds
x+w
x+w
22h
note
mutual
informations
exponents
lhs
rhs
respectively
correspond
smaller
larger
mutual
informations
corresponding
data
processing
inequalities
one
would
expect
theorem
also
admits
vector
generalization
may
regarded
main
result
theorem
suppose
n-dimensional
random
vectors
conditionally
independent
given
moreover
conditionally
gaussian
given
satisfying
x+w|q
x|q
x+w
w|q
following
section
see
strengthening
classical
epi
afforded
theorem
generalizes
costa
epi
vector
generalization
found
applications
ranging
interference
channels
secrecy
capacity
e.g.
also
leads
new
reverse
epi
applied
improve
stam
inequality
equivalently
gaussian
logarithmic
sobolev
inequality
moreover
see
theorem
leads
brief
proof
converse
rate
region
quadratic
gaussian
two-encoder
source-coding
problem
applications
one-sided
interference
channels
strong
data
processing
inequalities
also
given
remark
restriction
conditionally
gaussian
theorem
severe
limitation
practice
indeed
applications
epi
typically
case
one
variables
gaussian
noted
rioul
examples
include
scalar
gaussian
broadcast
channel
problem
generalization
multiple-input
multiple-output
case
secrecy
capacity
gaussian
wiretap
channel
multiple
access
extension
determination
corner
points
scalar
gaussian
interference
channel
problem
scalar
gaussian
source
multiple-description
problem
characterization
rate-distortion
regions
several
multiterminal
gaussian
source
coding
schemes
tempting
conjecture
holds
distribution
unconstrained
however
suspect
true
counterexample
immediately
apparent
generalized
costa
entropy
power
inequality
applications
costa
epi
states
independent
n-dimensional
random
vectors
x+αw
α22
x+w
|α|
result
generalized
vector
setting
liu
using
perturbation
i-mmse
arguments
demonstrate
generalization
follows
easy
corollary
theorem
taking
equal
contaminated
additive
gaussian
noise
sense
theorem
may
interpreted
generalization
costa
epi
additive
noise
longer
restricted
gaussian
theorem
semideﬁnite
matrix
cid:22
let
independent
n-dimensional
random
vectors
positive
x+a1/2w
a|1/n2
|a|1/n2
x+w
proof
let
denote
two
independent
copies
put
a1/2w1
1/2w2
note
distribution
similarly
1/2w
follows
theorem
since
x+a1/2w
x+w
a|1/n2
x+w
i−a
1/2w
|a|1/n2
a1/2w1
x+w
|a|1/n2
multiplying
sides
x+w
completes
proof
costa
epi
may
interpreted
concavity
property
enjoyed
entropy
powers
proof
theorem
suggests
generalization
property
non-gaussian
noise
indeed
following
may
viewed
reverse
epi
theorem
let
independent
n-dimensional
random
vectors
x+w
z+w
x+z+w
proof
immediate
consequence
theorem
putting
x+z+w
rearranging
exponents
brieﬂy
remark
madiman
observed
following
inequality
submodularity
differential
entropy
proved
via
data
processing
independent
random
variables
gaussian
theorem
sharpens
inequality
reducing
lhs
factor
x+w
z+w
x+z+w
reverse
epi
reﬁnement
stam
inequality
theorem
admits
several
interesting
corollaries
deeply
connected
celebrated
gaussian
logarithmic
sobolev
inequality
lsi
start
deﬁne
entropy
power
fisher
information
random
vector
density
respect
lebesgue
measure
follows
2πe
cid:20
k∇f
cid:21
avoid
degeneracy
assume
throughout
section
entropies
fisher
informations
exist
ﬁnite
exploring
similarity
brunn-minkowski
inequality
epi
costa
cover
proved
following
information
isoperimetric
inequality
n-dimensional
inequality
commonly
referred
stam
inequality
due
fact
ﬁrst
observed
classic
1959
paper
one-dimensional
case
1975
gross
rediscovered
establishing
mathematically
equivalent
lsi
standard
gaussian
measure
every
gradient
zrn
log
h2dγn
2zrn
|∇h|2dγn
cid:18
zrn
h2dγn
cid:19
log
cid:18
zrn
dγn
cid:19
paper
gross
also
proved
equivalent
hypercontractivity
ornstein-uhlenbeck
semigroup
1990
carlen
showed
equivalence
stam
inequality
gross
lsi
refer
reader
concise
proof
historical
details
since
proved
using
bruijn
identity
special
case
shannon
epi
one
summand
gaussian
theorem
naturally
leads
sharpening
surprisingly
strengthening
takes
form
reverse
epi
upper
bounds
terms
marginal
entropies
fisher
informations
theorem
independent
n-dimensional
random
vectors
proof
may
assume
else
nothing
prove
begin
let
√tg
particular
√tg
independent
recall
bruijn
identity
√tg
cid:12
cid:12
cid:12
t=0
identifying
√tg
theorem
rearranging
ﬁnd
√tg
√tg
√tg
letting
applying
proves
claim
chosen
reduces
straightforward
recover
stam
inequality
theorem
indeed
let
σ2i
variance
second
inequality
follows
epi
since
follows
stated
another
way
reads
entropy
power
sum
according
using
epi
may
sandwich
appropriately
normalized
met
equality
throughout
gaussian
proportional
covariance
matrices
next
let
independent
identically
distributed
ﬁnite
entropy
deﬁne
doubling
constant
denoted
cid:17
cid:16
x+x′
remark
doubling
constant
relationship
functionals
discussed
one-
dimensional
setting
general
dimension
letting
independent
identically
distributed
theorem
yields
following
inequality
expresses
deﬁcit
terms
doubling
constant
corollary
n-dimensional
random
vector
recalling
conditions
equality
epi
equality
gaussian
therefore
corollary
represents
strict
strengthening
since
equivalent
gaussian
lsi
corollary
provides
bound
deﬁcit
lsi
bounds
recent
interest
interpreted
stability
estimate
lsi
let
random
vector
density
respect
lsi
may
written
as1
zrn
log
dγn
2zrn
|∇f|2
dγn
1in
fact
completely
equivalent
gross
formulation
assumptions
zero-mean
satisﬁes
poincar´e
inequality
every
smooth
lsi
may
improved
cid:2
cid:3
cid:2
|∇s
cid:3
|∇f|2
log
dγn
zrn
zrn
dγn
spectral
gap
fact
using
corollary
self-strengthening
argument
constant
established
may
improved
incorporating
var
ball
barthe
naor
showed
poincar´e
inequality
implies
2+2ζ
since
due
var
obtain
sharpening
stam
inequality
corollary
let
zero-mean
random
variable
var
ﬁnite
entropy
satisﬁes
poincar´e
inequality
1+ζ
account
doubling
constant
weaker
assumption
presence
spectral
gap
therefore
inequality
may
viewed
improvement
stability
estimate
sense
less
restrictive
hypothesis
required
closing
remark
inequality
also
holds
replaced
provided
density
log-concave
thus
corollary
modiﬁed
accordingly
converse
two-encoder
quadratic
gaussian
source
coding
problem
characterizing
rate
region
two-encoder
quadratic
gaussian
source
coding
problem
longstanding
open
problem
ﬁeld
network
information
theory
ultimate
resolution
wagner
landmark
paper
established
separation-based
scheme
optimal
wagner
al.
work
built
upon
oohama
earlier
solution
one-helper
problem
independent
solutions
gaussian
ceo
problem
due
prabhakaran
tse
ramachandran
oohama
see
self-contained
treatment
since
wagner
al.
original
proof
sum-rate
constraint
proofs
proposed
based
estimation-theoretic
arguments
semideﬁnite
programming
e.g.
however
known
proofs
quite
complex
show
converse
result
entire
rate
region
direct
consequence
theorem
thus
unifying
results
common
succinct
inequality
theorem
variables
correlation
let
2nrx
2nry
deﬁne
i=1
independent
identically
distributed
pairs
jointly
gaussian
random
let
cid:2
x|φx
cid:3
cid:2
y|φx
cid:3
cid:0
ρ22−2ry
cid:1
cid:19
cid:0
ρ22−2rx
cid:1
cid:19
log
cid:18
log
cid:18
log
2dx
+q1
4ρ2d
1−ρ2
key
ingredient
following
consequence
theorem
proposition
satisfying
proof
since
mutual
information
invariant
scaling
may
assume
without
loss
generality
ρxi
independent
theorem
implies
y|u
ρ22
ρx|u
x|u
2πe
since
2πe
multiplying
2πe
establishes
claim
proof
theorem
convenience
put
using
markov
relationship
may
rearrange
exponents
proposition
obtain
equivalent
inequality
cid:16
ρ22−
cid:17
left-
right-hand
sides
monotone
decreasing
respectively
therefore
log
pair
2−2r
cid:0
ρ22−2r
cid:1
quadratic
inequality
respect
term
2−2r
easily
solved
using
quadratic
formula
obtain
+q1
4ρ2d
log
imply
2−2r
1−ρ2
note
jensen
inequality
maximum-entropy
property
gaussians
log
log
log
establishing
since
22rx
+log
u|v
similarly
proposition
implies
ρ22−2ry
ρ22−
rearranging
symmetry
yields
remark
proposition
special
case
theorem
ﬁrst
established
author
jiao
fact
proposition
establishes
stronger
result
converse
two-terminal
gaussian
source
coding
problem
shows
rate
regions
coincide
problems
distortion
measured
quadratic
loss
logarithmic
loss
one-sided
gaussian
interference
channel
one-sided
gaussian
interference
channel
z-gaussian
discrete
memoryless
channel
input-output
relationship
given
αy1
channel
inputs
observations
corresponding
encoder
decoder
respectively
independent
channel
inputs
assumed
|α|
since
setting
|α|
referred
strong
interference
regime
capacity
known
coincide
han-kobayashi
inner
bound
observe
expressed
one-sided
gaussian
degraded
form
capacity
region
identical
corresponding
non-degraded
version
proved
costa
despite
receiving
signiﬁcant
attention
researchers
several
decades
capacity
region
one-sided
gaussian
remains
unknown
regime
|α|
described
already
discussed
connections
costa
epi
theorem
remark
costa
epi
apparently
motivated
gaussian
since
theorem
generalizes
costa
result
one-sided
gaussian
presents
natural
application
toward
end
establish
new
multi-letter
outer
bound
give
simple
demonstration
theorem
might
applied
one-sided
gaussian
theorem
nα222r1−
satisfying
power
constraints
sup
npi
proof
nontrivial
inequality
prove
thus
begin
noting
theorem
implies
α22
since
rewritten
sup
nα22
log
log
2−2r2+o
independent
therefore
2−2
r2−ǫn
sup
sup
nα22
nα222
r1−ǫn
hold
due
fano
inequality
multiplying
sides
22ǫn
proves
claim
expressed
set
rate
pairs
satisfying
han-kobayashi
achievable
region
evaluated
gaussian
inputs
without
power
control
2−2r2
22r1
α2p1
interestingly
takes
similar
form
however
known
transmission
without
power
control
suboptimal
gaussian
z-interference
channel
general
nevertheless
may
possible
identify
random
variable
possibly
depending
ultimately
improves
known
bounds
leave
future
work
relationship
strong
data
processing
strong
data
processing
inequalities
connection
hypercontractivity
garnered
much
recent
attention
random
variables
standard
data
processing
inequality
asserts
random
variable
satisfying
pab
natural
deﬁne
best-possible
data
processing
function
pab
sup
a→b→v
pab
sharpest
possible
data
processing
inequality
joint
distribution
pab
thus
theorem
may
rephrased
−gi
pxy
22h
given
close
relationship
sharpened
epi
strong
data
processing
might
appropriate
call
theorem
strong
entropy
power
inequality
case
rearranging
ﬁnd
following
simple
bound
gaussian
channels
corollary
let
independent
moreover
gaussian
inequality
equality
pxy
log
cid:18
2πe
cid:19
remark
calmon
polyanskiy
recently
considered
complementary
setting
bound
best-possible
data
processing
function
deﬁned
according
sup
supremum
iii
proof
main
results
give
main
ideas
behind
proving
theorem
technical
details
provided
section
referred
needed
random
variables
pxy
write
denote
random
variable
conditional
note
uniquely
deﬁned
sense
different
versions
equal
-a.e
sequence
random
variables
indexed
denoted
shorthand
convergence
distribution
random
variable
written
d−→
order
minimize
difference
inequality
would
like
simultaneously
minimize
exponent
maximizing
exponent
valid
choices
toward
end
random
variable
let
deﬁned
via
additive
gaussian
noise
channel
given
√snrx
deﬁne
family
functionals
snr
x→y
parameterized
similarly
pxqpy
deﬁne
functional
pxq
x→y
|qni
snr|q
x|q
inf
inf
let
snr
denote
lower
convex
envelope
snr
snr
inf
pq|x
snr|q
consider
optimization
problem
snr
inf
snr
inf
pxq
snr|q
remark
note
optimization
problem
sufﬁces
consider
|q|
indeed
fenchel-caratheodory-bunt
theorem
taking
supported
two
points
sufﬁcient
preserve
values
=pq
2|q
snr|q
=pq
snr|q
following
explicit
characterization
snr
theorem
snr
=
λ−1
cid:17
log
cid:16
2πe
2hλ
log
cid:16
λ2πe
2hλ
log
2πe
snr
log
2πe
λ−1
cid:17
log
snr
snr
λ−1
snr
λ−1
essential
idea
needed
establish
theorem
need
consider
gaussian
random
variables
optimization
problem
establish
using
weak
convergence
argument
critical
ingredients
proved
sections
v-c
v-d
respectively
assert
claim
exists
sequence
satisfying
lim
n→∞
snr|qn
snr
d−→
x∗|
pq∗-a.e
depending
claim
d−→
supn
lim
inf
n→∞
snr
snr
words
claim
states
exists
sequence
approaches
inﬁmum
optimization
problem
converging
weakly
gaussian
claim
notes
functional
snr
weakly
lower
semicontinuous
gaussian
combining
two
claims
allows
restrict
attention
gaussian
optimization
problem
facts
hand
proof
theorem
follows
elementary
calculus
classical
epi
require
following
proposition
consequence
conditional
epi
dual
formulation
inequality
observed
oohama
proposition
let
independent
deﬁne
√snrx
inf
cid:2
log
snr
log
cid:0
λ−1
x→y
cid:16
cid:17
proof
let
let
denote
random
variables
conditioned
since
jointly
gaussian
γ√snr
1+γ
snr
cid:17
independent
entropy
power
inequality
holds
snr
22h
2πe
cid:18
1+γ
snr
cid:16
22h
x|v
22h
22h
snr
λ−1
snr
λ−1
snr
cid:1
cid:3
snr
cid:19
snr
snr
snr
upon
applying
jensen
inequality
rearranging
yields
2−2i
snr
2−2i
snr
follows
log
cid:16
snr
2−2i
cid:17
log
snr
cid:2
log
snr
log
cid:0
λ−1
snr
λ−1
snr
λ−1
λ−1
trivially
achieved
λ−1
easy
see
lower
bound
achieved
proof
theorem
noting
snr
invariant
translations
follows
claims
snr
cid:1
cid:3
second
inequality
follows
minimizing
snr
setting
constant
hand
snr
taking
snr
λ−1
1+γ
snr
snr
inf
0≤γ≤1
snr
recalling
deﬁnition
snr
proposition
implies
snr
=
λ−1
cid:17
log
snr
2hλ
log
cid:16
λ2πe
log
2πe
snr
log
2πeγ
log
2πe
snr
log
2πeγ
decreasing
λ−1
therefore
taking
minimizes
snr
interval
proving
given
explicit
characterization
snr
dual
form
inequality
position
differentiating
respect
quantity
ﬁnd
provided
snr
claim
snr
λ−1
snr
λ−1
λ−1
cid:17
log
cid:16
2πe
prove
theorem
proof
theorem
ﬁrst
establish
additional
assumption
generalize
end
via
truncation
argument
toward
goal
since
mutual
information
invariant
scaling
sufﬁcient
prove
√snrx
independent
snr
22h
satisfying
multiplying
sides
choosing
snr
var
inequality
thus
prove
observe
deﬁnition
snr
snr
gives
desired
minimizing
rhs
proves
inequality
particular
rhs
concave
derivative
given
∂λnλ
snr
=
since
log
2πe
snr
implying
particular
rhs
minimized
satisﬁes
log
2πe
snr
maximum
entropy
property
gaussians
follows
λ−1
snr
λ−1
snr
λ−1
∂λnλ
snr
satisfying
snr
log
cid:16
λ2πe
λ−1
cid:17
log
2πe
snr
2πe
2−2
substituting
recalling
22h
2πe
proves
eliminate
assumption
toward
end
let
density
let
gaussian
independent
consider
satisfying
deﬁne
random
variable
conditioned
event
|x|
let
deﬁne
via
since
bounded
22h
follows
lemma
limn→∞
provided
exists
moreover
since
d−→
lemma
see
section
v-a
asserts
limn→∞
easy
see
d−→
lim
inf
n→∞
lower
semicontinuity
relative
entropy
finally
chain
rule
mutual
information
implies
|x|≤n
|x|≤n
|x|
giving
lim
supn→∞
putting
observations
together
established
desired
22h
extension
random
vectors
vector
generalization
classical
epi
usually
proved
combination
conditioning
jensen
in-
equality
induction
e.g.
problem
2.9
argument
appear
readily
apply
generalizing
theorem
vector
version
due
complications
arising
markov
constraint
however
desired
generalization
may
established
noting
additivity
property
enjoyed
dual
form
random
vector
let
deﬁned
via
additive
gaussian
noise
channel
γ1/2x
independent
diagonal
matrix
nonnegative
diagonal
entries
analogous
scalar
case
deﬁne
family
functionals
parameterized
similarly
pxqpy|x
deﬁne
γ|q
x|q
y|q
consider
optimization
problem
inf
x→y→v
x→y→v
|qni
inf
theorem
diag
snr1
snr2
snrn
inf
pxq
γ|q
snri
xi=1
proof
let
block
diagonal
matrix
blocks
given
diag
partition
follows
γ1/2
lemma
see
section
v-c
γ|q
γ1|x2
γ2|y1
hence
deﬁnition
induction
proves
claim
proof
theorem
deﬁne
convenience
scalar
setting
establish
unconditional
claim
constant
constraint
kxk2
general
result
follows
truncation
argument
exactly
scalar
setting
moreover
may
assume
else
inequality
reduces
trivially
true
data
processing
inequality
fact
conditioning
reduces
entropy
thus
due
positive
deﬁniteness
invariance
mutual
information
one-one
transformations
may
multiply
sides
|σw|−1/n
obtain
equivalent
inequality
σ−1/2
σ−1/2
σ−1/2
σ−1/2
σ−1/2
kς−1/2
however
σ−1/2
generality
establishing
unconditional
version
trivial
since
therefore
equivalent
xk2
provided
kxk2
may
assume
without
loss
note
may
assume
snr
else
claimed
inequality
simplify
put
snr
max1≤i≤n
snr
holding
√snrx
independent
max1≤i≤n
established
exactly
proof
theorem
since
snr
nvλ
snr
proof
claims
section
dedicated
proof
claims
section
iii
several
steps
proof
require
properties
characterizations
gaussian
random
variables
recalled
proved
needed
ﬁrst
two
subsections
third
subsection
dedicated
proof
claim
fourth
subsection
dedicated
proof
claim
properties
gaussian
perturbation
collect
facts
random
variables
contaminated
gaussian
noise
particular
interest
weakly
convergent
sequences
random
variables
corresponding
continuity
properties
perturbation
gaussian
noise
lemma
non-vanishing
probability
density
function
derivatives
orders
lemma
5.1.3
independent
random
variables
normal
propositions
let
d−→
supn
lemma
kxnk2
let
σ2i
non-degenerate
gaussian
independent
let
finally
let
denote
density
respectively
d−→
kfn
lemma
suppose
d−→
supn
let
σ2i
pairwise
independent
deﬁne
n+zi
∗+zi
d−→
lim
inf
n→∞
n|y1
∗|y1
proof
fact
d−→
follows
lemma
lemma
also
establishes
account
markov
chains
identity
n|y1
lim
inf
n→∞
due
lower
semicontinuity
observe
relative
entropy
limn→∞
due
fact
∗|x1
n|x1
constant
thus
proved
applying
identity
lemma
let
lemma
fix
channel
deﬁne
according
exists
sequence
depending
satisfying
limn→∞
|yn|
|y∗|
log
|yn|
|y∗|
log
cid:12
cid:12
cid:12
cid:12
|yn|
|y∗|
cid:12
cid:12
cid:12
cid:12
proof
let
denote
density
respectively
lemma
density
continuous
vanish
therefore
bounded
away
zero
interval
lemma
kfn
follows
note
depend
consequence
sup
y∈b
cid:12
cid:12
cid:12
cid:12
=zb
cid:12
cid:12
cid:12
cid:12
sup
y∈b
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
hence
conditional
densities
yn|
y∗|
satisfy
fyn|
yn∈b
fy∗|
y∗∈b
symmetric
argument
fyn|
yn∈b
2fy∗|
y∗∈b
therefore
borel
set
a|yn
=zbza
fyn|
yn∈b
2zbza
fy∗|
y∗∈b
a|y∗
consequence
dpvn
|yn
dpv∗
|y∗
combining
observations
cid:19
dpv
dpv∗|y∗∈b
yn|yn
=zbz
fyn|
yn∈b
log
cid:18
dpv
dpvn|yn∈b
2zbz
fy∗|
y∗∈b
log
cid:18
y∗|y∗
log
cid:19
symmetric
argument
also
yn|yn
y∗|y∗
log
proves
inequality
established
logic
lemma
let
let
non-degenerate
gaussian
independent
holds
lim
b→∞
|x|
|x|
101
100
proof
proof
follows
theorem
lower
semicontinuity
relative
entropy
also
lim
inf
b→∞
|x|
102
103
limb→∞
|x|
limb→∞
|x|
|x|
chain
rule
|x|
|x|
|x|
|x|
|x|≤b
|x|
|x|
claim
proved
since
|x|≤b
vanishes
characterizations
gaussian
random
variables
goal
subsection
establish
following
characterization
gaussian
random
variables
lemma
suppose
d−→
supn
let
σ2i
pairwise
independent
deﬁne
independent
independent
gaussian
random
variables
identical
variances
lim
inf
n→∞
n|y1
104
require
two
facts
first
fundamental
result
bernstein
asserts
following
theorem
5.1.1
independent
random
variables
lemma
independent
normal
identical
variances
remark
formally
bernstein
theorem
comment
identical
variances
however
assuming
without
loss
generality
zero-mean
observation
identical
variances
immediate
since
fact
explicitly
noted
geng
nair
second
need
following
observation
lemma
let
non-degenerate
gaussian
independent
normal
-a.e
variance
depending
normal
variance
σ2σ2
σ2−σ2
proof
normal
-a.e
variance
depending
x|y
a.s.
independent
particular
density
lemma
also
lemma
density
conditional
density
exists
gaussian
deﬁnition
fx|y
valid
gaussian
density
-a.e
corresponding
variance
depending
thus
log
log
log
y|x
log
fx|y
x|y
105
key
observation
rhs
105
quadratic
function
since
density
must
integrate
unity
must
therefore
gaussian
direct
computation
reveals
variance
σ2σ2
σ2−σ2
proof
lemma
let
statement
lemma
recall
lemma
asserts
d−→
deﬁnition
random
variables
independent
gaussian
respective
variances
2σ2
thus
noting
assumption
104
equivalent
lim
inf
n→∞
n|y1
106
may
apply
lemma
sequences
obtain
∗|y1
∗|y1
107
using
independence
lemma
applied
directly
yields
∗|y1
108
particular
py1
∗y2
∗-a.e
random
variables
independent
independent
therefore
lemma
implies
normal
identical
variances
starting
third
claim
lemma
applying
lower
semicontinuity
relative
entropy
observe
lim
n→∞
lim
n→∞
109
110
111
follows
therefore
similarly
may
conclude
random
variables
normal
identical
variances
depending
invoking
lemma
ﬁnd
normal
identical
variances
completing
proof
∗|y1
existence
sequences
satisfying
limn→∞
snr|qn
snr
converge
weakly
gaussian
goal
section
prove
following
result
ﬁrst
essential
ingredient
needed
proof
theorem
i.e.
claim
lemma
exists
sequence
satisfying
lim
n→∞
snr|qn
snr
pq∗-a.e
d−→
x∗|
rough
outline
proof
follows
ﬁrst
establish
superadditivity
property
snr|q
exploit
property
conjunction
characterization
gaussians
proved
lemma
verify
existence
sequence
satisfying
limn→∞
snr|qn
snr
converges
weakly
gaussian
begin
straightforward
observation
lemma
10.
let
joint
distribution
pxyq
px1x2qpy1|x1py2|x2
satisﬁes
depending
113
112
x2|q
y2|q
|x2
x1|x2
|x2
y1|x2
|y1
x2|y1
|y1
y2|y1
114
moreover
proof
second
claim
straightforward
indeed
using
pxyq
px1x2qpy1|x1py2|x2
factor
joint
distribution
pxyv
px1x2qpy1|x1py2|x2pv
|y1y2q
px1x2qpy1|x1py2v
|y1x2q
marginalizing
ﬁnd
symmetric
markov
chain
follows
similarly
writing
pxyv
px1x2y1qpy2|x2pv
|y1y2q
marginalizing
prove
claimed
inequality
note
following
identities
x2|q
x2|q
x1|q
x2|q
x1|q
y1|q
x2|q
x1|q
y1|q
y2|q
y1|q
y2|q
y1|q
y2|q
y1|q
y1|q
y2|q
y1|q
115
116
117
118
119
120
therefore
x2|q
y2|q
|x2
x1|x2
|x2
y1|x2
|y1
x2|y1
|y1
y2|y1
y1|v
proves
inequality
114
since
lemma
leads
desired
superadditivity
property
snr|q
lemma
11.
let
gaussian
channel
√snrx
independent
suppose
pxqpy
let
denote
two
independent
copies
deﬁne
121
122
x2√2
x2√2
similar
manner
deﬁne
letting
2sλ
snr|q
snr|x−
snr|y+
123
124
2sλ
snr|q
snr|y−
snr|x+
125
proof
crucial
observation
unitary
transformation
preserves
gaussian
√snrx−
nature
channel
√snrxi
√snrx+
1√2
pair
1√2
1√2
thus
consider
arbitrary
satisfying
lemma
equal
distribution
1√2
observation
x2|q
y2|q
x−|q
y−|q
|x−
x+|x−
|x−
y+|x−
|y+
x−|y+
|y+
y−|y+
snr|x−
snr|y+
126
127
128
proves
124
since
inf
x2|q
y2|q
inf
xi=1
snr|q
xi→yi→v
|qi
|qi
xi|qi
|qi
yi|qi
129
130
131
inequality
follows
since
inﬁmum
taken
smaller
set
remark
sense
lemma
key
whole
proof
subadditivity
property
ultimately
implies
optimizing
distribution
optimization
problem
rotationally
invariant
therefore
gaussian
idea
introduced
information
theory
literature
geng
nair
origins
doubling
trick
used
great
success
literature
functional
inequalities
attributed
ball
reader
referred
detailed
discussion
duality
extremisation
information
measures
functional
inequalities
ready
prove
lemma
proof
lemma
convenience
refer
sequence
satisfying
112
113
admissible
since
snr|qn
invariant
translations
mean
may
restrict
attention
admissible
sequences
satisfying
without
loss
generality
begin
letting
admissible
sequence
property
132
lim
n→∞
yn|qn
xn|qn
lim
inf
n→∞
cid:0
′n|q′n
x′n|q′n
cid:1
admissible
sequence
x′n
q′n
clearly
sequence
always
constructed
diagonaliza-
tion
argument
moreover
lhs
132
must
ﬁnite
see
note
ﬁrst
yn|qn
xn|qn
since
conditioning
reduces
entropy
hand
snr|qn
snr
sufﬁciently
large
hence
satisfying
vn|qn
yn|qn
xn|qn
snr
vn|qn
vn|qn
yn|qn
snr
vn|qn
yn|qn
snr
yn|qn
yn|qn
snr
yn|xn
133
134
135
136
134
135
due
data
processing
inequality
since
snr
trivially
yn|xn
logic
remark
following
may
assume
|q|
since
sufﬁcient
preserve
values
snr|qn
yn|qn
xn|qn
thus
since
ﬁnite
sequence
tight
prokhorov
theorem
may
assume
d−→
restricting
attention
subsequence
necessary
moreover
next
given
let
denote
two
independent
copies
deﬁne
log
2πe
conclude
lhs
132
ﬁnite
claimed
fatou
lemma
lim
inf
n→∞
137
similar
manner
deﬁne
put
applying
lemma
variables
obtain
2sλ
snr|qn
snr|x−
nqn
snr|y+
nqn
138
symmetric
inequality
independence
assumption
2sλ
snr|qn
snr|y−
nqn
snr|x+
nqn
139
140
hence
follows
terms
rhs
138
rhs
139
lower
bounded
snr
since
limn→∞
snr|qn
snr
deﬁnition
must
also
lim
n→∞
cid:16
snr|y−
nqn
snr|y+
nqn
cid:17
snr
particular
letting
random
pair
x′n
q′n
correspond
equal
time-sharing
pairs
nqn
nqn
constructed
admissible
sequence
x′n
q′n
satisﬁes
142
lim
n→∞
x′n
snr|q′n
snr
using
markovity
following
identity
readily
established
yn|qn
xn|qn
n|qn
n|qn
n|y+
n|y+
n|y−
n|y−
n|y+
′n|q′n
x′n|q′n
n|y+
145
since
sequence
x′n
q′n
admissible
must
also
satisfy
132
therefore
view
145
fact
lhs
132
ﬁnite
implies
particular
pq∗
pq∗-a.e
lim
inf
n→∞
n|y1
lim
inf
n→∞
n|y1
completes
proof
since
lemma
guarantees
pq∗-a.e
random
variable
x∗|
normal
variance
depending
moreover
already
observed
variance
x∗|
unity
claimed
weak
semicontinuity
snr
subsection
devoted
establishing
following
semicontinuity
property
snr
second
essential
ingredient
needed
proof
theorem
i.e.
claim
lemma
12.
d−→
supn
148
recall
snr
deﬁned
terms
gaussian
channel
√snrx
however
purposes
proof
convenient
omit
snr
scaling
factor
instead
parametrize
channel
terms
snr
snr
lim
inf
n→∞
141
143
144
146
147
noise
variance
toward
end
let
random
variable
independent
deﬁne
functionals
inf
x→y
cid:16
cid:17
149
150
lemma
immediate
corollary
weak
lower
semicontinuity
gaussian
facts
established
separately
lemmas
respectively
former
straightforward
latter
requires
effort
lemma
13.
d−→
supn
proof
fix
deﬁne
pairwise
independent
observe
lim
inf
n→∞
151
152
third
claim
lemma
thus
153
lim
inf
n→∞
log
cid:0
2πe
since
lemma
14.
continuous
furthermore
cid:16
cid:17
log
cid:16
λ−1
2hlog
cid:16
cid:1
continuous
may
take
prove
claim
cid:17
cid:17
particular
continuous
parameters
gaussian
=
154
proof
function
pointwise
inﬁmum
linear
functions
therefore
concave
continuous
open
interval
distribution
explicit
expression
154
follows
identifying
snr
lemma
15.
d−→
proposition
supn
lim
inf
n→∞
155
proof
fix
interval
channel
satisfying
σ2/2
recalling
deﬁnition
decompose
mutually
independent
deﬁne
note
deﬁned
stochastic
transformation
using
notation
lemma
also
deﬁned
via
deﬁnitions
hand
may
apply
lemma
processes
conclude
existence
sequence
depending
satisﬁes
156
157
158
159
∗|x
log
log
n|x
n|y
160
161
162
163
164
165
166
167
n∈b
cid:17
n∈b
cid:17
log
cid:17
cid:17
following
sequence
inequalities
log
v∗|y
yn|x
n∈b
vn|y
vn|x
n∈b
cid:17
vn|y
vn|y
v∗|x
vn|x
vn|x
log
v∗|y
yn|x
cid:16
cid:16
cid:16
cid:16
cid:16
cid:16
cid:16
cid:16
3fλn
σ2−2δ
cid:16
cid:16
1−ǫn
cid:17
cid:16
1+ǫn
steps
justiﬁed
follows
160
follows
data
processing
inequality
161
follows
since
162
follows
chain
rule
mutual
information
163
follows
non-negativity
mutual
information
fact
yn|x
164
follows
156
157
165
follows
chain
rule
mutual
information
implies
v∗|y
log
log
log
n∈b
cid:17
log
n∈b
cid:17
log
n∈b
cid:17
y∗|y
data
processing
inequality
implies
n∈b
functions
cid:17
cid:17
yn|x
yn|x
yn|x
v∗|y
y∗|y
vn|x
cid:16
respectively
n∈b
n∈b
cid:17
168
combined
non-negativity
mutual
information
implies
166
follows
158
159
fact
y∗|y
σ2−2δ
167
follows
deﬁnition
summarizing
shown
v∗|y
v∗|x
taking
inﬁmum
satisfying
data
processing
inequality
169
σ2−2δ
170
y∗|y
cid:16
cid:16
log
yn|x
log
n∈b
cid:17
cid:17
lim
inf
n→∞
y∗|y
note
rhs
170
depend
i.e.
thus
taking
inﬁmum
satisfying
letting
arrive
cid:16
fλn
σ2−2δ
σ2−2δ
cid:16
∗∈b
cid:17
follows
due
following
continuity
lemma
d−→
third
claim
lemma
since
lim
supn
due
fact
supn
∗∈b
continuity
binary
entropy
function
since
y∗|x
ﬁrst
claim
lemma
token
take
continuity
binary
entropy
function
lemma
together
imply
latter
two
terms
rhs
171
vanish
yielding
inequality
cid:1
cid:0
positive
constant
yn|x
σ2−2δ
cid:17
y∗|x
n∈b
171
since
arbitrary
σ2−2δ
lim
inf
n→∞
continuous
lemma
proof
complete
letting
172
σ2−2δ
remark
given
tedious
chain
inequalities
proof
lemma
easy
lose
sight
overall
picture
crucial
idea
perturbing
allows
eventually
eliminate
dependence
channel
rhs
167
resisting
temptation
take
limits
dependence
particular
channel
eliminated
i.e.
inequality
170
also
essential
note
hypothesis
needed
proof
lemma
last
step
indeed
may
actually
conclude
following
general
result
holds
may
independent
interest
proposition
suppose
d−→
supn
following
holds
lim
inf
n→∞
independent
σ2−δ′
173
proof
claim
follows
proof
lemma
stopping
172
particularizing
gaussian
replacement
straightforward
decomposing
differently
ﬁrst
step
proof
remark
possible
establish
weak
upper
semicontinuity
needed
purposes
references
c.e
shannon
mathematical
theory
communication
bell
system
technical
journal
:623–656
oct
1948
stam
inequalities
satisﬁed
quantities
information
fisher
shannon
information
control
:101–112
1959
nelson
blachman
convolution
inequality
entropy
powers
information
theory
ieee
transactions
:267–271
apr
1965
m.h.m
costa
new
entropy
power
inequality
information
theory
ieee
transactions
:751–760
nov
1985
ruoheng
liu
tie
liu
h.v
poor
shamai
vector
generalization
costa
entropy-power
inequality
applications
information
theory
ieee
transactions
:1865–1879
april
2010
m.h.m
costa
gaussian
interference
channel
information
theory
ieee
transactions
:607–615
sep
1985
yury
polyanskiy
yihong
wasserstein
continuity
entropy
outer
bounds
interference
channels
arxiv
preprint
arxiv:1504.04419
2015
bagherikaram
a.s.
motahari
a.k
khandani
secrecy
capacity
region
gaussian
mimo
broadcast
channel
information
theory
ieee
transactions
:2673–2682
may
2013
a.b
wagner
tavildar
viswanath
rate
region
quadratic
gaussian
two-encoder
source-coding
problem
information
theory
ieee
transactions
:1938–1961
may
2008
oohama
gaussian
multiterminal
source
coding
information
theory
ieee
transactions
:1912–1923
nov
1997
rioul
information
theoretic
proofs
entropy
power
inequalities
information
theory
ieee
transactions
:33–55
jan
2011
bergmans
simple
converse
broadcast
channels
additive
white
gaussian
noise
corresp.
information
theory
ieee
transactions
:279–280
mar
1974
weingarten
steinberg
shamai
capacity
region
gaussian
multiple-input
multiple-output
broadcast
channel
information
theory
ieee
transactions
:3936–3964
sept
2006
mohseni
j.m
ciofﬁ
proof
converse
capacity
gaussian
mimo
broadcast
channels
information
theory
2006
ieee
international
symposium
pages
881–885
july
2006
leung-yan-cheong
m.e
hellman
gaussian
wire-tap
channel
information
theory
ieee
transactions
:451–456
jul
1978
tekin
yener
gaussian
multiple
access
wire-tap
channel
information
theory
ieee
transactions
:5747–5755
dec
2008
ozarow
source-coding
problem
two
channels
three
receivers
bell
system
technical
journal
:1909–1921
dec
1980
oohama
rate-distortion
theory
gaussian
multiterminal
source
coding
systems
several
side
informations
decoder
information
theory
ieee
transactions
:2577–2593
july
2005
prabhakaran
tse
ramachandran
rate
region
quadratic
gaussian
ceo
problem
information
theory
2004.
isit
2004.
proceedings
international
symposium
pages
119–
june
2004
mokshay
madiman
entropy
sums
proc
ieee
inform
theory
workshop
pages
303–307
2008
maice
costa
thomas
cover
similarity
entropy
power
inequality
brunn-minkowski
inequality
corresp.
information
theory
ieee
transactions
:837–839
1984
leonard
gross
logarithmic
sobolev
inequalities
american
journal
mathematics
:1061–1083
1975
edward
nelson
free
markoff
ﬁeld
journal
functional
analysis
:211–227
1973
eric
carlen
superadditivity
fisher
information
logarithmic
sobolev
inequalities
journal
functional
analysis
101
:194–
211
1991
maxim
raginsky
igal
sason
concentration
measure
inequalities
information
theory
communications
coding
foundations
trends
communications
information
theory
1-2
:1–247
2013
ioannis
kontoyiannis
mokshay
madiman
sumset
inverse
sumset
inequalities
differential
entropy
mutual
information
information
theory
ieee
transactions
:4503–4514
2014
mokshay
madiman
ioannis
kontoyiannis
ruzsa
divergence
random
elements
locally
compact
abelian
groups
arxiv
preprint
arxiv:1508.04089
2015
sergey
bobkov
nathael
gozlan
cyril
roberto
p-m
samson
bounds
deﬁcit
logarithmic
sobolev
inequality
journal
functional
analysis
267
:4110–4138
2014
max
fathi
emanuel
indrei
michel
ledoux
quantitative
logarithmic
sobolev
inequalities
stability
estimates
arxiv
preprint
arxiv:1410.6922
2014
jean
dolbeault
giuseppe
toscani
stability
results
logarithmic
sobolev
gagliardonirenberg
inequalities
international
mathematics
research
notices
page
rnv131
2015
keith
ball
franck
barthe
assaf
naor
entropy
jumps
presence
spectral
gap
duke
mathematical
journal
119
:41–63
2003
keith
ball
van
hoang
nguyen
entropy
jumps
isotropic
log-concave
random
vectors
spectral
gap
studia
mathematica
213
:81–96
2012
berger
multiterminal
source
coding
longo
information
theory
approach
communications
springer-verlag
new
york
usa
1977
s.-y
tung
multiterminal
source
coding
phd
thesis
cornell
university
ithaca
1978
abbas
gamal
young-han
kim
network
information
theory
cambridge
university
press
2012
jia
wang
jun
chen
xiaolin
sum
rate
gaussian
multiterminal
source
coding
new
proofs
results
information
theory
ieee
transactions
:3946–3960
2010
thomas
courtade
jiantao
jiao
extremal
inequality
long
markov
chains
communication
control
computing
allerton
2014
52nd
annual
allerton
conference
pages
763–770
ieee
2014
thomas
courtade
tsachy
weissman
multiterminal
source
coding
logarithmic
loss
ieee
transactions
information
theory
2014
jiantao
jiao
thomas
courtade
kartik
venkat
tsachy
weissman
justiﬁcation
logarithmic
loss
via
beneﬁt
side
information
information
theory
ieee
transactions
:5357–5365
2015
hiroshi
sato
capacity
gaussian
interference
channel
strong
interference
corresp.
information
theory
ieee
transactions
:786–788
1981
han
sun
kingo
kobayashi
new
achievable
rate
region
interference
channel
ieee
transactions
information
theory
:49–60
1981
max
costa
noisebergs
z-gaussian
interference
channels
information
theory
applications
workshop
ita
2011
pages
1–6
ieee
2011
chandra
nair
max
costa
gaussian
z-interference
channel
around
corner
information
theory
applications
workshop
ita
2016.
ieee
2016
rudolf
ahlswede
peter
g´acs
spreading
sets
product
spaces
hypercontraction
markov
operator
annals
probability
pages
925–939
1976
chandra
nair
equivalent
formulations
hypercontractivity
using
information
measures
international
zurich
seminar
communications
page
2014
venkat
anantharam
amin
aminzadeh
gohari
sudeep
kamath
chandra
nair
hypercontractivity
mutual
information
boolean
functions
allerton
pages
13–19
2013
maxim
raginsky
strong
data
processing
inequalities
φ-sobolev
inequalities
discrete
channels
arxiv
preprint
arxiv:1411.3575
2014
yury
polyanskiy
yihong
dissipation
information
channels
input
constraints
arxiv
preprint
arxiv:1405.3629
2014
fl´avio
pin
calmon
yury
polyanskiy
yihong
strong
data
processing
inequalities
power-constrained
gaussian
channels
isit
pages
2558–2562
ieee
2015
venkat
anantharam
amin
gohari
sudeep
kamath
chandra
nair
maximal
correlation
hypercontractivity
data
processing
inequality
studied
erkip
cover
arxiv
preprint
arxiv:1304.6133
2013
thomas
courtade
outer
bounds
multiterminal
source
coding
via
strong
data
processing
inequality
information
theory
proceedings
isit
2013
ieee
international
symposium
pages
559–563
ieee
2013
harold
gordon
eggleston
convexity
number
47.
cup
archive
1958
s.g.
bobkov
g.p
chistyakov
entropy
power
inequality
r´enyi
entropy
information
theory
ieee
transactions
:708–714
feb
2015
wlodzimierz
bryc
normal
distribution
characterizations
applications
volume
100.
springer
science
business
media
2012
yanlin
geng
nair
capacity
region
two-receiver
gaussian
vector
broadcast
channel
private
common
messages
information
theory
ieee
transactions
:2087–2104
april
2014
yihong
verdu
functional
properties
minimum
mean-square
error
mutual
information
information
theory
ieee
transactions
:1289–1301
march
2012
bernstein
property
characteristic
normal
law
trudy
leningrad
polytech
inst
3:21–22
1941
elliott
lieb
gaussian
kernels
gaussian
maximizers
inventiones
mathematicae
102
:179–208
1990
franck
barthe
optimal
young
inequality
converse
simple
proof
geometric
functional
analysis
gafa
:234–242
1998
eric
carlen
dario
cordero-erausquin
subadditivity
entropy
relation
brascamplieb
type
inequalities
geometric
functional
analysis
:373–405
2009
jingbo
liu
thomas
courtade
paul
cuff
sergio
verd´u
information
theoretic
perspectives
brascamp-lieb
inequalities
2016
international
symposium
information
theory
submitted
2016
rick
durrett
probability
theory
examples
cambridge
university
press
2010
