rate-distortion
risk
estimation
compressed
data
alon
kipnis∗
stefano
rini†
andrea
goldsmith‡
department
statistics
stanford
university
stanford
94305
usa
department
electrical
engineering
stanford
university
stanford
94305
usa
department
electrical
engineering
national
chiao-tung
university
hsinchu
30010
taiwan
abstract
consider
problem
estimating
latent
signal
lossy
compressed
version
data
assume
data
generated
underlying
signal
compressed
using
lossy
compression
scheme
agnostic
signal
reconstruction
underlying
signal
estimated
minimize
prescribed
loss
measure
setting
arbitrary
distortion
measure
data
compressed
version
deﬁne
rate-distortion
risk
estimator
risk
respect
distribution
achieving
shannon
function
respect
distortion
derive
conditions
risk
describes
risk
estimating
compressed
data
main
theoretical
tools
obtain
conditions
transportation-cost
inequalities
conjunction
properties
source
codes
achieving
shannon
function
show
conditions
hold
various
settings
including
settings
alphabet
underlying
signal
ﬁnite
achieving
distribution
multivariate
normal
evaluate
risk
special
cases
settings
risk
provides
achievable
loss
compress-and-estimate
settings
i.e.
data
ﬁrst
compressed
communicated
stored
using
procedure
agnostic
underlying
signal
later
estimated
compressed
version
data
results
imply
following
general
procedure
designing
estimators
datasets
undergoing
lossy
compression
without
specifying
actual
compression
technique
train
estimator
based
perturbation
data
according
achieving
distribution
general
conditions
estimator
achieves
risk
applied
lossy
compressed
version
data
motivation
introduction
digital
systems
subject
constraints
number
bits
store
communicate
process
necessarily
data
acquired
compressed
lossy
manner
procedure
extract
information
take
place
result
performance
procedures
intrinsically
dictated
quality
compression
applied
raw
data
particular
number
bits
available
compressed
representation
following
study
effect
lossy
compression
performance
estimation
procedures
paper
presented
part
ieee
international
symposium
information
theory
isit
2016
2017
information
theory
workshop
itw
2015
n|θ
estimator
lossy
compression
bit
rate
distortion
measure
fig
estimation
compressed
data
formulation
figure
ﬁgure
signal
represented
latent
variable
data
represented
n-dimensional
sequence
sequence
compressed
bitrate
i.e.
using
bits
yielding
compressed
representation
denoted
finally
sequence
utilized
produce
estimate
underlying
signal
setting
optimal
tradeoff
compression
bitrate
loss
estimation
well
known
classical
results
source
coding
show
optimal
tradeoff
achieved
estimate-and-compress
approach
approach
ﬁrst
compute
estimate
sufﬁcient
statistic
underlying
signal
data
compress
estimate
according
available
bit
budget
many
modern
applications
however
estimate-and-compress
approach
infeasible
example
data
science
machine
learning
applications
data
acquired
compressed
general-purpose
use
without
assumption
possible
future
uses
infeasibility
estimation
compression
arises
also
low
complexity
signal
acquisition
scenarios
simple
hardware
employ
complex
estimation
procedures
quantization
scenarios
compression
data
figure
may
designed
considering
distortion
measure
e.g.
euclidean
distance
empirical
mutual
information
agnostic
rather
considering
end
estimation
task
unknown
compressor
paper
strive
characterize
performance
estimating
class
lossy
compression
schemes
consider
setting
figure
given
loss
function
deﬁne
risk
estimator
equation
risk
respect
using
estimating
underlying
signal
compressed
version
data
paper
assume
compressed
version
designed
attain
prescribed
expected
distortion
level
respect
distortion
measure
since
underlying
signal
estimated
compressed
representation
data
denote
setting
compress-and-estimate
source
coding
consequently
refereed
risk
rate-distortion
risk
consider
lossy
compression
problem
determining
minimal
coding
rate
describing
data
function
inverse
distortion-rate
function
describe
optimal
tradeoff
compression
rate
expected
distortion
problem
good
code
problem
said
sequence
codes
satisﬁes
target
distortion
whose
bitrate
converges
shannon
provided
variational
characterization
minimal
mutual
information
family
transition
probability
kernels
alphabet
data
alphabet
compressed
representation
whenever
minimizer
pz∗n|y
unique
deﬁne
risk
estimator
respect
loss
function
ﬁdelity
criteria
bitrate
expected
risk
respect
joint
distribution
n|θpz∗n|y
namely
paper
characterize
risk
estimating
compressed
data
using
risk
speciﬁcally
derive
conditions
risk
converges
risk
show
conditions
hold
various
important
cases
whenever
compressed
using
good
code
respect
beneﬁt
characterization
twofold
evaluate
risk
estimation
datasets
undergoing
lossy
compression
well
design
estimators
compressed
data
minimizing
risk
application
example
characterization
useful
assume
represents
high-resolution
image
bitrate
lossy
compressed
version
using
standard
image
compression
technique
consider
problem
estimating
latent
information
image
represented
identities
person
image
traditionally
estimator
task
depends
particular
lossy
compression
technique
e.g.
obtained
training
convolutional
neural
network
compressed
data
results
imply
one
derive
estimator
works
well
compression
technique
designing
estimate
assuming
random
perturbation
distribution
n|θpz∗n|y
conditions
derived
later
paper
estimator
guaranteed
attain
risk
bitrate
lossy
compression
technique
approaching
shannon
compression
limit
respect
given
distortion
related
works
problem
estimation
compressed
data
long
history
source
coding
underlying
signal
ergodic
information
source
problem
encoding
estimating
known
indirect
a.k.a
remote
noisy
source
coding
problem
3.5
consequently
inﬁmum
achievable
distortions
using
compression
code
rate
exceeding
denoted
indirect
distortion-rate
function
i-drf
ﬁnite
vector
parameters
distribution
governing
generation
random
data
problem
compressing
recovering
sometimes
referred
compression
estimation
task-speciﬁc
compression
problems
particular
multi-terminal
versions
received
much
recent
interest
mostly
due
relevance
machine
learning
setting
departs
works
since
compression
assumed
optimal
respect
data
rather
underlying
signal
parameter
works
study
multi-terminal
version
problem
motivated
robustness
performance
due
encoders
oblivious
system
components
works
provided
achievable
coding
bitrate
regions
random
codebook
generation
joint
typicality
encoding
prescribed
distortion
level
knowledge
term
distortion
coined
inspired
compress-and-forward
coding
scheme
relay
channel
special
settings
exact
characterization
include
compressing
samples
wiener
process
gaussian
random
walk
well
compressing
measurements
obtained
via
sequence
random
linear
projections
recently
work
provided
conditions
risk
estimator
data
compressed
using
random
spherical
code
equals
risk
estimator
corrupted
version
data
using
additive
white
gaussian
noise
awgn
result
implies
form
compression
risk
estimator
converges
risk
whenever
achieving
distribution
pz∗n
gaussian
current
work
generalizes
last
statement
broad
class
achieving
distributions
compression
procedures
attaining
function
respect
data
speciﬁcally
setup
applies
situation
achieving
distribution
satisﬁes
suitable
transportation-cost
inequality
explain
condition
holds
quite
broadly
model
separable
across
dimensions
sense
data
i.i.d
single
letter
distortion
measure
follows
comment
connection
setting
problems
source
coding
work
considered
ability
wiretap
recover
underlying
signal
data
undergoing
lossy
compression
derived
minimal
distortion
wiretap
one
must
tolerate
order
guarantee
target
distortion
respect
data
case
encoder
chooses
code
pessimistic
estimating
underlying
signal
hence
distortion
bounds
risk
bayes-optimal
estimator
consequently
risk
provides
lower
bound
distortion
whenever
equals
risk
estimating
one
may
also
view
scheme
instance
mismatched
source
coding
considered
indeed
pointed
situations
attain
i-drf
compressing
data
according
ﬁdelity
criterion
˜zn
inf
thus
reducing
indirect
source
coding
problem
standard
one
mismatch
arises
data
encoded
minimize
ﬁdelity
criterion
instead
finally
donoho
considered
risk
estimating
n-dimensional
vector
means
standard
normal
vector
using
shortest
computer
program
produces
summary
expected
quadratic
distortion
showed
generated
i.i.d
compressed
using
optimal
lossy
compression
code
attains
mse
equal
true
underlying
noise
level
coordinates
distributed
according
true
posterior
pθ1|y1
even
though
posterior
never
revealed
compressor
consequently
resulting
expected
quadratic
loss
risk
sampling
posterior
var
θ1|y1
translated
terminology
donoho
results
says
model
risk
cid:17
log2
cid:16
var
respect
quadratic
loss
maximum
likelihood
estimator
coding
rate
compression
distortion
converges
cid:2
cid:3
var
θ1|y1
problem
dimension
goes
inﬁnity
results
specialized
model
imply
var
θ1|y1
risk
quadratic
loss
applied
compressed
version
satisfying
following
two
conditions
expected
quadratic
distortion
compression
coding
rate
approaches
contributions
main
result
paper
described
section
relates
cases
achieving
distribution
pz∗n
satisﬁes
transportation-cost
inequality
constant
case
difference
risk
l-lipschitz
estimator
risk
fed
output
rate-rn
code
n|y
satisfying
target
distortion
bounded
constant
times
prn
plus
relative
entropy
marginals
pz∗n
particular
difference
vanishes
whenever
relative
entropy
vanishes
last
condition
indicates
code
utilized
compression
good
rate-distortion
code
target
distortion
next
sections
show
condition
holds
quite
broadly
whenever
pz∗n
product
measure
discrete
domain
pz∗n|y
gaussian
measure
sections
also
evaluate
risk
three
special
cases
i.i.d
binary
signal
noisy
observation
binary
symmetric
channel
estimator
bayes-optimal
estimator
gaussian
i.i.d
signal
dimensional
vector
representing
sequence
noisy
observations
ﬁnally
iii
parameter
representing
unknown
mean
normal
distribution
estimator
averaging
operation
ﬁrst
two
examples
compare
risk
optimal
source
coding
performance
described
i-drf
example
iii
derive
relative
asymptotic
efﬁciency
mean
estimator
compressed
data
compared
efﬁcient
estimator
access
uncompressed
data
paper
organization
remainder
paper
organized
follows
section
presents
problem
formulation
section
iii
reviews
relevant
results
optimal
transport
theory
main
results
given
section
sections
specialize
main
result
settings
data
discrete
gaussian
respectively
concluding
remarks
provided
section
vii
notation
use
capital
calligraphic
letters
respectively
denote
random
variable
alphabet
therefore
explicitly
provided
implicitly
assume
probability
space
topology
n|θ
enc
dec
signal
data/measurements
compressed
representation
respectively
lipschitz
metric
fig
spaces
sub-additive
distortion
measure
obtained
compressing
data
using
nrn
bits
according
ﬁdelity
criterion
estimator
compressed
representation
data
measurable
denote
n-length
sequence
speciﬁc
case
denote
rm×n
matrices
also
denoted
bold
capital
letters
hence
depending
context
may
denote
either
random
vector
matrix
also
denote
euclidean
norm
normalized
hamming
distance
discrete
space
multivariate
normal
distribution
mean
covariance
matrix
denoted
bernoulli
distribution
probability
success
denote
use
notation
iid∼
denote
random
sequence
obtained
sampling
independently
times
distribution
support
distribution
denoted
supppx
problem
formulation
consider
lossy
compression
estimation
problem
illustrated
figure
ﬁgure
detailed
version
figure
separates
lossy
compression
operation
encoder
decoder
indicates
alphabet
loss
function
associated
variables
data
represented
random
n-dimensional
vector
alphabet
distribution
depends
underlying
signal
parameter
vector
conditional
distribution
kernel
n|θ
encoder
encodes
data
using
nrn
bits
decoder
produces
bit-restricted
representation
data
representation
alphabet
throughout
paper
assume
induced
stationary
ergodic
process
metric
spaces
complete
hence
polish
estimator
mensurable
respect
borel
σ-algebras
pth
risk
deﬁned
expectation
respect
n|θ
possible
randomness
encoding
decoding
explained
whenever
prior
given
expectation
also
respect
prior
case
refereed
pth
bayes
risk
respect
distortion-rate
function
deﬁned
bayes-optimal
risk
given
namely
inf
note
loss
function
pth
power
metric
notation
setting
restricted
loss
functions
form
given
lossy-compression
n-block
code
fenc
fdec
simply
n-block
code
deﬁned
pair
mappings
fenc
fdec
deﬁne
log2
rate
code
bits
following
consider
codes
fenc
fdec
random
mapping
fdec
fenc
measurable1
case
convenient
refer
code
transition
probability
kernel
n|y
deﬁned
namely
fdec
fenc
a|y
=za
n|y
dzn|yn
borel
measurable
output
distribution
code
n|y
marginal
distribution
i.e.
dzn
=zy
n|y
dzn|yn
dyn
distortion
measured
using
function2
assumed
sub-additive
sense
yk+m
zk+m
ym+k
k+1
zm+k
k+1
ym+k
m+k
zm+k
m+k
also
set
dmin
inf
y∈y
z∈z
sub-additivity
also
implies
inf
ndmin
code
fenc
fdec
said
d-admissible
expected
distortion
satisﬁes
target
distortion
dmin
distribution
shannon
function
deﬁned
min
pzn
≤dn
1we
specify
sigma
algebras
borel
sigma
algebras
sigma
algebra
implicit
notation
2the
dependency
explicitly
indicated
minimum
conditional
distribution
kernels
n|y
satisfying
prescribed
distortion
constraint
also
deﬁne
single
letter
function
respect
lim
n→∞
nry
note
sub-additivity
guarantees
limit
right-hand
side
rhs
exits
single
letter
distortion-rate
function
deﬁned
inverse
function
smaller
entropy
rate
zero
otherwise
particular
deﬁned
whenever
n|θ
absolutely
continuous
respect
lebesgue
measure
wide
class
distributions
describes
inﬁmum
rate
d-admissible
codes
describes
inﬁmum
codes
n|y
words
exists
sequences
codes
n|y
lim
n→∞
sequence
codes
satisfying
refereed
good
rate-distortion
code
distortion
level
throughout
paper
consider
following
condition
minimum
achieved
pz∗n|y
also
satisﬁes
condition
relatively
mild
usually
holds
whenever
dmin
inf
i.e.
distortion
levels
source
coding
problem
respect
non-trivial
kostina
verdu
provided
characterization
conditional
relative
entropy
good
code
achieving
distribution
pz∗n
proposition
2.1
properties
good
codes
thm
assume
holds
code
n|y
supppz
supppz∗n
cid:0
n|z
n||py
n|z∗n
cid:1
recall
relative
entropy
two
distributions
deﬁned
pu||pv
log2
cid:18
dpu
dpv
cid:19
dpu
dpu
dpv
radon-nikodym
derivative
respect
estimator
measurable
respect
borel
σ-algebras
deﬁne
pth
risk
respect
expectation
respect
n|θpz∗n
whenever
prior
provided
consider
bayes
risk
bayes-optimal
risk
inf
expectation
respect
pθpy
n|θpz∗n
goal
work
establish
connection
risk
given
encoding
deﬁnes
good
code
throughout
paper
make
frequent
use
following
assumptions
code
n|y
-almost
every
n|y
n=yn
absolutely
continuous
respect
pz∗n|y
n=yn
indicated
pz|y
pz∗|y
output
distribution
n|y
satisﬁes
lim
n→∞
n||pz∗n
hold
general
deterministic
codes
indeed
codes
n|y
n=yn
mass
distribution
hence
hold
n|y
absolutely
continuous
respect
lebesgue
measure
examples
deterministic
codes
violating
provided
prop
nevertheless
hold
following
important
random
coding
scenarios
codewords
randomly
drawn
pz∗n
encoding
done
using
joint
typically
whenever
i.i.d
discrete
alphabet
distortion
separable
output
distribution
code
uniform
codewords
iii
whenever
standard
normal
codewords
drawn
spherically
symmetric
distribution
absolutely
continuous
respect
lebesgue
measure
encoding
done
selecting
codeword
maximal
cosine
similarity
input
cases
iii
incorporate
codes
attaining
optimal
source
coding
performance
noted
analyzing
random
coding
results
conﬁrm
existence
deterministic
codes
satisfying
average
performance
guarantee
relevant
since
practice
deterministic
codes
needed
problem
formulation
encompasses
two
classical
settings
case
unknown
parameter
case
information
source
two
scenarios
introduced
next
together
examples
revisited
later
paper
parameter
estimation
setting
one
scenarios
encompassed
figure
one
unknown
parameter
family
distributions
estimated
lossy
compressed
observations
focus
scenario
respect
efﬁciency
estimator
compressed
data
compared
efﬁcient
estimator
uncompressed
data
explore
efﬁciency
using
setting
figure
letting
√n|θ
ˆθ|
considering
scaling
estimation
error
number
observations
available
scalar
gaussian
location
example
one
example
parameter
estimation
setting
normal
location
model
example
unknown
mean
normal
distribution
data
consists
independent
draws
distribution
namely
σwi
distortion
iid∼
data
compressed
using
procedure
aiming
minimize
quadratic
kyn
znk2
xi=1
i.e.
squared
euclidean
distance
compressed
representation
compression
said
optimal
respect
data
nk2/n
approaches
σ22−2r
quadratic
gaussian
drf
variance
importantly
setting
require
description
actual
compression
procedure
long
optimal
memoryless
information
source
setting
another
example
setting
figure
case
represents
independent
samples
distribution
case
underlying
sequence
whenever
n|θ
n|x
decomposes
product
distortion
additive
bayes-optimal
risk
corresponds
bayes-optimal
risk
yi|xi
estimating
distribution
prescribed
distortion
sections
explore
case
single-letter
rate-distortion
achieving
pxi|z∗
pxi|yip
pyi|z∗
yi|zi
aforementioned
form
following
speciﬁc
examples
binary
signal
observed
bitﬂip
noise
setting
n-dimensional
bernoulli
i.i.d
sequence
data
obtained
passing
binary
symmetric
channel
denotes
addition
modulo
also
bernoulli
i.i.d
sequence
namely
setting
estimation
compressed
version
according
metric
case
taken
normalized
hamming
distance
xi=1
ui6=vi
distortion
measure
also
taken
hamming
distance
namely
consider
estimation
binary
sequence
hamming
distance
latter
obtained
compressing
attain
target
expected
hamming
distance
prescribed
rate
constraint
single
input
multiple
output
gaussian
channel
setting
generated
sampling
standard
gaussian
distribution
data
point
m-dimensional
jointly
gaussian
vector
yi,1
√γjxi
iid∼
positive
constants
determining
channel
strength
realization
observation
jth
output
channel
accordingly
assume
data
compressed
minimize
quadratic
distortion
compressed
representation
i.e.
ehky
nk2i
scenario
achieving
distribution
pz∗n
product
distribution
two
m-dimensional
multivariate
gaussian
according
pinsker
kolmogorov
waterﬁlling
formula
iii
transportation
theory
interlude
let
review
section
transportation
theoretic
notions
useful
remainder
paper
transportation
cost-information
inequalities
expressed
wasserstein
distance
distance
widely
used
source
coding
ergodic
theory
since
many
relevant
quantities
continuous
respect
optimal
quantizer
performance
shannon
distortion-rate
function
transportation
cost
distance-divergence
inequalities
discovered
marton
easy
proof
concentration
measure
property
property
generalization
blowing
property
used
providing
various
converse
channel
coding
results
refer
background
applications
transport
theory
information
theory
probability
deﬁnition
3.1
wasserstein
distance
let
two
borel
probability
measures
respect
metric
polish
space
p-wassesrstein
distance
deﬁned
cid:18
inf
cid:2
cid:3
cid:19
1/p
inﬁmum
joint
probability
distributions
product
space
marginals
similarly
deﬁnition
3.1
p-wasserstein
distance
conditioned
deﬁned
pu|v
cid:18
inf
cid:2
cid:3
cid:19
1/p
deﬁned
product
space
v×u
inﬁmum
joint
probability
distributions
product
space
marginals
motivation
using
wasserstein
distance
setting
follows
following
continuity
argument
stated
arbitrary
triplet
random
vectors
metric
spaces
proposition
3.1
continuity
risk
wassesrstein
distance
let
distribution
let
pz|y
¯z|y
two
conditional
distributions
kernels
l-lipschitz
cid:2
cid:0
cid:1
cid:3
proof
markov
chain
triangle
inequality
lipschitz
continuity
imply
1/p
cid:0
cid:2
cid:0
cid:1
cid:3
cid:1
1/p
cid:12
cid:12
cid:12
lwp
pz|y
¯z|y
cid:12
cid:12
cid:12
cid:0
cid:2
cid:0
cid:1
cid:3
cid:1
1/p
1/p
cid:0
cid:2
cid:0
cid:1
cid:3
cid:1
1/p
cid:0
cid:2
cid:0
cid:1
cid:3
cid:1
1/p
cid:3
cid:1
1/p
cid:0
cid:2
one
side
inequality
obtained
taking
minimum
joint
distributions
marginals
second
inequality
follows
interchanging
role
cid:3
probability
measures
metric
space
whose
wasserstein
distance
respect
measure
bounded
relative
entropy
said
satisfy
transportation
cost
inequality
per
following
deﬁnition
deﬁnition
3.2
transportation-cost
inequality
def
22.1
def
3.4.2
let
polish
space
said
probability
measure
satisﬁes
inequality
constant
every
borel
probability
measure
holds
say
≤pcd
q||p
following
result
marton
shows
product
measure
discrete
space
1/2n
respect
normalized
hamming
distance
space
proposition
3.2
let
countable
set
let
two
probability
measures
consider
space
normalized
hamming
distance
=qn
i=1
qz1
n||qz
following
result
talagrand
shows
standard
gaussian
measure
respect
euclidean
distance
proposition
3.3
thm
1.1
assume
¯zn
kzn
¯znk
let
measure
absolutely
continuous
respect
dzn
−n/2e−
kznk2
dzn
≤p2d
n||γn
straightforward
extension
proposition
3.3
case
shifted
scaled
gaussian
measure
follows
proposition
3.4
assume
¯zn
kzn
¯znk
let
borel
measure
absolutely
continuous
respect
dzn|yn
∗σ−1
dzn
−n/2
exp
pdet
kσk2
n||γn
positive
deﬁnite
kσk2
operator
norm
proof
see
appendix
cid:3
main
result
section
present
main
result
characterization
difference
risk
risk
whenever
compressed
using
good
rate-distortion
code
theorem
4.1
consider
distribution
sub-additive
distortion
measure
distortion
level
dmin
assume
following
conditions
hold
minimum
achieved
pz∗n|y
also
satisﬁes
n-almost
every
exists
pz∗n|y
n=yn
let
n|y
d-admissible
random
code
satisfying
l-lipschitz
estimator
cid:12
cid:12
cid:12
1/p
cid:0
lsne
cid:18
cid:1
1/p
cid:12
cid:12
cid:12
n||z
cid:19
provided
proof
sake
brevity
clarity
omit
superscripts
deduced
context
assumption
implies
supppz|y
supppz∗|y
hence
||py
|z∗
||py
pz||pz∗
||py
=zy
pz|y
=y||pz∗|y
dpy
therefore
almost
every
cid:0
pz|y
pz∗|y
cid:1
≤qc
pz|y
=y||pz∗|y
proposition
3.1
cid:12
cid:12
cid:12
cid:1
1/p
cid:12
cid:12
cid:12
1/p
cid:0
lwp
pz|y
pz∗|y
lzy
cid:0
pz|y
pz∗|y
cid:1
dpy
lzyqc
pz|y
=y||pz∗|y
dpy
lpe
||pz∗
follows
cauchy-schwartz
inequality
next
use
theorem
2.1
obtain
||pz∗
|z||py
|z∗|pz
pz||pz∗
pz||pz∗
implies
cid:12
cid:12
cid:12
1/p
cid:0
lpe
n||z
cid:1
1/p
cid:12
cid:12
cid:12
equation
follows
deﬁnition
data
processing
inequality
d-admissible
rate
code
also
satisﬁes
nrn
cid:3
theorem
4.1
bounds
distance
pth
risk
lipschitz
estimator
compressed
data
pth
risk
estimator
using
expected
constant
measure
pz∗n|y
n=yn
metric
space
typical
use
case
theorem
4.1
explore
next
section
situations
1/n
cases
theorem
4.1
provides
sufﬁcient
conditions
good
rate-distortion
codes
satisfying
next
two
sections
consider
special
cases
conditions
met
n−1/p
cid:12
cid:12
cid:12
1/p
cid:0
cid:1
1/p
cid:12
cid:12
cid:12
separable
discrete
information
sources
section
focus
special
cases
setting
section
ii-b
space
discrete
problem
separable
across
information
source
dimension
shown
next
setting
satisﬁes
many
conditions
theorem
4.1
leading
precise
characterization
risk
data
encoded
using
good
rate-distortion
code
deﬁned
prop
2.1
estimation
discrete
separable
models
let
denote
separable
information
model
setting
section
ii-b
n|x
pyi|xi
yi=1
py1|x1
yi=1
ˆxn
ˆxi
xi=1
xi=1
27a
27b
due
additivity
mutual
information
product
measures
follows
pz∗n
satisﬁes
equality
also
product
form
thus
pz∗n
pz∗
yi=1
inf
pz1
|y1
also
note
bayes-optimal
estimator
respect
also
separable
ith
coordinate
given
˜ψi
argmin
x∈x
addition
assumption
information
model
separable
also
assume
section
alphabet
countable
assumption
normalized
hamming
distance
provides
canonical
metric
proposition
3.2
thus
implies
pz∗
|y1=y
satisﬁes
transportation-cost
inequality
combining
inequality
proposition
3.2
result
theorem
4.1
obtain
following
result
theorem
5.1
consider
discrete
separable
model
assume
dmin
holds
every
sequence
i=1
let
n|y
code
rate
satisfying
codes
n|y
lipschitz
estimator
n=1
satisﬁes
npn
lim
n→∞
xi=1
sequence
lipschitz
estimators
whose
lipschitz
constants
uniformly
bounded
lim
inf
n→∞
xi=1
cid:3
proof
see
appendix
emphasize
theorem
5.1
provides
positive
i.e
achievability
negative
i.e
converse
coding
results
speciﬁcally
implies
achievable
employing
bayes-optimal
estimator
coordinate
hand
implies
bounds
distortion
estimator
separable
output
good
sequence
codes
satisfying
another
important
conclusion
theorem
5.1
among
estimators
compressed
representation
separable
estimator
minimal
asymptotic
risk
example
binary
signal
bitﬂip
noise
let
return
setting
section
ii-b1
utilize
theorem
5.1
characterize
distortion
scenario
let
indicated
bernoulli
distribution
probability
success
iid∼
iid∼
setting
iid∼
henceforth
assume
simplicity
1/2
also
implies
that3
1/2
conditional
distribution
pz∗
|y1
obtained
described
memoryless
channel
iid∼
independent
resulting
shannon
function
given
=
binary
entropy
function
next
consider
estimation
binary
sequence
normalized
hamming
distortion
bayes-optimal
estimator
setting
argmin
0,1
estimator
lipschitz
respect
normalized
hamming
distance
case
estimator
spaces
ﬁnite
alphabet
appendix
show
risk
target
distortion
respect
data
i.e.
bayes-optimal
risk
bitrate
given
conditions
theorem
5.1
risk
estimator
applied
output
code
data
target
distortion
whose
rate
converges
converges
3note
complementary
cases
obtained
replacing
min
1/2
min
1/2
0.30
0.25
0.20
0.15
0.10
0.05
0.00
0.0
0.1
0.2
0.3
0.5
0.4
bits
0.6
0.7
0.8
0.9
data
obtained
observing
binary
0.25
signal
binary
symmetric
channel
bitﬂip
probability
0.05.
fig
red
minimal
risk
setting
data
compressed
using
good
rate-r
code
respect
data
i-drf
dotted
minimal
risk
estimating
signal
rate-r
encoding
data
also
shown
shannon
drf
dashed
achievable
compressing
noiseless
version
binary
signal
case
expression
also
obtain
number
bits
required
attain
prescribed
distortion
respect
cid:18
cid:19
note
coincides
classical
rate-distortion
function
binary
i.i.d
source
estimated
hamming
distortion
furthermore
1/2
reduces
indirect
rate-distortion
function
inverse
function
i-drf
binary
source
observed
binary
symmetric
channel
crossover
probability
exercise
3.8
rind
cid:18
cid:19
equality
1/2
implies
asymptotically
loss
compression
performance
encoder
uses
optimal
lossy
compression
code
respect
rather
ﬁrst
estimating
compressing
estimate
nevertheless
strictly
greater
rind
whenever
1/2
difference
dind
inverse
rind
illustrated
figure
note
i-drf
bayes-optimal
risk
attain
minimum
distortions
dmin
rate
rate
entropy
observation
sequence
latter
described
almost
losslessly
decoder
rate
indeed
dmin
probability
making
error
estimating
output
channel
hand
i-drf
attains
maximum
distortion
rate
zero
already
difference
rather
interesting
highlights
rate
less
encoded
representation
provides
information
38a
38b
quadratic
gaussian
setting
section
focus
case
data
follows
normal
distribution
distortion
measure
squared
euclidean
distance
counterpart
theorem
5.1
setting
follows
theorem
6.1
main
results
gaussian
data
euclidean
distance
let
let
eigenvalues
1/n
deﬁne
finally
deﬁne
joint
distribution
z∗n
max
log2
λi/η
xi=1
xi=1
min
+pdiuw
min
matrix
right
eigenvectors
let
n|y
d-admissible
code
rate
l-lipschtiz
cid:12
cid:12
cid:12
−qd∗
kσθk2s2
cid:18
cid:12
cid:12
cid:12
n||z
cid:19
assume
lim
supn
kσθk
sequence
codes
n|y
provided
cid:2
cid:3
n=1
satisﬁes
sequence
estimators
cid:2
cid:3
whose
lipschtiz
constants
uniformly
bounded
proof
see
appendix
lim
n→∞
cid:12
cid:12
cid:2
cid:3
cid:12
cid:12
cid:3
theorem
6.1
provides
powerful
characterization
asymptotic
performance
estimators
gaussian
data
compressed
using
good
rate-distortion
code
satisfying
part
theorem
provides
bound
risk
lipschitz
estimator
output
d-admissible
code
terms
excess
coding
bitrate
particular
implies
risk
estimation
compressed
representation
converges
ﬁnite
limit
provided
limit
risk
exists
part
says
estimators
attain
risk
applied
random
vector
deﬁned
work
provides
result
similar
theorem
6.1
case
data
necessarily
gaussian
encoder
limited
use
random
spherical
code
next
apply
result
theorem
6.1
examples
section
ii-a
section
ii-b2
respectively
example
location
parametric
estimation
consider
setting
described
subsection
ii-a
location
parameter
estimated
lossy
compressed
version
proposition
6.2
assume
iid∼
let
rate-rn
code
n|y
satisfying
cid:2
nk2
cid:3
σ22−2r
exists
estimator
proof
see
appendix
2−2r
n||z
2−2r
2−2r
cid:3
proposition
6.2
shows
exists
estimator
parameter
mean
squared
error
mse
decreasing
1/n
output
code
attains
distortion
target
respect
data
equals
function
data
natural
measure
loss
efﬁciency
due
compressing
measurements
asymptotic
relative
efﬁciency
estimator
compared
efﬁcient
estimator
i.e.
estimator
attaining
mse
order
σ2/n
setting
deﬁne
as4
intuitively
indicates
increase
number
samples
distribution
required
attain
lim
sup
n→∞
neh
prescribed
accuracy
estimating
compared
efﬁcient
estimator
access
uncompressed
data
e.g.
mean
data
proposition
6.2
implies
2−2r
n||z
2−2r
2−2r
n||z
rhs
reduces
particular
rate-distortion
codes
2−2r
2−2r
example
since
3/5
conclude
compressing
using
good
rate-distortion
code
one
bit
per
sample
increases
sample
size
required
attain
prescribed
mse
5/3
important
note
possible
design
code
especially
estimating
rather
compressing
measurements
attains
indeed
straightforward
represent
average
using
bits
mse
exponentially
small
point
highlights
difference
estimate-and-compress
approach
use
source
code
tailored
speciﬁc
end
estimation
problem
approach
compressor
agnostic
4this
deﬁnition
coincides
standard
deﬁnition
whenever
asymptotically
normal
ﬁnite
second
moment
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
dind
2γ+1
γ+1
γ+1
bits
dind
fig
quadratic
gaussian
measurement
model
gaussian
signal
prior
optimal
bayes
estimation
dind
describe
optimal
source
coding
risks
respectively
example
single
input
multiple
output
gaussian
channel
let
return
next
setting
section
ii-b2
setting
following
evaluation
bayes-optimal
risk
proposition
6.3
let
iid∼
input
independent
awgn
channels
deﬁned
j=1
obtained
m−1
log
1+γ
log
log
=
2−2r
proof
see
appendix
note
example
data
deﬁned
sequence
m-dimensional
vectors
therefore
order
use
theorem
6.1
characterizing
risk
terms
may
stack
vectors
single
sequence
length
indeed
transformation
data
change
rate-distortion
function
leads
similar
transformation
coordinates
rate-distortion
achieving
distribution
comparison
i-drf
setting
describing
minimal
distortion
optimal
rate
source
code
applied
given
dind
2−2r
comparing
recover
two
observations
ﬁrst
made
risk
coincides
i-drf
either
smaller
0.5
log
words
choice
observation
distortion
possible
attain
optimal
source
coding
performance
despite
knowledge
joint
distribution
source
observations
hand
rate
larger
0.5
log
minimal
distortion
decreases
roughly
times
slower
equivalent
term
dind
comparison
dind
illustrated
figure
different
values
parameter
vii
conclusions
considered
problem
estimation
compressed
data
encapsulates
indirect
source
coding
compression
estimation
settings
setup
considered
compress-and-estimate
approach
data
ﬁrst
compressed
using
good
rate-distortion
code
i.e.
code
attaining
shannon
rate-
distortion
function
data
equality
underlying
signal
parameter
ultimately
estimated
compressed
version
data
showed
whenever
rate-distortion
achieving
distribution
satisﬁes
suitable
transportation
cost
inequality
performance
robust
enough
estimation
procedure
data
given
risk
estimator
risk
estimator
evaluated
data
sampled
rate-distortion
achieving
distribution
particular
conditions
hold
broadly
speaking
whenever
compressed
representation
embedded
discrete
space
rate-distortion
achieving
distribution
gaussian
order
illustrate
usefulness
characterization
evaluated
risk
case
bernoulli
source
observed
binary
symmetric
channel
gaussian
source
observed
multiple
awgn
channels
parametric
estimation
problem
gaussian
location
model
work
presented
leaves
number
interesting
research
directions
first
unclear
whether
equivalence
risk
true
risk
estimating
compressed
data
follows
directly
concentration
measure
property
another
line
future
work
extension
results
provided
multi-terminal
setting
i.e
data
compressed
multiple
locations
finally
good
rate-distortion
codes
violate
condition
given
analysis
leaves
open
question
whether
true
risk
converges
risk
cases
work
supported
part
nsf
center
science
information
csoi
grant
ccf-0939370
nsf
grants
dms-1418362
dms-1407813
nsf-bsf
grant
1609695.
acknowledgment
kipnis
rini
goldsmith
multiterminal
compress-and-estimate
source
coding
information
theory
isit
2016
ieee
international
symposium
ieee
2016
540–544
references
coding
theorems
compress
estimate
source
coding
problem
2017
ieee
international
symposium
information
theory
isit
june
2017
2568–2572
indirect
rate-distortion
function
binary
i.i.d
source
information
theory
workshop
fall
itw
2015
ieee
oct
2015
352–356
dobrushin
tsybakov
information
transmission
additional
noise
ire
transactions
information
theory
vol
293–304
1962
berger
rate-distortion
theory
mathematical
basis
data
compression
englewood
cliffs
prentice-hall
1971
witsenhausen
indirect
rate
distortion
problems
information
theory
ieee
transactions
vol
518–521
1980
shannon
mathematical
theory
communication
bell
system
tech
vol
379–423
623–656
1948
wolf
ziv
transmission
noisy
information
noisy
receiver
minimum
distortion
ieee
transactions
information
theory
vol
406–411
1970
han
hypothesis
testing
multiterminal
data
compression
ieee
transactions
information
theory
vol
759–772
1987
han
amari
statistical
inference
multiterminal
data
compression
ieee
transactions
information
theory
vol
2300–2324
oct
1998
kassam
optimum
quantization
signal
detection
ieee
transactions
communications
vol
479–484
may
1977
poor
thomas
applications
ali-silvey
distance
measures
design
generalized
quantizers
binary
decision
systems
ieee
transactions
communications
vol
893–900
sep.
1977
gray
quantization
task-driven
sensing
distributed
processing
2006
ieee
international
conference
acoustics
speech
signal
processing
proceedings
vol
may
2006
v–v
duchi
jordan
wainwright
zhang
optimality
guarantees
distributed
statistical
estimation
arxiv
preprint
arxiv:1405.0782
2014
kipnis
duchi
mean
estimation
one-bit
measurements
corr
vol
abs/1901.03403
2019
barnes
han
ozgur
learning
distributions
samples
communication
constraints
2019
ishwar
puri
ramchandran
pradhan
rate-constrained
distributed
estimation
unreliable
sensor
networks
ieee
journal
selected
areas
communications
vol
765–775
april
2005
wang
xie
zhang
chen
robust
distributed
compression
symmetrically
correlated
gaussian
sources
corr
vol
abs/1807.06799
2018
schizas
giannakis
jindal
distortion-rate
bounds
distributed
estimation
using
wireless
sensor
networks
eurasip
adv
signal
process
vol
2008
118:1–118:12
jan.
2008
online
available
https
//doi.org/10.1155/2008/748605
cover
gamal
capacity
theorems
relay
channel
ieee
transactions
information
theory
vol
572–584
1979
kipnis
goldsmith
eldar
distortion-rate
function
sampled
wiener
processes
ieee
transactions
information
theory
vol
482–499
jan
2019
murray
kipnis
goldsmith
lossy
compression
decimated
gaussian
random
walks
2018
52nd
annual
conference
information
sciences
systems
ciss
march
2018
1–6
kipnis
reeves
eldar
single
letter
formulas
quantized
compressed
sensing
gaussian
codebooks
2018
ieee
international
symposium
information
theory
isit
june
2018
71–75
kipnis
reeves
gaussian
approximation
quantization
noise
estimation
compressed
data
2019
submitted
isit2019
raginsky
sason
al.
concentration
measure
inequalities
information
theory
communications
coding
foundations
trends
cid:13
communications
information
theory
vol
1-2
1–246
2013
villani
optimal
transport
old
new
springer
science
business
media
2008
vol
338
yamamoto
rate-distortion
problem
communication
system
secondary
decoder
hindered
ieee
transactions
information
theory
vol
835–842
1988
lapidoth
role
mismatch
rate
distortion
theory
ieee
transactions
information
theory
vol
38–47
1997
kontoyiannis
zamir
mismatched
codebooks
role
entropy
coding
lossy
data
compression
ieee
transactions
information
theory
vol
1922–1938
2006
donoho
kolmogorov
sampler
department
statistics
stanford
university
2002
gray
entropy
information
theory
springer
science
business
media
2011
kanlis
compression
transmission
information
multiple
resolutions
ph.d.
dissertation
university
maryland
1997
shamai
verd´u
empirical
distribution
good
codes
ieee
transactions
information
theory
vol
836–846
1997
weissman
ordentlich
empirical
distribution
rate-constrained
source
codes
information
theory
ieee
transactions
vol
3718–3733
2005
schieler
cuff
connection
good
rate-distortion
codes
backward
dmcs
information
theory
workshop
itw
2013
ieee
ieee
2013
1–5
kostina
verdu
output
distribution
good
lossy
source
codes
information
theory
applications
workshop
ita
2015
feb
2015
308–312
kanlis
khudanpur
narayan
typicality
good
rate-distortion
code
problems
information
transmission
vol
96–103
1996
sakrison
geometric
treatment
source
encoding
gaussian
random
variable
ieee
transactions
information
theory
vol
481–486
1968
zhang
berger
estimation
via
compressed
information
ieee
transactions
information
theory
vol
198–211
1988
kipnis
duchi
mean
estimation
adaptive
one-bit
measurements
2017
55th
annual
allerton
conference
communication
control
computing
allerton
oct
2017
1000–1007
kolmogorov
shannon
theory
information
transmission
case
continuous
signals
ire
transactions
information
theory
vol
102–108
december
1956
gray
probability
random
processes
ergodic
properties
springer
2009
gray
neuhoff
shields
generalization
ornstein
distance
applications
information
theory
annals
probability
315–328
1975
marton
simple
proof
blowing-up
lemma
corresp
ieee
transactions
information
theory
vol
445–446
1986
ahlswede
g´acs
¨orner
bounds
conditional
probabilities
applications
multi-user
communication
probability
theory
related
fields
vol
157–177
1976
gamal
y.-h.
kim
network
information
theory
cambridge
university
press
2011
marton
al.
bounding
ρ-bar
distance
informational
divergence
method
prove
measure
concentration
annals
probability
vol
857–866
1996
talagrand
transportation
cost
gaussian
product
measures
geometric
functional
analysis
gafa
vol
587–600
1996
cover
thomas
elements
information
theory
2nd
ed.
wiley
2006
lehmann
casella
theory
point
estimation
springer
science
business
media
2006
gastpar
lower
bound
awgn
remote
rate-distortion
function
statistical
signal
processing
2005
ieee/sp
13th
workshop
ieee
2005
1176–1181
appendix
let
arbitrary
absolutely
continuous
respect
γ′n
write
uλu∗
unitary
diagonal
coupling
rotation
matrix
proof
proposition
3.4
ynk2
nk2
cid:13
cid:13
λ1/2
cid:13
cid:13
cid:13
cid:13
λ1/2
cid:13
cid:13
cid:13
cid:13
cid:13
λ1/2
λ1/2
cid:13
cid:13
cid:13
cid:13
cid:13
λ1/2
cid:13
cid:13
cid:13
cid:13
cid:13
uλ1/2
uλ1/2
cid:13
cid:13
cid:13
cid:13
cid:13
λ1/2
cid:13
cid:13
cid:13
cid:13
λ1/2
cid:13
cid:13
′nk2
γ′n
arbitrary
absolutely
continuous
measure
respect
γ′n
taking
inﬁmum
coupling
γ′n
proposition
3.3
implies
n||γn
γ′n
cid:13
cid:13
λ1/2
cid:13
cid:13
finally
follows
since
γ′n
obtained
parameter
transformation
n||γn
invariant
appendix
proof
proposition
6.3
data
distribution
independent
blocks
length
distribution
m-length
block
jointly
gaussian
mean
covariane
matrix
σy1
aa∗
distribution
pz∗
achieving
rdf
satisﬁes
backward
channel
unitary
matrix
u∗σu
diag
eigenvalues
eigenvalues
given
a∗a
48a
48b
50a
50b
gaussian
vector
independent
whose
m-th
coordinate
variance
min
chosen
xi=1
xi=1
log+
cid:18
cid:19
u∗y1
˜z1
deﬁne
˜z1
u∗z1
equivalent
also
tre
treh
u∗y1
˜z1
u∗y1
˜z1
treh
u∗y1
˜z1
xi=1
equivalent
representation
pz∗
follows
˜z1
cid:0
2−2ri
cid:1
u∗y1
+qdi
2−2ri
+qdi
2−2ri
cid:0
2−2ri
cid:1
cid:0
2−2ri
cid:1
aθ1
cid:0
2−2ri
cid:1
aθ1
+qdi
22ri
2−2r
2vi
i.i.d
standard
normal
independent
write
50b
matrix
form
˜z1
bθ1
bayes-optimal
estimator
conditional
expectation
expected
quadratic
risk
minimal
mean
squared
error
given
mmse
θ1|z1
mmse
θ1|˜z1
cid:0
b∗σ−1
xi=1
xi=1
cid:1
2−2ri
2−2ri
a∗ui
2−2ri
min
2−2ri
a∗ui
since
a/kak
log
a∗a
i=2
...
orthogonal
get
mmse
θ|˜z1
cid:18
a∗a
a∗a
cid:19
a∗a
a∗a
a∗a
a∗a
obtained
setting
a∗a
appendix
proof
theorem
5.1
ﬁrst
note
separable
b-bounded
estimator
b-lipschtiz
respect
hamming
distance
per
following
proposition
proposition
c.1
let
b-bounded
deﬁne
b-lipschtiz
respect
hamming
distance
proof
proposition
c.1
ˆzn
ˆzn
ˆzi
xi=1
ˆzi
xzi6=ˆzi
bρh
ˆzn
xzi=ˆzi
ˆzi
show
conditions
theorem
4.1
hold
special
case
since
bounded
note
notation
theorem
4.1
use
given
xi=1
ˆxi
finally
proposition
3.2
implies
pz∗n|y
n=yn
1/2n
apply
theorem
4.1
parameter
space
whose
ith
coordinate
equals
since
assumed
bounded
proposition
c.1
implies
lipschitz
hence
lipschitz
denote
lipschitz
constant
follows
cid:12
cid:12
cid:12
cid:12
cid:12
xi=1
lrrn
xi=1
n||z
cid:12
cid:12
cid:12
cid:12
cid:12
equation
follows
since
holds
assumption
complete
proof
order
show
ﬁrst
note
trivially
holds
whenever
hence
assume
use
theorem
4.1
assuming
arbitrary
l-lipschitz
necessarily
separable
obtain
lrrn
xi=1
xi=1
n||z
since
z∗n
i=1
pxi
bounded
bayes
optimal
estimator
consequently
left
hand
side
bounded
ehρ
cid:16
cid:17
xi=1
inequality
follows
since
lprn
n−1d
n||z
appendix
proof
theorem
6.1
show
conditions
theorem
4.1
met
case
first
note
single
letter
distortion
measure
sub-additive
case
dmin
addition
holds
trς
since
case
pz∗n
deﬁned
unique
solution
minimization
problem
pz∗|y
n=yn
gaussian
measure
proposition
3.4
kσk2
respect
euclidean
distance
use
theorem
4.1
also
note
supppz
supppz∗n
conditions
part
follows
part
follows
due
additional
condition
n−1d
n||z
uniformly
bounded
lipschitz
constant
sequence
appendix
derivation
equation
consider
achieving
distribution
pz∗
deﬁned
denote
since
drf
given
h−1
cid:0
cid:1
since
1/2
also
1/2
thus
bayes-optimal
estimator
equals
follows
risk
equals
probability
bitﬂip
channel
probability
consider
estimator
metric
ˆzn
appendix
proof
proposition
6.2
xi=1
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
xi=1
last
transition
used
inequalitypn
follows
cauchy-schwartz
inequality
says
α-lipschitz
k·k
use
theorem
6.1
σ22−2r
obtain
i=1
xi=1
ˆzn
ˆzi
cid:12
cid:12
cid:12
cid:12
cid:12
ˆzi|
αkzn
ˆznk
i=1
√nppn
cid:12
cid:12
cid:12
cid:12
cid:12
rneh
−rneh
cid:12
cid:12
cid:12
cid:12
cid:12
neh
√αrrn
hence
√αrrn
n||z
n||z
+rneh
next
evaluate
term
set
2−2r
inverse
channel
given
¯dyi
σp2−2r
¯dn
¯d2−2rn
¯dθ
2−2r
set
2−2r
follows
iid∼
thus
2−2r
xi=1
neh
2−2r
2−2r
proof
completed
substituting
last
expression
