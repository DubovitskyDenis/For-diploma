performance
analysis
norm
constrained
recursive
least
squares
algorithm
samrat
mukhopadhyay
student
member
ieee
bijit
kumar
das
student
member
ieee
mrityunjoy
chakraborty
senior
member
ieee
abstract
performance
analysis
norm
constrained
recursive
least
squares
rls
algorithm
attempted
paper
though
performance
pretty
attractive
compared
various
alternatives
thorough
study
theoretical
analysis
performed
like
popular
least
mean
squares
lms
algorithm
rls
norm
penalty
added
provide
zero
tap
attractions
instantaneous
ﬁlter
taps
thorough
theoretical
performance
analysis
conducted
paper
white
gaussian
input
data
assumptions
suitable
many
practical
scenarios
expression
steady
state
msd
derived
analyzed
variations
different
sets
predeﬁned
variables
also
taylor
series
expansion
based
approximate
linear
evolution
instantaneous
msd
performed
finally
numerical
simulations
carried
corroborate
theoretical
analysis
shown
match
well
wide
range
parameters
adaptive
ﬁlters
sparsity
norm
recursive
least
squares
rls
algorithm
mean
square
deviation
performance
analysis
index
terms
introduction
sparse
systems
frequently
encountered
many
applications
echo
paths
wireless
communication
channels
hdtv
etc
system
vector
called
sparse
small
number
nonzero
entries
compared
dimension
becomes
necessary
ﬁnd
identiﬁcation
algorithms
suitable
sparse
systems
adaptive
algorithms
frequently
used
identify
systems
whose
parameters
changing
time
due
simplicity
ease
implementation
least
mean
squares
lms
algorithm
enjoyed
much
success
long
time
another
frequently
used
adaptive
algorithm
recursive
least
squares
rls
recursively
tries
minimize
error
estimated
unknown
system
vectors
using
information
conveyed
data
beginning
reception
algorithms
sparsity
agnostic
generally
perform
well
unknown
system
sparse
inspired
introduction
sparse
signal
processing
nascent
ﬁeld
compressive
sensing
last
decade
saw
ﬂurry
activities
sparse
adaptive
ﬁlters
produced
number
several
new
algorithms
exploit
knowledge
sparsity
many
algorithms
use
knowledge
sparsity
unknown
system
vector
add
norm
penalty
cost
function
za-lms
uses
norm
penalty
lms
uses
norm
penalty
exert
zero
attraction
ﬁlter
taps
norm
regularized
rls
algorithms
also
proposed
researchers
sparls
algorithm
suggests
use
expectation-maximization
algorithm
minimize
norm
penalized
rls
cost
function
authors
propose
algorithm
uses
online
coordinate
descent
algorithm
together
regularized
rls
cost
function
rls
algorithm
proposed
cost
function
conventional
rls
algorithm
modiﬁed
adding
penalty
term
results
zero
point
attracted
rls
algorithm
general
convex
penalty
term
added
rls
cost
function
result
sparsity
aware
convex
regularized
rls
algorithm
among
different
penalty
terms
used
regularizer
cost
function
rls
particular
interests
convex
functions
used
approximate
penalty
term
introduced
since
norm
penalty
introduce
strong
zero
point
attraction
small
taps
estimated
parameter
step
algorithm
sparse
system
algorithm
expected
converge
faster
lower
steady
state
mean
square
deviation
though
author
numerically
shown
mean
square
deviation
performance
norm
penalized
rls
superior
conventional
rls
neither
anyone
else
best
knowledge
found
make
attempt
establish
claim
theoretical
analysis
algorithm
detailed
theoretical
analysis
algorithm
could
corroborate
superior
performance
promised
numerical
simulations
rls
also
ﬁnd
spectrum
different
set
predeﬁned
variables
algorithm
may
even
become
worse
conventional
algorithm
detailed
theoretical
analysis
lms
carried
inspired
present
work
present
work
aimed
providing
thorough
analysis
rls
algorithm
along
presenting
salient
features
limitations
performance
algorithm
samrat
mukhopadhyay
dept
electronics
electrical
communication
engineering
indian
institute
technology
kharagpur
721302
india
email
samratphysics
gmail.com
bijit
kumar
das
dept
electronics
electrical
communication
engineering
indian
institute
technology
kharagpur
721302
india
email
bijitbijit
gmail.com
mrityunjoy
chakraborty
dept
electronics
electrical
communication
engineering
indian
institute
technology
kharagpur
721302
india
email
mrityun
ece.iitkgp.ernet.in
preliminaries
let
system
unknown
parameter
vector
cid:2
···
sn−1
cid:3
denoted
cid:2
···
cid:3
additive
noise
sequence
let
adaptive
ﬁlter
produces
estimate
cid:2
···
wn−1
cid:3
system
tap
vector
time
instantaneous
estimation
error
output
unknown
system
output
adaptive
ﬁlter
system
produces
output
sequence
let
input
vector
time
cost
function
conventional
rls
adaptive
ﬁlter
forgetting
factor
deﬁned
order
take
account
sparsity
unknown
system
vector
l0-rls
modiﬁes
cost
function
iteration
adding
penalty
function
gives
measure
sparsity
system
l0-rls
chooses
norm
penalty
function
result
cost
becomes
λn−m
xm=0
λn−m
kwnk0
xm=0
norm
deﬁned
number
non-zero
entries
vector
parameter
penalty
factor
controls
balance
estimation
error
penalty
general
norm
optimization
problem
known
hard
often
approximated
continuous
often
convex
functions
popular
approximation
introduced
results
manipulations
following
evolution
equation
rls
adaptive
ﬁlter
wn−1
knξn
κpng
wn−1
=yn
n−1xn
xm=0
pn−1xn
pn−1xn
λn−mxmxt
wn−1
cid:2
n−1
n−1
···
wn−1
n−1
cid:3
function
deﬁned
cid:26
β2t
βsgn
|t|
1/β
elsewhere
third
term
zero-point
attraction
term
range
−1/β
1/β
called
attraction
range
following
approach
adopted
et.al
based
magnitudes
entries
unknown
system
vector
iii
modelling
assumptions
partition
set
indices
···
three
sets
|sk|
1/β
|sk|
1/β
adopt
following
assumptions
thus
k-sparse
|cl
cs|
|c0|
a.1
data
sequence
white
sequence
zero
mean
variance
independent
additive
noise
a.2
independence
assumption
incoming
sequence
vectors
independent
a.3
chosen
sufﬁciently
close
sequence
also
assumed
zero
mean
sequence
large
1−λ
1−λn+1
r−1
autocorrelation
matrix
incoming
data
sequence
a.4
parameters
chosen
β2κ
a.5
tap
weights
gaussian
distributed
a.6
assumed
sign
a.7
assumed
attraction
range
inside
attraction
range
elsewhere
following
points
attempt
justify
use
assumptions
assumption
a.1
generally
adopted
leverage
simple
properties
gaussian
data
sequence
assumption
slightly
generalized
dropping
assumption
sequence
independent
forces
one
work
coloured
gaussian
sequence
however
coloured
sequence
easily
pre-whitened
pre-multiplying
vector
interest
unitary
matrix
diagonalizes
covariance
matrix
gaussian
sequence
assumption
a.1
considered
without
loss
generality
assumption
a.2
independence
assumption
widely
used
literature
simpliﬁed
analysis
adaptive
algorithms
assumption
a.3
generally
used
literature
simpliﬁed
analysis
rls
one
justiﬁcation
assumption
provided
following
lemma
assumes
assumptions
a.1
a.2
lemma
3.1.
sequence
assumed
follow
assumption
a.1
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:18
cid:19
1−λ
1−λn+1
proof
short
proof
provided
appendix
lim
n→∞
k·k
denotes
2-matrix
norm
lemma
3.1
encourages
use
assumption
a.3
furthermore
seen
performance
analysis
rls
assumption
simpliﬁes
analysis
signiﬁcantly
without
assumption
nonlinear
contribution
past
data
vector
xn−1
matrix
makes
carrying
analysis
difﬁcult
small
compared
signal
power
assumption
a.4
result
experimental
observation
basically
implies
rls
stable
use
assumptions
a.5
a.6
a.7
found
suitable
analysis
exactly
assumptions
taken
analysis
lms
justiﬁcations
assumptions
based
intuitive
discussion
logical
assumptions
also
probably
justiﬁed
experimental
observations
spirit
also
performed
extensive
simulations
verify
assumptions
also
since
structure
rls
algorithm
similar
lsm
algorithm
save
time
varying
gain
matrix
expected
logical
discussions
similar
justifying
use
assumptions
work
etal
also
justify
use
assumptions
work
convergence
analysis
rls
easy
presence
time
dependent
gain
matrix
however
use
assumption
a.3
signiﬁcantly
simpliﬁes
analysis
use
assumptions
taken
section
iii
carry
analysis
simpliﬁed
manner
performance
analysis
mean
convergence
analysis
deﬁne
weight
deviation
vector
recalling
equation
evolution
adaptive
ﬁlter
recursive
update
equation
written
knxt
hn−1
knνn
κpng
wn−1
deﬁnition
equation
evoked
sequence
inverse
matrices
evolve
according
following
well
known
riccati
equation
λ−1
knxt
pn−1
using
update
quation
ﬁlter
evolution
equation
takes
form
utilizing
assumptions
a.3
a.1
simplify
evolution
equation
get
large
λpnφn−1hn−1
knνn
κpng
wn−1
ηnhn−1
knνn
ρng
wn−1
following
symbols
used
compactly
represent
expressions
derived
paper
cid:18
λn+1
cid:19
λn+1
cid:19
cid:18
λn+1
cid:19
=λn
cid:18
cid:18
λn+1
cid:19
following
theorem
describes
evolution
convergence
mean
deviation
vector
theorem
4.1.
mean
deviation
coordinates
ehk
evolve
according
following
recursive
equation
result
ehk
cid:26
cnehk,0
dng
cnehk,0
proof
proof
postponed
appendix
ehk
cid:26
mean
square
convergence
analysis
begin
investigating
evolution
correlation
matrix
mean
deviation
vector
i.e
ehnht
get
ehnht
=λ2e
cid:0
pnφn−1hn−1ht
n−1φn−1pn
cid:1
=λe
cid:0
pnφn−1hn−1kt
cid:1
=λκe
cid:0
pnφn−1hn−1gt
wn−1
cid:1
=κe
cid:0
png
wn−1
cid:1
=κ2e
cid:0
png
wn−1
wn−1
cid:1
cid:0
nknkt
cid:1
using
assumptions
a.1
a.2
a.3
get
following
simpliﬁed
equations
terms
right
hand
side
n−1
=η2
ehn−1ht
=ηnρn
cid:18
λn+1
cid:19
cid:0
hn−1gt
wn−1
cid:1
=ρ2
=pν
eknkt
cid:0
wn−1
wn−1
cid:1
thus
evolution
equation
correlation
matrix
expressed
ehnht
=η2
taking
kth
diagonal
element
error
covariance
matrix
get
corresponding
evolution
equation
n−1
ηnρne
cid:0
hn−1gt
wn−1
wn−1
n−1
cid:1
cid:0
wn−1
wn−1
cid:1
eknkt
ehn−1ht
eh2
eh2
n−1
2ηnρne
n−1g
n−1
n−1
mean
square
convergence
analysis
introduce
notations
henceforth
used
succinctly
represent
results
mean
square
convergence
analysis
=eh2
=ekhnk2
xk∈c0
eh2
xk∈c0
xk∈c0
skg
ωn−1
cid:21
cid:21
cid:20
dn−1
cid:20
2βρnηn√2πω2
2βρnηn√2πω2
cid:20
cid:21
instantaneous
approximate
mean
square
deviation
analysis
section
provide
result
approximate
analysis
instantaneous
msd
theorem
4.2.
instantaneous
power
nonzero
zero
taps
rls
ﬁlter
evolve
approximately
according
following
linear
dynamical
system
pνp2
β2ρ2
βρnηnω2
2ρncnηng′
2ρndnηn
β2ρ2
βρnηnω2
=−2λθ/√2π
+p2λ2θ2/π
∞/p2πω2
∞/p2πω2
deﬁned
equation
19.
proof
proof
postponed
appendix
steady
state
mean
square
deviation
analysis
unlike
instantaneous
analysis
get
expression
steady
state
msd
exactly
assumptions
taken
sec
iii
result
analysis
showed
form
following
theorem
theorem
4.3.
steady
state
msd
following
expression
β1θ2
β2θpθ2
4λ2
√2π
2λ2
1−λ2
2λ2
proof
proof
postponed
appendix
appearance
form
steady
state
msd
identical
one
derived
authors
since
analysis
actually
follows
methodology
terms
calculate
msd
quite
different
also
way
terms
depend
upon
attraction
parameter
different
way
dependence
lms
see
details
ﬁrst
term
steady
state
msd
conventional
rls
second
third
terms
comprise
excess
msd
produced
attraction
term
note
excess
msd
well
negative
certain
range
results
improved
performance
rls
fact
paralleling
corollary
get
following
corollaries
straight
forward
calculations
corollary
4.1.
ﬁxed
rls
outperforms
conventional
rls
parameter
chosen
following
holds
proof
proof
follows
noticing
rls
outperforms
conventional
rls
steady
state
msd
1−λ2
corollary
4.2.
terms
minimum
obtainable
msd
rls
best
choice
found
β1θ2
β2θpβ3
recalling
√β3
θopt
minimum
msd
dmin
cid:18
qβ2
cid:19
proof
proof
proof
corollary
readers
referred
appendix
details
deﬁnitions
theorem
4.3
evident
corollary
4.2
minimum
msd
given
rls
function
attraction
parameter
following
corollary
shows
minimum
msd
fact
constant
large
corollary
4.3.
minimum
steady
state
msd
dmin
msd
rls
found
corollary
4.2
steady
state
msd
conventional
rls
converges
decreasing
function
ratio
minimum
lim
β→∞
dmin
drls
2λ2
k/n
close
proof
first
observe
independent
dependence
steady
state
msd
term
equation
clear
steady
state
msd
increasing
function
expression
clear
decreasing
function
proves
ﬁrst
part
corollary
see
second
part
corollary
comes
observe
expression
rewritten
theorem
4.3
make
expressions
look
less
formidable
let
pνp2
lim
β→∞
cid:18
4λ2
cid:19
cid:18
4λ2
cid:19
2λ2
2λ2
2f1f2f3
lim
β→∞
lim
β→∞
dmin
f0f1
f0f1
f0f1
=drls
cid:18
cid:18
qβ2
f0f1
f0f1f
cid:19
cid:18
cid:19
cid:19
result
follows
plugging
expressions
another
important
observation
expression
minimum
steady
state
msd
equation
dependent
upon
unknown
system
parameters
set
dependence
via
appears
expression
interestingly
extent
dependence
controlled
attraction
parameter
seen
corollary
4.3
dependence
vanishes
becomes
large
msd
function
system
sparsity
length
ratio
k/n
regard
following
simple
corollary
connects
behaviour
minimum
steady
state
msd
sparsity
system
attraction
small
unknown
parameters
corollary
4.4.
minimum
steady
state
msd
monotonically
increasing
function
small
set
attraction
sparsity
proof
write
expression
minimum
steady
state
msd
dmin
p∞2
2β3
pβ2
shows
dmin
monotonically
increasing
function
increases
increase
increasing
function
dmin
also
investigate
dependence
minimum
steady
state
msd
sparsity
ﬁrst
note
ﬁrst
term
independent
hence
behaviour
second
term
sufﬁce
purpose
let
deﬁne
sake
simplicity
expressions
2λ2
2λ2
note
express
second
term
function
hence
function
following
manner
2f1f2f3
cid:19
cid:18
qβ2
cid:18
2f1f4
−2β3f
2f1f4
cid:19
trivial
note
negative
decreases
increases
way
easy
verify
positive
increases
thus
second
term
decreases
increases
implies
second
term
increases
increases
proves
minimum
steady
state
msd
increases
increase
sparsity
numerical
experiments
numerical
experiments
carried
verify
accuracy
analysis
order
perform
experiments
unknown
system
vector
generated
generating
components
independent
samples
random
variable
simulation
result
averaged
100
iterations
table
documents
various
parameter
values
used
experiments
snr
50db/25db
50db
50db
parameter
values
different
numerical
experiments
table
10−7
100.1κmax
κopt
κopt
experiment
0.995
0.995
0.995
10−1
simulation
analysis
10−5
10−6
10−7
10−6
10−5
10−4
fig
steady
state
msd
snr
50db
figures
compare
steady
state
msd
conventional
rls
msd
rls
obtained
simulation
msd
rls
obtained
analysis
resulted
varied
ﬁgure
clearly
shows
theory
good
agreement
simulation
also
value
optimal
κopt
seen
well
matched
found
simulation
seen
tally
better
snr
snr
25db
expected
since
decrease
snr
makes
assumptions
a.5
a.7
weak
figures
plot
variation
steady
state
msd
seen
result
analysis
matches
well
theory
also
interesting
observe
decrease
msd
rls
almost
factor
1/10
compared
steady
state
msd
conventional
rls
result
matches
quite
closely
result
stated
corollary
4.3
according
factor
be≈
k/n
1/10.667
using
values
table
experiment
figure
plots
variation
steady
state
msd
sparsity
ﬁgure
clearly
veriﬁes
claim
corollary
4.4
simulation
analysis
10−3.3
10−3.4
10−3.5
10−3.6
10−3.7
10−6
10−5
10−4
10−3
fig
steady
state
msd
snr
25db
10−5
10−6
10−7
10−1
fig
steady
state
msd
snr
50db
simulation
analysis
100
101
102
simulation
analysis
100
101
102
10−3
10−4
10−5
10−1
fig
steady
state
msd
snr
25db
10−5
10−6
10−7
10−8
simulation
analy
sis
fig
steady
state
msd
sparsity
snr
50db
conclusion
paper
theoretical
analysis
rls
carried
inspired
work
relevant
common
assumptions
taken
along
new
ones
applicability
discussed
also
taps
divided
different
sets
according
magnitudes
effect
set
coefﬁcients
small
magnitude
analyzed
detail
expressions
steady
state
msd
well
linear
evolution
model
instantaneous
msd
derived
analyzed
effects
different
parameter
settings
several
numerical
simulations
done
verify
claims
made
analysis
seen
match
well
theoretical
predictions
range
parameter
values
appendix
proof
lemma
3.1
proof
follows
ergodicity
sequence
note
equation
one
write
λn+1
xm=0
λn−m
cid:0
xmxt
cid:1
cid:0
ǫnǫ
cid:1
=xl
=xl
cid:1
following
way
cid:1
cid:3
λ2n−l−m
cid:0
cid:0
xlxt
λ2n−l−m
cid:0
cid:0
xlxt
xmxt
xmxt
cid:1
xlxt
cid:1
cid:1
xmxt
cid:1
n−1
n−1
xmxt
xmxt
using
gaussian
mean
factoring
theorem
one
ﬁnd
expression
element
cid:0
xlxt
cid:2
cid:0
xlxt
xn=0
xn=0
xn=0
=
recalling
assumption
a.1
pxi
get
otherwise
using
assumption
a.1
n−1
ǫnǫ
λ2n−2m
n+1
xm=0
1−λn+1
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:19
cid:18
λn+1
cid:18
1−λ
λn+1
cid:19
kuk2
proves
claim
appendix
proof
theorem
4.1
taking
expectations
sides
using
assumptions
a.1
a.2
get
solve
linear
system
expression
wn−1
needed
using
assumptions
a.5
a.6
a.7
get
ehn
ηnehn−1
ρneg
wn−1
thus
follows
β2hk
n−1
n−1
n−1
=
ehk
=
ηnehk
n−1
ηnehk
n−1
ρng
ηnehk
n−1
assumption
a.4
used
simplify
expression
n−1
expression
n−1
obtained
following
way
using
assumption
a.5
deﬁnition
function
β2x
sgn
e−x2/2ω2
n−1dx
−1/β
eh2
n−1
q2πω2
n−1
1/β
follows
ehk
k=1
ehk,0
cid:16
deﬁnitions
ﬁnd
yj=k+1
k=1
1−λ
k=1
ehk,0
xk=1
j=k+1
cid:17
k=1
ρkqn
1−λn+1
λk+1
cid:19
cid:18
λn−k
λk+1
λn+1
cid:19
evolution
equation
ehk
follows
taking
trivially
results
solve
recursion
need
evaluate
terms
n−1g
n−1
n−1
···
evaluating
n−1g
n−1
recalling
get
taking
expectations
sides
get
n−1g
n−1
=
ehk
n−1g
n−1
=
β2h2
n−1g
n−1
n−1
n−1
β2eh2
ehk
n−1g
n−1
n−1
ehk
n−1
xk=1
cid:18
xk=1
λn+1
λn−k
=ρn
appendix
proof
theorem
4.2
get
expression
ehk
n−1g
n−1
note
using
deﬁnition
function
get
ehk
n−1g
n−1
n−1
1/β
−1/β
q2πω2
β2x2
β|x|
e−x2/2ω2
n−1dx
note
assumption
a.5
implies
1/β
permits
approximate
integral
ehk
n−1g
n−1
√2π
n−1
=β2ω2
β2ω2
n−1x2
ωn−1β|x|
e−x2/2dx
2βωn−1√2π
thus
ehk
n−1g
n−1
=
n−1
eh2
n−1
β2eh2
β2ω2
n−1
ehk
n−1
n−1
2βωn−1√2π
evaluating
eg2
n−1
using
deﬁnition
function
get
eg2
n−1
=
using
assumption
a.5
get
√2π
=β2
2β3ωn−1
4β3ωn−1√2π
=β2
eg2
n−1
β4eh2
eg2
n−1
n−1
2β2g
n−1
β2ωn−1x
βsgn
2e−x2/2dx
|x|e−x2/2dx
β4ω2
√2π
√2π
x2e−x2/2dx
n−1
β4ω2
n−1
thus
evaluating
ek2
eg2
n−1
=
β4eh2
n−1
2β2g
n−1
4β3ωn−1
√2π
β4ω2
n−1
deﬁnition
gain
vector
along
assumptions
a.1
a.3
get
following
simpliﬁed
expression
kxnk2
1−λ
let
ek2
follows
assumption
a.1
···
pn−1
kxnk2
kxnk2
kxnk2
choice
assumption
a.3
approximation
1−λ
simplify
expression
kxnk2
2px
λ2px
putting
everything
together
thus
using
expressions
found
equations
using
assumption
a.4
get
along
produces
following
linear
recursion
eh2
=
=η2
eh2
eh2
eh2
eh2
eh2
n−1
pνp2
ng2
2ρnηng
ehk
n−1
n−1
pνp2
n−1
pνp2
β2ρ2
4βρnηn
=
n−1
n−1
2ρncnηnskg
2ρndnηng2
n−1
4βρnηn
√eh2
√2π
√eh2
√2π
eh2
eh2
n−1
ng2
β2ρ2
assumed
ew0
follows
n−1
dn−1
ωn−1
kpνp2
2ρncnηng′
2ρndnηn
also
follows
nωn−1
β2ρ2
pνp2
4βρnηn√2π
ωn−1
observing
follows
nωn−1
β2ρ2
follows
thus
using
β2ρ2
=η2
4βρnηn√2π
pωn−1
=−2βρ∞η∞/√2π
+p2β2ρ2
∞/π
β2ρ2
1−λ
theorem
4.2.
recalling
hence
desired
parametric
expression
terms
promised
large
however
approximate
linear
evolution
obtained
ﬁrst
order
taylor
series
approximation
4βρ∞η∞√2π
∞η2
qeh2
n−1
get
eh2
n−1
≈qeh2
eh2
n−1
eh2
2qeh2
become
eh2
n−1
eh2
2qeh2
nωn−1
β2ρ2
ωn−1
large
thus
together
produce
results
equations
2βρnηn
p2πω2
4βρ∞η∞√2π
appendix
proof
theorem
4.3
taking
using
get
β2ρ2
pνp2
2ρ∞c∞η∞g′
2ρ∞d∞η∞
observing
get
together
yields
desired
result
pνp2
1−λ
2λθ2
n−k
√2π
references
duttweiler
proportionate
normalized
least-mean-squares
adaptation
echo
cancelers
speech
audio
processing
ieee
transactions
vol
508–518
2000
schreiber
advanced
television
systems
terrestrial
broadcasting
problems
proposed
solutions
proceedings
ieee
vol
958–981
1995
widrow
stearns
adaptive
signal
processing
englewood
cliffs
prentice-hall
inc.
1985
491
vol
1985
haykin
adaptive
ﬁlter
theory
pearson
education
india
2008
cand
romberg
tao
robust
uncertainty
principles
exact
signal
reconstruction
highly
incomplete
frequency
information
information
theory
ieee
transactions
vol
489–509
2006
candes
romberg
tao
stable
signal
recovery
incomplete
inaccurate
measurements
communications
pure
applied
mathematics
vol
1207–1223
2006
candes
tao
near-optimal
signal
recovery
random
projections
universal
encoding
strategies
information
theory
ieee
transactions
vol
5406–5425
2006
das
chakraborty
sparse
adaptive
ﬁlters-an
overview
new
results
circuits
systems
iscas
2012
ieee
international
symposium
ieee
2012
2745–2748
chen
hero
iii
sparse
lms
system
identiﬁcation
acoustics
speech
signal
processing
2009.
icassp
2009.
ieee
international
conference
ieee
2009
3125–3128
jin
mei
norm
constraint
lms
algorithm
sparse
system
identiﬁcation
signal
processing
letters
ieee
vol
774–777
2009
babadi
kalouptsidis
tarokh
sparls
sparse
rls
algorithm
signal
processing
ieee
transactions
vol
4013–4025
2010
angelosante
bazerque
giannakis
online
adaptive
estimation
sparse
signals
rls
meets
the-norm
signal
processing
ieee
transactions
vol
3436–3447
2010
eks¸io˘glu
rls
adaptive
ﬁltering
sparsity
regularization
information
sciences
signal
processing
applications
isspa
2010
10th
international
conference
ieee
2010
550–553
eksioglu
al.
rls
algorithm
convex
regularization
signal
processing
letters
ieee
vol
470–473
2011
jin
wang
performance
analysis
norm
constraint
least
mean
square
algorithm
signal
processing
ieee
transactions
vol
2223–2235
2012
kushner
yin
stochastic
approximation
recursive
algorithms
applications
springer
science
business
media
2003
vol
uncini
fundamentals
adaptive
signal
processing
springer
2015
priestley
spectral
analysis
time
series
1981
