sparsity
nuv-em
gaussian
message
passing
kalman
smoothing
hans-andrea
loeliger
lukas
bruderer
hampus
malmberg
federico
wadehn
nour
zalmai
eth
zurich
dept
information
technology
electrical
engineering
abstractâ€”normal
priors
unknown
variance
nuv
long
known
promote
sparsity
blend
well
parameter
learning
expectation
maximization
paper
advocate
approach
linear
state
space
models
applications
estimation
impulsive
signals
detection
localized
events
smoothing
occasional
jumps
state
space
detection
removal
outliers
actual
computations
boil
multivariate-gaussian
message
passing
algorithms
closely
related
kalman
smoothing
give
improved
tables
gaussian-message
com-
putations
algorithms
easily
synthesized
point
two
preferred
algorithms
introduction
paper
two
topics
particular
approach
modeling
estimating
sparse
parameters
based
zero-mean
normal
priors
un-
known
variance
nuv
concerning
second
topic
kalman
smoothing
models
multivariate-gaussian
message
passing
variations
main
point
paper
two
things
well
together
combine
versatile
toolbox
entirely
new
course
body
related
literature
large
nonetheless
speciï¬c
perspective
paper
far
known
authors
advocated
linear
state
space
models
continue
essential
tool
broad
variety
appli-
cations
primary
algorithms
models
variations
generalizations
kalman
ï¬ltering
smoothing
equivalently
multivariate-gaussian
message
passing
corresponding
factor
graph
similar
graphical
model
variety
algorithms
easily
synthesized
tables
message
computations
paper
give
new
version
tables
many
improvements
point
two
preferred
algorithms
concerning
ï¬rst
topic
nuv
priors
zero-mean
normal
priors
unknown
variance
originated
bayesian
infer-
ence
sparsity-promoting
nature
priors
basis
automatic
relevance
determination
ard
sparse
bayesian
learning
developed
neal
tipping
wipf
others
basic
properties
nuv
priors
illustrated
following
simple
example
let
variable
parameter
interest
model
zero-mean
real
scalar
gaussian
random
variable
unknown
variance
assume
observe
noise
zero-mean
gaussian
known
variance
independent
maximum
likelihood
estimate
single
sample
easily
determined
Ë†s2
cid:52
argmax
cid:112
max
eâˆ’Âµ2/2
s2+Ïƒ2
second
step
map/mmse/lmmse
estimate
ï¬xed
Ë†s2
cid:40
Ë†s2
Ë†s2
Âµ2âˆ’Ïƒ2
otherwise
equations
continue
hold
scalar
observation
generalized
observation
ï¬xed
likelihood
function
y|u
gaussian
scale
factor
mean
variance
fact
need
know
paper
nuv
priors
per
estimate
pleasing
properties
ï¬rst
promotes
sparsity
thus
used
select
features
relevant
parameters
second
priori
preference
scale
large
values
scaled
note
latter
property
lost
estimation
replaced
map
estimation
based
proper
prior
paper
stick
basic
nuv
regularization
prior
unknown
variances
variables
parameters
interest
modeled
independent
gaussian
random
variables
unknown
variance
estimated
exactly
approximately
maximum
likelihood
advocate
use
nuv
regularization
linear
state
space
models
applications
estimation
impulsive
signals
detection
localized
events
smoothing
occasional
jumps
state
space
detection
removal
outliers
concerning
actual
computations
estimating
un-
known
variances
substantially
different
learning
parameters
state
space
models
carried
expectation
maximization
methods
way
actual
computations
essentially
amount
gaussian
message
passing
paper
structured
follows
section
begin
quick
look
nuv
regularization
standard
linear
model
estimation
unknown
variances
addressed
section
iii
factor
graphs
state
space
models
reviewed
sections
respectively
nuv
regularization
models
addressed
section
new
tables
gaussian-message
computations
given
appendix
sum
gaussians
least
squares
nuv
regularization
begin
elementary
linear
model
special
case
relevance
vector
machine
follows
let
equality
proof
given
appendix
matrices
positive
deï¬nite
former
depends
latter
depends
also
iii
variance
estimation
following
standard
approach
sections
estimated
algorithm
follows
unknown
variances
section
analogous
quantities
later
cid:88
k=1
bkuk
unknown
random
variables
independent
zero-mean
real
scalar
gaussian
variances
noise
rn-valued
zero-mean
gaussian
covariance
matrix
Ïƒ2i
independent
given
observation
wish
maximum
likelihood
estimate
ï¬rst
second
ï¬rst
step
achieve
sparsity
estimate
zero
ï¬xed
second
step
second
step
estimation
ï¬xed
standard
gaussian
estimation
problem
map
estimation
mmse
estimation
lmmse
estimation
coincide
amount
minimizing
ï¬xed
cid:13
cid:13
cid:13
cid:88
kâˆˆk+
cid:13
cid:13
cid:13
cid:88
kâˆˆk+
bkuk
cid:107
cid:107
denotes
set
indices
closed-form
solution
minimization
Ë†uk
kbt
cid:52
cid:32
cid:88
k=1
cid:33
kbkbt
Ïƒ2i
may
obtained
standard
least-squares
equations
see
also
alternative
proof
given
appendix
also
point
computed
without
matrix
inversion
estimate
zero
two
different
generalizations
condition
setting
section
given
following
theorem
let
denote
probability
density
variables
according
theorem
let
ï¬xed
local
maximum
saddle
point
y|Ïƒ2
cid:52
cid:33
cid:96
Ïƒ2i
kbkbt
cid:1
cid:0
cid:32
cid:88
cid:0
cid:1
cid:96
cid:96
cid:96
moreover
begin
initial
guess
compute
mean
muk
variance
gaussian
posterior
distribution
uk|y
according
update
repeat
steps
convergence
ï¬xed
pragmatic
stopping
criterion
met
optionally
update
standard
update
variances
according
cid:3
cid:2
k|Ïƒ2
given
required
quantities
respectively
update
basic
theory
guaran-
decrease
tees
likelihood
y|Ïƒ2
normally
increase
step
algorithm
stated
algorithm
safe
convergence
slow
following
alternative
update
rule
due
mackay
often
converges
much
faster
however
alterative
update
rule
comes
without
guaran-
tees
sometimes
agressive
algorithm
fails
completely
/Ïƒ2
individual
variance
also
estimated
â†âˆ’muk
â†âˆ’Ïƒ
cid:9
maximum-likelihood
step
y|Ïƒ2
argmax
max
cid:8
meanâ†âˆ’muk
given
104
varianceâ†âˆ’Ïƒ
given
however
parallel
updates
simultaneously
step
algorithm
rule
normally
agressive
fails
later
algorithm
used
estimating
parameters
variables
linear
state
space
models
case
useful
analytical
expressions
analogs
muk
quantities
easily
computed
gaussian
message
passing
factor
graphs
gaussian
message
passing
heavily
use
factor
graphs
reasoning
describing
algorithms
use
factor
graphs
nodes/boxes
represent
factors
-Ïƒ1
-Ïƒ2
Ëœu1
Ëœu2
-Ïƒk
Ïƒ2i
Ëœuk
fig
cycle-free
factor
graph
nuv
regularization
edges
represent
variables
contrast
factor
graphs
variable
nodes
factor
nodes
cid:52
bkuk
cid:52
xkâˆ’1
Ëœuk
figure
example
represents
probability
density
uk|Ïƒ1
model
auxiliary
cid:52
variables
Ëœuk
nodes
labeled
represent
zero-mean
normal
densities
variance
node
labeled
Ïƒ2i
represents
zero-
mean
multivariate
normal
density
covariance
matrix
Ïƒ2i
nodes
figure
represent
deterministic
constraints
ï¬xed
figure
cycle-free
linear
gaus-
sian
factor
graph
map/mmse/lmmse
estimation
variables
carried
gaussian
message
passing
described
detail
interestingly
particular
example
message
passing
carried
sym-
bolically
i.e.
technique
derive
closed-form
expressions
estimates
every
message
paper
scalar
multivariate
gaussian
distribution
scale
factor
sometimes
also
allow
degenerate
limit
gaussian
gaussian
variance
zero
inï¬nity
discuss
detail
scale
factors
ignored
paper
messages
thus
parameterized
mean
vector
denote
mean
vector
covariance
matrix
respectively
message
traveling
forward
edge
figure
covariance
matrix
example
âˆ’â†’mxk
whileâ†âˆ’mxk
precision
matrix
matrix
respectively
message
traveling
backward
edge
alternatively
messages
parameterized
wxk
inverse
covariance
matrix
â†âˆ’v
denote
mean
vector
covariance
precision-weighted
mean
vector
wxkâˆ’â†’mxk
cid:52
backward
message
along
edge
denoted
reversed
arrows
directed
graphical
model
without
cycles
figure
forward
messages
represent
priors
backward
messages
represent
likelihood
functions
scale
factor
addition
also
work
marginals
posterior
distribution
i.e.
product
forward
message
back-
ward
message
along
edge
example
mxk
vxk
denote
posterior
mean
vector
posterior
covariance
matrix
respectively
important
role
paper
played
alternative
parameterization
dual
precision
matrix
cid:52
cid:0
â†âˆ’v
cid:1
â†âˆ’mxk
Ëœwxk
âˆ’â†’mxk
cid:52
Ëœwxk
ËœÎ¾xk
dual
mean
vector
message
computations
parameterizations
given
tables
iâ€“vi
appendix
contain
numerous
improvements
corresponding
tables
linear
state
space
models
consider
standard
linear
state
space
model
state
observation
evolving
according
axkâˆ’1
buk
cxk
rnÃ—n
rnÃ—m
rlÃ—n
values
values
independent
zero-mean
white
gaussian
noise
processes
usually
assume
ï¬rst
second
covariance
matrix
identity
matrix
assumptions
essential
cycle-free
factor
graph
model
shown
figure
section
vary
augment
models
nuv
priors
various
quantities
inference
state
space
model
amounts
kalman
ï¬ltering
smoothing
equivalently
gaussian
message
passing
factor
graph
figure
estimating
input
usually
considered
kalman
ï¬lter
literature
essential
signal
processing
tables
appendix
easy
put
together
large
variety
algorithms
relative
merits
different
algorithms
depend
particulars
-xkâˆ’1
Â·Â·Â·
Â·Â·Â·
Ïƒ2i
Ëœyk
fig
one
section
factor
graph
linear
state
space
model
whole
factor
graph
consists
many
sections
optional
initial
and/or
terminal
conditions
dashed
block
varied
section
problem
however
ï¬nd
following
two
algo-
rithms
usually
advantageous
terms
computational
complexity
terms
numerical
stability
input
output
scalar
decomposed
multiple
scalar
inputs
outputs
neither
two
algorithms
requires
matrix
inversion
ï¬rst
algorithms
essentially
modiï¬ed
brysonâ€“frazier
mbf
smoother
augmented
input-signal
estimation
mbf
message
passing
perform
forward
message
passing
âˆ’â†’mxk
using
ii.1
ii.2
iii.1
iii.2
v.1
v.2
standard
kalman
ï¬lter
perform
backward
message
passing
ËœÎ¾xk
Ëœwxk
beginning
ËœÎ¾xn
Ëœwxn
end
horizon
using
ii.6
ii.7
iii.7
iii.8
either
v.4
v.6
v.8
v.5
v.7
v.9
inputs
may
estimated
using
ii.6
ii.7
iii.7
iii.8
iv.9
iv.13
posterior
mean
mxk
covariance
matrix
vxk
state
individual
component
thereof
may
obtained
iv.9
iv.13
mated
using
i.5
i.6
iii.5
iii.6
cid:52
cxk
may
obviously
esti-
outputs
Ëœyk
step
initialization
Ëœwxn
corresponds
typical
situation
priori
information
state
end
horizon
mbf
message
passing
especially
attractive
input
signal
estimation
step
without
steps
second
algorithm
exact
dual
mbf
message
passing
especially
attractive
state
estimation
output
signal
estimation
i.e.
standard
kalman
smoothing
with-
steps
algorithmâ€”backward
recursion
time-reversed
information
ï¬lter
forward
recursion
marginals
bifm
â€”does
seem
widely
known
bifm
message
passing
â†âˆ’Î¾
â†âˆ’wxk
perform
backward
message
passing
using
i.3
i.4
iii.3
iii.4
vi.1
vi.2
changes
reverse
direction
stated
ta-
ble
time-reversed
version
standard
information
ï¬lter
perform
forward
message
passing
mxk
vxk
using
i.5
i.6
iii.5
iii.6
either
vi.4
vi.6
vi.8
vi.5
vi.7
vi.9
outputs
Ëœyk
may
obviously
estimated
using
i.5
i.6
iii.5
iii.6
dual
means
ËœÎ¾xk
dual
precision
matrices
Ëœwxk
may
obtained
iv.3
iv.7
inputs
may
estimated
using
ii.6
ii.7
iii.7
iii.8
iv.9
iv.13
sparsity
nuv
state
space
models
sparse
input
signals
easily
introduced
simply
replace
normal
prior
figure
nuv
prior
shown
figure
approach
used
estimate
input
signal
however
may
also
interested
clean
output
signal
Ëœyk
cxk
example
consider
problem
approximating
given
signal
constant
segments
illustrated
figure
constant
segments
represented
simplest
possible
state
space
model
input
occasional
jumps
constant
segments
use
sparse
input
signal
nuv
prior
figure
sparsity
levelâ€”i.e.
number
constant
segmentsâ€”can
controlled
assumed
observation
noise
sparse
scalar
input
signal
figure
generalized
several
different
directions
ï¬rst
obvious
generalization
combine
primary
white-noise
input
secondary
sparse
input
shown
figure
example
constant
segments
figure
thus
generalized
random-walk
segments
figure
another
generalization
figure
shown
figure
constant-level
segments
replaced
straight-line
segments
represented
state
space
model
order
corresponding
input
block
two
separate
sparse
scalar
input
signals
shown
figure
ï¬rst
input
uk,1
affects
magnitude
second
input
uk,2
affects
slope
line
model
generalization
polynomial
segments
obvious
continuity
enforced
omitting
input
uk,1
continuity
derivatives
enforced
likewise
generally
figure
generalized
arbitrary
number
sparse
scalar
input
signals
used
allow
occasional
jumps
individual
components
state
arbitrary
state
space
models
-Ïƒk
fig
alternative
input
block
replace
dashed
box
figure
sparse
scalar
input
signal
-Ïƒk
uk,2
uk,1
fig
input
block
white
noise
additional
sparse
scalar
input
-Ïƒk,1
uk,1
-Ïƒk,2
uk,2
fig
input
block
two
separate
sparse
scalar
inputs
two
degrees
freedom
figure
10.
uk,1
-bk
Ëœuk,1
uk,2
Ëœyk
Ëœzk
fig
alternative
output
block
scalar
signal
outliers
fig
estimating
ï¬tting
piecewise
constant
signal
fig
estimating
random
walk
occasional
jumps
fig
10.
approximation
straight-line
segments
fig
signature
addition
full-rank
white
noise
input
block
allowing
general
sparse
pulses
fig
11.
outlier
removal
according
figure
200300400500600700 10010100200300400500600700012200400600800 50050100150200 20 10010201200300400500600700 10010100200300400500600700012200400600800 50050100150200 20 10010201200300400500600700 10010100200300400500600700012200400600800 50050100150200 20 10010201200300400500600700 10010100200300400500600700012200400600800 50050100150200 20 10010201
cid:96
muk
cid:96
examples
parameters
cid:96
learned
described
section
iii
required
quantities
muk
respectively
computed
message
passing
pertinent
factor
graph
described
section
substantial
generalization
figure
shown
figure
figure
generalized
mention
without
proof
generalized
nuv
prior
cid:52
Ëœuk,1
bkuk,1
still
promotes
sparsity
learned
provided
bbt
full
rank
input
model
allows
quite
general
events
happen
signature
estimated
nonzero
vectors
Ë†b1
Ë†b2
may
viewed
features
given
signal
used
analysis
finally
turn
output
block
figure
simple
effective
method
detect
remove
outliers
scalar
output
signal
state
space
model
replace
cxk
Ëœzk
sparse
Ëœzk
shown
figure
parameters
estimated
essentially
described
section
iii
required
quantities
Ëœzk
computed
message
passing
described
section
example
method
shown
figure
state
space
model
order
details
irrelevant
paper
Ëœzk
vii
conclusion
given
improved
tables
gaussian-message
com-
putations
estimation
linear
state
space
models
pointed
two
preferred
message
passing
algorithms
ï¬rst
algorithm
essentially
modiï¬ed
bryson-frazier
smoother
second
algorithm
dual
addition
advocated
nuv
priors
together
algorithms
sparse
bayesian
learning
introducing
sparsity
linear
state
space
models
outlined
several
applications
paper
factor
graphs
cycle-free
gaussian
message
passing
yields
exact
marginals
use
nuv
regularization
factor
graphs
cycles
relative
merits
comparison
e.g.
amp
remains
investigated
tabulated
gaussian-message
computations
appendix
tables
iâ€“vi
improved
versions
corresponding
tables
notation
different
parameterizations
messages
deï¬ned
section
main
novelties
new
version
following
cid:52
new
notation
wâˆ’â†’m
â†âˆ’wâ†âˆ’m
â†âˆ’Î¾
introduction
dual
marginal
iv.1
pertinent
new
expressions
tables
iâ€“v
new
expressions
dual
precision
matrix
especially
v.4
v.9
results
used
appendix
two
preferred
algorithms
section
cid:52
gaussian
message
passing
equality-constraint
table
constraint
expressed
factor
â†âˆ’Î¾
â†âˆ’wy
â†âˆ’Î¾
â†âˆ’Î¾
â†âˆ’Î¾
â†âˆ’wz
â†âˆ’wy
â†âˆ’wx
ËœÎ¾x
ËœÎ¾y
ËœÎ¾z
gaussian
message
passing
adder
node
table
i.1
i.2
i.3
i.4
i.5
i.6
i.7
constraint
expressed
factor
âˆ’â†’mz
âˆ’â†’mx
âˆ’â†’my
â†âˆ’mz
âˆ’â†’my
â†âˆ’mx
â†âˆ’v
â†âˆ’v
ËœÎ¾x
ËœÎ¾y
ËœÎ¾z
Ëœwx
Ëœwy
Ëœwz
ii.1
ii.2
ii.3
ii.4
ii.5
ii.6
ii.7
new
expressions
vi.4
vi.9
marginals
essential
bifm
kalman
smoother
sec-
tion
proofs
given
new
expressions
proofs
refer
proof
i.7
using
iv.3
i.3
i.4
i.5
â†âˆ’wx
â†âˆ’Î¾
ËœÎ¾x
gaussian
message
passing
matrix
multiplier
node
arbitrary
real
matrix
table
iii
constraint
expressed
factor
âˆ’â†’my
aâˆ’â†’mx
aâˆ’â†’
â†âˆ’Î¾
atâ†âˆ’Î¾
â†âˆ’wx
atâ†âˆ’wy
amx
avx
ËœÎ¾x
ËœÎ¾y
Ëœwx
Ëœwy
iii.1
iii.2
iii.3
iii.4
iii.5
iii.6
iii.7
iii.8
gaussian
single-edge
marginals
duals
cid:52
ËœÎ¾x
cid:52
table
Ëœwx
ËœÎ¾x
Ëœwx
âˆ’â†’mx
â†âˆ’mx
â†âˆ’Î¾
â†âˆ’wx
â†âˆ’v
â†âˆ’wx
vxâˆ’â†’
â†âˆ’wx
â†âˆ’wx
â†âˆ’wx
â†âˆ’Î¾
âˆ’â†’mx
â†âˆ’v
ËœÎ¾x
â†âˆ’mx
â†âˆ’wx
â†âˆ’v
Ëœwxâˆ’â†’
â†âˆ’v
â†âˆ’v
Ëœwx
â†âˆ’v
cid:0
â†âˆ’Î¾
â†âˆ’Î¾
cid:0
â†âˆ’wy
â†âˆ’wz
cid:1
cid:1
â†âˆ’Î¾
â†âˆ’Î¾
cid:1
cid:0
â†âˆ’wzmz
cid:0
â†âˆ’wy
â†âˆ’mx
âˆ’â†’mx
âˆ’â†’my
â†âˆ’mz
âˆ’â†’mx
âˆ’â†’mz
â†âˆ’mz
Ëœwx
ËœÎ¾y
ËœÎ¾z
iv.1
iv.2
iv.3
iv.4
iv.5
iv.6
iv.7
iv.8
iv.9
iv.10
iv.11
iv.12
iv.13
iv.14
cid:1
proof
ii.6
ï¬rst
note
gaussian
message
passing
observation
block
table
cid:52
atg
v.1
v.2
v.3
âˆ’â†’mz
âˆ’â†’mx
â†âˆ’my
aâˆ’â†’mx
atgaâˆ’â†’
cid:0
â†âˆ’v
aâˆ’â†’
cid:1
ËœÎ¾x
ËœÎ¾z
atâ†âˆ’wy
cid:0
aâˆ’â†’mz
â†âˆ’my
cid:1
ËœÎ¾z
atg
aâˆ’â†’mx
â†âˆ’my
Ëœwx
Ëœwzf
atâ†âˆ’wy
zatâ†âˆ’wy
â†âˆ’v
â†âˆ’v
exchange
ËœÎ¾x
ËœÎ¾z
exchange
reverse
direction
replace
âˆ’â†’mz
byâ†âˆ’mx
âˆ’â†’mx
â†âˆ’mz
Ëœwzf
atga
cid:52
v.4
v.5
v.6
v.7
v.8
v.9
Ëœwx
Ëœwz
change
v.4
v.5
atga
proof
iv.9
iv.2
using
iv.13
iv.12
â†âˆ’my
ii.6
follows
ii.7
proof
iii.7
using
iii.9
â†âˆ’mx
ËœÎ¾x
Ëœwx
âˆ’â†’mx
Ëœwxâˆ’â†’mx
Ëœwx
â†âˆ’mx
Ëœwy
aâˆ’â†’mx
Ëœwy
Ëœwy
âˆ’â†’my
â†âˆ’my
â†âˆ’Î¾
vxâˆ’â†’
cid:17
cid:16
Ëœwxâˆ’â†’
âˆ’â†’mx
â†âˆ’mx
cid:0
âˆ’â†’mx
cid:1
âˆ’â†’mx
iv.2
follows
multiplication
atâ†âˆ’wy
atâ†âˆ’wy
proof
v.9
i.2
iii.4
obtain
Ëœwx
ËœÎ¾x
Ëœwx
wzf
â†âˆ’v
â†âˆ’Î¾
gaussian
message
passing
input
block
table
cid:52
vi.1
vi.2
vi.3
ahat
â†âˆ’Î¾
tvz
ahat
cid:52
vi.4
vi.5
vi.6
vi.7
vi.8
vi.9
atâˆ’â†’
ahatâˆ’â†’
atâˆ’â†’
cid:0
cid:1
cid:0
atâˆ’â†’
tmz
aâˆ’â†’
cid:1
tmz
cid:0
atâˆ’â†’
cid:1
tvz
aâˆ’â†’
wzaâˆ’â†’
reverse
direction
replace
â†âˆ’wx
â†âˆ’Î¾
â†âˆ’wz
exchange
exchange
replace
âˆ’âˆ’â†’
thus
zâˆ’â†’
fâˆ’â†’
atga
cid:1
cid:0
v.2
ËœÎ¾z
cid:0
â†âˆ’wy
cid:0
âˆ’â†’mz
proof
v.4
using
i.7
iii.7
iv.3
using
iii.5
iv.9
â†âˆ’my
cid:1
hand
atga
follows
ËœÎ¾x
ËœÎ¾z
ËœÎ¾y
â†âˆ’wy
amz
cid:1
ËœÎ¾z
inserting
yields
v.4
proof
v.5
begin
using
iv.9
âˆ’â†’mx
ËœÎ¾x
âˆ’â†’mz
âˆ’â†’mx
atg
ËœÎ¾z
â†âˆ’my
aâˆ’â†’mx
ËœÎ¾z
using
v.9
yields
v.7
yields
proof
v.6
already
established
v.7
need
prove
last
step
follows
atga
atga
insert-
ing
yields
proof
vi.9
ii.2
iii.2
v.5
proof
v.7
begin
using
iv.13
second
step
uses
v.1
fâˆ’â†’
fâˆ’â†’
subtracting
âˆ’â†’mx
multiplying
Ëœwzâˆ’â†’
Ëœwxâˆ’â†’
atgaâˆ’â†’
Ëœwzfâˆ’â†’
second
step
uses
v.2
subtracting
multiplying
atga
atâ†âˆ’wy
atga
zatâ†âˆ’wy
wxâˆ’â†’
atâ†âˆ’wy
aâˆ’â†’
zâˆ’â†’
aâˆ’â†’
aâˆ’â†’
Ëœfâˆ’â†’
ahat
cid:1
cid:0
vi.2
cid:0
âˆ’â†’my
cid:0
atâˆ’â†’
inserting
yields
vi.4
proof
vi.5
begin
ËœÎ¾x
ËœÎ¾z
ii.6
using
iv.2
proof
vi.4
using
ii.3
iii.5
iv.9
using
ii.6
iii.7
iv.2
thus
wzâˆ’â†’
hand
amy
obtain
ahat
follows
tmz
ËœÎ¾y
ËœÎ¾z
wzmz
wzmz
cid:1
cid:1
ËœÎ¾y
Ëœfâˆ’â†’
yields
vi.5
proof
vi.7
begin
Ëœwx
Ëœwz
ii.7
using
iv.6
second
step
uses
vi.1
subtracting
multiplying
wzvzâˆ’â†’
vxâˆ’â†’
tvz
Ëœfâˆ’â†’
ahatâˆ’â†’
second
step
uses
vi.2
subtracting
multiplying
yields
vi.7
proof
vi.6
since
already
established
vi.7
need
prove
using
vi.9
ahat
aâˆ’â†’
ahat
wzaâˆ’â†’
xâˆ’â†’
aâˆ’â†’
atâˆ’â†’
wzâˆ’â†’
last
step
follows
ahat
ahat
inserting
yields
appendix
message
passing
figure
proofs
appendix
demonstrate
quantities
pertaining
computations
mentioned
sections
iii
well
proof
theorem
section
obtained
symbolic
message
passing
using
tables
appendix
key
ideas
section
section
v.c
throughout
section
ï¬xed
key
quantities
ËœÎ¾xk
Ëœwxk
pivotal
quantities
section
dual
mean
concerning
dual
precision
matrix
Ëœuk
vector
Ëœuk
former
follows
ii.6
follows
Ëœuk
ËœÎ¾xk
ËœÎ¾x0
ËœÎ¾y
Ëœwy
ËœÎ¾y
Ëœwy
âˆ’â†’my
â†âˆ’my
Ëœwy
since
âˆ’â†’my
concerning
Ëœwxk
Ëœuk
Ëœwxk
Ëœwx0
Ëœwy
deï¬ned
Ëœwy
cid:0
â†âˆ’v
cid:1
follows
ii.7
follows
â†âˆ’v
matrix
computed
without
matrix
inversion
follows
first
note
k=1
kbkbt
Ïƒ2i
cid:88
â†âˆ’v
Ëœwx0
cid:0
cid:1
â†âˆ’v
cid:0
cid:1
â†âˆ’wx0
â†âˆ’wx0
computed
â†âˆ’wxk
â†âˆ’wxk
Ïƒâˆ’2
â†âˆ’wxk
â†âˆ’wxk
â†âˆ’wxk
Ïƒâˆ’2i
complexity
alter-
second
using
vi.2
matrix
backward
recursion
â†âˆ’wxkâˆ’1
starting
native
computation
n2k
contrast
direct
computation
using
gauss-jordan
elimination
matrix
inversion
complexity
n2k
posterior
distribution
map
estimate
ï¬xed
map
estimate
mean
muk
gaussian
posterior
iv.9
iii.7
muk
âˆ’â†’muk
kbt
Ëœuk
ËœÎ¾uk
yields
proves
muk
kbt
re-estimating
variance
need
variance
iv.13
iii.8
kbt
Ëœwukâˆ’â†’
bkÏƒ2
Ëœuk
yields
section
iii
also
posterior
distribution
kbt
bkÏƒ2
likelihood
function
backward
message
consider
backward
message
along
edge
likelihood
function
y|uk
ï¬xed
ï¬xed
scale
factor
use
section
b-d
give
two
different
expressions
message
meanâ†âˆ’muk
varianceâ†âˆ’Ïƒ
latter
haveâ†âˆ’wuk
â†âˆ’w
Ëœuk
â†âˆ’w
Ëœuk
iii.4
thusâ†âˆ’Ïƒ
cid:0
cid:1
xkâˆ’1
cid:1
Ëœuk
alternatively
cid:0
deï¬ned
also
note
ii.4
used
iv.4
iii.8
â†âˆ’Î¾
â†âˆ’w
Ëœuk
â†âˆ’Ïƒ
â†âˆ’v
meanâ†âˆ’muk
â†âˆ’Î¾
Ëœuk
â†âˆ’w
Ëœuk
â†âˆ’mËœuk
â†âˆ’w
Ëœuk
â†âˆ’mxk
âˆ’â†’mxkâˆ’1
â†âˆ’w
Ëœuk
â†âˆ’Î¾
â†âˆ’muk
â†âˆ’Ïƒ
â†âˆ’w
Ëœuk
â†âˆ’w
Ëœuk
cid:0
cid:1
â†âˆ’muk
âˆ’â†’muk
alternatively
iii.3
ii.3
thus
ËœÎ¾uk
Ëœuk
âˆ’1bt
âˆ’1bt
Ëœuk
100
101
102
103
104
used
iv.1
iii.8
iii.7
proof
theorem
section
let
ï¬xed
local
maximum
saddle
106
105
â†âˆ’Ïƒ
argmax
â†âˆ’m2
point
likelihood
y|Ïƒ1
y|Ïƒ1
101
â†âˆ’Ïƒ
â†âˆ’w
Ëœuk
max
â†âˆ’m2
â†âˆ’w
Ëœuk
cid:1
cid:0
â†âˆ’w
Ëœuk
cid:0
cid:1
â†âˆ’Ïƒ
obvious
thatâ†âˆ’m2
cid:1
cid:0
cid:0
cid:1
cid:1
cid:1
104
distinguish
two
cases
106
108
together
imply
â†âˆ’m2
â†âˆ’Ïƒ
cid:0
cid:0
holds
109
107
108
cid:0
cid:0
cid:1
cid:1
hand
106
108
imply
110
combining
two
cases
yields
references
roweis
ghahramani
unifying
review
linear
gaussian
models
neural
computation
vol
305â€“345
feb.
1999
kailath
sayed
hassibi
linear
estimation
prentice
hall
2000
durbin
koopman
time
series
analysis
state
space
methods
oxford
univ
press
2012
bishop
pattern
recognition
machine
learning
springer
2006
h.-a
loeliger
introduction
factor
graphs
ieee
signal
proc
mag.
jan.
2004
28â€“41
h.-a
loeliger
dauwels
junli
korl
ping
kschis-
chang
factor
graph
approach
model-based
signal
processing
proceedings
ieee
vol
1295â€“1322
june
2007
mackay
bayesian
interpolation
neural
comp.
vol
415â€“447
1992
gull
bayesian
inductive
inference
maximum
entropy
maximum-entropy
bayesian
methods
science
engineering
erickson
smith
eds.
kluwer
1988
53â€“74
neal
bayesian
learning
neural
networks
new
york
springer
verlag
1996
tipping
sparse
bayesian
learning
relevance
vector
machine
machine
learning
research
vol
211â€“244
2001
tipping
faul
fast
marginal
likelihood
maximisation
sparse
bayesian
models
proc
9th
int
workshop
artiï¬cial
intelligence
statistics
2003
wipf
nagarajan
new
view
automatic
relevance
determi-
nation
advances
neural
information
processing
systems
1625â€“
1632
2008
wipf
rao
sparse
bayesian
learning
basis
selection
ieee
trans
signal
proc.
vol
aug.
2004
2153â€“2164
dempster
laird
rubin
maximum
likelihood
incomplete
data
via
algorithm
journal
royal
statistical
society
vol
series
1â€“38
1977
stoica
selÂ´en
cyclic
minimizers
majorization
techniques
expectation-maximization
algorithm
refresher
ieee
signal
proc
mag.
january
2004
112â€“114
ghahramani
hinton
parameter
estimation
linear
dynamical
systems
techn
report
crg-tr-96-2
univ
toronto
1996
dauwels
eckford
korl
h.-a
loeliger
expectation
maximization
message
passingâ€”part
principles
gaussian
messages
arxiv:0910.2832
kschischang
frey
h.-a
loeliger
factor
graphs
sum-product
algorithm
ieee
trans
information
theory
vol
498â€“519
feb.
2001
bruderer
h.-a
loeliger
estimation
sensor
input
signals
neither
bandlimited
sparse
2014
information
theory
applications
workshop
ita
san
diego
feb.
9â€“14
2014
bruderer
input
estimation
dynamical
system
identiï¬cation
new
algorithms
results
phd
thesis
eth
zurich
22575
2015
bierman
factorization
methods
discrete
sequential
estima-
tion
new
york
academic
press
1977
bruderer
malmberg
h.-a
loeliger
deconvolution
weakly-sparse
signals
dynamical-system
identiï¬cation
gaussian
message
passing
2015
ieee
int
symp
information
theory
isit
hong
kong
june
14â€“19
2015
zalmai
malmberg
loeliger
blind
deconvolution
sparse
ï¬ltered
pulses
linear
state
space
models
41th
ieee
int
conf
acoustics
speech
signal
processing
icassp
shanghai
china
march
20â€“25
2016
wadehn
bruderer
sahdeva
h.-a
loeliger
outlier-
insensitive
kalman
smoothing
marginal
message
passing
prepa-
ration
donoho
maleki
montanari
message-passing
algo-
rithms
compressed
sensing
proc
national
academy
sciences
vol
106
18914â€“18919
2009
