sparsity
nuv-em
gaussian
message
passing
kalman
smoothing
hans-andrea
loeliger
lukas
bruderer
hampus
malmberg
federico
wadehn
nour
zalmai
eth
zurich
dept
information
technology
electrical
engineering
abstract—normal
priors
unknown
variance
nuv
long
known
promote
sparsity
blend
well
parameter
learning
expectation
maximization
paper
advocate
approach
linear
state
space
models
applications
estimation
impulsive
signals
detection
localized
events
smoothing
occasional
jumps
state
space
detection
removal
outliers
actual
computations
boil
multivariate-gaussian
message
passing
algorithms
closely
related
kalman
smoothing
give
improved
tables
gaussian-message
com-
putations
algorithms
easily
synthesized
point
two
preferred
algorithms
introduction
paper
two
topics
particular
approach
modeling
estimating
sparse
parameters
based
zero-mean
normal
priors
un-
known
variance
nuv
concerning
second
topic
kalman
smoothing
models
multivariate-gaussian
message
passing
variations
main
point
paper
two
things
well
together
combine
versatile
toolbox
entirely
new
course
body
related
literature
large
nonetheless
speciﬁc
perspective
paper
far
known
authors
advocated
linear
state
space
models
continue
essential
tool
broad
variety
appli-
cations
primary
algorithms
models
variations
generalizations
kalman
ﬁltering
smoothing
equivalently
multivariate-gaussian
message
passing
corresponding
factor
graph
similar
graphical
model
variety
algorithms
easily
synthesized
tables
message
computations
paper
give
new
version
tables
many
improvements
point
two
preferred
algorithms
concerning
ﬁrst
topic
nuv
priors
zero-mean
normal
priors
unknown
variance
originated
bayesian
infer-
ence
sparsity-promoting
nature
priors
basis
automatic
relevance
determination
ard
sparse
bayesian
learning
developed
neal
tipping
wipf
others
basic
properties
nuv
priors
illustrated
following
simple
example
let
variable
parameter
interest
model
zero-mean
real
scalar
gaussian
random
variable
unknown
variance
assume
observe
noise
zero-mean
gaussian
known
variance
independent
maximum
likelihood
estimate
single
sample
easily
determined
ˆs2
cid:52
argmax
cid:112
max
e−µ2/2
s2+σ2
second
step
map/mmse/lmmse
estimate
ﬁxed
ˆs2
cid:40
ˆs2
ˆs2
µ2−σ2
otherwise
equations
continue
hold
scalar
observation
generalized
observation
ﬁxed
likelihood
function
y|u
gaussian
scale
factor
mean
variance
fact
need
know
paper
nuv
priors
per
estimate
pleasing
properties
ﬁrst
promotes
sparsity
thus
used
select
features
relevant
parameters
second
priori
preference
scale
large
values
scaled
note
latter
property
lost
estimation
replaced
map
estimation
based
proper
prior
paper
stick
basic
nuv
regularization
prior
unknown
variances
variables
parameters
interest
modeled
independent
gaussian
random
variables
unknown
variance
estimated
exactly
approximately
maximum
likelihood
advocate
use
nuv
regularization
linear
state
space
models
applications
estimation
impulsive
signals
detection
localized
events
smoothing
occasional
jumps
state
space
detection
removal
outliers
concerning
actual
computations
estimating
un-
known
variances
substantially
different
learning
parameters
state
space
models
carried
expectation
maximization
methods
way
actual
computations
essentially
amount
gaussian
message
passing
paper
structured
follows
section
begin
quick
look
nuv
regularization
standard
linear
model
estimation
unknown
variances
addressed
section
iii
factor
graphs
state
space
models
reviewed
sections
respectively
nuv
regularization
models
addressed
section
new
tables
gaussian-message
computations
given
appendix
sum
gaussians
least
squares
nuv
regularization
begin
elementary
linear
model
special
case
relevance
vector
machine
follows
let
equality
proof
given
appendix
matrices
positive
deﬁnite
former
depends
latter
depends
also
iii
variance
estimation
following
standard
approach
sections
estimated
algorithm
follows
unknown
variances
section
analogous
quantities
later
cid:88
k=1
bkuk
unknown
random
variables
independent
zero-mean
real
scalar
gaussian
variances
noise
rn-valued
zero-mean
gaussian
covariance
matrix
σ2i
independent
given
observation
wish
maximum
likelihood
estimate
ﬁrst
second
ﬁrst
step
achieve
sparsity
estimate
zero
ﬁxed
second
step
second
step
estimation
ﬁxed
standard
gaussian
estimation
problem
map
estimation
mmse
estimation
lmmse
estimation
coincide
amount
minimizing
ﬁxed
cid:13
cid:13
cid:13
cid:88
k∈k+
cid:13
cid:13
cid:13
cid:88
k∈k+
bkuk
cid:107
cid:107
denotes
set
indices
closed-form
solution
minimization
ˆuk
kbt
cid:52
cid:32
cid:88
k=1
cid:33
kbkbt
σ2i
may
obtained
standard
least-squares
equations
see
also
alternative
proof
given
appendix
also
point
computed
without
matrix
inversion
estimate
zero
two
different
generalizations
condition
setting
section
given
following
theorem
let
denote
probability
density
variables
according
theorem
let
ﬁxed
local
maximum
saddle
point
y|σ2
cid:52
cid:33
cid:96
σ2i
kbkbt
cid:1
cid:0
cid:32
cid:88
cid:0
cid:1
cid:96
cid:96
cid:96
moreover
begin
initial
guess
compute
mean
muk
variance
gaussian
posterior
distribution
uk|y
according
update
repeat
steps
convergence
ﬁxed
pragmatic
stopping
criterion
met
optionally
update
standard
update
variances
according
cid:3
cid:2
k|σ2
given
required
quantities
respectively
update
basic
theory
guaran-
decrease
tees
likelihood
y|σ2
normally
increase
step
algorithm
stated
algorithm
safe
convergence
slow
following
alternative
update
rule
due
mackay
often
converges
much
faster
however
alterative
update
rule
comes
without
guaran-
tees
sometimes
agressive
algorithm
fails
completely
/σ2
individual
variance
also
estimated
←−muk
←−σ
cid:9
maximum-likelihood
step
y|σ2
argmax
max
cid:8
mean←−muk
given
104
variance←−σ
given
however
parallel
updates
simultaneously
step
algorithm
rule
normally
agressive
fails
later
algorithm
used
estimating
parameters
variables
linear
state
space
models
case
useful
analytical
expressions
analogs
muk
quantities
easily
computed
gaussian
message
passing
factor
graphs
gaussian
message
passing
heavily
use
factor
graphs
reasoning
describing
algorithms
use
factor
graphs
nodes/boxes
represent
factors
-σ1
-σ2
˜u1
˜u2
-σk
σ2i
˜uk
fig
cycle-free
factor
graph
nuv
regularization
edges
represent
variables
contrast
factor
graphs
variable
nodes
factor
nodes
cid:52
bkuk
cid:52
xk−1
˜uk
figure
example
represents
probability
density
uk|σ1
model
auxiliary
cid:52
variables
˜uk
nodes
labeled
represent
zero-mean
normal
densities
variance
node
labeled
σ2i
represents
zero-
mean
multivariate
normal
density
covariance
matrix
σ2i
nodes
figure
represent
deterministic
constraints
ﬁxed
figure
cycle-free
linear
gaus-
sian
factor
graph
map/mmse/lmmse
estimation
variables
carried
gaussian
message
passing
described
detail
interestingly
particular
example
message
passing
carried
sym-
bolically
i.e.
technique
derive
closed-form
expressions
estimates
every
message
paper
scalar
multivariate
gaussian
distribution
scale
factor
sometimes
also
allow
degenerate
limit
gaussian
gaussian
variance
zero
inﬁnity
discuss
detail
scale
factors
ignored
paper
messages
thus
parameterized
mean
vector
denote
mean
vector
covariance
matrix
respectively
message
traveling
forward
edge
figure
covariance
matrix
example
−→mxk
while←−mxk
precision
matrix
matrix
respectively
message
traveling
backward
edge
alternatively
messages
parameterized
wxk
inverse
covariance
matrix
←−v
denote
mean
vector
covariance
precision-weighted
mean
vector
wxk−→mxk
cid:52
backward
message
along
edge
denoted
reversed
arrows
directed
graphical
model
without
cycles
figure
forward
messages
represent
priors
backward
messages
represent
likelihood
functions
scale
factor
addition
also
work
marginals
posterior
distribution
i.e.
product
forward
message
back-
ward
message
along
edge
example
mxk
vxk
denote
posterior
mean
vector
posterior
covariance
matrix
respectively
important
role
paper
played
alternative
parameterization
dual
precision
matrix
cid:52
cid:0
←−v
cid:1
←−mxk
˜wxk
−→mxk
cid:52
˜wxk
˜ξxk
dual
mean
vector
message
computations
parameterizations
given
tables
i–vi
appendix
contain
numerous
improvements
corresponding
tables
linear
state
space
models
consider
standard
linear
state
space
model
state
observation
evolving
according
axk−1
buk
cxk
rn×n
rn×m
rl×n
values
values
independent
zero-mean
white
gaussian
noise
processes
usually
assume
ﬁrst
second
covariance
matrix
identity
matrix
assumptions
essential
cycle-free
factor
graph
model
shown
figure
section
vary
augment
models
nuv
priors
various
quantities
inference
state
space
model
amounts
kalman
ﬁltering
smoothing
equivalently
gaussian
message
passing
factor
graph
figure
estimating
input
usually
considered
kalman
ﬁlter
literature
essential
signal
processing
tables
appendix
easy
put
together
large
variety
algorithms
relative
merits
different
algorithms
depend
particulars
-xk−1
···
···
σ2i
˜yk
fig
one
section
factor
graph
linear
state
space
model
whole
factor
graph
consists
many
sections
optional
initial
and/or
terminal
conditions
dashed
block
varied
section
problem
however
ﬁnd
following
two
algo-
rithms
usually
advantageous
terms
computational
complexity
terms
numerical
stability
input
output
scalar
decomposed
multiple
scalar
inputs
outputs
neither
two
algorithms
requires
matrix
inversion
ﬁrst
algorithms
essentially
modiﬁed
bryson–frazier
mbf
smoother
augmented
input-signal
estimation
mbf
message
passing
perform
forward
message
passing
−→mxk
using
ii.1
ii.2
iii.1
iii.2
v.1
v.2
standard
kalman
ﬁlter
perform
backward
message
passing
˜ξxk
˜wxk
beginning
˜ξxn
˜wxn
end
horizon
using
ii.6
ii.7
iii.7
iii.8
either
v.4
v.6
v.8
v.5
v.7
v.9
inputs
may
estimated
using
ii.6
ii.7
iii.7
iii.8
iv.9
iv.13
posterior
mean
mxk
covariance
matrix
vxk
state
individual
component
thereof
may
obtained
iv.9
iv.13
mated
using
i.5
i.6
iii.5
iii.6
cid:52
cxk
may
obviously
esti-
outputs
˜yk
step
initialization
˜wxn
corresponds
typical
situation
priori
information
state
end
horizon
mbf
message
passing
especially
attractive
input
signal
estimation
step
without
steps
second
algorithm
exact
dual
mbf
message
passing
especially
attractive
state
estimation
output
signal
estimation
i.e.
standard
kalman
smoothing
with-
steps
algorithm—backward
recursion
time-reversed
information
ﬁlter
forward
recursion
marginals
bifm
—does
seem
widely
known
bifm
message
passing
←−ξ
←−wxk
perform
backward
message
passing
using
i.3
i.4
iii.3
iii.4
vi.1
vi.2
changes
reverse
direction
stated
ta-
ble
time-reversed
version
standard
information
ﬁlter
perform
forward
message
passing
mxk
vxk
using
i.5
i.6
iii.5
iii.6
either
vi.4
vi.6
vi.8
vi.5
vi.7
vi.9
outputs
˜yk
may
obviously
estimated
using
i.5
i.6
iii.5
iii.6
dual
means
˜ξxk
dual
precision
matrices
˜wxk
may
obtained
iv.3
iv.7
inputs
may
estimated
using
ii.6
ii.7
iii.7
iii.8
iv.9
iv.13
sparsity
nuv
state
space
models
sparse
input
signals
easily
introduced
simply
replace
normal
prior
figure
nuv
prior
shown
figure
approach
used
estimate
input
signal
however
may
also
interested
clean
output
signal
˜yk
cxk
example
consider
problem
approximating
given
signal
constant
segments
illustrated
figure
constant
segments
represented
simplest
possible
state
space
model
input
occasional
jumps
constant
segments
use
sparse
input
signal
nuv
prior
figure
sparsity
level—i.e.
number
constant
segments—can
controlled
assumed
observation
noise
sparse
scalar
input
signal
figure
generalized
several
different
directions
ﬁrst
obvious
generalization
combine
primary
white-noise
input
secondary
sparse
input
shown
figure
example
constant
segments
figure
thus
generalized
random-walk
segments
figure
another
generalization
figure
shown
figure
constant-level
segments
replaced
straight-line
segments
represented
state
space
model
order
corresponding
input
block
two
separate
sparse
scalar
input
signals
shown
figure
ﬁrst
input
uk,1
affects
magnitude
second
input
uk,2
affects
slope
line
model
generalization
polynomial
segments
obvious
continuity
enforced
omitting
input
uk,1
continuity
derivatives
enforced
likewise
generally
figure
generalized
arbitrary
number
sparse
scalar
input
signals
used
allow
occasional
jumps
individual
components
state
arbitrary
state
space
models
-σk
fig
alternative
input
block
replace
dashed
box
figure
sparse
scalar
input
signal
-σk
uk,2
uk,1
fig
input
block
white
noise
additional
sparse
scalar
input
-σk,1
uk,1
-σk,2
uk,2
fig
input
block
two
separate
sparse
scalar
inputs
two
degrees
freedom
figure
10.
uk,1
-bk
˜uk,1
uk,2
˜yk
˜zk
fig
alternative
output
block
scalar
signal
outliers
fig
estimating
ﬁtting
piecewise
constant
signal
fig
estimating
random
walk
occasional
jumps
fig
10.
approximation
straight-line
segments
fig
signature
addition
full-rank
white
noise
input
block
allowing
general
sparse
pulses
fig
11.
outlier
removal
according
figure
200300400500600700 10010100200300400500600700012200400600800 50050100150200 20 10010201200300400500600700 10010100200300400500600700012200400600800 50050100150200 20 10010201200300400500600700 10010100200300400500600700012200400600800 50050100150200 20 10010201200300400500600700 10010100200300400500600700012200400600800 50050100150200 20 10010201
cid:96
muk
cid:96
examples
parameters
cid:96
learned
described
section
iii
required
quantities
muk
respectively
computed
message
passing
pertinent
factor
graph
described
section
substantial
generalization
figure
shown
figure
figure
generalized
mention
without
proof
generalized
nuv
prior
cid:52
˜uk,1
bkuk,1
still
promotes
sparsity
learned
provided
bbt
full
rank
input
model
allows
quite
general
events
happen
signature
estimated
nonzero
vectors
ˆb1
ˆb2
may
viewed
features
given
signal
used
analysis
finally
turn
output
block
figure
simple
effective
method
detect
remove
outliers
scalar
output
signal
state
space
model
replace
cxk
˜zk
sparse
˜zk
shown
figure
parameters
estimated
essentially
described
section
iii
required
quantities
˜zk
computed
message
passing
described
section
example
method
shown
figure
state
space
model
order
details
irrelevant
paper
˜zk
vii
conclusion
given
improved
tables
gaussian-message
com-
putations
estimation
linear
state
space
models
pointed
two
preferred
message
passing
algorithms
ﬁrst
algorithm
essentially
modiﬁed
bryson-frazier
smoother
second
algorithm
dual
addition
advocated
nuv
priors
together
algorithms
sparse
bayesian
learning
introducing
sparsity
linear
state
space
models
outlined
several
applications
paper
factor
graphs
cycle-free
gaussian
message
passing
yields
exact
marginals
use
nuv
regularization
factor
graphs
cycles
relative
merits
comparison
e.g.
amp
remains
investigated
tabulated
gaussian-message
computations
appendix
tables
i–vi
improved
versions
corresponding
tables
notation
different
parameterizations
messages
deﬁned
section
main
novelties
new
version
following
cid:52
new
notation
w−→m
←−w←−m
←−ξ
introduction
dual
marginal
iv.1
pertinent
new
expressions
tables
i–v
new
expressions
dual
precision
matrix
especially
v.4
v.9
results
used
appendix
two
preferred
algorithms
section
cid:52
gaussian
message
passing
equality-constraint
table
constraint
expressed
factor
←−ξ
←−wy
←−ξ
←−ξ
←−ξ
←−wz
←−wy
←−wx
˜ξx
˜ξy
˜ξz
gaussian
message
passing
adder
node
table
i.1
i.2
i.3
i.4
i.5
i.6
i.7
constraint
expressed
factor
−→mz
−→mx
−→my
←−mz
−→my
←−mx
←−v
←−v
˜ξx
˜ξy
˜ξz
˜wx
˜wy
˜wz
ii.1
ii.2
ii.3
ii.4
ii.5
ii.6
ii.7
new
expressions
vi.4
vi.9
marginals
essential
bifm
kalman
smoother
sec-
tion
proofs
given
new
expressions
proofs
refer
proof
i.7
using
iv.3
i.3
i.4
i.5
←−wx
←−ξ
˜ξx
gaussian
message
passing
matrix
multiplier
node
arbitrary
real
matrix
table
iii
constraint
expressed
factor
−→my
a−→mx
a−→
←−ξ
at←−ξ
←−wx
at←−wy
amx
avx
˜ξx
˜ξy
˜wx
˜wy
iii.1
iii.2
iii.3
iii.4
iii.5
iii.6
iii.7
iii.8
gaussian
single-edge
marginals
duals
cid:52
˜ξx
cid:52
table
˜wx
˜ξx
˜wx
−→mx
←−mx
←−ξ
←−wx
←−v
←−wx
vx−→
←−wx
←−wx
←−wx
←−ξ
−→mx
←−v
˜ξx
←−mx
←−wx
←−v
˜wx−→
←−v
←−v
˜wx
←−v
cid:0
←−ξ
←−ξ
cid:0
←−wy
←−wz
cid:1
cid:1
←−ξ
←−ξ
cid:1
cid:0
←−wzmz
cid:0
←−wy
←−mx
−→mx
−→my
←−mz
−→mx
−→mz
←−mz
˜wx
˜ξy
˜ξz
iv.1
iv.2
iv.3
iv.4
iv.5
iv.6
iv.7
iv.8
iv.9
iv.10
iv.11
iv.12
iv.13
iv.14
cid:1
proof
ii.6
ﬁrst
note
gaussian
message
passing
observation
block
table
cid:52
atg
v.1
v.2
v.3
−→mz
−→mx
←−my
a−→mx
atga−→
cid:0
←−v
a−→
cid:1
˜ξx
˜ξz
at←−wy
cid:0
a−→mz
←−my
cid:1
˜ξz
atg
a−→mx
←−my
˜wx
˜wzf
at←−wy
zat←−wy
←−v
←−v
exchange
˜ξx
˜ξz
exchange
reverse
direction
replace
−→mz
by←−mx
−→mx
←−mz
˜wzf
atga
cid:52
v.4
v.5
v.6
v.7
v.8
v.9
˜wx
˜wz
change
v.4
v.5
atga
proof
iv.9
iv.2
using
iv.13
iv.12
←−my
ii.6
follows
ii.7
proof
iii.7
using
iii.9
←−mx
˜ξx
˜wx
−→mx
˜wx−→mx
˜wx
←−mx
˜wy
a−→mx
˜wy
˜wy
−→my
←−my
←−ξ
vx−→
cid:17
cid:16
˜wx−→
−→mx
←−mx
cid:0
−→mx
cid:1
−→mx
iv.2
follows
multiplication
at←−wy
at←−wy
proof
v.9
i.2
iii.4
obtain
˜wx
˜ξx
˜wx
wzf
←−v
←−ξ
gaussian
message
passing
input
block
table
cid:52
vi.1
vi.2
vi.3
ahat
←−ξ
tvz
ahat
cid:52
vi.4
vi.5
vi.6
vi.7
vi.8
vi.9
at−→
ahat−→
at−→
cid:0
cid:1
cid:0
at−→
tmz
a−→
cid:1
tmz
cid:0
at−→
cid:1
tvz
a−→
wza−→
reverse
direction
replace
←−wx
←−ξ
←−wz
exchange
exchange
replace
−−→
thus
z−→
f−→
atga
cid:1
cid:0
v.2
˜ξz
cid:0
←−wy
cid:0
−→mz
proof
v.4
using
i.7
iii.7
iv.3
using
iii.5
iv.9
←−my
cid:1
hand
atga
follows
˜ξx
˜ξz
˜ξy
←−wy
amz
cid:1
˜ξz
inserting
yields
v.4
proof
v.5
begin
using
iv.9
−→mx
˜ξx
−→mz
−→mx
atg
˜ξz
←−my
a−→mx
˜ξz
using
v.9
yields
v.7
yields
proof
v.6
already
established
v.7
need
prove
last
step
follows
atga
atga
insert-
ing
yields
proof
vi.9
ii.2
iii.2
v.5
proof
v.7
begin
using
iv.13
second
step
uses
v.1
f−→
f−→
subtracting
−→mx
multiplying
˜wz−→
˜wx−→
atga−→
˜wzf−→
second
step
uses
v.2
subtracting
multiplying
atga
at←−wy
atga
zat←−wy
wx−→
at←−wy
a−→
z−→
a−→
a−→
˜f−→
ahat
cid:1
cid:0
vi.2
cid:0
−→my
cid:0
at−→
inserting
yields
vi.4
proof
vi.5
begin
˜ξx
˜ξz
ii.6
using
iv.2
proof
vi.4
using
ii.3
iii.5
iv.9
using
ii.6
iii.7
iv.2
thus
wz−→
hand
amy
obtain
ahat
follows
tmz
˜ξy
˜ξz
wzmz
wzmz
cid:1
cid:1
˜ξy
˜f−→
yields
vi.5
proof
vi.7
begin
˜wx
˜wz
ii.7
using
iv.6
second
step
uses
vi.1
subtracting
multiplying
wzvz−→
vx−→
tvz
˜f−→
ahat−→
second
step
uses
vi.2
subtracting
multiplying
yields
vi.7
proof
vi.6
since
already
established
vi.7
need
prove
using
vi.9
ahat
a−→
ahat
wza−→
x−→
a−→
at−→
wz−→
last
step
follows
ahat
ahat
inserting
yields
appendix
message
passing
figure
proofs
appendix
demonstrate
quantities
pertaining
computations
mentioned
sections
iii
well
proof
theorem
section
obtained
symbolic
message
passing
using
tables
appendix
key
ideas
section
section
v.c
throughout
section
ﬁxed
key
quantities
˜ξxk
˜wxk
pivotal
quantities
section
dual
mean
concerning
dual
precision
matrix
˜uk
vector
˜uk
former
follows
ii.6
follows
˜uk
˜ξxk
˜ξx0
˜ξy
˜wy
˜ξy
˜wy
−→my
←−my
˜wy
since
−→my
concerning
˜wxk
˜uk
˜wxk
˜wx0
˜wy
deﬁned
˜wy
cid:0
←−v
cid:1
follows
ii.7
follows
←−v
matrix
computed
without
matrix
inversion
follows
first
note
k=1
kbkbt
σ2i
cid:88
←−v
˜wx0
cid:0
cid:1
←−v
cid:0
cid:1
←−wx0
←−wx0
computed
←−wxk
←−wxk
σ−2
←−wxk
←−wxk
←−wxk
σ−2i
complexity
alter-
second
using
vi.2
matrix
backward
recursion
←−wxk−1
starting
native
computation
n2k
contrast
direct
computation
using
gauss-jordan
elimination
matrix
inversion
complexity
n2k
posterior
distribution
map
estimate
ﬁxed
map
estimate
mean
muk
gaussian
posterior
iv.9
iii.7
muk
−→muk
kbt
˜uk
˜ξuk
yields
proves
muk
kbt
re-estimating
variance
need
variance
iv.13
iii.8
kbt
˜wuk−→
bkσ2
˜uk
yields
section
iii
also
posterior
distribution
kbt
bkσ2
likelihood
function
backward
message
consider
backward
message
along
edge
likelihood
function
y|uk
ﬁxed
ﬁxed
scale
factor
use
section
b-d
give
two
different
expressions
message
mean←−muk
variance←−σ
latter
have←−wuk
←−w
˜uk
←−w
˜uk
iii.4
thus←−σ
cid:0
cid:1
xk−1
cid:1
˜uk
alternatively
cid:0
deﬁned
also
note
ii.4
used
iv.4
iii.8
←−ξ
←−w
˜uk
←−σ
←−v
mean←−muk
←−ξ
˜uk
←−w
˜uk
←−m˜uk
←−w
˜uk
←−mxk
−→mxk−1
←−w
˜uk
←−ξ
←−muk
←−σ
←−w
˜uk
←−w
˜uk
cid:0
cid:1
←−muk
−→muk
alternatively
iii.3
ii.3
thus
˜ξuk
˜uk
−1bt
−1bt
˜uk
100
101
102
103
104
used
iv.1
iii.8
iii.7
proof
theorem
section
let
ﬁxed
local
maximum
saddle
106
105
←−σ
argmax
←−m2
point
likelihood
y|σ1
y|σ1
101
←−σ
←−w
˜uk
max
←−m2
←−w
˜uk
cid:1
cid:0
←−w
˜uk
cid:0
cid:1
←−σ
obvious
that←−m2
cid:1
cid:0
cid:0
cid:1
cid:1
cid:1
104
distinguish
two
cases
106
108
together
imply
←−m2
←−σ
cid:0
cid:0
holds
109
107
108
cid:0
cid:0
cid:1
cid:1
hand
106
108
imply
110
combining
two
cases
yields
references
roweis
ghahramani
unifying
review
linear
gaussian
models
neural
computation
vol
305–345
feb.
1999
kailath
sayed
hassibi
linear
estimation
prentice
hall
2000
durbin
koopman
time
series
analysis
state
space
methods
oxford
univ
press
2012
bishop
pattern
recognition
machine
learning
springer
2006
h.-a
loeliger
introduction
factor
graphs
ieee
signal
proc
mag.
jan.
2004
28–41
h.-a
loeliger
dauwels
junli
korl
ping
kschis-
chang
factor
graph
approach
model-based
signal
processing
proceedings
ieee
vol
1295–1322
june
2007
mackay
bayesian
interpolation
neural
comp.
vol
415–447
1992
gull
bayesian
inductive
inference
maximum
entropy
maximum-entropy
bayesian
methods
science
engineering
erickson
smith
eds.
kluwer
1988
53–74
neal
bayesian
learning
neural
networks
new
york
springer
verlag
1996
tipping
sparse
bayesian
learning
relevance
vector
machine
machine
learning
research
vol
211–244
2001
tipping
faul
fast
marginal
likelihood
maximisation
sparse
bayesian
models
proc
9th
int
workshop
artiﬁcial
intelligence
statistics
2003
wipf
nagarajan
new
view
automatic
relevance
determi-
nation
advances
neural
information
processing
systems
1625–
1632
2008
wipf
rao
sparse
bayesian
learning
basis
selection
ieee
trans
signal
proc.
vol
aug.
2004
2153–2164
dempster
laird
rubin
maximum
likelihood
incomplete
data
via
algorithm
journal
royal
statistical
society
vol
series
1–38
1977
stoica
sel´en
cyclic
minimizers
majorization
techniques
expectation-maximization
algorithm
refresher
ieee
signal
proc
mag.
january
2004
112–114
ghahramani
hinton
parameter
estimation
linear
dynamical
systems
techn
report
crg-tr-96-2
univ
toronto
1996
dauwels
eckford
korl
h.-a
loeliger
expectation
maximization
message
passing—part
principles
gaussian
messages
arxiv:0910.2832
kschischang
frey
h.-a
loeliger
factor
graphs
sum-product
algorithm
ieee
trans
information
theory
vol
498–519
feb.
2001
bruderer
h.-a
loeliger
estimation
sensor
input
signals
neither
bandlimited
sparse
2014
information
theory
applications
workshop
ita
san
diego
feb.
9–14
2014
bruderer
input
estimation
dynamical
system
identiﬁcation
new
algorithms
results
phd
thesis
eth
zurich
22575
2015
bierman
factorization
methods
discrete
sequential
estima-
tion
new
york
academic
press
1977
bruderer
malmberg
h.-a
loeliger
deconvolution
weakly-sparse
signals
dynamical-system
identiﬁcation
gaussian
message
passing
2015
ieee
int
symp
information
theory
isit
hong
kong
june
14–19
2015
zalmai
malmberg
loeliger
blind
deconvolution
sparse
ﬁltered
pulses
linear
state
space
models
41th
ieee
int
conf
acoustics
speech
signal
processing
icassp
shanghai
china
march
20–25
2016
wadehn
bruderer
sahdeva
h.-a
loeliger
outlier-
insensitive
kalman
smoothing
marginal
message
passing
prepa-
ration
donoho
maleki
montanari
message-passing
algo-
rithms
compressed
sensing
proc
national
academy
sciences
vol
106
18914–18919
2009
