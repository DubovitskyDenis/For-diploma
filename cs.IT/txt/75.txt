beneﬁt
1-bit
jump-start
necessity
stochastic
encoding
jamming
channels
bikash
kumar
dey
sidharth
jaggi
michael
langberg
anand
sarwate
september
2018
abstract
consider
problem
communicating
message
presence
malicious
jamming
adversary
calvin
erase
arbitrary
set
bits
transmitted
bits
capacity
channel
calvin
exactly
causal
i.e
calvin
decision
whether
erase
bit
depends
observations
recently
characterized
work
show
two
perhaps
surprising
phenomena
firstly
demonstrate
via
novel
code
construction
calvin
delayed
even
single
bit
i.e
calvin
decision
whether
erase
bit
depends
xi−1
independent
current
bit
capacity
increases
encoder
allowed
stochastic
secondly
show
via
novel
jamming
strategy
calvin
single-bit-delay
setting
encoding
deterministic
i.e
transmitted
codeword
deterministic
function
message
rate
asymptotically
larger
possible
vanishing
probability
error
hence
stochastic
encoding
using
private
randomness
encoder
essential
achieve
capacity
one-bit-delayed
calvin
introduction
two
traditional
methods
information
theory
modeling
uncertainty
communication
channels
shannon
approach
treats
uncertainty
channel
random
phenomenon
requires
probability
decoding
error
vanish
blocklength
tends
inﬁnity
capacity
governed
behaviour
typical
channel
realizations
example
binary
erasure
channel
bec
erasure
probability
channel
erase
approximately
symbols
classical
error-control
coding
might
call
hamming
approach
considers
problem
worst-case
recovery
assuming
channel
erases
symbols
goal
design
codes
exactly
recover
transmitted
message
one
way
view
diﬀerences
two
models
anthropomorphize
channel
assume
controlled
adversary
call
calvin
wishes
foil
communication
transmitter
receiver
hereafter
referred
alice
bob
restricting
information
available
calvin
recover
models
communication
two
regimes
information
could
transmitted
message
codeword
example
bec
could
modeled
oblivious
adversary
knows
neither
message
codebook
used
transmitter
receiver
restricted
erase
symbols
bec
allow
probability
error
average
maximum
messages
tends
hamming
approach
pessimistic
calvin
knows
transmitted
message
codeword
codebook
adversarially
choose
positions
figure
binary
adversarial
erasure
channels
erase
create
uncertainty
decoder
good
code
hamming
sense
protects
erasure
attacks
guarantees
zero
error
subject
adversary
constraint
advantage
perhaps
paranoid
adversarial
modeling
reveals
plethora
intermediate
models
shannon
hamming
models
potentially
shed
light
diﬀerence
average
worst
case
analysis
arbitrarily
varying
channel
avc
time-varying
state
e.g
presence/absence
erasure
chosen
calvin
avc
models
distinctions
error
criteria
maximum
average
presence
common
randomness
shared
alice
bob
become
important
sometimes
avc
capacity
displays
dichotomy
calvin
simulate
sending
legitimate
message
bob
may
able
decode
correctly
avcs
called
symmetrizable
capacity
case
paper
ﬁnd
new
dichotomy
calvin
observe
transmitted
codeword
subject
delay
time
calvin
knowledge
transmitted
codeword
time
particular
study
case
model
calvin
erase
transmitted
bits
encoder
decoder
share
common
randomness
prior
work
shows
capacity
model
bec
capacity
show
model
study
capacity
coding
scheme
uses
randomness
encoder
speciﬁcally
rate
maximum
probability
error
encoder
randomness
goes
result
may
come
surprise
capacity
strictly
lower
equals
moreover
show
encoder
randomness
essential
proving
deterministic
coding
scheme
capacity
contrast
show
sec
omniscient
adversary
noncausal
knowledge
full
codeword
capacity
stochastic
encoding
deterministic
encoding
1.1
prior
work
contributions
focus
two
aspects
communication
models
adversaries
impact
delay
knowledge
adversary
diﬀerence
deterministic
stochastic
encoding
ﬁrst
paper
knowledge
examined
issues
ahlswede
wolfowitz
gave
several
equivalences
classes
avc
models
showed
stochastic
encoding
alone
beneﬁt
deterministic
encoding
traditional
works
avc
00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91pcapacitycob
=c1−bit−delay
=1−pccausal
=c1−bit−delaydet
=1−2plp
boundgv
bound
focused
case
calvin
oblivious
erasure
adversaries
capacity
average
error
deterministic
codes
encoder
decoder
share
common
randomness
capacity
even
calvin
omniscient
deterministic
codes
best-known
achievable
rate
equals
via
codes
best-known
outer
bound
given
bound
results
show
sharp
diﬀerence
bassily
smith
proved
outer
bound
chen
constructed
code
stochastic
encoding
achieves
1−2p
−n
n-lookahead
code
achieves
1−2p−
showing
sublinear
lookahead
improve
calvin
jamming
strategy
ﬁnd
capacity
hence
title
one-bit
delay
1−p
thereby
establishing
result
positive
demonstrates
sharp
asymmetry
eﬀect
lookahead
delay
capacities
problems
spectrum
plotted
figure
second
issue
address
importance
stochastic
encoding
private
randomization
ﬁrst
paper
avcs
considered
case
full
common
randomness
proof
extend
constrained
adversaries
avc
results
focus
diﬀerence
common
randomness
deterministic
coding
oblivious
omniscient
adversaries
stochastic
encoding
oﬀers
beneﬁts
settings
dmcs
avcs
although
useful
wiretap
scenarios
paper
show
deterministic
codes
achieve
rates
higher
whereas
stochastic
encoding
achieve
rate
shows
stochastic
encoding
essential
speciﬁc
channel
considered
work
related
fascinating
open
question
whether
true
exactly
causal
binary
erasure
channels
rate-optimal
codes
achieving
used
stochastic
encoding
unclear
whether
rate
achievable
via
deterministic
codes
however
omniscient
adversary
capacity
stochastic
encoding
argued
deterministic
encoding
sec
1.2
comparison
large
alphabet
channels
often
analyzing
large
alphabet
channels
channel
input/output
alphabet
sizes
larger
blocklength
gives
one
insight
general
channels
including
channel
models
challenging
characterize
binary
channels
large
alphabet
erasure
channels
situation
somewhat
diﬀerent
considered
paper
input
alphabet
size
least
p-fraction
symbols
may
erased
capacity
exactly
equals
capacity
q-ary
random
erasure
channel
erasure
probability
symbol
rate
attainable
regardless
knowledge
adversary
computationally-eﬃciently
attainable
deterministic
reed-solomon
codes
hence
neither
behaviours
observed
binary
adversarial
erasure
channel
observed
hand
large
alphabet
symbol
errors
output
symbols
may
diﬀer
input
symbols
may
observe
similar
behaviour
binary
erasure
case
demonstrated
capacity
exactly
causal
channels
equals
capacity
adversary
omniscient
hence
advantage
lookahead
adversary
cases
achieve
rates
using
computationally
eﬃcient
deterministic
reed-
solomon
codes
however
adversary
delayed
depending
symbol-error
model
capacity
may
higher
two
symbol-error
models
considered
symbol
errors
additive
output
symbol
equals
+ei
input
symbol
error
symbol
eis
may
non-zero
symbols
addition
ﬁnite
ﬁeld
delay
even
single
symbol
capacity
equals
thereby
exhibiting
similar
phase-transition
capacity
paper
contrast
overwrite
errors
output
symbol
equals
non-zero
eis
delay
symbols
function
xi−dn
capacity
1/2
otherwise
hence
demonstrating
less
sharp
transition
throughput
diﬀerences
optimal
rates
obtainable
stochastic
deterministic
encoding
large
alphabets
causally-constrained
adversaries
best
knowledge
considered
literature
may
worthy
investigation
channel
model
integers
let
denote
set
r+1
let
denote
set
set
let
complement
let
denote
erasure
output
symbol
random
variables
typically
denoted
capital
letters
vectors
boldface
vector
set
write
vector
i∈s
components
ordered
increasing
order
index
hamming
weight
binary
vector
wth
hamming
distance
ﬁrst
set
channel
model
generally
specializing
case
considered
writeup
let
discrete
alphabets
consider
variants
arbitrarily
varying
channel
models
channels
whose
state
partially
controlled
malicious
adversary
wishes
prevent
reliable
communication
across
channel
model
parameterized
set
discrete
channels
y|x
blocklength
input
state
output
blocklength-n
extension
channel
y|x
yt|xt
cid:89
i=1
2nr
code
randomized
encoding
channel
pair
maps
2nr
randomized
encoding
map
2nr
deterministic
decoding
map
deterministic
code
encoder
also
deterministic
assigns
unique
codeword
2nr
messages
consider
channel
models
chosen
adversarially
partial
knowledge
transmitted
codeword
deﬁne
adversarial
strategy
delay
sequence
maps
t−∆
randomized
map
t−∆
allow
map
depend
code
alternatively
strategy
deﬁnes
conditional
probability
distribution
zt|x
t−∆
chooses
corresponds
scenario
adversary
observe
channel
input
delay
choose
channel
state
based
information
structure
code
say
strategy
satisﬁes
cost
constraint
respect
cost
function
cid:88
t=1
let
set
strategies
satisﬁes
cost
constraint
probability
error
code
message
2nr
adversarial
strategy
cid:88
cid:88
cid:88
cid:54
x∈x
z∈z
perr
y|x
cid:89
t=1
maximum
probability
error
perr
max
γ∈g
max
2nr
perr
zt|x
t−∆
supremum
set
achievable
rates
note
probabilities
encoder
randomness
potential
randomness
adversary
strategy
possible
randomness
channel
say
rate
achievable
model
exists
sequence
cid:98
cid:99
codes
perr
capacity
take
channel
model
given
cost
function
corresponds
binary-input
channel
adversary
observe
past
inputs
erase
bits
larger
delay
capacity
however
delay
capacity
remainder
paper
show
stochastic
encoding
capacity
deterministic
encoding
capacity
main
results
take
form
two
theorems
theorem
achievability
result
says
stochastic
encoding
achieve
rate
bit-erasing
adversary
erase
bits
subject
delay
theorem
capacity
binary
channel
bit-erasing
adversary
erase
fraction
codeword
based
causal
1-bit-delayed
observation
next
theorem
contrasts
result
say
transmitter
restricted
using
deterministic
codes
capacity
therefore
stochastic
encoding
crucial
take
advantage
adversary
delayed
observation
theorem
capacity
binary
channel
deterministic
encoding
bit-erasing
adversary
erase
upto
fraction
codeword
based
causal
1-bit-delayed
observation
analysis
stochastic
encoding
proof
theorem
consider
coding
online
adversarial
channel
binary
inputs
adversary
observes
channel
input
subject
unit
delay
erase
fraction
bits
larger
delay
capacity
bits
however
delay
capacity
bits
3.1
code
construction
encoding
decoding
given
parameter
rate
blocklength
let
cid:98
2nr
cid:99
number
messages
3.1.1
random
code
construction
code
construction
relies
following
parameter
settings
1/4
log2
−1/2
2k−1n
message
base
codeword
selected
uniformly
random
message
encoder
partition
set
set
set
indices
codeword
generate
partitions
binning
indices
bins
independently
uniformly
random
addition
encoder
maintains
set
probabilities
given
3.1.2
encoding
encoding
randomized
encode
message
encoder
transmits
bernoulli
encoder
adds
bernoulli
noise
components
indexed
encoder
depicted
figure
3.1.3
decoding
given
received
codeword
decoder
ﬁrst
ﬁnds
smallest
index
ﬁrst
positions
contain
/2
unerased
bits
min
cid:54
/2
list
decoding
decoder
constructs
list
based
preﬁx
speciﬁcally
message
put
list
cid:54
cid:54
n3/4
codewords
suﬃciently
close
hamming
distance
unerased
bits
put
list
list
disambiguation
decoder
turns
suﬃx
tuple
deﬁne
set
unerased
bits
k1-th
part
k2-th
part
vm1
pair
decoder
ﬁrst
checks
see
exists
cid:54
|vm1
k2|
figure
encoder
message
encoded
ﬁrst
noise
added
according
subsets
corresponding
noise
level
probabilities
subsets
represented
diﬀerent
shades
grey
resulting
codeword
corrupted
calvin
may
use
erasures
received
word
xzyns
m,1
channelencoderbasecodewordencodernoisemessagereceivedcodewordnoise-levelq1qknr=n
1 p  
 block-lengthbaseencodercodeword0101001011001011001011000101000101001010110
0101001011001011001011000101000101001010110
 nperasures0101001011001011001011000101000101001010110
noise-level
pair
cid:54
exists
decoder
declares
decoding
error
pair
exists
decoder
takes
ﬁrst
pair
lexicographically
ordered
pairs
denote
vm1
adopt
simpliﬁed
maximum
likelihood
decoding
rule
partition
set
indices
positions
agree
disagree
vm1
vm1
cid:54
set
larger
two
sets
|vm1
m2|/2
apply
maximum
likely
decoder
let
say
beats
qk1
|−α
qk2
|−α
otherwise
say
beats
exists
message
list
beats
elements
list
condorcet
winner
output
message
else
declares
error
3.2
analysis
analysis
follow
usual
recipe
show
suﬃciently
large
high
proba-
bility
randomly
constructed
code
perr
vanishes
thereby
showing
code
exists
recall
code
construction
choose
pure
codewords
independently
uniformly
random
i.e
i.i.d
bernoulli
1/2
generate
partitions
binning
indices
bins
uniformly
random
codebook
consists
pure
codewords
well
partitions
message
codebook
nice
properties
probability
super-exponentially
close
let
random
variable
representing
codebook
denoted
prove
fix
recall
prove
sequence
lemmas
prove
achieve
rate
lemma
probability
least
exp
weight
n3/4
proof
since
probability
upper
bounded
probability
i.i.d
variables
bernoulli
hamming
weight
greater
n3/4
expected
weight
qkn
log
−1n−1/2
n3/4
encoder
random
noise
hamming
therefore
hoeﬀding
inequality
cid:16
cid:17
cid:88
exp
cid:18
cid:19
n3/4
figure
example
demonstrating
coherence
two
base
codewords
cid:48
let
i.e.
comprises
ﬁrst
locations
codeword
let
noise-levels
codeword
sets
locations
cid:48
cid:48
respectively
expected
size
cid:48
cid:48
therefore
|/k2
veriﬁed
largest
size
cid:48
sets
size
least
cid:54
cid:48
hence
cid:48
2-coherent
exactly
locations
cid:48
coherent
8th
12th
16th
18th
locations
highlighted
ﬁgure
remaining
decoherent
locations
potentially
usable
decoder
bob
disambiguate
cid:48
example
cid:48
comprising
two
locations
possible
choice
disambiguation
set
reasonable
size
lexicographically
ﬁrst
set
cid:54
cid:48
lemma
length
given
preﬁx
least
n/2
unerased
bits
proof
since
1−p−
adversary
erase
locations
number
unerased
least
/2
/2
required
deﬁnition
bits
let
number
unerased
bits
deﬁnitions
holds
/2
thus
number
erased
bits
ﬁnally
implies
least
implying
erased
bits
/2
suﬃx
1−/2
unerased
bits
/2
n/2
deﬁne
useful
properties
random
code
decoder
diﬃculty
resolving
diﬀerence
codewords
share
similar
partitions
two
messages
cid:48
set
expected
number
common
locations
cid:48
call
pair
base
codewords
cid:48
η1-coherent
cid:35
cid:88
k=1
cid:34
cid:88
k=1
cid:88
k=1
cid:48
m ,1,2
m,1
m ,2
coherentlocations
factor
greater
expected
number
locations
call
codebook
number
locations
cid:48
noise
levels
-coherent
pair
messages
cid:48
size
least
η2n
pair
base
codewords
cid:48
η1-coherent
cid:48
ordered
subset
denote
ith
entry
cid:48
deﬁne
restriction
let
cid:48
cid:48
cid:48
length-|t
base
codeword
binary
vector
whose
i-th
entry
equals
cid:48
ith
entry
deﬁne
restriction
codebook
cid:48
denoted
cid:48
analogously
deﬁned
codebook
possible
repetitions
generated
restricting
base
codeword
size
least
unerased
bits
restricted
codebook
cid:48
decodable
weight
errors
list
size
precisely
unrestricted
codebook
set
size
hamming
ball
radius
contains
less
codewords
restricted
lemma
suﬃciently
small
exists
suﬃciently
large
probability
least
cid:48
call
codebook
-list-decodable
set
cid:48
cid:48
cid:48
design
codebook
following
two
properties
hold
log
log
codebook
k/2
/2
-coherent
codebook
/2
n3/4
log
log
/2
-list-decodable
proof
ﬁrst
prove
high
probability
k/2
/2
-coherent
since
k/2
must
show
pair
base
codewords
cid:48
cid:48
cid:88
k=1
recall
sets
selected
partitions
positions
cid:48
select
written
follows
fix
set
size
least
n/2
probability
code
construction
generates
k=1
partition
calculate
probability
randomly
cid:88
cid:18
cid:19
cid:18
cid:19
cid:18
cid:19
n−i
cid:19
cid:18
cid:19
cid:18
cid:18
cid:19
cid:18
cid:19
cid:19
cid:18
log
cid:88
cid:88
n2n
n2n
log
log
1+/2
n+log
log
log
4+/2
n+log
last
inequality
follows
setting
log
size
least
n/2
taking
union
bound
pairs
base
codewords
strictly
less
22n
pairs
since
rate
code
less
possible
sets
strictly
less
sets
shows
probability
code
k/2
/2
-coherent
prove
high
probability
appropriately
list-decodable
broadly
similar
classical
derivations
list-decoding
bounds
due
speciﬁc
combination
error/erasure
decoding
required
proof
asymptotically
vanishing
fraction
errors
constant
fraction
erasures
re-derive
proof
since
base
codeword
codebook
generated
uniformly
random
true
codewords
cid:48
therefore
suﬃciently
large
probability
codeword
restricted
cid:48
falls
ﬁxed
hamming
ball
radius
n3/4
1−p−/2
cid:48
cid:1
cid:0
1−p−/2
1−p−/2
+n3/4
log
n1/4
n1/4
1−p−2/3
n3/4
1−p−/2
equality
follows
stirling
approximation
let
2−n
1−p−2/3
probability
design
codebook
hamming
ball
contains
least
log
log
codewords
restricted
2nr
cid:88
cid:18
2nr
cid:19
2nr−i
log
log
/2
cid:48
most1
cid:18
2nr
cid:19
2nr
cid:88
2nr
cid:88
2nr
cid:88
log
log
/2
log
log
/2
log
log
/2
log
log
2nr2
2nriνi
1−p−
cid:16
1−p−2/3
cid:17
log
log
taking
union
bound
1−p−/2
hamming
balls
cid:0
cid:48
1−p−/2
cid:48
exists
log
log
+3n
hence
probability
least
one
two
properties
approximate
decoherence
log
log
4+/2
n+log
log
log
suﬃciently
small
suﬃciently
large
implies
probability
design
exists
set
hamming
ball
least
log
log
/2
codewords
restricted
list-decodability
required
hold
codebook
cid:1
sets
cid:48
log
log
+3n
lemma
probability
least
design
codebook
adversarial
erasure
pattern
1note
expected
number
codewords
hamming
ball
q2nr
equals
2−n/3
log
log
every
message
size
list
equation
log
log
/2
wth
n3/4
every
pair
messages
exists
pair
cid:54
satisfying
equation
proof
lemma
preﬁx-length
/2
hence
using
part
lemma
gives
required
bound
list-decodability.2
set
indices
corresponding
unerased
bits
size
least
log
log
hence
n/2
lemma
part
lemma
shows
probability
least
k=1
size
set
cid:83
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
set
cid:91
cid:48
cid:54
cid:48
cid:48
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
values
pair
cid:48
cid:54
cid:48
hence
least
one
pair
size
cid:48
least
k2−k
required
lemma
code
satisfying
two
conditions
lemma
exists
constant
suﬃciently
large
probability
least
exp
cid:0
decoder
outputs
transmitted
message
−βn1/2/2
log2
cid:1
encoder
noise
proof
suppose
transmitted
consider
test
decoding
rule
cid:48
cid:54
let
qk1
cid:48
qk2
let
denote
fraction
i.e
type
decoder
maximum
likelihood
detector
observations
hypotheses
bernoulli
bernoulli
cid:48
message
beats
cid:48
11.194
cid:107
cid:107
log
cid:48
log
cid:48
cid:48
cid:48
bernoulli
bernoulli
cid:48
beats
cid:48
cid:107
cid:107
log
cid:48
log
cid:48
cases
solve
threshold
left
side
equals
sanov
theorem
theorem
11.4.1
probability
error
cid:48
beats
exp
−|v
cid:107
2in
fact
lemma
3.2
provides
stronger
guarantees
required
proof
one
shows
list-decodability
appropriate
size
whereas
decoder
ever
decodes
using
furthermore
part
lemma
guarantees
hamming
ball
appropriate
radius
correspond
many
messages
rather
hamming
balls
centred
sub-vectors
neither
relaxations
asymptotically
worsens
parameters
obtainable
proof
advantage
signiﬁcantly
simplifying
presentation
thus
must
lower
bound
divergence
cases
since
cid:48
cid:28
1/2
clear
case
smaller
upper
bound
focus
case
error
largest
hypotheses
closest
|k1
k2|
ﬁrst
prove
useful
lower
bound
divergences
using
taylor
expansion
cid:107
cid:88
j=1
λjrj
cid:88
cid:88
j=1
j=2
λr2
cid:88
j=2
cid:18
cid:19
λjrj
coeﬃcient
strictly
positive
cid:54
thus
suﬃciently
small
cid:54
exists
cid:107
apply
divergence
threshold
either
cid:48
q/2
cid:48
means
q/2
cid:107
therefore
either
|ζ∗
cid:107
3r/2
cid:107
cid:107
3r/2
cid:107
either
case
previous
argument
shows
exists
r/2
means
r/2
|2r
cid:107
cid:107
therefore
cid:107
−1/2
let
event
conditions
lemmas
hold
taking
union
bound
messages
cid:48
list
use
fact
cid:48
cid:54
beats
m|e
|l||v
exp
−|v
exp
log
log
log2
cid:16
exp
−βn1/2/2
log2
cid:16
cid:107
cid:17
−βn1/2/
log2
cid:17
lemmas
together
imply
theorem
argued
proof
theorem
converse
follows
considering
adversary
erases
ﬁrst
bits
fix
set
fix
erasure
pattern
lemma
probability
least
list
contains
log
log
/2
codewords
wth
n3/4
cid:48
list
exists
set
cid:48
cid:48
size
least
k2−k
n/
log2
union
bound
erasure
patterns
messages
show
high
probability
code
construction
satisﬁes
conditions
complete
proof
note
lemma
weight
list
size
log
log
/2
probability
exp
−n1/2/2
thus
lemma
decoding
succeeds
probability
exp
cn1/2
log
log
log3
therefore
probability
error
goes
showing
achievable
completes
proof
theorem
deterministic
codes
proof
theorem
section
show
stochastic
nature
code
design
essential
speciﬁcally
show
series
2nr
deterministic
codes
i.e.
2nr
depends
2nr
allow
communication
channel
model
average
probability
error
tending
zero
must
satisfy
example
illustrating
proof
appears
end
section
see
figure
show
presenting
adversarial
strategy
constant
suﬃciently
large
values
deterministic
code
average
error
depend
depend
adversarial
strategy
follows
wait
push
strategy
used
causal
binary
bit-ﬂip
channel
erasure
case
adversary
waits
certain
amount
time
without
performing
action
based
information
adversary
seen
far
pushes
i.e.
corrupts
transmitted
codeword
malicious
manner
causing
decoding
error
probability
given
message
time
parameter
cid:96
let
cid:96
set
messages
corresponding
codewords
agree
ﬁrst
cid:96
entries
set
cid:96
plays
important
role
analysis
referred
cid:96
-consistency
set
notice
calvin
construct
cid:96
cid:96
bits
transmitted
due
delay
knowledge
cid:96
bit
however
delay
calvin
1-bit
time
step
cid:96
calvin
construct
two
potential
consistency
sets
set
cid:96
corresponding
case
cid:96
bit
transmitted
one
set
cid:96
corresponding
case
bit
holds
cid:96
cid:96
start
deﬁning
analyzing
wait
phase
calvin
turn
discussing
cid:96
push
phase
4.1
wait
phase
wait
phase
calvin
proceeds
follows
wait-1
calvin
starts
waiting
bits
transmitted
codeword
sent
value
cid:96
transmission
cid:96
bit
transmitted
codeword
calvin
constructs
sets
cid:96
cid:96
clearly
cid:96
cid:96
addition
cid:96
cid:96
exactly
cid:96
cid:96
min
|φ0
show
shortly
high
probability
messages
holds
cid:96
1−2p+δ
size
cid:96
cid:96
least
based
value
cid:96
cid:96
calvin
decides
either
continue
waiting
move
push
phase
speciﬁcally
cid:96
let
cid:96
max
|φ0
cid:96
|φ1
cid:96
|φ1
cid:96
wait-2
cid:96
cid:96
greater
cid:48
calvin
nothing
waits
next
bit
transmitted
cid:48
δ/4
attack
cid:96
cid:96
less
cid:48
least
large
calvin
sets
transition
time
cid:96
equal
current
value
cid:96
stops
wait
phase
moves
push
phase
discussed
section
4.2
detail
set
transition
time
cid:96
equal
current
value
error
cid:96
cid:96
less
cid:96
declare
error
type
deﬁnitions
calvin
either
declares
error
move
push
phase
point
time
cid:96
latter
say
transition
push
phase
successful
message
namely
size
cid:96
cid:96
cid:48
least
suﬃciently
large
constant
determined
shortly
otherwise
say
transition
failed
error
type
show
constant
probability
messages
transition
push
phase
successful
error
type
lemma
let
suﬃciently
large
constant
determined
shortly
let
suﬃciently
large
let
cid:96
ﬁrst
point
time
cid:96
cid:96
cid:48
probability
least
2−8c/δ2
messages
holds
cid:96
cid:96
size
least
address
probability
given
transition
calvin
push
phase
proof
ﬁrst
note
shown
using
pigeonhole
principle
probability
messages
cid:96
size
cid:96
thus
cid:96
cid:96
least
2δn/2
least
2−δn/2
let
event
alice
chooses
message
corresponding
consistency
set
cid:96
size
least
2δn/2
failed
happen
messages
cid:96
∗−2
cid:96
∗−1
cid:96
∗−1
cid:48
cid:96
∗−1
cid:96
cid:96
size
less
words
failure
happens
messages
point
time
consecutive
consistency
sets
sizes
jump
cid:48
consider
codeword
chosen
uniformly
random
codebook
alice
corresponds
choosing
uniformly
distributed
message
one
may
expose
codeword
bit
bit
according
conditional
probability
given
choices
made
thus
far
process
time
parameter
cid:96
cid:96
cid:96
least
cid:48
value
cid:96
cid:96
less
failure
calvin
probability
cid:96
cid:96
cid:96
cid:96
notice
cid:96
cid:96
equal
either
cid:96
cid:96
exposure
process
moreover
suﬃciently
turn
implies
cid:96
cid:96
large
cid:96
cid:96
cid:96
otherwise
cid:96
cid:96
cid:96
contradiction
cid:96
cid:96
cid:48
implies
cases
conditional
probability
error
time
cid:96
cid:96
cid:96
cid:96
equivalently
cases
conditional
probability
exposure
process
induce
failed
transition
cid:96
cid:96
cid:96
conclude
probability
codewords
i.e.
messages
transition
successful
calvin
cid:18
cid:89
cid:96
cid:96
cid:96
cid:96
cid:18
cid:89
k=1
δxk
cid:19
cid:18
cid:19
cid:17
product
cid:96
speciﬁed
cid:96
cid:96
cid:48
cid:96
values
cid:96
cid:96
strictly
decreasing
sequence
integers
greater
cid:48
holds
setting
lower
bound
minimum
consecutive
integers
increasing
order
starting
cid:48
i.e
cid:48
conclude
suﬃciently
large
values
bounded
cid:17
cid:16
cid:16
cid:19
cid:89
k=1
cid:19
cid:16
cid:17
cid:48
δ2n
cid:18
cid:89
k=1
cid:17
cid:16
cid:48
cid:17
cid:16
least
success
2−δn/2
using
union
bound
event
suﬃciently
large
probability
calvin
transition
push
phase
result
4.2
push
phase
calvin
corrupting
algorithm
proceeds
follows
calvin
chooses
plausible
transmission
cid:48
uniformly
random
value
cid:96
cid:96
either
|φ0
cid:96
calvin
uncertainty
cid:96
since
possible
cid:96
equal
either
|φi
cid:96
calvin
certain
cid:96
since
surviving
codewords
cid:96
calvin
following
calvin
uncertain
cid:96
|φ0
calvin
certain
cid:96
|φi
cid:96
|φ1
cid:96
cid:96
calvin
erases
cid:96
cid:96
|φ1
cid:96
cid:96
erasing
disambiguating
information
cid:48
cid:96
hence
cid:96
cid:48
action
cid:48
cid:96
cid:96
hence
cid:96
cid:54
cid:48
cid:96
calvin
erases
cid:96
calvin
erase
cid:96
calvin
successfully
continue
process
end
without
violating
total
erasure
budget
clearly
codewords
cid:48
consistent
vector
received
bob
indices
diﬀer
erased
calvin
argue
calvin
tighter
analysis
indicates
better
lower
bound
cid:0
indeed
complete
process
total
number
erasures
required
due
intricacy
analysis
omit
cid:1
erasures
introduced
calvin
steps
let
consider
full
binary
tree
depth
edges
labeled
let
consider
code
subtree
path
root
leaf
represents
codeword
composed
bits
labeling
branches
along
path
calvin
perspective
beginning
push
phase
encoder
state
either
two
nodes
representing
subsequences
cid:96
∗−10
cid:96
∗−11
paths
via
two
nodes
represent
two
sets
codewords
cid:96
respectively
since
total
number
paths
cid:48
δn/4
δn/4
branchings
subtree
rooted
node
corresponding
cid:96
∗−1
i.e.
subtree
spanning
codewords
cid:96
implies
along
path
subtree
calvin
encounter
δn/4
branching
nodes
words
calvin
encounter
step
δn/4
times
upper
bounds
total
number
erasures
due
step
δn/4
cid:96
cid:96
counting
number
required
erasures
step
done
following
similar
analysis
using
plotkin
bound
turan
theorem
brieﬂy
reprise
analysis
2p−δ
formed
completions
cid:96
∗−1
let
consider
consider
codebook
length
cid:48
graph
codewords
nodes
two
codewords
connected
hamming
distance
δn/4
plotkin
bound
independent
set
graph
4p/δ
nodes
implies
turan
theorem
average
degree
number
nodes
satisfy
implying
thus
probability
two
randomly
chosen
codewords
distance
|e|
∆|v|
2|v
constant
probability
calvin
remaining
erasure
budget
pn−
δn/4
suﬃcient
erase
step
positions
cid:48
diﬀer
success
probability
calvin
bounded
success
wait
phase
cid:16
16c
times
push
phase
constant
independent
cid:16
16p
cid:17
cid:17
16p
remark
note
proof
theorem
hold
stochastic
codes
wait
phase
may
analogous
analysis
ﬁts
stochastic
setting
push
phase
breaks
speciﬁcally
crucial
part
push
phase
step
erases
location
uncertainty
behalf
calvin
regarding
current
symbol
deterministic
codes
step
may
occur
locations
whereas
stochastic
setting
number
branchings
subtree
rooted
node
corresponding
calvin
view
far
may
large
thus
step
may
costly
indeed
code
design
achievability
proof
presented
section
every
location
includes
branching
point
figure
illustration
proof
theorem
4.3
illustration
proof
theorem
figure
demonstrate
toy
example
showing
adversarial
attack
deterministic
code
block-length
comprising
codewords
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
cid:48
hence
rate
code
equals
log2
.258.
toy-example
4/9
erasures
possible
length-9
transmission
though
example
bits
actually
erased
hence
claim
theorem
suﬃciently
large
rate
asymptotically
larger
1/9
achievable
implying
many
messages
reliably
transmitted
via
deterministic
code
particular
example
aims
show
speciﬁc
code
shown
messages
corresponding
codewords
chosen
reliably
transmitted
keep
example
dimensions
manageable
parameters
example
match
proofs
particular
suitable
value
rate-excess
parameter
exists
matches
required
proofs
small
value
chosen
zig-zag
lines
bottom
figure
show
code-tree
binary
tree
representing
length-9
codewords
paths
incomplete
depth-9
binary
tree
segments
angled
upwards
indicate
location
segments
angled
downwards
indicate
location
hence
ﬁve
codewords
respectively
100101011
cid:48
100111010
cid:48
cid:48
1001010001
cid:48
cid:48
cid:48
100001010
cid:48
cid:48
cid:48
cid:48
111101010.
codeword
actually
transmitted
shown
black-shaded
path
code-tree
note
codewords
code
ﬁrst
bit
hence
calvin
wait
cid:96
knows
transmitted
codeword
cid:48
cid:48
cid:48
cid:48
general
calvin
initial
two
phases
wait
phases
erase
bits
speciﬁcally
calvin
always
wait-1
phase
exactly
ﬁrst
bits
shown
ﬁgure
ﬁrst
bits
wait-1wait-2xxx x  x   x    erasures
erasing
branch-points
erasing
disambiguity
 =1 =2 =3 =4 =5 =6 =7 =8 =9 14
 04
1 2p+ 
n+1  =5attack
continues
waiting
wait-2
phase
time
cid:96
number
codewords
consistent
observations
time
cid:96
somewhere
range
c/δ
constant
speciﬁed
theorem
example
wait-2
phase
length
since
time
cid:96
calvin
observes
4th
transmitted
bit
realizes
transmitted
codeword
equal
cid:48
cid:48
cid:48
hence
consistency
set
time
denoted
shrinks
become
cid:48
cid:48
cid:48
size
point
calvin
segues
attack
phase
speciﬁcally
ﬁrst
chooses
random
codeword
consistency
set
example
cid:48
denoted
shared
black-red
path
code-tree
tries
confuse
bob
cid:48
speciﬁcally
whenever
calvin
sees
branch-point
i.e.
location
cid:96
alice
may
transmitted
either
i.e.
codewords
corresponding
cid:96
erases
corresponding
bit
step
push
phase
theorem
happens
cid:96
∗−1
locations
thereby
denies
bob
knowledge
value
bits
example
branch-points
cid:96
cid:96
also
branch-point
cid:96
cid:54
cid:48
cid:96
bit
would
enable
bob
disambiguate
cid:48
calvin
erases
bits
well
happens
cid:96
end
cid:48
equally
likely
bob
perspective
care
required
ensure
calvin
run
erasures
attack
phase
analyzed
theorem
detail
cid:96
omniscient
adversary
stochastic
vs.
deterministic
encoding
argue
bit-ﬂipping
adversary
ﬂip
upto
fraction
bits
codeword
capacity
stochastic
encoding
deterministic
encoding
suppose
rate
achievable
stochastic
encoding
average
error
probability
let
suppose
sequence
stochastic
codes
achieving
average
probability
error
length
let
consider
ﬁxed
let
denote
set
vectors
decoder
outputs
message
let
cid:54
e|m
message
good
codeword
adversary
power
move
outside
words
ball
radius
around
completely
contained
let
clearly
αnp
e|m
e|m
cid:54
let
set
good
codewords
messages
argue
sequence
deterministic
codes
decoder
decision
regions
zero
error
asymptotic
rate
code
zero
error
probability
follows
codeword
adversary
power
move
outside
αnh
m|m
m|m
cid:54
⇒αnh
m|m
m|m
⇒αnh
m|m
nh
m|m
⇒αnh
m|m
nnr
m|m
cid:18
cid:19
cid:18
log2
|mn|
cid:19
since
rate
converges
references
raef
bassily
adam
smith
causal
erasure
channels
proceedings
acm-siam
symposium
discrete
algorithms
soda
pages
1844–1857
2014
zitan
chen
sidharth
jaggi
michael
langberg
characterization
capacity
online
causal
binary
channels
foundations
computer
science
2015
claude
shannon
warren
weaver
mathematical
theory
communication
urbana
university
illinois
press
1949
david
blackwell
leo
breiman
thomasian
capacities
certain
channel
classes
random
coding
annals
mathematical
statistics
pages
558–567
1960
amos
lapidoth
prakash
narayan
others
reliable
communication
channel
uncer-
tainty
ieee
transactions
information
theory
:2148–2177
1998
imre
csisz´ar
prakash
narayan
capacity
arbitrarily
varying
channel
revisited
positivity
constraints
ieee
transactions
information
theory
:181–193
1988
thomas
cover
joy
thomas
elements
information
theory
john
wiley
sons
2012
ahlswede
wolfowitz
correlated
decoding
channels
arbitrarily
varying
channel
probability
functions
information
control
14:457–473
1969
anand
sarwate
michael
gastpar
rateless
codes
avc
models
ieee
transactions
information
theory
:3105–3114
2010
gilbert
comparison
signalling
alphabets
bell
systems
technical
journal
31:504–
522
1952
varshamov
estimate
number
signals
error
correcting
codes
dokl
acad
nauk
117:739–741
1957
robert
mceliece
eugene
rodemich
howard
rumsey
lloyd
welch
new
upper
bounds
rate
code
via
delsarte-macwilliams
inequalities
ieee
transactions
information
theory
:157–166
1977
csisz´ar
narayan
arbitrarily
varying
channels
constrained
inputs
states
ieee
transactions
information
theory
:27–34
1988
ahlswede
elimination
correlation
random
codes
arbitrarily
varying
channels
wahrsch
verw
gebiete
33:159–175
march
1978
michael
langberg
private
codes
succinct
random
codes
almost
perfect
focs
volume
pages
325–334
2004
adam
smith
scrambling
adversarial
errors
using
random
bits
optimal
information
rec-
proceedings
eighteenth
annual
acm-siam
onciliation
better
private
codes
symposium
discrete
algorithms
pages
395–404
society
industrial
applied
mathe-
matics
2007
aaron
wyner
wire-tap
channel
bell
system
technical
journal
:1355–1387
1975
dey
jaggi
langberg
codes
online
adversaries
part
large
alpha-
bets
ieee
transactions
information
theory
:3304–3316
2013
bikash
kumar
dey
sidharth
jaggi
michael
langberg
anand
sarwate
improved
information
upper
bounds
capacity
binary
channels
causal
adversaries
theory
proceedings
isit
2012
ieee
international
symposium
pages
681–685
ieee
2012
bikash
kumar
dey
sidharth
jaggi
michael
langberg
anand
sarwate
upper
bounds
capacity
binary
channels
causal
adversaries
ieee
transactions
information
theory
:3753–3763
2013
michael
langberg
sidharth
jaggi
bikash
kumar
dey
binary
causal-adversary
channels
information
theory
2009.
isit
2009.
ieee
international
symposium
pages
2723–2727
ieee
2009
