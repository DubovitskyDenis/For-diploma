converse
bounds
noisy
group
testing
arbitrary
measurement
matrices
jonathan
scarlett
volkan
cevher
laboratory
information
inference
systems
lions
école
polytechnique
fédérale
lausanne
epfl
email
jonathan.scarlett
volkan.cevher
epﬂ.ch
abstract—we
consider
group
testing
problem
one
seeks
identify
subset
defective
items
within
larger
set
items
based
number
noisy
tests
matching
achievability
converse
bounds
known
several
cases
interest
i.i.d
measurement
matrices
less
known
regarding
converse
bounds
arbitrary
measurement
matrices
address
presenting
two
converse
bounds
arbitrary
matrices
general
noise
models
first
provide
strong
converse
bound
error
matches
existing
achievability
bounds
several
cases
interest
second
provide
weak
converse
bound
error
matches
existing
achievability
bounds
greater
generality
introduction
group
testing
problem
consists
determining
small
subset
defective
items
within
larger
set
items
problem
history
areas
medical
testing
fault
detection
regained
signiﬁ-
cant
attention
following
new
applications
areas
communication
protocols
pattern
matching
database
systems
new
connections
compressive
sensing
let
items
labeled
let
subset
defective
items
consider
general
group
testing
model
observation
associated
single
test
randomly
generated
according
|xs
y|xs
|vs
y|vs
xi∈s
counts
number
defective
items
test
measurement
vector
indicates
items
included
test
techniques
allow
arbitrary
ﬁnite
output
alphabets
focus
binary
case
concreteness
noiseless
setting
simply
additive
modulo-
noise
models
form
also
common
general
permitting
forms
dependence
dilution
noise
goal
recover
based
number
inde-
pendent
non-adaptive
tests
i-th
measurement
vector
i-th
observation
henceforth
let
denote
matrix
whose
i-th
row
let
n-dimensional
binary
vector
whose
i-th
entry
consider
ﬁxed
number
defective
items
assume
support
set
uniform
subsets
cardinality
ﬁxed
measurement
matrix
error
probability
given
estimate
based
probability
respect
randomness
information-theoretic
limits
problem
studied
decades
e.g.
see
recently
become
increasingly
well-understood
particular
exact
asymptotic
threshold
known
several
cases
interest
consider
error
probability
averaged
i.i.d
bernoulli
matrix
xij
ν/k
speciﬁcally
broad
range
scaling
regimes
max
ℓ=1
...
max
ℓ=1
...
log
xsdif
|xseq
log
xsdif
|xseq
equations
sdif
seq
denotes
arbitrary
partition
ﬁxed
defective
set
|sdif|
see
intuition
mutual
information
respect
independent
random
vectors
xsdif
xseq
sizes
k−ℓ
containing
independent
bernoulli
ν/k
entries
model
sdif
seq
converse
bound
additional
optimization
i.e.
main
goal
paper
obtain
variants
min
max
ℓ=1
...
log
xsdif
|xseq
case
arbitrary
measurement
matrices
rather
i.i.d
measurement
matrices.1
brieﬂy
mention
exist-
ing
works
direction
noiseless
setting
threshold
simpliﬁes
cid:0
log2
cid:1
converse
holds
arbitrary
matrices
so-called
counting
bound
although
measurement
matrix
may
arbitrary
ﬁnal
results
still
written
terms
random
vectors
xsdif
xseq
independent
bernoulli
entries
directly
related
log2
symmetric
noise
model
bernoulli
log
2−h2
threshold
simpliﬁes
log
log
binary
entropy
function
nats
moreover
converse
remains
valid
arbitrary
matrices
proved
combining
analysis
simple
symmetry
argument
information-density
random
variables
alternatively
obtained
non-asymptotic
bound
given
general
noise
models
weak
converse
statement
corresponding
i.e.
seq
known
arbitrary
matrices
i.e.
showing
opposed
strong
converse
initial
preparation
work
learned
result
similar
second
one
theorem
presented
russian
literature
630-631
giving
weak
converse
case
seq
however
proof
techniques
appear
signiﬁcantly
different
focus
therein
case
scale
contrast
work
contributions
paper
prove
strong
converse
corresponding
arbitrary
matrices
prove
weak
converse
note
former
interest
since
often
achieves
maximum
true
noiseless
model
symmetric
noise
model
numerical
investigations
suggest
also
case
|vs
corresponds
passing
z-channel
however
known
cases
smaller
values
achieve
maximum
notation
write
denote
submatrix
containing
columns
indexed
complement
respect
set
denoted
similarly
given
joint
distribution
pxy
corresponding
marginal
distributions
denoted
similarly
conditional
marginals
e.g.
use
usual
notations
entropy
mutual
information
e.g
make
use
standard
asymptotic
notations
deﬁne
function
max
write
ﬂoor
function
⌊·⌋
function
log
base
total
variation
distance
two
probability
mass
functions
written
dtv
strong
converse
seq
ﬁrst
main
result
follows
theorem
consider
observation
model
|vs
deﬁne
maxν∈
i.i.d
bernoulli
ν/k
entries
sequence
measure-
ment
matrices
indexed
cid:18
cid:19
provided
log
cid:0
cid:1
arbitrarily
small
remark
typically
case
seq
cid:1
behaves
hence
remainder
term
cid:0
cid:0
cid:1
case
lower
bound
error
probability
yields
strong
converse
statement
proof
theorem
let
n×p
ﬁxed
measurement
matrix
analysis
shows
cid:20
xi=1
cid:0
cid:1
log
log
cid:18
cid:19
|xs
log
cid:12
cid:12
cid:12
cid:21
arbitrary
auxiliary
output
distribution
specif-
ically
proved
case
i.i.d
induced
output
distribution
proof
reveals
general
form
i=1
log
|xs
letting
denote
mean
variance
given
defective
set
obtain
chebyshev
inequality
cid:1
log
arbitrary
n∆i
mean
directly
computed
provided
log
cid:0
n∆i
|xs
y|x
log
|xs
y|x
|vs
y|v
|vs
y|vs
|vs
y|v
log
|vs
y|vs
log
xi=1
xi=1
nxvs
pj∈s
empirical
distribu-
tion
across
tests
given
choice
choosing
unique
capacity-achieving
output
distribution
channel
|vs
follows
well-known
saddlepoint
result
mutual
information
thm
4.4
sets
cardinality
deﬁned
theorem
statement
claim
corresponding
variance
behaves
shown
case
equals
induced
output
distribution
app
analysis
reveals
holds
true
min
bounded
away
zero
proof
theorem
concluded
combining
above-mentioned
application
chebyshev
inequality
choosing
arbitrarily
small
true
1+∆
since
sdif
uniform
set
subsets
\seq
size
necessary
iii
weak
converse
seq
second
main
result
follows
theorem
observation
model
|vs
sequence
measurement
matrices
indexed
provided
max
ℓ=1
...
min
cid:0
p−k+ℓ
cid:1
xsdif
|xseq
maxn1
log
universal
constant
mutual
mation
respect
i.i.d
bernoulli
ν/k
entries
along
infor-
pair
xsdif
xseq
remark
remainder
term
typically
always
dominated
mutual
information
example
mutual
information
true
regardless
value
remark
min-max
ordering
opposite
thus
making
potentially
weaker
threshold
however
proof
also
show
threshold
improved
min
px|u
max
log
cid:0
p−k+ℓ
cid:1
xsdif
|xseq
thus
recovering
correct
min-max
ordering
additional
random
variable
ﬁnite
alphabet
threshold
shown
achievable
hence
establishing
tight
bound
broad
range
scaling
regimes
using
i-non-i.d
coding
fix
sequence
···
empirical
distribution
generate
i-th
row
according
i.i.d
bernoulli
distribution
px|u
·|ui
whose
parameter
may
depend
achievability
analysis
follows
chosen
state
theorem
terms
weakened
threshold
since
bears
stronger
resemblance
familiar
threshold
since
aware
cases
gap
two
proof
theorem
proof
given
four
steps
step
fano
inequality
starting
point
analysis
necessary
condition
based
fano
inequality
genie
argument
follows
directly
analysis
see
also
sec
iii-d
speciﬁcally
ﬁxing
letting
revealed
indices
denoted
seq
uniform
set
subsets
size
letting
non-revealed
indices
denoted
log
cid:0
p−k+ℓ
cid:1
sdif
y|seq
upper
bound
mutual
information
writing
sdif
y|seq
xi=1
xi=1
sdif
|seq
dif
dif
dif
standard
property
independent
observa-
tion
models
7.96
follows
deﬁning
count
number
defective
items
test
non-revealed
revealed
indices
recalling
depends
defective
set
sdif
seq
pi∈s
step
approximate
distributions
binomials
pro-
ceed
showing
pairs
distri-
bution
close
enough
product
binomial
distributions
probability
parameter
since
defective
set
uniformly
random
joint
distribution
dif
pair
information
dif
depends
number
non-zeros
i-th
row
denote
proceeding
recall
hypergeometric
distribution
counts
number
special
items
obtained
sampling
items
population
items
without
replacement
labeled
special
random
variable
distribution
probability
mass
function
course
sampling
replacement
hence
mutual
simply
gives
binomial
m/p
distribution
p−m
k−i
following
recalling
seq
uniform
cid:0
k−ℓ
cid:1
sets
cardinality
number
ones
revealed
indices
distributed
hypergeometric
approximate
binomial
random
variable
binomial
cid:16
cid:17
speciﬁcally
denoting
corresponding
distributions
pveq
respectively
omitting
superscripts
total
variation
distance
two
satisﬁes
dtv
pveq
cid:19
cid:18
denote
upper
bound
suppose
condition
value
veq
recalling
sdif|seq
seq
uniform
cid:0
p−k+ℓ
cid:1
possible
realizations
dif
hypergeometric
veq
veq
max
log
cid:0
k−ℓ
step
form
single-letter
expression
deﬁning
random
variable
equiprobable
write
average
denominator
k−ℓ
cid:1
dif
approximate
conditional
distribution
veq
cid:17
veq
binomial
cid:16
approximate
unconditional
distribution
dif
binomial
cid:16
cid:17
speciﬁcally
corresponding
distributions
satisfy
dtv
pvdif
·|veq
dif
·|veq
proved
appendix
cid:19
cid:18
dtv
dif
·|veq
dif
cid:18
cid:19
uniformly
veq
denoting
bounds
δ2,1
δ2,2
obtain
triangle
inequality
dtv
pvdif
·|veq
min
δ2,1
δ2,2
upper
bound
one
trivial
dif
step
infer
bounds
mutual
informations
next
formalize
statement
two
joint
distri-
butions
close
distance
conditional
mutual
informations
also
close
using
deﬁnitions
vdif
veq
following
dif
prove
appendix
also
prove
appendix
cid:12
cid:12
vdif
|veq
vdif
dif
cid:12
cid:12
vdif
cid:12
cid:12
log
cid:12
cid:12
log
fact
show
logarithmic
term
usually
improved
constant
sometimes
even
see
remark
focus
slightly
looser
bound
sake
simplicity
combining
gives
cid:12
cid:12
vdif
|veq
cid:18
dif
cid:12
cid:12
maxn1
log
cid:19
substituting
maximizing
obtain
necessary
condition
max
log
cid:0
p−k+ℓ
cid:1
dif
i=1
xi=1
dif
dif
dif
conditional
distributions
given
independent
binomial
random
variables
trials
common
parameter
thus
overall
bound
becomes
max
log
cid:0
p−k+ℓ
dif
cid:1
upper
bounding
right-hand
side
maximizing
px|u
yields
since
output
depends
measurement
vector
pi∈s
safely
replace
binomial
random
variables
corresponding
i.i.d
bernoulli
vectors
xseq
xsdif
mutual
information
weakening
swapping
min-max
ordering
yields
thus
concluding
proof
theorem
dif
conclusion
provided
two
converse
bounds
noisy
group
testing
arbitrary
measurement
matrices
ﬁrst
result
strengthens
existing
result
obtain
strong
converse
statement
second
result
provides
weak
converse
potentially
improved
threshold
several
cases
converse
bounds
known
achievable
using
i.i.d
matrices
scales
sufﬁciently
slowly
compared
thus
results
support
use
matrices
regimes
contrast
known
i.i.d
matrices
suboptimal
settings
linear
scaling
cases
may
room
improve
converse
bounds
presented
paper
another
direction
future
work
determine
extent
bounds
remain
valid
case
adaptive
group
testing
test
designed
based
past
obser-
vations
work
direction
given
conclusive
results
therein
limited
symmetric
noise
proof
appendix
recall
considering
distance
dif
cid:1
deﬁne
difference
two
dif|veq
veq
binomial
cid:0
−veq
binomial
cid:0
p−k+ℓ
cid:1
binomial
parameters
veq
simple
asymptotic
expansion
fact
veq
satisﬁes
cid:18
cid:19
uniformly
veq
moreover
bound
comparing
binomial
distributions
states
dtv
dif
·|veq
dif
c√η
+p2η
e2η
1/4e1/242−1/2
∆2ℓ
∆2ℓ2
upper
bound
behaves
cid:0
k−ℓ
cid:1
whenever
thus
establishing
trivial
anyway
since
gives
k−ℓ
upper
bound
always
holds
proof
obtain
writing
cid:12
cid:12
vdif
|veq
vdif
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
veq
cid:1
vdif
|veq
cid:12
cid:12
cid:12
cid:12
xveq
cid:0
pveq
veq
xveq
cid:12
cid:12
pveq
veq
veq
cid:12
cid:12
log
dtv
pveq
log
holds
since
mutual
bounded
log
binary
outputs
information
upper
proof
since
conditional
mutual
information
average
unconditional
mutual
informations
uniform
veq
sufﬁces
show
common
alphabet
inequality
dtv
implies
|ip
log
subscripts
denote
distribution
used
whereas
conditional
distribution
y|x
given
cases
use
similar
notations
entropies
since
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
dtv
log
second
term
follow
deduce
moreover
reasoning
along
identities
y|x
y|x
gives
dtv
dtv
denotes
-marginal
y|x
similarly
may
thus
apply
result
continuity
entropy
obtain
dtv
cid:12
cid:12
cid:12
cid:12
dtv
log
estimates
combining
log
remark
logarithmic
factor
replaced
constant
whenever
yield
probabilities
whenever
dtv
desired
|ip
yields
strictly
bounded
away
one
entropy
bounded
derivatives
except
fact
vicinity
0.5
0.5
relevant
symmetric
settings
may
even
make
bound
behave
dtv
since
derivative
binary
entropy
function
0.5
zero
acknowledgment
work
supported
european
commission
erc
future
proof
snf
200021-146750
crsii2-
147633
epfl
fellows
program
horizon2020
665667
references
fernández
anta
mosteiro
ramón
muñoz
unbounded
contention
resolution
multiple-access
channels
distributed
com-
puting
springer
berlin
heidelberg
2011
vol
6950
225–236
clifford
efremenko
porat
rothschild
pattern
matching
cares
errors
comp
sys
sci.
vol
115–124
2010
cormode
muthukrishnan
hot
tracking
frequent
items
dynamically
acm
trans
database
sys.
vol
249–278
march
2005
gilbert
iwen
strauss
group
testing
sparse
signal
recovery
asilomar
conf
sig.
sys
comp.
oct.
2008
1059–
1063
gilbert
strauss
tropp
vershynin
one
sketch
fast
algorithms
compressed
sensing
proc
acm-siam
symp
disc
alg
soda
new
york
2007
237–246
atia
saligrama
boolean
compressed
sensing
noisy
group
testing
ieee
trans
inf
theory
vol
1880–1901
march
2012
malyutov
separating
property
random
matrices
math
notes
acad
sci
ussr
vol
84–91
1978
atia
saligrama
mutual
information
characterization
sparse
signal
processing
int
colloq
aut.
lang
prog
icalp
zürich
2011
tan
atia
strong
impossibility
results
sparse
signal
processing
ieee
sig
proc
letters
vol
260–264
march
2014
laarhoven
asymptotics
ﬁngerprinting
group
testing
tight
bounds
channel
capacities
ieee
trans
inf
forens
sec.
vol
1967–1980
2015
scarlett
cevher
limits
support
recovery
framework
2015
probabilistic
models
information-theoretic
http
//infoscience.epﬂ.ch/record/204670
phase
transitions
group
testing
proc
acm-siam
symp
disc
alg
soda
2016
aldridge
capacity
bernoulli
nonadaptive
group
testing
2015
http
//arxiv.org/abs/1511.05201
baldassini
johnson
aldridge
capacity
adaptive
group
testing
ieee
int
symp
inf
theory
july
2013
2676–
2680
johnson
strong
converses
group
testing
ﬁnite
blocklength
regime
2015
http
//arxiv.org/abs/1509.06188
malyutov
search
sparse
active
inputs
review
inf
theory
comb
search
theory
2013
609–647
cover
thomas
elements
information
theory
john
wiley
sons
inc.
2001
malyutov
mateev
screening
designs
non-symmetric
response
function
mat
zametki
vol
109–127
1980
polyanskiy
lecture
notes
information
theory
2014
http
//people.lids.mit.edu/yp/homepage/data/itlectures_v2.pdf
soon
binomial
approximation
dependent
statistica
sinica
vol
703–714
1996.
indicators
roos
binomial
approximation
poisson
binomial
distribution
krawtchouk
expansion
theory
probability
applications
vol
258–272
2001
csiszár
körner
information
theory
coding
theorems
discrete
memoryless
systems
2nd
cambridge
university
press
2011
