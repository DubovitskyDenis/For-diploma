compressed
sensing
implantable
neural
recordings
using
co-sparse
analysis
model
weighted
ℓ1-optimization
biao
sun
member
ieee
wenfeng
zhao
member
ieee
xinshan
zhu∗
member
ieee
abstract
reliable
energy-efﬁcient
wireless
data
transmission
remains
major
challenge
resource-
constrained
wireless
neural
recording
tasks
data
compression
generally
adopted
relax
burdens
wireless
data
link
recently
compressed
sensing
theory
successfully
demon-
strated
potential
neural
recording
application
main
limitation
however
neural
signals
good
sparse
representation
commonly
used
dictionaries
learning
reliable
dictionary
often
data
dependent
computationally
demanding
paper
novel
approach
implantable
neural
recording
proposed
main
contributions
co-sparse
analysis
model
adopted
enforce
co-sparsity
neural
signals
therefore
overcoming
drawbacks
conventional
synthesis
model
enhancing
reconstruction
performance
multi-fractional-order
difference
matrix
constructed
analysis
dictionary
thus
avoiding
dictionary
learning
procedure
reducing
need
previously
acquired
data
computational
resources
exploiting
statistical
priors
analysis
coefﬁcients
weighted
analysis
ℓ1-minimization
walm
algorithm
proposed
reconstruct
neural
signals
experimental
results
leicester
neural
signal
database
reveal
proposed
approach
outperforms
state-of-the-art
cs-based
methods
challenging
high
compression
ratio
task
proposed
approach
still
achieves
high
reconstruction
performance
spike
classiﬁcation
accuracy
sun
zhu
school
electrical
engineering
automation
tianjin
university
tianjin
300072
china
email
sunbiao
xszhu
tju.edu.cn
zhao
department
electrical
computer
engineering
national
university
singapore
117583
singapore
email
elezhwf
nus.edu.sg
work
supported
national
natural
science
foundation
china
grants
61271321
61473207
61401303
ph.d.
programs
foundation
ministry
education
china
grant
20120032110068
tianjin
key
technology
research
development
program
grant
14zczds
f00025
february
2016
draft
index
terms
compressed
sensing
implantable
neural
recording
co-sparse
analysis
model
fractional
order
differ-
ence
sequence
weighted
ℓ1-minimization
introduction
large-scale
multi-channel
extracellular
neural
recording
simultaneously
various
brain
regions
desired
investigate
neural
activities
different
neuron
ensembles
local
circuits
brain
networks
technological
capabilities
would
advance
understanding
brain
functions
moreover
brain-machine
interfaces
translational
neurotechnologies
would
become
feasible
sophis-
ticated
prosthetic
devices
disease
treatment
conventional
static
recording
scenario
large
amounts
neural
data
generated
order
tens
megabytes
per
second
tethered
cables
wires
commonly
adopted
data
streaming
purposes
however
applications
would
limited
owing
tissue
infection
subcutaneous
chronic
recording
tasks
well
neuroscience
experiments
study
awake
free
behaving
animals
models
wireless
neural
recording
devices
overcome
above-mentioned
limitations
would
greatly
expand
research
application
scenarios
nevertheless
wireless
neural
recording
devices
would
compromise
among
various
system-level
con-
siderations
including
system
complexity
power
budget
volume
miniaturization
component-
level
design
aspects
neural
recording
ampliﬁers
analog-to-digital
converters
adcs
neural
signal
processors
data
transceivers
antenna
designs
arguably
challenging
component
wireless
neural
recording
device
reliable
high-throughput
energy-efﬁcient
wireless
data
link
wireless
link
dominates
system
channel
count
resolution
energy-efﬁciency
although
continuous
progress
made
data
rate
energy
efﬁciency
transceivers
still
prohibitive
adopt
wireless
links
due
practical
limitations
experimental
clinical
procedures
another
straightforward
approach
perform
on-chip
compression
transmission
relax
bandwidth
constraints
spike
detection
based
approaches
lossy
data
compression
via
dwt
etc
approaches
signiﬁcantly
reduce
neural
data
needed
transmitted
yet
compression
hardware
overhead
on-chip
resources
excessive
power
consumption
neglected
recently
ﬁeld
compressed
sensing
shown
potential
achieving
compression
reconstruction
performance
comparable
previous
approaches
simpler
hardware
re-
sources
approach
requires
set
random
measurements
original
signals
avoids
need
dedicated
dsps
leaves
computational
burden
off-chip
processing
main
challenge
however
spike
segments
sparse
common
dictionaries
february
2016
draft
discrete
cosine
transform
dct
basis
discrete
fourier
transform
dft
basis
reconstructing
spikes
using
dictionaries
severely
degrade
performance
therefore
careful
design
sparsifying
dictionary
needed
guarantee
compression
performance
alleviate
issue
various
dictionary-learning
based
algorithms
proposed
neural
data
compression
zhang
proposed
learning
dictionaries
using
k-svd
developed
signal-dependent
approach
compress
data
suo
proposed
use
recorded
neural
data
directly
sparsity
dictionary
however
algorithms
computational
demanding
highly
signal-dependent
indicates
iterative
training
processes
required
practical
neural
recording
applications
unfavorable
experimental
settings
paper
proposes
novel
framework
implantable
neural
recordings
capable
recov-
ering
neural
spikes
high
compression
ratio
avoiding
sparsity
dictionary
learning
procedure
main
contributions
work
follows
instead
using
conventional
synthesis
model
analysis
model
adopted
enforce
co-
sparsity
neural
signals
overcoming
drawbacks
conventional
model
enhancing
reconstruction
performance
best
knowledge
ﬁrst
time
neural
signal
reconstruction
problem
solved
using
analysis
model
based
piecewise
smooth
structures
neural
signals
multiple-fractional-order-difference
matrix
constructed
analysis
dictionary
high
co-sparsity
neural
signals
also
avoids
dictionary
learning
procedure
saving
computational
resources
data
storage
space
iii
statistical
priors
analysis
coefﬁcients
among
difference
orders
deduced
associated
reconstruction
algorithm
dubbed
weighted
analysis
ℓ1-minimization
walm
proposed
improve
reconstruction
performance
embedding
multiple
orders
knowledge
within
penalty
weights
remainder
paper
organized
follows
section
describes
cs-based
implantable
neural
recording
system
architecture
relevant
background
synthesis
model
co-sparse
analysis
model
section
iii
introduces
proposed
construction
method
multiple-fractional-order-difference
dictionary
section
covers
weighted
analysis
ℓ1-minimization
algorithm
neural
spike
recon-
struction
section
experimental
results
presented
compared
state-of-the-art
cs-based
reconstruction
methods
section
concludes
paper
throughout
paper
boldface
capital
letters
e.g.
denote
matrices
boldface
lowercase
letters
e.g.
denote
vectors
bold
letters
e.g.
denote
scalars
boldface
calligraphic
letters
e.g.
specify
number
sets
vector
use
denote
ith
entry
use
kxk2
kxk1
kxk0
indicate
norms
respectively
matrix
use
denote
ith
february
2016
draft
raw
neural
signal
spike
detection
random
encoding
spikes
compressed
measurements
spike
reconstrcution
reconstructed
spikes
spikes
fig
diagram
compressed
sensing
system
implant
neural
recording
row
column
depending
situation
used
set
use
|i|
indicate
cardinality
random
variable
probability
distribution
function
pdf
denoted
standard
deviation
indicated
system
overview
sparse
models
system
overview
cs-based
wireless
neural
recording
system
architecture
brieﬂy
depicted
fig
recorded
raw
neural
data
ﬁrst
conditioned
appropriate
signal
amplitude
bandwidth
ampliﬁers
ﬁlters
digitized
via
nyquist-rate
adcs
second
neural
spike
events
detected
threshold
crossing
techniques
aligned
temporally
spike
detection
ﬁgure
aligned
segments
containing
spikes
compressed
via
randomized
encoding
circuit
random
encoding
ﬁgure
based
compressive
sensing
theory
compressed
data
transmitted
via
wireless
transmitters
e.g.
bluetooth
zig-bee
wi-fi
receiver
side
random
measurements
aligned
spikes
reconstructed
speciﬁc
algorithms
spike
reconstruction
ﬁgure
workstations
fusion
centers
february
2016
draft
compressed
sensing
synthesis
model
compressed
sensing
emerging
low-rate
sampling
scheme
signals
known
sparse
compressible
certain
basis
assume
signal
measured
simple
matrix-vector
multiplication
rm×n
called
sensing
matrix
compressed
measurement
vector
denotes
measurement
noise
usually
undetermined
i.e.
ratio
m/n
called
compression
ratio
undetermined
system
signal
uniquely
retrieved
sensing
matrix
measurements
however
described
using
synthesis
model
i.e
rn×n
pre-deﬁned
dictionary
signal
representation
assumed
k-sparse
i.e.
kθk0
|supp
well-approximated
k-sparse
vector
name
synthesis
comes
relation
obvious
interpretation
model
describes
way
synthesize
signal
therefore
based
compressed
measurements
represented
φψθ
due
sparse
prior
knowledge
possible
estimate
via
minimization
formulation
tolerance
noise
modeling
errors
calculating
solution
hard
minkθk0
s.t
aθk2
np-hard
problem
generally
one
seeks
solution
relaxed
convex
optimization
problem
kθk0
replaced
kθk1
minkθk1
aθk2
s.t
condition
restricted
isometry
property
rip
minimizing
theoretically
proven
equivalent
minimizing
moreover
minimization
convex
solved
within
polynomial
time
estimating
sparse
coefﬁcient
original
signal
recovered
directly
february
2016
draft
co-sparse
analysis
model
synthesis
model
extensively
studied
twin
model
takes
analysis
point
view
left
aside
almost
untouched
alternative
assumes
signal
interest
analysis
coefﬁcients
vector
expected
sparse
rl×n
possibly
redundant
analysis
dictionary
ratio
l/n
called
redundant
ratio
co-sparsity
signal
respect
deﬁned
number
zeros
vector
i.e.
kco
kzk0
index
set
zero
entries
called
co-support
worth
noting
square
invertible
dictionary
synthesis
analysis
models
ω−1
analysis
model
may
seem
similar
synthesis
counterpart
one
in-fact
different
dealing
redundant
dictionary
traditional
synthesis
model
puts
emphasis
non-zeros
sparse
vector
co-sparse
analysis
model
draws
strength
zeros
analysis
vector
optimization
problem
co-sparse
signal
recovery
formulated
call
analysis
ℓ0-minimization
also
classical
way
relax
nonconvex
minkωxk0
s.t
φxk2
norm
convex
norm
i.e.
minkωxk1
s.t
φxk2
call
analysis
ℓ1-minimization
al1
several
sufﬁcient
conditions
theoretically
guarantee
successful
recovery
original
signal
compressed
measurement
using
restricted
isometry
property
adapted
dictionary
d-rip
restricted
orthogonal
projection
property
ropp
etc
iii
analysis
dictionary
construction
section
focus
construction
analysis
dictionary
analysis
coefﬁcients
sparse
worth
noting
dealing
square
invertible
matrix
analysis
model
completely
equivalent
synthesis
one
case
synthesis-dictionary
construction
methods
used
build
work
concentrate
redundant
case
two
models
depart
analysis
model
becomes
powerful
february
2016
draft
multiple-integer-order-difference
matrix
prior
works
show
many
types
signals
e.g.
eeg
ecg
signals
often
reveals
approximately
piecewise
smooth
structure
structure
exhibits
gradient
sparsity
i.e.
signals
become
sparse
differenced
speciﬁc
orders
moreover
investigations
statistical
properties
using
available
implantable
neural
signals
show
neural
spikes
also
approximately
piecewise
smooth
implying
neural
spikes
co-sparse
signal
model
well
integer-
order-difference
iod
sequence
analysis
dictionary
n-length
signal
iod
sequence
deﬁned
xk=0
cid:18
cid:19
xi+k
denotes
iod
operator
difference
order
iod
sequence
reformulated
matrix
form
proposed
simplicity
distinguish
two
forms
rest
paper
use
matrix
form
build
analysis
dictionary
fig
presents
histogram
co-sparsities
1000
spikes1
using
2nd
order
iod
matrix
analysis
dictionary
seen
co-sparsities
strictly
high
furthermore
construct
redundant
analysis
dictionary
seek
promote
co-sparsity
multiple
orders
difference
sequence
rather
single
one
propose
using
multiple-integer-order-difference
miod
matrix
composed
concatenation
iod
matrices
i.e.
ωmiod
dr0
dr1
drq−1
dri
denotes
iod
matrix
order
corresponding
order
set
rq−1
nq×1
given
miod
matrix
propose
following
prior
proportional
multiple-order
sparsity
worth
noting
setting
analysis
coefﬁcients
order
contain
signal
kωmiodxk0
xri
kdri
xk0
information
formulated
synthesis
model
1the
spikes
randomly
chosen
leicester
easy2
dataset
february
2016
draft
✶✶✡
✶✡☛
❈ ✁✂✄☎✆✂✝✞✟
fig
histogram
effective
co-sparsities
1000
spikes
using
2nd
order
iod
matrix
analysis
dictionary
spikes
randomly
chosen
leicester
easy2
dataset
length
spike
128
✶✵✵
■✍✎✏✑✏✒
✓✒✔✏✒
✔✕✖✖✏✒✏✍✗✏
✘✏✙✚✏✍✗✏
❋✒✛✗✎✕✓✍✛✜
✓✒✔✏✒
✔✕✖✖✏✒✏✍✗✏
✘✏✙✚✏✍✗✏
✵✄☎
✶✄☎
✷✄☎
✸✄☎
✹✄☎
☎✄☎
❖ ✁✂ 
fig
average
co-sparsities
1000
spikes
using
iod
matrix
blue-circle
curve
fod
matrix
red-times
curve
analysis
dictionary
respectively
spikes
randomly
chosen
leicester
easy2
dataset
length
spike
128.
multiple-fractional-order-difference
matrix
although
miod
matrix
promotes
higher
co-sparsity
single-order
one
choose
order
set
appropriately
remains
problem
blue-circle
curve
fig
shows
co-sparsity
1000
spikes
using
iod
matrix
analysis
dictionary
notice
4-th
order
analysis
coefﬁcients
maximum
co-sparsity
adding
difference
matrix
orders
however
co-sparsity
analysis
coefﬁcients
optimal
signal
reconstruction
performance
severely
degrade
solve
problem
build
multiple-fractional-orders-difference
mfod
matrix
using
fractional-order-difference
fod
sequence
generalization
iod
sequence
fractional
order
fod
sequence
deﬁned
xk=0
xi+k
draft
february
2016
denotes
gamma
function
easy
verify
summation
convergent
nonnegative
integer
inﬁnite
sum
deﬁned
reduces
ﬁnite
sum
i.e.
xk=0
xi+k
operator
generalizes
one
deﬁned
red-times
curve
fig
shows
co-sparsity
1000
spikes
using
fod
matrix
analysis
dictionary
based
fod
matrix
mfod
matrix
constructed
order
set
deﬁne
ωmfod
df0
df1
dfq−1
fq−1
rq×1
max|fi
fj|
maximum
order
distance
presents
trade-off
co-sparsity
analysis
coefﬁcients
mutual
coherence
ωmfod
small
increases
co-sparsity
analysis
coefﬁcients
leads
condition
highly
coherent
order
difference
matrix
vice
versa
observations
different
suggest
good
compromise
remark
redundant
ratio
presents
trade-off
signal
reconstruction
accuracy
computational
cost
large
increases
reconstruction
performance
consumes
computational
resources
work
showed
good
compromise
weighted
analysis
ℓ1-minimization
constructed
multiple
fractional
orders
difference
matrix
analysis
dictionary
herein
propose
weighted
analysis
ℓ1-minimization
walm
method
reconstruct
original
neural
spike
considering
measurement
model
assume
components
independent
identically
distributed
i.i.d
gaussian
variables
unknown
variance
entry
independent
laplacian
distribution
standard
deviation
i.e.
√2σi
exp
cid:18
√2kzik1
cid:19
note
reconstructing
identical
therefore
ﬁrst
infer
maximizing
conditional
probability
distribution
z|y
expressed
bayes
rule
february
2016
z|y
y|φ
draft
✵☛☞
✲✵☛☞
✵☛✵☞
✵☛✵✁☞
✲✵☛✵✁☞
✲✵☛✵☞
❙✌✍✎✏
✸✆✝✞✟✠
✡☛☞✌☛
✶✶✁
✶✁✂
✗✘✙✚✛✜✍✜
✢✏✣✤✥✦
✸☛☞✲✑✒
✓✔✕✖✔
✹✲✑✒
✓✔✕✖✔
✹☛☞✲✑✒
✓✔✕✖✔
✶✹✹
✶✾✁
✁✹✵
✁✂✂
✸✸ 
✸✂✹
✄✵✵
☎✵✵
☎✵✵
✹✞✟✠
✡☛☞✌☛
✹✆✝✞✟✠
✡☛☞✌☛
fig
top
bottom
original
neural
spike
corresponding
analysis
coefﬁcients
3.5-th
4-th
4.5-th
order
difference
top
bottom
histograms
analysis
coefﬁcients
3.5-th
4-th
4.5-th
order
respectively
noise
assumed
gaussian
likelihood
function
given
y|φ
exp
cid:18
−ky
φxk2
2σ2
hence
maximizing
posterior
distribution
z|y
leads
cid:19
ˆzmap
arg
max
z|y
substituting
obtain
arg
max
log
y|φ
+xi
φxk2
2σ2
+xi
log
√2kωixk1
ˆxmap
arg
min
ˆxmap
arg
min
2ky
φxk2
λkdiag
ωxk1
denotes
ith
row
problem
equivalent
denotes
tuning
parameter
hence
ℓ1-minimization
interpreted
map
estimation
hypothesis
equal
however
multiple
fractional
orders
analysis
matrix
hypothesis
entries
equal
standard
deviations
reﬂect
fact
illustrate
argument
fig
plots
analysis
coefﬁcients
3.5-th
4-th
4.5-th
order
difference
typical
neural
spike2
time
2the
spike
randomly
chosen
leicester
easy2
dataset
february
2016
draft
corresponding
histograms
analysis
coefﬁcients
three
orders
simultaneously
reported
fig
clearly
standard
deviations
analysis
coefﬁcients
distinct
orders
identical
cope
issue
divide
standard
deviation
vector
multiple
groups
incorporate
aforementioned
multiple
orders
prior
suppose
constructed
difference
matrices
fractional
orders
partitioned
groups
i.e.
=hwt
gq−1it
gq−1
represent
standard
deviations
corresponding
orders
respectively
note
group
equal
variance
analysis
coefﬁcients
tends
decrease
ﬁrst
increase
across
orders
propose
model
variance
across
orders
quadratic
functions
ci2−2aif
−2bifi
model
parameters
difference
order
model
made
equal
coefﬁcients
within
order
refers
variance
analysis
coefﬁcients
order
therefore
wgi
σfi
+bifi
2aif
√ci
entries
depend
value
problem
solved
parameters
calculated
leads
propose
training
stage
estimate
values
ﬁrst
part
predicts
standard
deviations
σfi
using
maximum
likelihood
estimation
variances
estimated
simple
quadratic
regression
employed
solve
following
equation
derived
log2
2aif
example
ﬁtted
regression
curve
shown
fig
log2
2bifi
building
problem
easily
solved
using
ℓ1-minimization
algorithms
complete
walm
algorithm
outlined
algorithm
remark
proposed
walm
nearly
signal
independent
approach
although
requires
training
step
estimate
regression
parameters
amount
data
needed
much
less
signal
dependent
approaches
therefore
walm
signiﬁcantly
reduce
data
storage
computational
resource
february
2016
draft
❚✡☛☞✌☞✌✍
✎✏✑
❋☞✑✑✏✒
✡✏✍✡✏✎✎☞✓✌
✔☞✌✏
✲✞✹ ✂
✲✞✹ ☎✂
✲✞✞
✲✞✞ ✞✂
✲✞✞ ✂
✲✞✞ ☎✂
✲✞✸
✸ ✁
✸ ✂
✸ ✄
✸ ☎
✸ ✆
✸ ✝
✁ ✹
✁ ✞
✁ ✸
✁ ✁
✁ ✂
✁ ✄
❖r❞❡r
fig
scatter
plot
fractional
order
versus
variance
log2
ﬁtted
regression
curve
order
variance
averaged
training
dataset
contains
100
spikes
randomly
chosen
leicester
easy2
dataset
remark
contrast
canonical
al1
method
main
advantage
walm
incorporation
multiple
orders
prior
analysis
coefﬁcients
including
positions
nonzero
coefﬁcients
standard
deviations
neighboring
difference
orders
allow
number
measurements
signiﬁcantly
reduced
without
leading
ambiguity
algorithm
weighted
analysis
ℓ1-minimization
input
construct
dfi
using
end
construct
using
estimate
using
construct
using
end
construct
using
solve
arg
min
2ky
φxk2
λkdiag
ωxk1
output
recovered
signal
experiment
validation
section
examine
performance
proposed
walm
method
state-of-the-art
compressed
sensing
schemes
implant
neural
recording
february
2016
draft
experimental
setup
use
leicester
neural
signal
database
contains
synthesized
datasets
dataset
contains
spikes
three
different
neurons
different
noise
levels
datasets
categorized
spike
sorting
difﬁculty
levels
leicester
difﬁcult1
difﬁcult2
easy1
easy2
take
128
samples
around
spike
form
signal
frame
simplify
comparison
retain
signal
containing
one
spike
random
i.i.d
bernoulli
matrix
chosen
sensing
matrix
guarantees
excellent
reconstruction
quality
implementation
efﬁciency
signals
compressed
reconstructed
times
using
different
sensing
matrix
trial
results
averaged
across
trials
measure
reconstruction
quality
employ
percentage
root-mean-square
difference
prd
quantify
error
percentage
original
reconstructed
prd
ˆxk2
kxk2
100
physiological
signal
reconstruction
zigel
classiﬁed
different
values
prd
based
signal
quality
perceived
specialists
work
prd
value
regarded
good
reconstruction
quality
following
state-of-the-art
algorithms
chosen
performance
comparison
basis
pursuit
de-noising
bpdn
described
orthonormal
basis
daubechies-4
wavelet
used
sparsity
dictionary
bpdn
implementation
used
solvers
solvebp
sparselab
toolbox
block
sparse
bayesian
learning
bsbl
proposed
zhang
used
solver
bsbl-
bsbl
implementation
analysis
ℓ1-minimization
al1
algorithm
described
al1
implementation
used
cvx
toolbox
stanford
university
signal
dependent
neural
compressed
sensing
sdncs
method
proposed
used
sparse
representation
dictionary
learned
data
method
dataset
divided
training
section
test
section
composed
dataset
training
section
used
construct
sparse
representation
dictionary
whereas
test
section
used
evaluate
performance3
3the
implementation
sdncs
downloaded
http
//etienne.ece.jhu.edu/projects/neural
cs/cs
code.rar
february
2016
draft
✰❢✑❀✒❀✓❣
✰❢✑✿✓❀✒❀✒✿✓❣
✰❢✒❣
✰❘❚❋
◆ ✁✂✄☎
✞✄✟✠ ☎✄✁✄✡☛
❖r✐❣✐♥❛✒
s♣✐❦❡
❘❡❝♦♥str✉❝t❡❞
s♣✐❦❡
✉s✐♥❣
✰❢✓❀✔❀✺✕
❘❡❝♦♥str✉❝t❡❞
s♣✐❦❡
✉s✐♥❣
✰❢✓✿✺❀✔❀✔✿✺✕
❘❡❝♦♥str✉❝t❡❞
s♣✐❦❡
✉s✐♥❣
✰❢✔✕
❘❡❝♦♥str✉❝t❡❞
s♣✐❦❡
✉s✐♥❣
✰✖❚❋
✶✶✞
✶✞✟
❙ ✁✂✄☎✆
✶✑✞
✵✑✟
✵✑✹
✲✵✑✹
✲✵✑✟
fig
averaged
prds
spikes
leicester
easy1
dataset
versus
different
number
measurements
3,4,5
3.5,4,4.5
ωrtf
respectively
original
spike
reconstructed
spikes
using
3,4,5
3.5,4,4.5
ωrtf
respectively
advantage
mfod
matrix
evaluate
effectiveness
proposed
mfod
matrix
compared
iod
matrix
miod
matrix
random
tight
frame
rtf
proposed
experiment
constructed
iod
matrix
miod
matrix
3.5,4,4.5
leicester
easy1
dataset
chosen
evaluation
four
dictionaries
al1
algorithm
3,4,5
mfod
matrix
used
reconstruct
spikes
average
prds
spikes
four
dictionaries
spike
reconstruction
example
shown
fig
respectively
among
four
different
dictionaries
ωrtf
worst
performance
mainly
ωrtf
general
analysis
dictionary
exploit
statistical
information
neural
spikes
furthermore
note
3.5,4,4.5
3,4,5
outperform
due
redundancy
addition
3.5,4,4.5
better
reconstruction
accuracy
3,4,5
especially
number
measurements
small
mainly
analysis
coefﬁcients
3.5,4,4.5
sparser
3,4,5
high
sparsity
reduces
number
measurements
signal
reconstruction
february
2016
draft
probabilities
reconstruction
good
quality
different
number
measurements
table
easy1
dataset
difﬁcult1
dataset
bpdn
bsbl
al1
sdncs
walm
bpdn
bsbl
al1
sdncs
walm
92.2
93.1
92.5
30.8
93.3
28.8
9.3
42.9
52.9
68.2
84.5
92.7
2.1
54.2
52.9
83.8
60.8
92.1
68.4
94.3
75.8
98.8
80.8
100
93.2
100
98.3
100
95.6
97.9
100
100
100
100
100
100
95.1
97.7
100
100
100
100
100
100
6.4
33.4
48.2
63.3
82.8
91.2
1.3
52.1
48.5
82.4
58.3
90.2
66.7
93.0
74.3
98.7
78.4
100
92.0
100
97.9
100
94.9
96.2
98.8
100
100
100
100
100
94.3
96.3
98.9
100
100
100
100
100
average
prd
probability
good
reconstruction
evaluate
performance
proposed
walm
algorithm
versus
number
measure-
ments
3.5,4,4.5
used
analysis
dictionary
al1
walm
leicester
neural
signal
database
easy1
difﬁcult1
datasets
chosen
evaluation
experimental
results
shown
fig
point
indicates
average
prd
spikes
speciﬁed
number
measurements
time
table
reports
probability
good
reconstruction
quality
different
situations
first
observe
analysis
model
based
algorithms
outperform
synthesis
model
based
ones
terms
averaged
prd
probability
good
reconstruction
quality
moreover
due
incorporation
multiple
orders
prior
analysis
coefﬁcients
walm
algorithm
performs
better
canonical
al1
algorithm
especially
number
measurements
small
walm
algorithm
averaged
prd
less
numbers
measurements
achieves
good
reconstruction
quality
16.
comparison
bpdn
bsbl
al1
recover
many
spikes
easy1
difﬁcult1
datasets
good
reconstruction
quality
condition
sdncs
walm
algorithms
show
good
reconstruction
quality
sdncs
even
slightly
outperforms
walm
proposed
walm
algorithm
simpliﬁes
sparse
dictionary
learning
much
fewer
computational
resources
preferred
practical
neural
recording
experiments
observe
prd
variance
across
individual
datasets
fig
shows
box
plots
algorithms
february
2016
draft
❇✑✒✓
❇✔❇✕
❆✕✖
✔✒✓❙✔
❲❆✕✗
◆ ✁✂✄☎
✞✄✟✠ ☎✄✁✄✡☛
❇✑✒✓
❇✔❇✕
❆✕✖
✔✒✓❙✔
❲❆✕✗
◆ ✁✂✄☎
✞✄✟✠ ☎✄✁✄✡☛
fig
prd
averaged
spikes
easy1
dataset
difﬁcult1
dataset
versus
different
number
measurements
bpdn
bsbl
al1
sdncs
walm
respectively
green
dash-dotted
line
denotes
good
prd
bound
number
measurements
32.
box
central
mark
indicates
median
edges
box
25th
75th
percentiles
whiskers
extend
extreme
data
points
obviously
easy1
difﬁcult1
datasets
walm
algorithm
adopted
multiple
fractional
orders
analysis
dictionary
outperforms
algorithms
sdncs
similar
performance
walm
moreover
although
prd
variances
two
datasets
similar
number
outliers
difﬁcult1
dataset
easy1
dataset
performance
classiﬁcation
using
reconstructed
spikes
illustrate
performance
walm
carried
spike
classiﬁcation
experiment
using
reconstructed
spikes
leicester
easy1
difﬁcult1
datasets
chosen
evaluation
firstly
spikes
compressed
reconstructed
using
ﬁve
algorithms
principal
component
analysis
pca
wavelet
decomposition
methods
used
extract
features
reconstructed
spikes
easy1
difﬁcult1
datasets
respectively
finally
ﬁrst
features
february
2016
draft
✠✡✍
✠✡✌
✠✡☞
✠✡☛
✠✡✍
✠✡✌
✠✡☞
✠✡☛
✠✡✍
✠✡✌
✠✡☞
✠✡☛
✠✡✍
✠✡✌
✠✡☞
✠✡☛
❇ ✁✂
❇✄❇☎
❆☎✆
✄✁✂❙✄
❲❆☎✝
❇ ✁✂
❇✄❇☎
❆☎✆
✄✁✂❙✄
❲❆☎✝
fig
box
plots
spikes
easy1
dataset
difﬁcult1
dataset
bpdn
bsbl
al1
sdncs
walm
respectively
number
measurements
32.
spike
used
classiﬁcation
superparamagnetic
clustering
spc
algorithm
fig
fig
show
three-dimensional
projections
ﬁrst
three
features
reconstructed
spikes
using
ﬁve
algorithms
easy1
difﬁcult1
datasets
respectively
cases
classiﬁcation
done
automatically
spc
represented
different
colors
easy1
dataset
observe
possible
identify
three
clusters
clearly
using
spikes
reconstructed
ﬁve
algorithms
furthermore
features
spikes
reconstructed
walm
sdncs
accurate
algorithms
implying
walm
sdncs
better
reconstruction
performance
difﬁcult1
dataset
observe
spikes
reconstructed
walm
sdncs
clearly
classiﬁed
whereas
classiﬁcation
got
failed
using
spikes
reconstructed
al1
bpdn
bsbl
spike
classiﬁcation
accuracy
also
used
performance
metric
calculated
percentage
total
number
spikes
correctly
classiﬁed
classiﬁcation
results
compared
ground
truth
labels
contained
datasets
spike
classiﬁcation
results
easy1
difﬁcult1
shown
fig
11.
observe
walm
sdncs
outperform
three
algorithms
spike
classiﬁcation
even
number
measurements
walm
achieve
classiﬁcation
accuracy
easy1
difﬁcult1
datasets
respectively
moreover
walm
provides
reliable
solution
yields
better
reconstruction
classiﬁcation
performance
much
fewer
computational
resource
pre-acquired
data
sdncs
february
2016
draft
✵ ✵✁
✵ ✵✂
✲✵ ✵✂
✲✵ ✵✁
✵ ✵✁
✵ ✵✁
✵ ✵✂
✲✵ ✵✂
✲✵ ✵✁
✵ ✵✁
✵ ✵✂
✵ ✵✁
✵ ✵✂
✵ ✵✂
✲✵ ✵✂
✲✵ ✵✂
✲✵ ✵✂
✲✵ ✵✂
✵ ✵✂
✵ ✵✁
✵ ✵✂
✵ ✵✁
✵ ✵✂
✲✵ ✵✂
✲✵ ✵✂
✵ ✵✁
✵ ✵✂
✲✵ ✵✂
✲✵ ✵✁
✵ ✵✁
✵ ✵✁
✵ ✵✂
✲✵ ✵✂
✲✵ ✵✁
✵ ✵✁
✵ ✵✁
✵ ✵✂
✲✵ ✵✂
✲✵ ✵✁
✵ ✵✁
✵ ✵✁
✵ ✵✂
✲✵ ✵✂
✲✵ ✵✁
✵ ✵✁
✵ ✵✂
✵ ✵✁
✵ ✵✂
✵ ✵✂
✲✵ ✵✂
✲✵ ✵✂
✲✵ ✵✂
✲✵ ✵✂
✵ ✵✂
✵ ✵✁
✵ ✵✂
✵ ✵✁
✵ ✵✂
✲✵ ✵✂
✲✵ ✵✂
fig
scatter
plot
ﬁrst
three
features
original
spikes
easy1
dataset
spikes
reconstructed
walm
spikes
reconstructed
sdncs
spikes
reconstructed
al1
spikes
reconstructed
bpdn
spikes
reconstructed
bsbl
respectively
features
extracted
using
pca
points
colored
according
cluster
assigned
number
measurements
16.
conclusion
paper
proposed
novel
compressed
sensing
method
implantable
neural
recording
proposed
method
enforces
sparsity
neural
spikes
traditional
synthesis
model
analysis
model
multiple
fractional
orders
difference
matrix
analysis
dictionary
therefore
pre-
acquired
data
computational
resource
dictionary
learning
signiﬁcantly
reduced
besides
exploiting
statistical
priors
analysis
coefﬁcients
among
difference
orders
weighted
analysis
ℓ1-minimization
algorithm
proposed
reconstruct
neural
spikes
experimental
results
proved
efﬁcacy
proposed
method
neural
signal
reconstruction
stevenson
kording
advances
neural
recording
affect
data
analysis
nature
neuroscience
vol
139–142
2011.
references
february
2016
draft
✵ ✁
✵ ✁
✶ ✁
✶ ✁
✵ ✁
✵ ✁
✵ ✁
✶ ✁
✶ ✁
✶ ✁
fig
10.
scatter
plot
ﬁrst
three
features
original
spikes
difﬁcult1
dataset
spikes
reconstructed
walm
spikes
reconstructed
sdncs
spikes
reconstructed
al1
spikes
reconstructed
bpdn
spikes
reconstructed
bsbl
respectively
features
extracted
using
wavelet
decomposition
points
colored
according
cluster
assigned
number
measurements
ber´enyi
somogyvari
nagy
roux
long
fujisawa
stark
leonardo
harris
buzs´aki
large-scale
high-density
512
channels
recording
local
circuits
behaving
animals
journal
neurophysiology
vol
111
1132–1149
2014
schwarz
lebedev
hanson
dimitrov
lehew
meloy
rajangam
subramanian
ifft
al.
chronic
wireless
recordings
large-scale
brain
activity
freely
moving
rhesus
monkeys
nature
methods
vol
670–676
2014
yin
borton
komar
agha
laurens
lang
bull
al.
wireless
neurosensor
full-spectrum
electrophysiology
recordings
free
behavior
neuron
vol
1170–1182
2014
rodriguez-perez
ruiz-amaya
delgado-restituto
rodriguez-vazquez
low-power
programmable
neural
spike
detection
channel
embedded
calibration
data
compression
biomedical
circuits
systems
ieee
transactions
vol
87–100
2012
chae
yang
yuce
hoang
liu
128-channel
wireless
neural
recording
spike
feature
extraction
uwb
transmitter
neural
systems
rehabilitation
engineering
ieee
transactions
vol
312–321
2009
gosselin
sawan
ultra
low-power
cmos
automatic
action
potential
detector
neural
systems
february
2016
draft
✶✹✹
❇✤✥✦
❇✧❇★
❆★✩
✧✥✦❙✧
❲❆★✪
◆ ✁✂✄☎
✞✄✟✠ ☎✄✁✄✡☛
✶✹✹
❇✤✥✦
❇✧❇★
❆★✩
✧✥✦❙✧
❲❆★✪
◆ ✁✂✄☎
✞✄✟✠ ☎✄✁✄✡☛
fig
11.
classiﬁcation
accuracy
averaged
spikes
easy1
dataset
difﬁcult1
dataset
versus
different
numbers
measurements
bpdn
bsbl
al1
sdncs
walm
respectively
rehabilitation
engineering
ieee
transactions
vol
346–353
2009
gosselin
ayoub
j.-f.
roy
sawan
lepore
chaudhuri
guitton
mixed-signal
multichip
neural
recording
interface
bandwidth
reduction
biomedical
circuits
systems
ieee
transactions
vol
129–141
2009
oweiss
mason
suhail
kamboh
thomson
scalable
wavelet
transform
vlsi
architecture
real-time
signal
processing
high-density
intra-cortical
implants
circuits
systems
regular
papers
ieee
transactions
vol
1266–1278
2007
donoho
compressed
sensing
information
theory
ieee
transactions
vol
1289–1306
2006
candes
tao
near-optimal
signal
recovery
random
projections
universal
encoding
strategies
information
theory
ieee
transactions
vol
5406–5425
2006
zhang
suo
mitra
chin
hsiao
yazicioglu
tran
etienne-cummings
efﬁcient
compact
compressed
sensing
microsystem
implantable
neural
recordings
biomedical
circuits
systems
ieee
transactions
vol
485–496
2014
suo
zhang
xiong
chin
etienne-cummings
tran
energy-efﬁcient
multi-mode
compressed
sensing
system
implantable
neural
recordings
biomedical
circuits
systems
ieee
transactions
vol
648–659
2014
zhang
rao
extension
sbl
algorithms
recovery
block
sparse
signals
intra-block
correlation
february
2016
draft
signal
processing
ieee
transactions
vol
2009–2015
2013
bulach
bihr
ortmanns
evaluation
study
compressed
sensing
neural
spike
recordings
engineering
medicine
biology
society
embc
2012
annual
international
conference
ieee
ieee
2012
3507–3510
bruckstein
donoho
elad
sparse
solutions
systems
equations
sparse
modeling
signals
images
siam
review
vol
34–81
2009
becker
bobin
cand
nesta
fast
accurate
ﬁrst-order
method
sparse
recovery
siam
journal
imaging
sciences
vol
1–39
2011
cand
restricted
isometry
property
implications
compressed
sensing
comptes
rendus
mathema-
tique
vol
346
589–592
2008
elad
milanfar
rubinstein
analysis
versus
synthesis
signal
priors
inverse
problems
vol
947
2007
nam
davies
elad
gribonval
cosparse
analysis
model
algorithms
applied
computational
harmonic
analysis
vol
30–56
2013
candes
eldar
needell
randall
compressed
sensing
coherent
redundant
dictionaries
applied
computational
harmonic
analysis
vol
59–73
2011
peleg
elad
performance
guarantees
thresholding
algorithm
cosparse
analysis
model
information
theory
ieee
transactions
vol
1832–1845
2013
liu
vos
van
huffel
compressed
sensing
multichannel
eeg
signals
simultaneous
cosparsity
low-rank
optimization
biomedical
engineering
ieee
transactions
vol
2055–2061
2015
chambolle
algorithm
total
variation
minimization
applications
journal
mathematical
imaging
vision
vol
1-2
89–97
2004
keogh
chakrabarti
pazzani
mehrotra
dimensionality
reduction
fast
similarity
search
large
time
series
databases
knowledge
information
systems
vol
263–286
2001
jiang
lan
zhang
new
representation
similarity
measure
time
series
data
mining
computational
intelligence
software
engineering
2009.
cise
2009.
international
conference
ieee
2009
1–5
zhou
new
method
piecewise
linear
representation
time
series
data
physics
procedia
vol
1097–1103
2012
yan
fang
approach
time
series
piecewise
linear
representation
based
local
maximum
minimum
extremum
journal
information
computational
science
vol
2747–2756
2013
olak
generalized
difference
sequence
spaces
soochow
journal
mathematics
vol
377–386
1995
quiroga
nadasdy
ben-shaul
unsupervised
spike
detection
sorting
wavelets
superparam-
agnetic
clustering
neural
computation
vol
1661–1687
2004
chen
chandrakasan
stojanovi´c
design
analysis
hardware-efﬁcient
compressed
sensing
architecture
data
compression
wireless
sensors
solid-state
circuits
ieee
journal
vol
744–756
2012
zigel
cohen
katz
weighted
diagnostic
distortion
wdd
measure
ecg
signal
compression
biomedical
engineering
ieee
transactions
vol
1422–1430
2000.
february
2016
draft
donoho
stodden
tsaig
sparselab
2007
see
http
//sparselab
stanford
edu..
accessed
january
2014
2008
grant
boyd
cvx
matlab
software
disciplined
convex
programming
2008
jolliffe
principal
component
analysis
wiley
online
library
2002
february
2016
draft
