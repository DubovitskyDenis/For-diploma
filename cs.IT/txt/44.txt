quantifying
synergistic
information
using
intermediate
stochastic
variables
rick
quax1
omri
har-shemesh1
peter
m.a
sloot1,2,3
computational
science
lab
university
amsterdam
netherlands
advanced
computing
lab
itmo
university
saint
petersburg
russia
complexity
institute
nanyang
technological
university
singapore
part
work
presented
conference
complex
systems
2016.
quantifying
synergy
among
stochastic
variables
important
open
problem
information
theory
information
synergy
occurs
multiple
sources
together
predict
outcome
variable
better
sum
single-source
predictions
essential
phenomenon
biology
neuronal
networks
cellular
regulatory
processes
different
information
flows
integrate
produce
single
response
also
social
cooperation
processes
well
statistical
inference
tasks
machine
learning
propose
metric
synergistic
entropy
synergistic
information
first
principles
proposed
measure
relies
so-called
synergistic
random
variables
srvs
constructed
zero
mutual
information
individual
source
variables
non-zero
mutual
information
complete
set
source
variables
prove
several
basic
desired
properties
measure
including
bounds
additivity
properties
addition
prove
several
important
consequences
measure
including
fact
different
types
synergistic
information
may
co-exist
sets
variables
numerical
implementation
provided
use
demonstrate
synergy
associated
resilience
noise
measure
may
marked
step
forward
study
multivariate
information
theory
numerous
applications
introduction
shannon
information
theory
natural
framework
studying
correlations
among
stochastic
variables
claude
shannon
proved
entropy
single
stochastic
variable
uniquely
quantifies
much
information
required
identify
sample
value
variable
follows
four
quite
plausible
axioms
non-negativity
continuity
monotonicity
additivity
using
similar
arguments
mutual
information
two
stochastic
variables
pairwise
correlation
measure
quantifies
much
information
shared
however
higher-order
informational
measures
among
three
stochastic
variables
remain
long-standing
research
topic
2–6
prominent
higher-order
informational
measure
synergistic
information
3–5,7–10
however
still
open
question
measure
quantify
idea
set
variables
taken
together
convey
information
summed
information
individual
variables
synergy
studied
instance
context
regulatory
processes
cells
networks
neurons
illustrate
idea
high
level
consider
recognition
simple
object
say
red
square
implemented
multi-layer
neuronal
network
input
neurons
implement
local
edge
detection
input
neurons
implement
local
color
detection
presence
red
square
defined
solely
presence
edges
red
color
alone
defined
particular
higher-order
relation
edges
color
therefore
neuronal
network
successfully
recognizes
object
must
integrate
multiple
pieces
information
synergistic
manner
however
unknown
exactly
implemented
dynamical
network
measure
exists
quantify
synergistic
information
among
arbitrary
number
variables
consider
task
predicting
values
outcome
variable
using
set
source
variables
total
predictability
given
quantified
information-theoretically
classic
shannon
mutual
information
entropy
bits
also
referred
uncertainty
denotes
total
amount
information
needed
determine
unique
value
obeys
conditional
variant
chain
rule
written
explicitly
denotes
remaining
entropy
given
value
observed
article
address
problem
quantifying
synergistic
information
illustrate
information
synergy
consider
classic
example
xor-gate
two
i.i.d
binary
inputs
defined
following
deterministic
input-output
table
table
transition
table
binary
xor-gate
priori
outcome
value
50/50
distributed
easily
verified
observing
inputs
simultaneously
fully
predicts
outcome
value
observing
either
input
individually
improve
prediction
indeed
find
words
means
case
information
outcome
stored
either
source
variable
individually
stored
synergistically
combination
two
inputs
case
stores
whether
independent
individual
values
either
yiixxyx
.ixyhyhyx2
logpr
yhyyyyyyyy
hyx
hxyhxhyx2
logpr
.xyhyxxxyyxxyyxxyxxy1x2xyy1x2xyy1212
1.ixyixyixxyy12xx1x2x
two
general
approaches
quantify
synergy
exist
current
literature
practical
side
methods
devised
approximate
synergistic
information
using
simplifying
assumptions
intuitive
example
whole
minus
sum
wms
method
simply
subtracts
sum
pairwise
individual
mutual
information
quantities
total
mutual
information
i.e.
formula
based
assumption
uncorrelated
presence
correlations
measure
may
become
negative
ambiguous
theoretical
side
search
ongoing
set
necessary
sufficient
conditions
general
synergy
measure
satisfy
knowledge
prominent
systematic
approach
partial
information
decomposition
framework
pid
proposed
williams
beer
synergistic
information
implicitly
defined
additionally
defining
so-called
unique
shared
information
together
required
sum
total
mutual
information
among
conditions
however
appears
original
axioms
shannon
information
theory
insufficient
uniquely
determine
functions
decomposition
framework
two
approaches
exist
extending
changing
set
axioms
3,7,8,12
finding
good
enough
approximations
3,6,9,10
work
differs
crucially
abovementioned
approach
fact
define
synergy
first
principles
incompatible
pid
use
simple
example
motivate
perceived
incongruence
pid
proposed
procedure
calculating
synergy
based
upon
newly
introduced
notion
perfect
orthogonal
decomposition
among
stochastic
variables
prove
important
basic
properties
feel
successful
synergy
measure
obey
non-negativity
insensitivity
reordering
subvariables
also
derive
number
intriguing
properties
upper
bound
amount
synergy
variable
given
set
variables
finally
provide
numerical
implementation
use
experimental
validation
demonstrate
synergistic
variables
increased
resilience
noise
important
property
large
specifically
biological
systems
results
2.1
preliminaries
2.1.1
definition
orthogonal
decomposition
following
intuition
linear
algebra
refer
two
stochastic
variables
orthogonal
case
independent
i.e.
given
joint
distribution
two
stochastic
variables
say
function
orthogonal
decomposition
respect
case
satisfies
following
five
properties

:iiixyixyix
ixy
ab:0iab
ab
,dbabbbasufficiency
orthogonality
parallelism
non-spuriousness
parsimony
:.ibbbhbibaibaibaibabibbiba
words
decomposed
two
orthogonal
stochastic
variables
two
parts
taken
together
informationally
equivalent
information
variable
iii
parallel
part
mutual
information
orthogonal
part
zero
mutual
original
2.1.2
related
literature
decomposing
correlated
variables
notion
orthogonal
decomposition
related
ongoing
study
common
random
variable
definitions
dating
back
around
1970.
particular
definition
appears
equivalent
definition
wyner
denoted
specific
case
appendix
6.6
show
condition
satisfies
three
requirements
involve
remains
undefined
wyner
work
brief
smallest
common
random
variable
makes
conditionally
independent
i.e.
show
case
minimization
able
reach
condition
open
question
whether
implies
exist
particular
required
minimization
step
calculate
highly
non-trivial
solutions
known
specific
cases
14,15
illustrate
different
approach
field
gács
körner
define
common
random
variable
largest
random
variable
extracted
deterministically
individually
i.e.
functions
chosen
maximize
show
appears
practice
typically
less
relation
actually
holds
variable
restricted
applications
zero-error
communication
cryptography
2.1.3
sufficiency
decomposition
definition
orthogonal
decomposition
sufficient
able
define
consistent
measure
synergistic
information
show
section
2.2.
however
leave
open
question
whether
actually
stringent
strictly
necessary
therefore
statement
orthogonal
decomposition
possible
synergy
measure
valid
case
possible
remains
open
question
whether
implies
impossible
define
synergy
similar
manner
important
future
work
thus
try
minimize
conditions
orthogonal
decomposition
leaving
synergy
measure
intact
2.1.4
satisfiability
decomposition
indeed
turns
always
possible
achieve
perfect
orthogonal
decomposition
according
depending
example
demonstrate
appendix
6.5
depends
case
binary-valued
impossible
achieve
decomposition
case
hand
one
sufficient
condition
able
achieve
perfect
orthogonal
decomposition
able
restate
valid
independent
case
easy
see
orthogonal
decomposition
restating
could
reached
reordering
relabeling
variables
states
bbbaabbwb
wibabibawbbwbabargmin
:wawbbiabw
:wibaiba
:wibaibab
abwbab0fagbbfg0hb0
:ibaibaababbaprbbapab
awx
bwy
,wxybwby
propose
following
line
reasoning
asymptotically
reach
restating
least
approximate
nevertheless
remainder
paper
simply
assumes
existence
orthogonal
decomposition
use
particular
method
achieve
consider
karhunen-loève
transform
klt
17–19
restate
stochastic
variable
mean
pairwise
independent
random
variables
coefficients
real
scalars
transform
could
seen
random
variables
analogy
well-known
principle
component
analysis
fourier
transform
typically
transform
defined
range
random
variables
context
continuous
stochastic
process
decomposed
defined
coefficients
scalars
become
functions
must
pairwise
orthogonal
zero
inner
product
square-integrable
otherwise
abovementioned
transform
applies
single
way
-dependent
coefficients
nevertheless
purpose
leave
open
chosen
part
stochastic
process
otherwise
also
note
transform
works
similarly
discrete
case
often
applied
image
analysis
let
choose
single
sequence
variable
basis
consider
two
random
variables
decomposed
sequences
respectively
particular
mutual
information
must
equal
transform
desired
restating
achieved
choice
common
could
either
natural
common
stochastic
process
otherwise
part
known
common
signal
two
receivers
intermittently
record
could
found
numerical
procedure
attempt
numerical
approximation
done
instance
image
analysis
tasks
abxklt1.kkkxzxkzktatbxtxkztx
.btkkaztxdt
kt
abtxt
ktkzkzabkzkkzkkz
iabab
awx
bwy:00
:00
:00.kkkkkkkkkwzxzyzkzabkz
2.2
proposed
framework
2.2.1
synergistic
random
variable
firstly
define
synergistic
random
variable
srv
satisfies
conditions
words
srv
stores
information
whole
information
individual
constitute
srv
defined
conditional
probability
distribution
thus
conditionally
independent
srv
given
denote
collection
possible
non-redundant
srvs
joint
random
variable
sometimes
refer
set
ordering
marginal
distributions
srvs
irrelevant
due
conditional
independence
2.2.2
maximally
synergistic
random
variables
set
may
general
uncountable
many
members
may
extremely
small
mutual
information
would
prevent
practical
use
therefore
introduce
notion
maximally
synergistic
random
variables
msrv
also
use
proofs
proof
set
countable
however
numerical
results
see
especially
figure
show
typical
msrv
substantial
mutual
information
maximum
possible
suggests
either
set
msrvs
countable
mutual
information
subset
msrvs
rapidly
converges
maximum
aiding
practical
use
define
set
msrvs
denoted
smallest
possible
subset
still
makes
redundant
i.e.
denotes
cardinality
set
minimized
intuitively
one
could
imagine
building
iteratively
removing
srv
case
completely
redundant
given
another
srv
i.e.
result
set
informational
content
entropy
since
redundant
variables
discarded
case
multiple
candidates
would
exist
choice
among
induce
synergy
quantity
proposed
synergy
measure
2.2.3
synergistic
entropy
interpret
representing
synergistic
information
stochastic
variable
could
possibly
store
therefore
define
synergistic
entropy
upper
bound
synergistic
information
variable
siixx:0
::0.iisxiisxxixxispriisxxx
xx
xxxxx
xxmin:0.xxhxxxisxjs:0ijjhssxxxxxxxhxx
2.2.4
orthogonalized
srvs
order
prevent
doubly
counting
synergistic
information
orthogonalize
msrvs
let
denote
particular
ordering
msrvs
convert
set
orthogonal
msrvs
osrvs
short
given
ordering
words
iteratively
take
msrv
add
orthogonal
part
set
given
order
result
osrv
completely
independent
others
set
still
informationally
equivalent
construction
discard
completely
redundant
variables
given
srvs
note
orthogonal
part
srv
srv
msrv
follows
contradiction
negation
srv
consequently
contradicts
since
definition
orthogonal
decomposition
2.2.5
total
synergistic
information
define
total
amount
synergistic
information
stores
contains
words
propose
quantify
synergy
sum
mutual
information
first
making
msrvs
independent
reordering
maximize
msrv
quantity
next
section
prove
several
desired
properties
definition
satisfies
finish
informal
outline
intuition
behind
definition
refer
corresponding
proofs
appropriate
2.2.5.1
outline
intuition
proposed
definition
initial
idea
quantify
synergistic
information
directly
however
found
results
undesired
counting
non-synergistic
information
demonstrate
section
2.4.3
appendix
6.2.1.
two
srvs
taken
together
necessarily
form
srv
meaning
combination
may
store
information
individual
inputs
reason
use
summation
osrvs
intuitively
term
sum
quantifies
unique
amount
synergistic
information
none
terms
quantifies
due
independence
among
osrvs
synergistic
information
doubly
counted
also
argue
appendix
6.2
proving
never
exceeds
hand
possible
type
synergistic
information
ignored
undercounted
seen
fact
fully
redundant
variables
ever
discarded
process
also
prove
example
section
2.3.6
sense
arbitrary
exists
equals
maximum
namely
12
...
iisss
x11
...
.iiiiisixsdsssis
xisiisxiisiisxiisx
xisisisis
i:0ijjsx
iissxisx
iiisssyxsynmax
.iiisiiissxixyiysyx
iyxiisxsynixy
iisiyxxysynixyhxyx
summation
sensitive
ordering
orthogonalization
srvs
reason
maximizing
orderings
possible
presence
synergies
among
srvs
prove
handles
correctly
synergy-among-synergies
i.e.
lead
counting
undercounting
appendix
6.3
2.3
basic
properties
first
list
important
minimal
requirements
definitions
obey
first
four
properties
typically
appear
related
literature
either
implicitly
explicitly
desired
properties
latter
two
properties
direct
consequences
first
principle
use
srvs
encode
synergistic
information
corresponding
proofs
straightforward
sketched
briefly
2.3.1
non-negativity
follows
non-negativity
underlying
mutual
information
function
making
every
term
sum
non-negative
2.3.2
upper-bounded
mutual
information
follows
data-processing
inequality
first
processed
follows
write
understood
denote
element
maximizing
sequence
used
construct
computing
2.3.3
equivalence
class
reordering
arguments
follows
property
underlying
mutual
information
function
sum
2.3.4
zero
synergy
single
variable
synixysyn0.ixysyn
.ixyixyxiisxsyn
iisixyixy1111syn
...
...
...
:.iinnsiniiiiiiixyhsshssyhshssyhshsyisyixyisthiiisxiisiisxsynixysynsyn
reordered
labelings
'.iiiiijiiixyixyijsyn10.ixy
follows
constraint
srv
ignorant
individual
variable
2.3.5
zero
synergy
single
variable
also
follows
constraint
srv
ignorant
individual
variable
terms
sum
necessarily
zero
2.3.6
identity
maximizes
synergistic
information
follows
fact
computed
therefore
completely
redundant
given
term
sum
must
maximal
equal
since
independent
2.4
consequential
properties
list
important
properties
induced
proposed
synergy
measure
along
corresponding
proofs
2.4.1
upper
bound
mutual
information
srv
maximum
amount
mutual
information
entropy
srv
set
variables
derived
analytically
start
case
two
input
variables
i.e.
generalize
maximizing
leads
two
constraints
using
constraint
since
first
term
third
line
change
varying
maximize
minimizing
second
term
since
holds
swapped
reordering
labelling
find
rewritten
generalization
variables
fairly
straightforward
induction
see
appendix
6.1
illustrated
case
one
particular
labeling
x1xsyn10.ixxxsynmax.ixxhxiiissxxxihsiiissxiiisihshxsynixy2x12
ixxs1
0ixs2
0ixs1212121212121
.ixxsixsixsxixsxhxxhxsxhxx1
0ixs21
hxxs12
ixxs21
hxsx21
0hxsx1x2x12211212
,min
.isxxhxxhxxsxx12121212
max
.isxxhxxhxhxsxxn3niix
since
inequality
must
true
labelings
particular
labeling
maximizes
extending
result
find
corollary
suppose
completely
synergistic
i.e.
mutual
information
bounded
follows
finally
assume
srv
efficient
sense
contains
additional
entropy
unrelated
i.e.
would
contain
additional
entropy
orthogonal
decomposition
assumption
distill
dependent
part
exactly
therefore
derived
upper
bound
srv
also
upper
bound
entropy
2.4.2
non-equivalence
srvs
indeed
possible
least
two
non-redundant
msrvs
i.e.
even
words
means
multiple
types
synergistic
relation
equivalent
demonstrated
following
example
uniform
distribution
fact
functions
msrvs
verified
numerically
trying
combinations
also
seen
visually
figure
adding
additional
states
changing
distribution
break
symmetries
needed
stay
uncorrelated
individual
inputs
case
two
msrvs
mutually
independent
whereas
fact
shown
section
2.4.1
since
actually
maximum
possible
mutual
information
srv
store
msrvs
subset
srvs
follows
trivially
srvs
non-equivalent
even
independent
123123122113122121312312213121231
.ixxxsixxsixsxxixsxixsxxhxxhxsxhxxxhxsxxhxxhxxxhxxxhxiix1hxn11
...
...
max
.nniiixxshxxhxsxyxyx1
...
max
.niiixyhxxhxyxx12
ixxshsx121
isshs12
ssx12:0issx12
xxx0,1,2ixpr
19x1122mod3sxx212mod3sxx1s2s12:0iss122
:log31.58isxisxx
figure
values
two
msrvs
mutually
independent
highly
synergistic
two
3-valued
variables
uniformly
distributed
independent
2.4.3
synergy
among
msrvs
combination
two
msrvs
srv
i.e.
otherwise
would
contradiction
would
true
follows
since
would
completely
redundant
given
therefore
discarded
construction
msrv
means
combinations
msrvs
must
necessarily
non-zero
mutual
information
least
one
individual
source
variables
i.e.
violating
since
individual
msrv
zero
mutual
information
individual
source
variable
definition
must
true
non-synergistic
information
results
synergy
among
msrvs
emphasize
type
synergy
among
different
synergy
among
intend
quantify
paper
could
appropriately
considered
synergy
synergies
fact
multiple
msrvs
possible
already
proven
example
used
previous
proof
section
2.4.2.
synergy
among
two
msrvs
example
indeed
easily
verified
whereas
since
msrvs
subset
srvs
follows
also
srvs
synergy-of-synergies
fact
existence
multiple
msrvs
means
necessarily
srvs
synergistic
another
srv
conversely
one
msrv
set
srvs
synergistic
another
srv
corollary
alternatively
quantifying
synergistic
information
using
directly
mutual
information
could
violate
fourth
desired
property
zero
synergy
single
variable
consists
two
msrvs
case
choice
would
non-zero
synergistic
information
undesired
12
ss12
ssx12
ssx12
ssx12
ssx1s2s12
ss12
ss12
,:0iiissxisxixx11:0isx21:0isx1211212
:log3issxissx
iisiyxx12
,:0iiissxiyxx
2.4.4
xor-gate
random
binary
inputs
msrv
lastly
use
definition
synergy
prove
common
intuition
xor-gate
maximally
synergistic
set
i.i.d
binary
variables
bits
suggested
introductory
example
start
case
two
bits
srv
take
entropy
srv
equals
fact
upper
bound
srv
make
completely
redundant
would
prevent
therefore
srv
becoming
msrv
section
2.2.2
possible
another
srv
make
redundant
case
converse
also
true
case
two
srvs
equivalent
example
would
nxor
not-xor
gate
informationally
equivalent
xor
consider
equivalent
srvs
one
general
case
bits
consider
srv
set
xor-gates
easily
verified
contain
mutual
information
individual
bit
indeed
moreover
also
easily
verified
independent
entropy
equals
upper
bound
srv
following
reasoning
two-bit
case
possible
set
xor
gates
necessarily
msrv
e.g.
indeed
msrv
remark
conversely
redundant
given
sets
xor-gates
redundant
given
others
therefore
member
set
construction
2.5
numerical
implementation
implemented
numerical
procedures
compute
part
python
library
named
jointpdf
available
online1
set
discrete
stochastic
variables
represented
matrix
joint
probabilities
dimensions
number
variables
number
possible
values
per
variable
matrix
uniquely
identified
parameters
unit
line
independent
brief
finding
msrv
amounts
numerically
optimizing
subset
bounded
parameters
order
maximize
satisfying
conditions
srvs
approximate
set
osrvs
constructing
iteratively
finding
next
osrv
addition
existing
set
independence
constraint
added
numerical
optimization
procedure
finishes
osrvs
found
optimization
ordering
implemented
restarting
sequence
numerical
optimizations
different
starting
points
taking
result
highest
synergistic
information
orthogonal
decomposition
also
implemented
even
though
used
since
osrv
set
built
directly
using
optimization
procedure
uses
fact
decomposed
part
srv
must
also
srv
assuming
perfect
orthogonal
decomposition
therefore
found
directly
optimization
numerical
optimizations
algorithm
scipy.optimize.minimize
version
0.11.0
used
probability
distribution
extended
set
osrvs
amount
synergistic
information
confidence
interval
due
approximate
nature
numerical
optimizations
https
//bitbucket.org/rquax/jointpdf
12,0,1xx112sxxx1s1s1sn1
...
,0,1nxx121
...
nssss11
...
iisxxsixsxis1hsns13xx12xx23xxxxnmnm1nmspr
xs
isxiisxns11
...
nss11
...
,0nnisss
one
osrvs
may
turn
store
small
amount
unwanted
information
individual
inputs
subtract
unwanted
quantities
mutual
information
term
order
estimate
synergistic
information
osrv
however
subtracted
terms
could
partially
redundant
extent
determined
general
thus
optimal
sequence
osrvs
found
take
lower
bound
estimated
synergistic
information
corresponds
case
subtracted
mutual
information
term
fully
independent
summed
leading
wms
form
hand
corresponding
upper
bound
would
occur
subtracted
mutual
information
terms
would
fully
redundant
case
take
middle
point
bounds
best
estimate
corresponding
measure
uncertainty
defined
relative
error
following
numerical
results
obtained
case
two
input
variables
one
output
variable
joint
probability
distribution
randomly
generated
unless
otherwise
stated
osrv
found
added
distribution
additional
variable
variables
constrained
number
possible
values
state
space
experiments
2.5.1
success
rate
accuracy
finding
srvs
first
result
ability
numerical
algorithm
find
single
srv
function
number
possible
states
per
individual
variable
namely
definition
synergistic
information
relies
perfect
orthogonal
decomposition
showed
perfect
orthogonal
decomposition
impossible
least
one
type
relation
among
binary
variables
appendix
6.5
whereas
previous
work
hints
continuous
variables
might
almost
perfectly
decomposed
section
2.1.4
figure
shows
probability
successfully
finding
srv
variables
state
space
values
success
defined
relative
error
entropy
srv
less
synixysyn1
...
.iijinjixyisxisxsyn1
...
:max
.ijijinixyisxisxsynixysynsyn
max
.ijjijjisxisxixyixy1x2xy12pr
,xxy
figure
effectiveness
numerical
implementation
find
single
srv
input
consists
two
variables
possible
values
x-axis
red
line
dots
probability
srv
could
found
relative
error
randomly
generated
distributions
fact
lowest
binary
variables
consistent
observation
perfect
orthogonal
decomposition
impossible
case
least
one
known
condition
appendix
6.5
fact
converges
consistent
suggestion
orthogonal
decomposition
could
possible
continuous
variables
section
2.1.4
blue
box
plot
expected
relative
error
entropy
single
srv
successfully
found
figure
also
show
expected
relative
error
entropy
srv
successfully
found
relevant
confidence
subsequent
results
values
per
variable
find
relative
error
low
range
1-3
indicating
finding
srv
bimodal
problem
either
successfully
found
relatively
low
error
found
successfully
high
error
values
per
variable
satisfactory
srv
always
successfully
found
indicates
additional
degrees
freedom
aid
finding
srvs
2.5.2
efficiency
single
srv
srv
successfully
found
next
question
much
synergistic
information
actually
contains
compared
maximum
possible
according
upper
bound
minimum
thus
single
added
variable
srv
principle
sufficient
entropy
store
information
however
depending
possible
single
srv
store
synergistic
information
regardless
much
entropy
demonstrated
section
2.4.3.
happens
two
srvs
would
mutually
incompatible
combined
single
large
srv
therefore
show
expected
synergistic
information
single
srv
normalized
corresponding
upper
bound
figure
12pr
,xxy21
hxx12
hxx12pr
decreasing
trend
indicates
incompatibility
among
srvs
plays
significant
role
state
space
variables
grows
would
imply
increasing
number
srvs
must
found
order
estimate
total
synergistic
information
fortunately
figure
also
suggests
efficiency
settles
non-zero
constant
implies
number
needed
srvs
grow
impractical
numbers
figure
synergistic
entropy
single
srv
normalized
theoretical
upper
bound
input
consists
two
randomly
generated
stochastic
variables
possible
values
per
variable
x-axis
srv
constrained
number
possible
values
initial
downward
trend
shows
individual
srvs
become
less
efficient
storing
synergistic
information
state
space
per
variable
grows
apparent
settling
non-zero
constant
suggests
estimating
synergistic
information
require
diverging
number
srvs
found
number
values
per
variable
2.5.3
resilience
implication
synergy
finally
compare
impact
two
types
perturbations
two
types
input-output
relations
namely
case
randomly
generated
versus
case
srv
local
perturbation
implemented
adding
random
vector
norm
0.1
point
unit
hypercube
defines
marginal
distribution
randomly
selected
input
variable
conversely
non-local
perturbation
similarly
applied
keeping
unchanged
impact
quantified
relative
change
mutual
synixy12pr
yxxyx1px2px21pxx1px2px
information
due
perturbation
ask
whether
small
perturbation
disrupts
information
transmission
viewing
communication
channel
significantly
less
susceptible
local
perturbations
figure
show
synergistic
compared
randomly
generated
non-local
perturbations
difference
susceptibility
smaller
still
significant
null-hypothesis
equal
population
median
rejected
local
non-local
perturbations
mood
median
test
p-values
threshold
respectively
difference
susceptibility
local
perturbations
intuitive
srv
zero
mutual
information
individual
inputs
arguably
insensitive
changes
individual
inputs
still
find
non-zero
expected
impact
could
partly
explained
algorithm
relative
error
order
order
relative
impact
found
order
test
intuition
devised
non-local
perturbations
compare
larger
susceptibility
indeed
found
non-local
perturbations
however
remains
unclear
synergistic
variables
still
less
susceptible
non-local
case
compared
randomly
generated
variables
nevertheless
numerical
results
indicate
synergy
plays
significant
role
resilience
noise
relevant
especially
biological
systems
continually
subject
noise
must
resilient
simple
use-case
using
jointpdf
package
estimate
synergies
done
included
appendix
6.7.
figure
left
median
relative
change
mutual
information
perturbing
'local
perturbation
error
bars
indicate
25th
single
input
variable
marginal
distribution
75th
percentiles
perturbation
implemented
adding
random
vector
norm
0.1
point
unit
hypercube
defines
marginal
distribution
bar
based
100
randomly
generated
joint
distributions
synergistic
case
constrained
srv
right
left
except
perturbation
non-local
sense
applied
keeping
unchanged
12
:ixxy12
xxyyy131.21055.5100.0112
:ixxy1px1px12
,pxxyy12
xx21pxx1px2px
discussion
theoretical
work
defining
synergistic
information
uses
pid
framework
informally
stated
requires
synergistic
information
stores
less
information
store
individual
two
types
information
required
sum
quantity
non-negative
terms
approach
incompatible
viewpoint
framework
amount
synergistic
information
makes
statement
amount
individual
information
may
also
store
fact
proposed
synergistic
information
maximized
identity
obviously
also
stores
maximum
information
individual
variables
fact
successful
synergy
measure
found
date
satisfies
pid
framework
led
explore
completely
different
viewpoint
proposed
measure
would
prove
successful
may
imply
decomposition
requirement
strong
synergy
measure
obey
whether
proposed
synergy
measure
used
define
different
notion
decomposition
remains
open
question
find
additional
argument
decomposition
requirement
example
section
2.4.2
2.4.3.
example
demonstrates
two
independent
srvs
zero
mutual
information
exist
synergistic
taken
together
evidently
two
distinct
ways
variable
however
impossible
store
information
srvs
maximum
synergy
still
zero
information
individual
variables
suggests
synergistic
information
individual
information
simply
completely
synergistic
considered
mutually
exclusive
therefore
propose
alternative
viewpoint
whereas
synergistic
information
could
measured
amount
individual
information
could
foreseeably
measured
similar
procedure
instance
set
could
replaced
individual
inputs
procedure
repeated
would
measure
amount
unique
information
stores
individual
inputs
also
stored
combinations
inputs
measure
would
upper
bounded
completely
random
independent
inputs
individual
information
would
upper
bounded
whereas
synergistic
information
would
upper
bounded
srv
quantities
measure
different
fully
independent
aspects
two
measures
relate
subject
future
work
proposed
definition
builds
upon
concept
orthogonal
decomposition
allows
rigorously
define
single
definite
measure
synergistic
information
first
principles
however
research
needed
determine
cases
decomposition
done
exactly
approximately
even
specific
case
would
turn
exactly
computable
due
imperfect
orthogonal
decomposition
definition
still
serve
reference
point
extent
necessary
orthogonal
decomposition
must
approximated
bounded
resulting
amount
synergistic
information
must
also
considered
approximation
bound
=synergy+individualixyyxix
ixysynixyyixsynixysynixxixyxixsynixyiisiixsynixyyhxn1nhx11nhx
final
point
discussion
choice
divide
stochastic
variable
subvariables
crucial
determines
amount
information
synergy
found
choice
strongly
depends
specific
research
question
instance
neurons
brain
may
divided
two
cerebral
hemispheres
many
anatomical
regions
individual
neurons
altogether
level
amount
information
synergy
may
differ
article
concerned
choosing
division
calculate
amount
information
synergy
subvariables
chosen
conclusion
paper
propose
measure
uniquely
quantify
synergistic
information
first
principles
briefly
first
extract
synergistic
entropy
set
variables
constructing
new
set
possible
maximally
synergistic
random
variables
msrvs
denoted
msrv
non-zero
mutual
information
set
individual
zero
mutual
information
set
msrvs
transformed
set
independent
orthogonal
srvs
osrv
denoted
prevent
counting
define
amount
synergistic
information
outcome
variable
set
source
variables
sum
osrv-specific
mutual
information
quantities
proposed
measure
satisfies
important
desired
properties
e.g.
non-negative
bounded
mutual
information
invariant
rearranging
always
zero
synergy
input
single
variable
also
prove
four
important
properties
synergy
measure
particular
derive
maximum
mutual
information
case
srv
demonstrate
synergistic
information
different
types
multiple
independent
srvs
prove
fact
combination
multiple
srvs
may
store
non-zero
information
individual
synergistic
way
latter
property
leads
intriguing
concept
synergy
among
synergies
show
must
necessarily
excluded
quantifying
synergy
might
turn
interesting
subject
study
right
finally
provide
software
implementation
proposed
synergy
measure
ability
quantify
synergistic
information
arbitrary
multivariate
setting
necessary
step
better
understand
dynamical
systems
implement
complex
information
processing
capabilities
proposed
framework
based
srvs
orthogonal
decomposition
provides
new
line
thinking
produces
general
synergy
measure
important
desired
properties
initial
numerical
experiments
suggest
synergistic
relations
less
sensitive
noise
important
property
biological
social
systems
studying
information
synergy
complex
adaptive
systems
certainly
lead
substantial
new
insights
various
emergent
behaviors
ranging
acknowledgements
authors
acknowledge
financial
support
future
emerging
technologies
fet
program
within
seventh
framework
programme
fp7
research
european
commission
fet-
proactive
grant
agreement
topdrim
number
fp7-ict-318121
well
financial
support
future
emerging
technologies
fet
program
within
seventh
framework
programme
fp7
research
european
commission
fet-proactive
grant
agreement
xiixxiixxxxxixiisxyx
isiiisxisyxyixyx
sophocles
number
fp7-ict-317534
pmas
acknowledges
support
russian
scientific
foundation
project
number
14-21-00137
appendix
6.1
upper
bound
possible
entropy
srv
induction
6.1.1
base
case
base
case
true
proven
section
2.4.1
6.1.2
induction
step
prove
base
case
induces
must
chosen
maximizes
maximization
term
understood
maximize
label
orderings
first
subvariables
note
upper
bound
relation
must
true
choices
orderings
labels
since
labeling
arbitrary
due
desired
property
section
2.3.3
therefore
possible
ordering
minimal
necessarily
also
satisfies
simultaneous
instances
inequality
one
satisfies
constraining
inequality
i.e.
r.h.s
inequalities
r.h.s
minimized
case
must
satisfy
overall
maximum
part
subset
words
inequality
minimal
r.h.s
true
substituting
find
indeed
6.2
overcount
synergistic
information
synergistic
information
store
encoded
set
srvs
informationally
equivalent
i.e.
equal
entropy
zero
conditional
entropy
therefore
upper
bound
since
otherwise
synergistic
information
must
doubly
counted
section
derive
111111
...
...
maxnniniixxshxxhx2n111
...
...
maxnniniixxshxxhxsx1111111111111111111111111
...
...
...
...
max
...
...
max
...
...
...
max
...
nnnnnnininnnininnnnnininnixxxsixxsixsxxhxxhxixsxxhxxhxhxxxhxsxxhxxhxhxsxx111
...
max.ninihxxhx1n11
...
nxx1iinxns
nixihx11
...
nxx111maxmax.iniinihxhx111
...
...
max.nniniixxshxxhxsynixyyxxiisx
iisiyxsynixy
appendix
6.2.1
use
derivation
demonstrate
positive
difference
undesirable
least
cases
start
proof
case
consists
two
osrvs
taken
base
case
reader
see
derivation
extends
increasing
induction
proof
induction
also
work
case
provide
proof
let
consist
arbitrary
number
osrvs
let
denote
first
osrvs
let
defined
using
instead
i.e.
first
terms
sum
use
property
construction
similarly
use
independence
properties
syn
iisixyiyxsyn
iisiyxixysyn
iisixyiyxiisx2n3nnn1
...
iinsxssn1
...
iinsnxssnnnsynnixyiisnxiisxn2n121hsshsiisx122121121121211211212syn2
.iisnniyxiyssiysiyssiysiyshsyhsshsyshsiysiyshsyhsysiysiysixy3n121hsshs12312
,hssshss123312131212112121312121123121231121212312123s
iisniyxiysssiysiyssiysssiysiyshsyhsshsyshsiysssiysiyshsyhsysiysssiysiysiyshsyhsyshssyhssyshsshsssiyn112121233syn3
,.nnxyhsyhsyshssyhssysixy
proceeds
rewriting
conditional
mutual
information
term
essentially
proof
mutual
information
term
plus
four
entropy
terms
third
equality
two
cancel
remaining
two
terms
summed
non-negative
thus
induction
thus
find
possible
proposed
exceed
mutual
information
suggests
overcount
synergistic
information
6.2.1
also
includes
non-synergistic
information
derivation
previous
section
observe
conversely
exceed
proceed
show
undesirable
least
cases
positive
difference
must
arise
one
non-negative
terms
square
brackets
derivations
suppose
therefore
zero
information
individual
osrv
definition
section-2.2.1
correlate
possible
synergistic
relation
srv
view
thus
said
store
zero
synergistic
information
however
even
though
construction
necessarily
imply
among
others
therefore
term
square
brackets
still
positive
words
possible
cooperate
synergy
one
osrvs
non-zero
mutual
information
another
osrv
concrete
example
given
section
2.4.3.
would
lead
non-zero
synergistic
information
quantified
undesirable
view
contrast
proposed
definition
purposely
ignores
synergy-
of-synergies
fact
always
yield
case
desirable
view
proved
section
2.3.5.
n12123
,hsshsss12123
hssyhssys1111111111111111112123syn11
...
...
...
...
...
...
...
...
...
:iinsnnnnnnnnnnnnnnnniyxiyssiyssiysssiyssiyshssyhssyshsshsssiyssiyshssyhssysixyiyshs2123syn
.nsyhssysixysynixy
iisiyxsynixy
iisiyx
iisiyxsynixysyn
iisiyxixyiyxiyxxiyxx
|iiihsyhs1111
...
...
,nnnhsyhssysiyx
iisiyxsynixysyn0ixyiyx
6.3
synergy
measure
correctly
handles
synergy-of-synergies
among
srvs
correctly
handled
mean
synergistic
information
neither
overcounted
undercounted
already
start
conjecture
non-synergistic
redundancy
among
pair
srvs
lead
overcounting
synergistic
information
suppose
consider
non-synergistic
mutual
information
correlates
one
neither
srv
optimal
ordering
trivial
correlates
ordering
assuming
respective
parallel
parts
see
section
2.2.5
informationally
equivalent
matter
one
retained
respective
orthogonal
parts
retained
case
therefore
proceed
handle
case
synergy
among
srvs
first
illustrate
apparent
problem
handle
section
suppose
suppose
words
construction
pair
synergistically
makes
fully
redundant
non-synergistic
redundancy
among
srvs
exists
finally
let
first
sight
appears
possible
happens
constructed
using
ordering
appears
unwanted
part
used
compute
i.e.
term
disappears
sum
potentially
leads
constribution
synergistic
information
ignored
appendix
show
contribution
always
counted
towards
construction
possibility
individual
term
disappear
synergistic
information
already
accounted
first
interpret
synergistic
mutual
information
set
srvs
another
single
srv
hyperedge
hypergraph
example
would
hyperedge
let
weight
hyperedge
equal
mutual
information
pair
subsection
6.3.1
prove
setting
one
hyperedge
hyperedge
possible
srvs
one
srv
implies
subsets
remaining
srv
weight
hypergraph
forms
fully
connected
clique
three
hyperedges
setting
finding
correct
ordering
translates
letting
appear
appeared
case
hyperedge
translates
traversing
path
steps
hyperedges
reverse
order
time
choosing
one
srv
ancestor
set
already
previously
chosen
srv
either
words
case
ancestor
srvs
chosen
zero
mutual
information
ordering
last
element
suffice
correlates
srvs
one
srvs
partially
discarded
order
maximization
process
desirable
otherwise
could
exceed
even
intuitively
correlates
srvs
automatically
correlates
srv
well
due
redundancy
among
srvs
counting
synergistic
information
would
overcounting
redundancy
leading
violation
boundedness
mutual
information
12:0issyiisx123
,xsss1233
:issshs
::0ijijiss12
ss3s3syiisxiis3s1s2s3siisxsynixy3
iys3ssynixy3
iys1n112
ss3s1n1n123
,xsssns11
...
nss11
...
nnsss:0niysnyis:0iiysisysynixysynixy
ixyhyy1nthn
example
demonstrates
phenomenon
given
consisting
three
i.i.d
binary
variables
four
pairwise-independent
msrvs
namely
three
pairwise
xor
functions
one
nested
xor-of-xor
function
verified
numerically
however
one
pairwise
xor
synergistically
fully
redundant
given
two
pairwise
xors
entropy
equals
taking
e.g
yields
indeed
bits
synergistic
information
according
proposed
definition
correctly
discarding
synergistic
redundancy
among
four
srvs
however
synergistically
redundant
srv
would
discarded
sum
would
find
bits
synergistic
information
counterintuitive
exceeds
intuitively
fact
correlates
two
pairwise
xors
necessarily
implies
also
correlates
third
pairwise
xor
redundant
correlation
counted
6.3.1
synergy
among
srvs
forms
clique
given
particular
set
srvs
arbitrary
order
suppose
set
fully
synergistic
i.e.
first
assume
assumption
dropped
subsection
question
also
synergistic
prove
fact
indeed
synergistic
exactly
amount
i.e.
following
proof
thus
case
two
variables
synergistic
third
trivially
generalizes
variables
case
condition
also
generalized
variables
first
find
given
condition
leads
known
quantities
two
conditional
mutual
information
terms
use
derive
different
combination
third
combination
derived
similarly
conclusion
find
set
srvs
synergistically
stores
mutual
information
amount
subsets
srvs
store
exactly
synergistic
information
respective
remaining
srv
synergistic
mutual
information
123
,xxxx3hxhxyxsynixyyxhxhy
ixyyx12
ss3s123
:0isssd
::0ijijiss23
ss1s13
ss2s232132
:isssisssdn
::0ijijiss1n123
:0isssd1231323123132231132
:.isssississsississsdisssisssd132
:isss1321232132131213211131212313132
.isssississsisshsshsshssshshshsshsshsssississsd11
...
snsnsd1n1
...
nss
set
srvs
another
srv
considered
directed
resulting
hypergraph
srvs
clique
hyperedge
hypergraph
6.3.2
generalize
partial
synergy
among
srvs
assumed
remove
constraint
thus
let
mutual
informations
general
arbitrary
proceed
first
see
appears
account
difference
mutual
information
quantities
among
obtained
mutual
information
among
variables
correction
term
variables
6.4
independence
two
decomposed
parts
first
constraint
follows
used
shorthand
resulting
combined
second
constraint
must
independent
namely
follows
1n11
...
nss
::0ijijiss21n1231323123132
:isssississsississsd1321232112321312132112321131212312321313212332133213
:.isssississsississhsshsshssshsississhshsshsshsssississississsisssississdississdn1n:0iba
,||
||log||||log|||log||log|||log|log|:0.abbabbabbabbabbabbpapbapbbibapapbapbbpapbpbapbbpapbapbbpbpbbpapbapbbpapbapbbpbapbpbbpbpbbpapbapbapbibbhbaprpbabbaa
ibbhba
:ibbibahbhbabb
impossibility
decomposition
binary
variables
6.5
consider
stochastic
binary
variables
orthogonal
decomposition
imposes
constraints
always
satisfied
perfectly
binary
case
show
next
use
following
model
particular
show
computed
without
storing
information
violating
orthogonality
condition
supposedly
independent
encode
dependence
fully
encoded
two
parameters
intuitively
case
binary
variables
indirectly
storing
information
insufficient
number
degrees
freedom
store
information
without
also
possible
explanation
binary
case
satisfy
condition
must
true
therefore
among
others
let
find
conditions
equality
conditions
satisfying
equality
either
first
condition
describes
trivial
case
independent
second
condition
less
trivial
severely
constrains
relations
fact
constrains
mutual
information
exactly
zero
regardless
show
next
using
shorthand

.ibbbibbibbbibbibbhbbhbbhbbbhbibbibbhbbhbbhbhbibbibb
,0,1abb
dabbbbbab,1.abpappbxaxpbbaabb.xcpbxbxpbba:0ibaprprbabpr11pr1bab10101010101010pr11pr1
.bcbcabcbcabcbcbcbcaabcbcbcbcbcbcbabpppppppppppppppppppppppppppppppp12bp101ccppbaba
ibbapbp
ababpbpppp
using
substitution
algebra
steps
verified
indeed
simplifies
zero
extending
parameters
also
depend
would
certainly
possible
add
degrees
freedom
however
create
non-zero
conditional
mutual
information
soon
calculated
extra
parameters
summed
certain
parameters
demonstrated
lead
zero
mutual
information
orthogonality
constraint
exists
perfect
result
demonstrates
class
correlated
binary
variables
orthogonal
decomposition
impossible
choices
binary
decomposition
indeed
possible
exist
trivial
independent
case
exactly
numerous
cases
currently
unknown
especially
number
possible
states
per
variable
increased
6.6
wyner
common
variable
satisfies
orthogonal
decomposition
wyner
common
variable
defined
non-trivial
minimization
procedure
namely
means
minimization
considers
random
variables
make
independent
i.e.
wyner
showed
general
show
cases
equality
condition
actually
reached
satisfies
three
orthogonal
decomposition
conditions
involve
also
wyner
leaves
undefined
therefore
work
satisfy
conditions
shows
least
one
potential
method
computing
two
starting
conditions
second
condition
follows
similarly
01000101001000001111
log
1log
log
log
log
log
ccccccccccccccccccccpbpppppbpppbppppbppppbpppppbbpbppip101ccpp
ibbxcpa
ibba
ibbxcpababwb
wibabiabargmin
:wawbbiabwawbwab:0iabw
:wibaibawbbbb
,:0.wwibabiabiabb
.wwwwwiabbiabiabbiabbiab
first
condition
derive
follows
firstly
implies
non-spuriousness
condition
last
line
combining
either
find
respectively
parallel
parsimony
conditions
concluding
proof
6.7
use-case
estimating
synergy
using
provided
code
code
run
using
python
interface
example
suppose
particular
probability
distribution
given
two
input
stochastic
variables
three
possible
values
generate
random
probability
distribution
follows
jointpdf
import
jointprobabilitymatrix
randomly
generated
joint
probability
mass
function
discrete
stochastic
variables
possible
values
p_ab
jointprobabilitymatrix
2,3
add
fully
redundant
fully
correlated
output
variable
follows
append
third
variable
deterministically
computed
i.e.
p_ab.append_redundant_variables
p_abc
p_ab
rename
clarity
finally
compute
synergistic
information
following
command

.wwwwwibabibbibabibabibb
wwwwwwwibabiabibbibabibabibabiabibbaibab
so:0
:0.wwwwibbaibabibbaibab
.wwiabiabiabibbsyn
iabc
compute
information
synergy
contains
p_abc.synergistic_information
0,1
jointpdf
package
also
easy
marginalize
stochastic
variables
joint
distribution
add
variables
using
various
constraints
compute
various
information-theoretic
quantities
estimate
distributions
data
samples
implemented
discrete
variables
details
found
website
https
//bitbucket.org/rquax/jointpdf
references
shannon
weaver
mathematical
theory
communication
university
illinois
press
1963
watanabe
information
theoretical
analysis
multivariate
correlation
ibm
res
dev
1960
66–82
williams
beer
nonnegative
decomposition
multivariate
information
arxiv
prepr
arxiv10042515
2010
timme
alford
flecker
beggs
synergy
redundancy
multivariate
information
measures
experimentalist
perspective
comput
neurosci
2014
119–140
schneidman
bialek
berry
synergy
redundancy
independence
population
codes
neurosci
2003
11539–11553
griffith
chong
james
ellison
crutchfield
intersection
information
based
common
randomness
entropy
2014
1985–2000
lizier
flecker
williams
towards
synergy-based
approach
measuring
information
modification
artificial
life
alife
2013
ieee
symposium
ieee
2013
43–51
bertschinger
rauh
olbrich
jost
shared
information—new
insights
problems
decomposing
information
complex
systems
proceedings
european
conference
complex
systems
2012
springer
2013
251–269
olbrich
bertschinger
rauh
information
decomposition
synergy
entropy
2015
3501
10.
griffith
koch
quantifying
synergistic
mutual
information
guided
self-organization
inception
springer
2014
159–190
11.
wibral
lizier
priesemann
bits
brains
biologically
inspired
computing
front
robot
2015
12.
bertschinger
rauh
olbrich
jost
quantifying
unique
information
entropy
2014
2161–2183
13.
wyner
common
information
two
dependent
random
variables
ieee
trans
inf
theory
1975
163–179
14.
witsenhausen
values
bounds
common
information
two
discrete
random
variables
siam
appl
math
1976
313–333
15.
liu
chen
wyners
common
information
continuous
random
variables-a
lossy
source
coding
interpretation
information
sciences
systems
ciss
2011
45th
annual
conference
ieee
2011
1–6
16.
gács
körner
common
information
far
less
mutual
information
probl
control
inf
theory
1973
149–162
17.
karhunen
zur
spektraltheorie
stochastischer
prozesse
ann
acad
sci
fenn
ser
math-phys
1946
1946
18.
loeve
probability
theory
graduate
texts
mathematics
springer-verlag
1994
vol
19.
phoon
huang
quek
simulation
strongly
non-gaussian
processes
using
karhunen–loeve
expansion
probabilistic
eng
mech
2005
188
198
20.
cover
thomas
elements
information
theory
wiley-interscience
1991
vol
