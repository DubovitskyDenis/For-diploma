conditional
dependence
via
shannon
capacity
axioms
estimators
applications∗
weihao
gao†
sreeram
kannan‡
sewoong
oh§
pramod
viswanath¶
abstract
consider
axiomatically
problem
estimating
strength
conditional
dependence
rela-
tionship
random
variables
random
variable
applications
determining
strength
known
causal
relationship
strength
depends
conditional
distri-
bution
eﬀect
given
cause
driving
distribution
cause
shannon
capacity
appropriately
regularized
emerges
natural
measure
axioms
examine
problem
calculating
shannon
capacity
observed
samples
propose
novel
ﬁxed-k
nearest
neigh-
bor
estimator
demonstrate
consistency
finally
demonstrate
application
single-cell
ﬂow-cytometry
proposed
estimators
signiﬁcantly
reduce
sample
complexity
introduction
axiomatic
study
dependence
measures
joint
distributions
two
random
variables
long
history
statistics
sha48
r´en59
csi08
paper
study
relatively
unexplored
terrain
measures
depend
conditional
distribution
motivated
study
conditional
dependence
measures
problem
causal
strength
estimation
causal
learning
basic
problem
many
areas
scientiﬁc
learning
one
wants
uncover
cause-eﬀect
relationship
usually
using
interventions
sometimes
directly
observational
data
pea09
re15
mpj+15
paper
interested
even
simpler
question
given
causal
relationship
one
measure
strength
relationship
problem
arises
many
contexts
example
one
may
know
causal
genetic
pathways
subset
maybe
active
particular
tissue
organ
therefore
deducing
much
inﬂuence
causal
link
exerts
becomes
necessary
focus
simple
model
consider
pair
random
variables
known
causal
direction
suppose
confounders
interested
quantifying
causal
inﬂuence
denote
causal
inﬂuence
quantity
two
philosophically
distinct
ways
model
quantity
ﬁrst
one
factual
inﬂuence
i.e.
much
inﬂuence
exert
current
probability
cause
second
possible
way
one
term
potential
inﬂuence
measures
much
inﬂuence
potentially
exert
without
cognizance
present
distribution
cause
example
consider
hypothetical
city
smokers
smoking
inevitably
leads
lung-cancer
city
factual
inﬂuence
smoking
lung-cancer
small
potential
inﬂuence
high
depending
setting
one
may
prefer
former
latter
paper
interested
potential
inﬂuence
cause
eﬀect
want
invariant
scaling
one-one
transformations
variables
naturally
suggests
information
theoretic
metrics
plausible
choices
starting
mutual
information
pxy
||px
least
case
factual
inﬂuence
measures
information
channel
given
prior
observe
metric
email
wgao9
illinois.edu
∗parts
manuscript
appeared
international
conference
machine
learning
icml
2016
†department
‡department
electrical
engineering
university
washington
email
ksreeram
uw.edu
§department
industrial
enterprise
engineering
university
illinois
urbana-champaign
email
swoh
illinois.edu
¶department
urbana-champaign
urbana-champaign
computer
computer
engineering
engineering
university
university
electrical
electrical
illinois
illinois
email
pramodv
illinois.edu
symmetric
respect
directions
property
always
desirable
fact
measure
taken
starting
point
develop
axiomatic
approach
studying
causal
strength
general
graphs
jbgw+13
recent
work
ksm+14
potential
causal
inﬂuence
posited
relevant
metric
spot
trends
gene
pathways
particular
application
considered
rare
biological
states
gene
given
data
may
nevertheless
correspond
important
biological
states
become
common
diﬀerent
biological
conditions
therefore
important
causal
measures
sensitive
cause
distribution
depend
relationship
cause
eﬀect
quantify
potential
inﬂuence
rare
following
approach
proposed
replace
observed
distribution
uniform
distribution
calculate
mutual
information
joint
distribution
resulting
causal
strength
quantiﬁcation
|x||pu
represents
distribution
output
channel
input
given
call
quantiﬁcation
uniform
mutual
information
umi
pronounced
you-me
key
challenge
compute
quantity
i.i.d
samples
statistical
eﬃcient
manner
especially
channel
output
continuous
valued
potentially
high
dimensions
ﬁrst
focus
point
paper
umi
invariant
bijective
transformations
since
uniform
distribution
diﬀerent
uniform
distribution
also
sensitive
estimated
support
size
even
fundamentally
unclear
one
would
prefer
uniform
prior
measure
potential
inﬂuence
channel
based
natural
axioms
data
processing
additivity
motivate
alternative
measure
causal
strength
largest
amount
information
sent
channel
namely
shannon
capacity
formally
maxqx
|x||qx
represents
distribution
output
channel
input
given
refer
quantiﬁcation
capacitated
mutual
information
cmi
pronounced
see-me
key
challenge
compute
quantity
i.i.d
samples
statistical
eﬃcient
manner
especially
channel
output
continuous
valued
potentially
high
dimensions
second
focus
point
paper
make
following
main
contributions
paper
umi
estimation
construct
novel
estimator
compute
umi
data
sampled
i.i.d
distribution
pxy
estimator
brings
together
ideas
three
disparate
threads
statistical
esti-
mation
theory
nearest-neighbor
methods
correlation
boosting
idea
estimation
standard
mutual
information
samples
ksg04
importance
sampling
estimator
single
hyper
parameter
number
nearest-neighbors
considered
set
practice
uses
oﬀ-
the-shelf
kernel
density
estimator
strong
connections
entropy
estimator
kl87
main
technical
result
show
estimator
consistent
probability
supposing
radon-nikodym
derivative
dpu
uniformly
bounded
support
simulations
dpx
estimator
strong
performance
terms
sample
complexity
compared
baseline
partition-based
estimator
mod89
cmi
estimation
build
upon
estimator
derived
umi
construct
optimization
problem
mimics
optimization
problem
inherent
computing
capacity
directly
conditional
probability
distribution
channel
main
technical
result
show
consistency
estimator
supposing
radon-nikodym
derivative
dpq
uniformly
bounded
support
dpx
optimizing
input
channel
simulation
results
show
strong
empirical
performance
compared
baseline
partition-based
method
followed
discrete
optimization
application
gene
pathway
inﬂuence
ksm+14
considered
important
result
single-cell
ﬂow-cytometry
data
analysis
causal
strength
metric
termed
dremi
proposed
measuring
causal
inﬂuence
gene
estimator
speciﬁc
way
implementing
umi
along
channel
ampliﬁcation
step
dremi
successfully
used
spot
gene-pathway
trends
show
proposed
cmi
umi
estimators
also
exhibit
performance
dremi
supplied
full
dataset
time
signiﬁcantly
smaller
sample
complexity
performance
axiomatic
approach
formally
model
inﬂuence
measure
conditional
probability
distributions
postulating
ﬁve
natural
axioms
let
drawn
alphabet
alphabet
let
probability
distribution
given
given
let
family
conditional
distributions
usually
consider
case
set
possible
conditional
distributions
inﬂuence
measure
function
conditional
distribution
non-negative
real
numbers
y|x
write
postulate
function
satisﬁes
ﬁve
axioms
show
cmi
satisﬁes
ﬁve
axioms
independence
measure
=y|x=x
depends
data
processing
processing
chain
i.e.
pz=z|x=x
cid:80
y∈y
pz=z|y
=ypy
=y|x=x
natural
data
processing
inequalities
hold
pz|x
pz|y
pz|x
additivity
parallel
channel
py1
y2|x1
py1|x1py2|x2
need
py1
y2|x1
py1|x1
py2|x2
monotonicity
causal
relationship
strong
many
possible
values
achievable
varying
input
probability
distribution
thus
consider
map
probability
simplex
probability
simplex
larger
range
map
stronger
causal
strength
depend
range
map
range
convex
hull
output
monotonic
function
range
map
distributions
|x=x
range
range
maximum
value
maximum
value
possible
conditional
distributions
particular
output
alphabet
achieved
exactly
relationship
fully
causal
i.e.
achieved
setting
begin
exploration
appropriate
inﬂuence
measures
alphabets
discrete
let
pxy
pxy
||px
denote
mutual
information
respect
joint
distribution
pxy
since
looking
potential
inﬂuence
measures
shannon
capacity
deﬁned
maximum
input
probability
distributions
mutual
information
natural
choice
cmi
max
ﬁrst
claim
following
proposition
cmi
satisﬁes
axioms
causal
inﬂuence
proof
proof
fairly
straightforward
clearly
axiom
holds
chapter
ct12
axiom
suppose
cmi
pz|x
achieved
consider
joint
distribution
pz|y
utilizing
data-processing
inequality
mutual
information
get
cmi
max
pz|x
cmi
pz|x
thus
axiom
satisﬁed
consider
axiom
joint
distribution
let
marginal
cmi
pz|y
max
pz|y
pz|x
cmi
pz|x
pz|y
axiom
standard
result
shannon
capacity
refer
interested
reader
chapter
ct12
axiom
first
rewrite
capacity
equivalently
information-centroid
see
cs04
cmi
max
min
min
min
max
max
cid:107
|px
cid:107
|px
|x=x
cid:107
conditional
divergence
cid:107
deﬁned
usual
way
px|z
x|z
y|z
px|z
x|z
log
cid:107
cid:88
cid:88
augment
input
alphabet
one
input
symbol
cid:48
|x=x
cid:48
cid:80
characterization
allows
make
observation
capacity
function
convex
hull
probability
distributions
|x=x
given
conditional
probability
distribution
αxpy
|x=x
convex
combination
conditional
distributions
claim
capacity
new
channel
unchanged
one
direction
obvious
i.e.
new
channel
capacity
greater
equal
original
channel
since
adding
new
symbol
decrease
capacity
show
direction
use
observe
due
convexity
divergence
arguments
get
cid:88
|x=x
cid:48
cid:107
αxpy
|x=x
cid:107
αxd
|x=x
cid:107
max
|x=x
cid:107
cid:88
thus
shannon
capacity
function
convex
hull
range
map
satisfying
axiom
function
monotonic
directly
thus
satisfying
axiom
axiom
ﬁxed
output
alphabet
clear
maxx
cmi
log
|y|
suppose
conditional
distribution
cmi
log
|y|
implies
optimizing
input
distribution
log
|y|
implies
log
|y|
thus
deterministic
function
essential
support
since
log
|y|
implies
uniform
distribution
deterministic
function
onto
axiomatic
view
umi
consider
alternative
metric
uniform
mutual
information
umi
deﬁned
mutual
information
uniform
input
distribution
uniform
distribution
estimator
motivated
recent
work
ksm+14
investigate
estimator
fares
terms
proposed
axioms
umi
umi
clearly
satisﬁes
axiom
also
satisﬁes
axioms
data-processing
inequality
mutual
information
joint
distribution
pz|y
implies
pz|x
umi
umi
pz|x
thus
pz|y
pz|x
umi
however
satisfy
axiom
general
however
transition
matrices
pz|y
doubly
stochastic
straightforward
calculation
shows
umi
satisﬁes
axiom
umi
satisﬁes
axiom
since
uniform
distribution
naturally
factors
ux1
ux1
ux2
umi
py1
y2|x1
ux1
py1
y2|x1
ux1
ux2py1|x1py2|x2
umi
py1|x1
umi
py2|x2
umi
satisfy
axiom
since
multiple
repeated
values
|x=x
alter
convex
hull
alters
umi
value
interestingly
umi
satisfy
axiom
reason
cmi
2.1
real-valued
alphabets
real-valued
shannon
mutual
information
ﬁnite
without
additional
regularizations
also
true
measures
relation
renyi
correlation
r´en59
case
measure
studied
context
form
penalty
term
typically
done
via
cost
constraint
real-valued
input
parameters
context
one
possibility
consider
following
norm-constrained
optimization
ensure
causal
eﬀect
ﬁnite
valued
cmi
max
cid:107
cid:107
2≤a
cid:80
i=1
cid:107
cid:107
practice
chosen
empirical
moments
samples
sam-
ples
regularization
turns
so-called
power
constraint
input
common
treatments
additive
noise
communication
channels
estimators
although
deﬁnition
umi
cmi
seamlessly
applies
discrete
continuous
random
variables
estimation
becomes
relatively
straightforward
discrete
estimation
conditional
distribution
computation
umi
cmi
separated
straightforward
manner
reason
also
due
application
genomic
biology
study
focus
challenging
regime
continuous
due
certain
subtleties
estimation
process
provide
separate
estimators
customized
case
discrete
continuous
respectively
3.1
uniform
mutual
information
idea
applying
umi
infer
strength
conditional
dependence
ﬁrst
proposed
ksm+14
oﬀ-the-shelf
2-dimensional
kernel
density
estimators
kde
used
ﬁrst
estimate
joint
distribution
pxy
given
samples
subsequently
channel
computed
directly
joint
distribution
umi
computed
via
either
numerical
integration
sampling
approach
suﬀers
known
drawbacks
kde
sensitivity
choice
bandwidth
increased
bias
higher
dimensional
however
critical
challenge
using
kde
estimate
joint
distribution
points
samples
overkill
nature
need
compute
single
functional
umi
joint
distribution
could
principle
computed
eﬃciently
directly
samples
clear
directly
estimate
umi
perhaps
surprisingly
bring
together
ideas
three
topics
statistical
estimation
introduce
novel
estimators
also
provably
convergent
estimator
based
k-nearest
neighbor
estimators
e.g
kl87
correlation
boosting
idea
estimator
ksg04
–which
widely
adopted
practice
kbg+07
importance
sampling
techniques
adjust
uniform
prior
umi
explain
idea
consider
simpler
task
computing
mutual
information
samples
several
approaches
exist
estimation
pan03
ksg04
wkv09
pps10
srhi10
pxs12
gsg14
gsg15
kkpw15
note
three
applications
entropy
estimator
bdgvdm97
gives
estimate
mutual
information
i.e
cid:98
cid:98
cid:98
cid:98
entropy
term
computed
using
example
kde
based
approach
suﬀers
challenges
umi
alternatively
bypass
estimating
pxy
every
point
diﬀerential
entropy
estimation
done
via
nearest
neighbor
knn
methods
pioneering
work
kl87
entropy
estimator
provides
ﬁrst
step
designing
umi
estimator
however
taking
route
estimating
mutual
information
via
estimating
three
diﬀerential
entropies
two
marginals
one
joint
entirely
unclear
estimate
two
quantities
diﬀerential
entropy
directly
samples
perhaps
surprisingly
innovative
approach
undertaken
ksg04
improve
upon
three
applications
cid:107
xjk
yjk
k-th
nearest
neighbor
precisely
ksg
estimator
cid:98
estimators
provides
solution
ksg
estimator
ksg04
based
knn
distance
deﬁned
distance
k-th
nearest
neighbor
cid:96
distance
i.e
max
cid:107
xjk
cid:107
cid:107
yjk
cid:88
i=1
cid:0
cid:1
digamma
function
cid:48
large
log
knn
statistics
deﬁned
cid:88
cid:88
cid:54
cid:54
cid:107
cid:107
cid:107
cid:107
note
number
nearest
neighbors
computed
respect
joint
space
innovative
idea
gives
simple
estimator
also
advantage
canceling
correlations
three
entropy
estimates
giving
improved
performance
however
despite
popularity
practice
due
simplicity
convergence
result
known
recently
gov16
showed
consistency
rate
convergence
properties
inspired
innovative
mutual
information
estimator
combine
importance
sampling
tech-
niques
adjust
uniform
prior
umi
propose
novel
estimator
top
provable
convergence
estimator
one
hyper-parameter
besides
choice
bandwidth
esti-
mating
marginal
distribution
signiﬁcantly
simpler
task
compared
estimating
joint
number
nearest
neighbors
consider
practice
set
small
integer
continuous
propose
novel
umi
estimator
based
kraskov
mutual
information
estimator
conditional
probability
density
want
compute
uniform
mutual
information
i.i.d
samples
generated
prior
umi
estimator
based
nearest
neighbor
knn
statistics
given
choice
samples
cid:16
cid:88
i=1
cid:17
cid:91
umi
log
cdxcdy
cdx+dy
rdx
rdy
self-normalized
importance
sampling
estimate
cmmr12
volume
d-dimensional
unit
ball
estimate
use
standard
kernel
density
estimator
bandwidth
cid:80
cid:0
cid:1
j=1
cid:88
cid:16
cid:17
hdx
i=1
deﬁne
knn
statistics
follows
sample
calculate
euclidean
distance
opposed
cid:96
distance
proposed
ksg04
k-th
nearest
neighbor
determines
random
number
samples
within
ﬁrst
deﬁned
euclidean
distance
second
weighted
number
samples
within
cid:88
cid:54
wji
cid:107
cid:107
compared
ﬁrst
exchange
log
function
digamma
functions
step
especially
crucial
proving
convergence
use
ideas
importance
sampling
introduce
new
variables
capture
correction
mismatch
prior
constants
cdx
cdy
cdx+dy
correct
volume
measured
cid:96
discrete
similarly
discrete
random
variable
joint
probability
density
function
denoted
y|x
propose
umi
estimator
overload
notation
discrete
case
cid:16
cid:88
i=1
cid:91
umi
wxi
log
nxi
nyi
cid:17
nxi
cid:54
number
samples
wxi
self-normalizing
estimate
|x|px
deﬁned
|x|nx
weighted
knn
statistics
deﬁned
follows
sample
let
distance
k-th
nearest
neighbor
samples
value
considered
euclidean
distance
measured
deﬁne
weighted
number
samples
within
cid:107
cid:107
wxj
cid:88
cid:54
3.2
capacitated
mutual
information
given
standard
estimators
mutual
information
entropy
clear
directly
estimate
cmi
changed
unknown
optimal
input
distribution
however
combining
mutual
information
estimator
importance
sampling
techniques
design
novel
estimator
solution
optimize
space
weights
estimator
one
hyper-parameter
number
nearest
neighbors
consider
continuous
conditional
distribution
compute
estimate
cmi
i.i.d
samples
generated
prior
introduce
novel
cmi
estimator
based
umi
estimator
given
choice
samples
estimated
cmi
solution
following
constrained
optimization
cid:88
cid:16
cid:17
cid:91
cmi
max
w∈ta
log
cdx
cdy
cdx+dy
iny
i=1
ond
moment
constraint
i.e
rn|wi
1/n
cid:80
deﬁned
optimize
sec-
i=1
cid:107
cid:107
observe
kde
needed
cmi
estimation
making
particularly
simple
robust
discrete
similarly
deﬁne
cmi
estimate
cid:91
cmi
solution
following
constrained
optimization
i=1
1/n
cid:80
cid:88
size
i.e
cid:8
mi∆
|x|
cid:12
cid:12
1/n
cid:80
|x|
/c1
cid:101
cid:9
quantization
crucial
proving
consistence
theorem
deﬁned
set
quantized
version
interval
step
x=1
|x|∆
|x|∆
cid:100
cid:91
cmi
max
w∈t∆
log
iny
cid:16
cid:17
wxi
i=1
convergence
guarantees
show
proposed
umi
cmi
estimators
consistent
typical
assumptions
distribution
consistency
estimators
large
sample
limit
generally
basic
ﬁrst
step
understanding
properties
ﬁxed-k
nearest
neighbor
based
estimators
far
know
estimator
based
ﬁxed-k
nearest
neighbors
known
consistent
entropy
estimator
kl87
convergence
rate
known
univariate
case
tvdm96
signiﬁcant
assumptions
univariate
density
result
consistency
umi
estimator
discrete
alphabet
marks
another
instance
consistency
ﬁxed-k
nearest
neighbor
based
estimators
established
uniform
mutual
information
estimators
use
oﬀ-the-shelf
kernel
density
estimator
dp84
sj91
also
ides
nearest-neighbor
methods
kl87
make
assumptions
conditional
density
typical
literature
one
extra
assumption
make
umi
radon-nikodym
derivative
dpu
uniformly
bounded
support
necessary
controlling
dpx
importance-sampling
estimates
refer
assumption
supplementary
material
precise
description
theorem
assumption
supplementary
material
umi
estimator
converges
true
value
probability
i.e
cid:0
cid:12
cid:12
cid:91
umi
umi
cid:12
cid:12
cid:1
lim
n→∞
log
discrete
max
dy/dx
dx/dy
continuous
log
1+δ
practice
regularize
knn
distance
case
much
smaller
expected
distance
order
n−1/
dx+dy
continuous
require
larger
ratio
dimensions
ﬁnite
constant
discrete
however
eﬀective
dimension
zero
makes
ratio
dy/dx
unbounded
hence
concentration
measure
hold
need
k1/dy
scaling
least
logarithmically
number
samples
capacitated
mutual
information
make
analogous
assumptions
described
precisely
assumption
supplementary
material
following
theorem
establishes
consistency
estimator
discrete
quantize
analysis
requires
uniform
convergence
possible
choices
weights
making
quantization
step
inevitable
improvements
technical
condition
natural
future
steps
theorem
assumption
supplementary
material
cmi
estimator
converges
log
1+δ
probability
true
value
resolution
quantization
i.e
log
cid:0
cid:12
cid:12
cid:91
cmi
cmi
cid:12
cid:12
cid:1
lim
n→∞
numerical
experiments
5.1
gene
causal
strength
single
cell
data
brieﬂy
describe
setup
ksm+14
motivate
numerical
experiments
consider
simple
genetic
pathway
cascade
genes
expression
values
interact
linearly
i.e.
key
question
interest
case
signaling
pathway
varies
diﬀerent
conditions
intervention
let
denote
time
intervention
example
giving
certain
drug
may
want
compare
strength
causal
relationship
two
genes
diﬀerent
times
intervention
experiments
usually
samples
taken
time
points
small
cardinality
example
drug
minutes
drug
minutes
drug
given
time
point
many
cells
interrogated
get
samples
distribution
resampling
rate
resampling
rate
figure
cmi
umi
estimators
signiﬁcantly
improve
dremi
capturing
biological
trend
ﬂow-cytometry
data
ﬁgures
refer
setting
figure
ksm+14
z|y
value
observe
i.i.d
samples
...
sampled
samples
obtained
using
technique
called
single-cell
mass
ﬂow
cytometry
see
ksm+14
details
interested
obtaining
causal
measure
another
measure
z|y
time
point
measure
serves
high
level
summary
signaling
proceeds
cascade
function
time
lets
one
compare
strengths
given
causal
relationship
diﬀerent
points
intervention
drug
indeed
activates
causal
pathway
one
may
expect
causal
relationship
follow
certain
trend
i.e.
earlier
strength
high
later
value
strength
high
eﬀect
drug
wears
time
expect
relationships
fall
back
low
nominal
value
analysis
conducted
ksm+14
causal
strength
function
evaluated
via
so-called
dremi
estimator
essentially
version
umi
estimation
channel
ampliﬁcation
step
careful
choice
hyper
parameters
therein
theoretical
properties
estimator
evaluated
paper
shown
two
example
pathways
dremi
recovers
correct
trend
i.e.
correctly
identiﬁes
time
causal
relationship
expected
peak
per
prior
biological
knowledge
demonstrates
utility
dremi
causal
strength
inference
gene
networks
see
figure
ksm+14
authors
also
demonstrate
metrics
depend
whole
joint
distribution
mutual
information
maximal
information
coeﬃcient
correlation
capture
trend
aside
note
somewhat
diﬀerent
set
trend
spotting
estimators
primarily
trying
ﬁnd
genes
demonstrate
monotonic
trend
time
single-cell
rna-sequencing
data
proposed
recently
mjg15
paper
studied
inﬂuence
measures
axiomatically
proposed
umi
cmi
mea-
sures
natural
apply
estimators
time
point
setting
ksm+14
look
understand
two
distinct
issues
experiments
ﬂow-cytometry
data
ﬁrst
whether
proposed
quantities
umi
cmi
able
capture
biological
trend
dremi
able
second
question
relates
sample
complexity
ability
recover
trend
vary
function
sample
complexity
study
subsample
original
data
ksm+14
multiple
times
100
experiments
subsampling
ratio
compute
fraction
times
recover
true
biological
trend
plotted
figure
ﬁgure
demonstrates
whole
dataset
made
available
umi
cmi
able
spot
trend
correctly
dremi
fewer
sam-
ples
available
umi
uniformly
dominates
dremi
turn
cmi
uniformly
dominates
umi
terms
capturing
biological
trend
function
number
samples
available
believe
strong
empirical
evidence
lends
credence
approach
completeness
note
datasets
represented
figure
refer
regular
t-cells
left
ﬁgure
t-cells
exposed
antigen
right
ﬁgure
expect
diﬀerent
biological
trends
correctly
captured
metrics
5.2
synthetic
data
demonstrate
accuracy
proposed
umi
cmi
estimators
synthetic
experiments
generate
samples
pxy
distributed
beta
distribution
beta
1.5
1.5
independent
present
three
results
varying
0.09
0.36
1.0
figure
number
samples
number
samples
number
samples
figure
proposed
umi
estimator
signiﬁcantly
outperforms
partition
based
methods
mod89
sample
complexity
additive
gaussian
channels
used
varying
variances
0.09
left
0.36
middle
1.0
right
number
samples
number
samples
number
samples
figure
proposed
cmi
estimator
signiﬁcantly
outperforms
partition
based
methods
bla72
ari72
sample
complexity
additive
gaussian
channels
used
varying
variances
0.36
left
1.0
middle
2.25
right
shows
estimate
umi
averaged
100
instances
compared
ground
truth
state-of-the-art
partition
based
estimators
mod89
ground
truth
computed
via
simulations
8192
samples
desired
distribution
using
kraskov
mutual
information
estimator
ksg04
cmi
use
exactly
distribution
pxy
umi
varying
0.36
1.0
2.25
illustrated
figure
power
constraint
ground
truth
given
log
1/16σ2
proposed
cmi
estimator
compared
blahut-
arimoto
algorithm
bla72
ari72
computing
discrete
channel
capacity
applied
quantized
data
ﬁgures
illustrate
proposed
estimators
signiﬁcantly
improves
state-of-the-art
partition
based
methods
terms
sample
complexity
log
discussion
paper
proposed
novel
information
theoretic
measures
potential
inﬂuence
one
variable
another
well
provided
novel
estimators
compute
measures
i.i.d
samples
technical
innovation
proposing
estimators
combining
separate
threads
ideas
statistics
including
importance
sampling
nearest-neighbor
methods
consistency
proofs
suggest
similar
analysis
popular
estimator
traditional
mutual
information
ksg04
conducted
successfully
work
recently
conducted
gov16
several
issues
statistical
estimation
theory
intersect
current
work
discuss
topics
main
technical
results
paper
weak
consistency
proposed
estimators
proving
stronger
consistency
guarantees
rates
convergence
would
natural
improvements
albeit
challenging
ones
rates
convergence
nearest-neighbor
methods
barely
known
literature
consistency
even
traditional
information
theoretic
quantities
instance
tvdm96
derives
single
dimensional
case
diﬀerential
entropy
estimation
strong
assumptions
underlying
pdf
leaving
higher
dimensional
scenarios
open
recently
successfully
addressed
gov16
natural
generalization
estimators
alphabet
high
dimensional
using
knn
approach
diﬀerential
entropy
estimator
kl87
mutual
information
estimator
ksg04
however
recent
works
gsg14
gsg15
lp16
shown
boundary
biases
common
high
dimensional
scenarios
much
better
handled
using
local
parametric
methods
l+96
hj96
adapting
approaches
estimators
umi
cmi
interesting
direction
future
research
considered
case
discrete
single
dimensional
continuous
alphabet
scenario
high
dimensional
signiﬁcantly
challenging
cmi
estimation
vastly
expanded
space
distributions
optimization
performed
also
challenging
consider
application
speciﬁc
regularization
inputs
scenario
focus
paper
quantifying
potential
causal
inﬂuence
related
question
involves
testing
direction
causality
pair
random
variables
widely
studied
topic
long
lineage
pea09
also
strong
topical
interest
jbgw+13
jsss15
mpj+15
sjsb15
natural
inclination
explore
eﬃcacy
umi
cmi
measures
test
direction
causality
especially
context
benchmark
data
sets
collected
mpj+15
results
follows
umi
probability
predict
correct
direction
cmi
gives
probability
directly
comparing
marginal
entropy
estimator
kl87
also
provides
accuracy
mpj+15
diﬀerent
entropy
estimators
appropriate
hyper
parameter
choices
applied
get
accuracy
-70
research
needed
shed
conclusive
light
although
point
benchmark
data
sets
mpj+15
substantial
confounding
factors
make
causal
direction
hard
measure
ﬁrst
place
axiomatic
derivation
potential
causal
inﬂuence
naturally
suggests
cmi
appropriate
measure
also
able
show
general
quantity
so-called
r´enyi
capacity
also
meets
axioms
cid:54
deﬁne
r´enyi
entropy
r´enyi
divergence
cid:107
log
deﬁne
asymmetric
information
measure
csi95
log
cid:34
cid:18
cid:19
cid:35
pxy
cid:107
inf
converges
traditional
mutual
information
deﬁne
r´enyi
capacity
parameter
ﬁxed
conditional
distribution
pxy
cid:107
observe
cmiλ
cmi
traditional
shannon
capacity
observe
following
proposition
cid:54
cmiλ
satisﬁes
axioms
section
cmiλ
sup
inf
proof
available
appendix
light
result
would
interesting
design
estimators
general
family
r´enyi
capacity
measures
conﬁrm
performance
empirical
tasks
ones
studied
ksm+14
would
also
interesting
understand
role
additional
axioms
would
lead
uniqueness
shannon
capacity
spirit
entropy
uniquely
characterized
somewhat
similar
axioms
csi08
finally
comment
optimization
problem
cmi
estimation
optimization
problem
involving
necessarily
concave
program
given
sample
realization
although
program
converges
shannon
capacity
computation
involves
maximizing
mutual
information
concave
function
input
probability
distribution
standard
stochastic
gradient
decent
used
experiments
face
disparity
convergent
values
set
synthetic
experiments
conducted
acknowledgements
work
supported
part
aro
w911nf1410220
nsf
satc
award
cns-1527754
nsf
cise
award
ccf-1553452
university
washington
startup
grant
references
ari72
suguru
arimoto
algorithm
computing
capacity
arbitrary
discrete
memoryless
channels
information
theory
ieee
transactions
:14–20
1972
bdgvdm97
jan
beirlant
edward
dudewicz
l´aszl´o
gy¨orﬁ
edward
van
der
meulen
non-
international
journal
mathematical
parametric
entropy
estimation
overview
statistical
sciences
:17–39
1997
bla72
richard
blahut
computation
channel
capacity
rate-distortion
functions
informa-
tion
theory
ieee
transactions
:460–473
1972
cmmr12
jean
cornuet
jean-michel
marin
antonietta
mira
christian
robert
adaptive
mul-
tiple
importance
sampling
scandinavian
journal
statistics
:798–812
2012
cs04
csi95
csi08
ct12
dp84
gov16
gsg14
gsg15
hj96
hv15
imre
csisz´ar
paul
shields
information
theory
statistics
tutorial
publishers
inc
2004.
imre
csisz´ar
generalized
cutoﬀ
rates
renyi
information
measures
information
theory
ieee
transactions
:26–34
1995.
imre
csisz´ar
axiomatic
characterizations
information
measures
entropy
:261–273
2008.
thomas
cover
joy
thomas
elements
information
theory
john
wiley
sons
2012.
luc
devroye
clark
penrod
consistency
automatic
kernel
density
estimates
annals
statistics
pages
1231–1249
1984.
weihao
gao
sewoong
pramod
viswanath
demystifying
ﬁxed
k-nearest
neighbor
information
estimators
arxiv
preprint
arxiv:1604.03006
2016.
shuyang
gao
greg
ver
steeg
aram
galstyan
eﬃcient
estimation
mutual
information
strongly
dependent
variables
arxiv
preprint
arxiv:1411.2003
2014.
shuyang
gao
greg
ver
steeg
aram
galstyan
estimating
mutual
information
local
gaussian
approximation
arxiv
preprint
arxiv:1508.00536
2015.
nils
lid
hjort
jones
locally
parametric
nonparametric
density
estimation
annals
statistics
pages
1619–1647
1996.
siu-wai
sergio
verd´u
convexity/concavity
renyi
entropy
α-mutual
informa-
tion
information
theory
isit
2015
ieee
international
symposium
pages
745–749
ieee
2015
jbgw+13
dominik
janzing
david
balduzzi
moritz
grosse-wentrup
bernhard
sch¨olkopf
quan-
tifying
causal
inﬂuences
annals
statistics
:2324–2358
2013
jsss15
janzing
steudel
shajarisales
sch¨olkopf
justifying
information-geometric
causal
inference
chapter
pages
253–265
springer
international
publishing
2015
kbg+07
shiraj
khan
sharba
bandyopadhyay
auroop
ganguly
sunil
saigal
david
erickson
iii
vladimir
protopopescu
george
ostrouchov
relative
performance
mutual
information
estimation
methods
quantifying
dependence
among
short
noisy
data
physical
review
:026209
2007
kkpw15
kirthevasan
kandasamy
akshay
krishnamurthy
barnabas
poczos
larry
wasserman
nonparametric
von
mises
estimators
entropies
divergences
mutual
informations
advances
neural
information
processing
systems
pages
397–405
2015
kl87
ksg04
ksm+14
l+96
lp16
mjg15
mod89
mpj+15
kozachenko
nikolai
leonenko
sample
estimate
entropy
random
vector
problemy
peredachi
informatsii
:9–16
1987.
kraskov
st¨ogbauer
grassberger
estimating
mutual
information
physical
review
:066138
2004.
smita
krishnaswamy
matthew
spitzer
michael
mingueneau
sean
bendall
oren
litvin
erica
stone
dana
garry
nolan
conditional
density-based
analysis
cell
signaling
single-cell
data
science
346
6213
:1250689
2014.
clive
loader
local
likelihood
density
estimation
annals
statistics
:1602–
1618
1996.
damiano
lombardi
sanjay
pant
nonparametric
k-nearest-neighbor
entropy
estimator
physical
review
:013310
2016.
jonas
mueller
tommi
jaakkola
david
giﬀord
modeling
trends
distributions
arxiv
preprint
arxiv:1511.04486
2015.
rudy
moddemeijer
estimation
entropy
mutual
information
continuous
distri-
butions
signal
processing
:233–248
1989.
j.m
mooij
peters
janzing
zscheischler
sch¨olkopf
distinguishing
cause
eﬀect
using
observational
data
methods
benchmarks
journal
machine
learning
research
2015
owe13
art
owen
monte
carlo
theory
methods
examples
2013
pan03
liam
paninski
estimation
entropy
mutual
:1191–1253
2003.
information
neural
computation
pea09
judea
pearl
causality
cambridge
university
press
2009
pps10
pv10
d´avid
p´al
barnab´as
p´oczos
csaba
szepesv´ari
estimation
r´enyi
entropy
mutual
information
based
generalized
nearest-neighbor
graphs
advances
neural
information
processing
systems
pages
1849–1857
2010.
yury
polyanskiy
sergio
verd´u
arimoto
channel
coding
converse
r´enyi
divergence
communication
control
computing
allerton
2010
48th
annual
allerton
conference
pages
1327–1333
ieee
2010
pxs12
barnab´as
p´oczos
liang
xiong
jeﬀ
schneider
nonparametric
divergence
estimation
applications
machine
learning
distributions
arxiv
preprint
arxiv:1202.3758
2012
re15
robin
richardson
thomas
evans
non-parametric
causal
models
2015
r´en59
sha48
alfr´ed
r´enyi
measures
dependence
acta
mathematica
hungarica
3-4
:441–451
1959.
c.e
shannon
mathematical
theory
communication
bell
system
tech
27:379423
623656
1948
sj91
sjsb15
smh+03
simon
sheather
michael
jones
reliable
data-based
bandwidth
selection
method
kernel
density
estimation
journal
royal
statistical
society
series
methodological
pages
683–690
1991.
shajarisales
janzing
sch¨olkopf
besserve
telling
cause
eﬀect
de-
terministic
linear
dynamical
systems
proceedings
32nd
international
conference
machine
learning
volume
jmlr
workshop
conference
proceedings
page
285
294
jmlr
2015.
harshinder
singh
neeraj
misra
vladimir
hnizdo
adam
fedorowicz
eugene
demchuk
nearest
neighbor
estimates
entropy
american
journal
mathematical
management
sciences
3-4
:301–321
2003
srhi10
kumar
sricharan
raviv
raich
alfred
hero
iii
empirical
estimation
entropy
functionals
conﬁdence
arxiv
preprint
arxiv:1012.4188
2010
tvdm96
alexandre
tsybakov
van
der
meulen
root-n
consistent
estimators
entropy
densities
unbounded
support
scandinavian
journal
statistics
pages
75–83
1996
veh14
wkv09
tim
van
erven
peter
harremo¨es
r´enyi
divergence
kullback-leibler
divergence
information
theory
ieee
transactions
:3797–3820
2014.
qing
wang
sanjeev
kulkarni
sergio
verd´u
divergence
estimation
multidimen-
sional
densities
via-nearest-neighbor
distances
information
theory
ieee
transactions
:2392–2405
2009
appendix
proof
umi
estimator
convergence
theorem
present
proof
theorem
two
separate
umi
estimators
ﬁrst
continuous
next
discrete
ﬁrst
state
formal
assumptions
theorem
holds
assumption
continuous
deﬁne
cid:90
y|x
cid:12
cid:12
y|x
make
following
assumptions
cid:82
cid:0
log
cid:1
exists
positive
constant
cid:48
conditional
pdfs
satisfy
cid:12
cid:12
cid:48
cid:12
cid:12
exists
ﬁnite
constant
hessian
matrix
exists
max
cid:107
cid:107
cid:107
cid:107
almost
everywhere
dxdy
cid:48
almost
everywhere
exist
positive
constants
marginal
pdf
satisfy
almost
everywhere
bandwidth
kernel
density
estimator
chosen
n−1/
2dx+3
discrete
deﬁne
cid:88
x∈x
|x|
y|x
make
following
assumptions
cid:82
y|x
cid:0
log
y|x
cid:1
exists
ﬁnite
constant
cid:48
conditional
pdf
cid:12
cid:12
cid:48
almost
everywhere
exists
ﬁnite
constant
hessian
matrix
exists
cid:107
cid:107
almost
everywhere
exists
ﬁnite
constants
prior
c1/|x|
c2/|x|
almost
everywhere
a.1
case
continuous
given
assumptions
deﬁne
log
log
cid:16
cdx
cdy
cdx+dy
cid:17
cid:0
log
log
cid:1
cid:80
cid:88
cid:48
cid:48
cid:48
cid:54
i=1
deﬁne
quantity
true
prior
cid:107
cid:107
cid:16
cdx
cdy
cdx+dy
cid:17
cid:0
log
log
cid:48
cid:1
cid:48
log
log
cid:12
cid:12
cid:12
cid:12
equal
uniform
distribution
support
apply
triangle
inequality
show
term
converges
zero
probability
y|x
log
wig
cid:90
cid:90
cid:48
cid:1
cid:12
cid:12
cid:12
cid:0
wig
cid:48
cid:90
cid:90
cid:48
cid:48
y|x
log
i=1
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:88
i=1
i=1
cid:82
cid:48
y|x
cid:48
cid:48
dxdy
y|x
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:82
cid:48
y|x
cid:48
cid:48
dxdy
y|x
ﬁrst
term
captures
error
kernel
density
estimator
following
claim
whose
proof
delegated
appendix
lemma
term
equation
converges
probability
cid:88
i=1
second
term
error
comes
sample
noise
density
estimation
similar
decomposition
mutual
information
decompose
estimator
three
terms
cid:48
cid:98
cid:48
i=1
cid:48
cid:88
cid:98
log
log
cid:98
cid:0
log
cdx+dy
log
cid:1
cid:0
log
log
log
cdx
log
cid:1
cid:0
log
log
log
cdy
log
cid:88
cid:98
cid:88
cid:98
cid:88
cid:98
cid:48
log
log
goes
goes
inﬁnity
desired
claim
follows
cid:48
cid:48
cid:48
cid:1
i=1
i=1
i=1
notice
cid:80
i=1
directly
following
two
lemmas
showing
convergence
entropy
estimates
corresponding
entropies
umi
lemma
hypotheses
theorem
lim
n→∞
cid:18
cid:12
cid:12
cid:12
cid:98
cid:90
cid:90
cid:0
cid:18
cid:12
cid:12
cid:12
cid:98
cid:0
cid:98
cid:19
y|x
log
y|x
dxdy
cid:1
cid:12
cid:12
cid:12
cid:90
cid:90
y|x
cid:0
log
cid:1
dxdy
cid:1
cid:12
cid:12
cid:12
lemma
hypotheses
theorem
cid:19
lim
n→∞
cid:82
y|x
crucial
technical
idea
proving
lemmas
concept
importance
sampling
function
importance
sampling
estimate
given
cid:88
i=1
˜hn
cid:48
cid:48
lemma
theorem
9.1
owe13
assume
cid:82
cid:82
y|x
dxdy
exists
following
lemma
gives
almost
sure
convergence
˜hn
cid:0
lim
n→∞
˜hn
cid:1
a.1.1
proof
lemma
given
denote
hdx
mean
given
cid:90
cid:90
cid:90
z∈rdx
u∈rdx
u∈rdx
cid:80
j=1
suﬃciently
small
xj−x
˜fx
hdx
cid:0
ut∇fx
cid:90
cid:107
cid:107
used
fact
kernel
centered
cid:82
udu
suﬃciently
small
u∈rdx
obtain
therefore
cid:16
cid:12
cid:12
˜fx
cid:12
cid:12
n−1/
2dx+3
cid:12
cid:12
cid:12
cid:0
cid:12
cid:12
cid:88
cid:0
cid:12
cid:12
cid:88
cid:17
cid:12
cid:12
2dx+2
2dx+3
cid:12
cid:12
cid:12
cid:1
cid:12
cid:12
2dx+2
2dx+3
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:1
j=1
cid:54
since
bounded
a/hdx
2dx+3
right
hand
side
lower
bounded
choosing
2dx+2
2dx+3
cid:12
cid:12
cid:12
cid:12
2dx+2
2dx+3
|ai
2dx+2
2dx+3
a/hdx
2dx+2
2dx+3
since
cid:54
cid:48
cid:0
cid:12
cid:12
cid:88
cid:0
cid:12
cid:12
cid:88
i.i.d
bounded
a/hdx
hoeﬀding
inequality
obtain
cid:12
cid:12
2dx+2
2dx+3
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:1
cid:12
cid:12
2dx+2
2dx+3
cid:12
cid:12
cid:12
cid:1
cid:54
cid:54
exp
2dx+2
2dx+3
a2h
−2dx
exp
9a2
exp
2dx+3
9a2
2dx+2
2dx+3
hdx
since
upper
bound
independent
take
expectation
obtain
desired
claim
a.1.2
proof
lemma
deﬁne
ˆfx
exp
cdx+dy
ρdx+dy
cid:88
cid:2
log
ˆfx
cid:12
cid:12
cid:3
log
log
ˆfx
cid:48
i=1
theorem
smh+03
notice
cid:48
log
identically
distributed
therefore
plugging
cid:48
/fx
lim
n→∞
lim
n→∞
lim
n→∞
lim
n→∞
lim
n→∞
lim
n→∞
cid:90
cid:90
cid:90
cid:90
cid:48
log
ˆfx
cid:90
cid:90
cid:90
cid:90
cid:90
cid:90
y|x
cid:0
lim
log
ˆfx
cid:12
cid:12
cid:3
dxdy
cid:2
y|x
cid:2
log
ˆfx
cid:12
cid:12
cid:3
dxdy
y|x
cid:2
log
ˆfx
cid:12
cid:12
cid:3
dxdy
cid:2
log
ˆfx
cid:12
cid:12
cid:3
cid:1
dxdy
n→∞
y|x
log
dxdy
want
show
follows
reverse
fatou
lemma
fact
cid:90
cid:90
cid:12
cid:12
cid:2
log
ˆfx
cid:12
cid:12
cid:3
cid:12
cid:12
cid:90
cid:90
cid:12
cid:12
cid:2
log
ˆfx
cid:12
cid:12
cid:3
cid:12
cid:12
cid:90
cid:90
cid:12
cid:12
cid:2
log
ˆfx
cid:12
cid:12
cid:3
cid:12
cid:12
cid:90
cid:90
cid:0
log
cid:1
lim
sup
n→∞
lim
sup
n→∞
dxdy
lim
sup
n→∞
lim
sup
n→∞
c−2
c−2
c−2
dxdy
dxdy
dxdy
explained
main
result
section
regularize
knn
distance
ρdx+dy
positive
constant
ensures
log
ˆfx
cid:48
almost
surely
follows
log
ˆfx
|xi
cid:48
one
apply
reverse
fatou
lemma
similar
interchange
limit
used
kl87
wkv09
without
regularization
context
pps10
claims
step
justiﬁed
although
counterexample
pointed
case
given
practical
way
algorithm
implemented
regularization
reverse
fatou
lemma
justiﬁed
therefore
y|x
log
dxdy
ck/n
cid:90
cid:90
lim
n→∞
moreover
theorem11
smh+03
n→∞
var
ˆfx
lim
cid:48
cid:48
var
log
cid:54
cid:48
1/c1
fact
ˆfx
identically
distributed
n→∞
cov
ˆfx
ˆfx
lim
cid:48
var
ˆfx
cid:48
cid:48
cov
ˆfx
ˆfx
cid:3
var
cid:2
cid:88
cid:88
i=1
var
ˆfx
cid:48
cid:48
var
log
cid:54
i=1
cov
ˆfx
ˆfx
cov
ˆfx
ˆfx
therefore
combining
get
lim
lim
n→∞
n→∞
var
cid:2
cid:90
cid:90
y|x
log
dxdy
cid:1
cid:1
cid:105
cid:104
cid:0
cid:98
cid:0
cid:16
cid:0
cid:3
lim
n→∞
var
cid:2
cid:90
cid:90
cid:16
cid:12
cid:12
cid:98
y|x
log
dxdy
cid:1
cid:12
cid:12
cid:0
converges
mean
hence
probability
i.e.
cid:90
cid:90
n→∞
lim
n→∞
lim
therefore
y|x
log
dxdy
cid:1
cid:17
cid:17
cid:88
cid:88
cid:54
cid:18
cid:19
cid:3
a.1.3
proof
lemma
deﬁne
ˆfx
ˆfu
cdx
ρdx
cdy
ρdy
cid:88
cid:90
cid:90
i=1
cid:0
log
ˆfx
log
ˆfu
cid:1
cid:48
triangle
inequality
write
formula
lemma
i=1
i=1
cid:48
cid:48
cid:48
cid:0
cid:12
cid:12
cid:98
cid:98
cid:12
cid:12
cid:88
cid:12
cid:12
cid:88
cid:88
y|x
cid:0
log
log
cid:1
dxdy
cid:1
cid:12
cid:12
cid:90
cid:90
cid:90
cid:90
y|x
cid:0
log
log
cid:1
dxdy
cid:12
cid:12
y|x
cid:0
log
log
cid:1
dxdy
cid:12
cid:12
cid:0
log
ˆfx
log
ˆfu
cid:1
cid:0
log
log
cid:1
cid:12
cid:12
cid:12
cid:0
log
ˆfx
log
ˆfu
cid:1
cid:0
log
log
cid:1
cid:12
cid:12
cid:12
cid:0
log
log
cid:1
i.i.d.
therefore
strong
law
large
numbers
cid:0
log
log
cid:1
cid:17
cid:0
log
log
cid:1
cid:16
cid:90
cid:90
cid:0
log
log
cid:1
dxdy
log
log
cid:1
cid:90
cid:90
y|x
cid:0
log
log
cid:1
dxdy
ﬁrst
term
comes
sampling
recall
cid:48
cid:0
cid:88
cid:48
i=1
i=1
almost
surely
mean
given
cid:48
/fx
since
random
variables
therefore
converges
almost
surely
second
term
comes
density
estimation
simplfy
notations
let
ﬁxed
union
bound
obtain
cid:0
cid:88
cid:0
cid:91
i=1
cid:48
cid:12
cid:12
cid:0
log
ˆfx
log
ˆfu
cid:1
cid:0
log
log
cid:1
cid:12
cid:12
cid:1
cid:88
cid:8
cid:12
cid:12
cid:0
log
ˆfx
log
ˆfu
cid:1
cid:0
log
log
cid:1
cid:12
cid:12
ε/2
cid:9
cid:1
cid:48
i=1
i=1
second
term
converges
zero
lemma
ﬁrst
term
bounded
i=1
cid:0
cid:91
cid:8
cid:12
cid:12
cid:0
log
ˆfx
log
ˆfu
cid:1
cid:0
log
log
cid:1
cid:12
cid:12
ε/2
cid:9
cid:1
cid:0
cid:12
cid:12
cid:0
log
ˆfx
log
ˆfu
cid:1
cid:0
log
log
cid:1
cid:12
cid:12
ε/2
cid:1
cid:90
cid:0
cid:12
cid:12
cid:0
log
ˆfx
log
ˆfu
cid:1
cid:0
log
log
cid:1
cid:12
cid:12
cid:12
ε/2
cid:12
cid:12
cid:1
cid:124
cid:125
cid:0
log
cdx+dy
cid:0
max
cid:8
log
cdx
cid:0
cid:12
cid:12
log
ˆfx
log
cid:12
cid:12
ε/4
cid:0
cid:12
cid:12
log
ˆfu
log
cid:12
cid:12
ε/4
cid:12
cid:12
cid:12
cid:1
fρk
cid:12
cid:12
cid:12
cid:1
fρk
cid:123
cid:122
dx+dy
cid:12
cid:12
cid:1
log
cdy
≤i1
+i2
+i3
+i4
cid:90
cid:90
r=r2
r=r2
cid:12
cid:12
cid:1
fρk
pdf
given
log
cdx+dy
max
cid:8
log
cdx
log
cdy
dx+dy
probability
k-nn
distance
large
small
given
gives
probability
estimator
deviates
true
value
given
medium
consider
four
terms
separately
let
cid:107
cid:107
-dimensional
ball
centered
radius
since
hessian
matrix
exists
cid:107
cid:107
almost
everywhere
suﬃciently
small
probability
mass
within
given
cid:90
cid:0
cid:1
cid:90
cid:2
cdx+dy
rdx+dy
cr2
cdx+dy
rdx+dy
cr2
cid:3
cid:107
u−z
cid:107
cid:107
u−z
cid:107
t∇f
cid:107
cid:107
suﬃciently
large
probability
mass
within
lower
bounded
cid:0
log
cdx+dy
cid:0
log
cdx+dy
dx+dy
cid:1
dx+dy
cid:1
dx+dy
cid:0
log
cdx+dy
dx+dy
cid:1
cdx+dy
log
dx+dy
probability
samples
fall
upper
bounded
cid:0
log
cdx+dy
dx+dy
cid:12
cid:12
cid:1
cid:18
cid:19
k−1
cid:88
k−1
cid:88
m=0
n−1−m
n−1−m
m=0
n−k−1
k−1
log
dx+dy
k−1
exp
log
dx+dy
k−1
exp
log
dx+dy
let
r2,1
log
cdx
r2,1
given
suﬃciently
large
probability
mass
within
p2,1
cid:0
log
cdx
cid:0
log
cdx
cid:1
cid:1
dx+dy
cid:0
log
cdx
cid:1
log
dx+dy
dx+dy
last
equation
comes
assumption
y|x
cid:48
similarly
let
r2,2
log
cdy
probability
p2,2
cid:0
log
cdy
cid:0
log
cdx
cid:1
cid:1
dx+dy
cid:0
log
cdx
cid:1
cdx+dy
cdx+dy
dx+dy
cdx+dy
cdx
cdx
2fy
y|x
cid:48
cdx+dy
cdx
log
dx+dy
dx+dy
log
dx+dy
dx+dy
cdx+dy
cdx+dy
cdy
dx+dy
cdx+dy
2c2
cdy
2c2c
cid:48
cdx+dy
cdy
cdy
log
dx+dy
dx+dy
log
dx+dy
dx+dy
cdx+dy
log
dx+dy
dx+dy
log
dx+dy
dx+dy
probability
least
samples
lie
max
r2,1
r2,2
upper
bounded
follows
cid:0
max
cid:8
log
cdx
log
cdy
cid:12
cid:12
cid:1
max
p2,1
p2,2
p2,1
p2,2
n−1−m
cid:18
cid:19
m=k
n−1
cid:88
n−1
cid:88
n−1
cid:88
m=k
max
p2,1
p2,2
cid:48
cdx+dy
min
cdx
cdy
log
dx+dy
cdx+dy
min
cdx
cdy
log
dx+dy
m=k
cid:48
cid:1
min
min
suﬃciently
large
cid:48
ity
comes
sum
geometric
series
holds
min
cdx
cdy
log
dx+dy
cdx+dy
dx+dy
min
dx+dy
1/2
last
inequal-
given
recall
ˆfx
cid:0
cid:12
cid:12
log
ˆfx
log
cid:12
cid:12
ε/4
cid:12
cid:12
cid:1
cid:0
cid:12
cid:12
log
log
log
cdx
log
log
cid:12
cid:12
ε/4
cid:12
cid:12
cid:1
cid:0
cid:12
cid:12
log
log
cdx
rdx
cid:12
cid:12
ε/4
cid:12
cid:12
cid:1
cid:0
cdx
rdxfx
eε/4
cid:12
cid:12
cid:1
cid:0
cdx
rdxfx
e−ε/4
cid:12
cid:12
cid:1
n−1
cdx
ρdx
given
r2,1
probability
distribution
given
following
lemma
lemma
given
deterministic
sequence
l=k+1
i.i.d
bernoulli
random
variables
mean
cdx
rdx
1−ε/8
cdx
rdx
1+ε/8
suﬃciently
large
limn→∞
positive
number
neighbors
distributed
cid:80
n−1
given
lemma
obtain
cid:0
cdx
rdxfx
eε/4
cid:12
cid:12
cid:12
cid:1
cid:0
n−1
cid:88
cid:0
n−1
cid:88
cdx
rdx
eε/4
cid:1
cdx
rdx
eε/4
cid:1
l=k+1
l=k+1
right
hand
side
probability
lower
bounded
cdx
rdx
eε/4
cdx
rdx
eε/4
cdx
rdx
ε/8
cdx
rdx
eε/4
ε/8
cdx
rdx
ε/16
suﬃciently
large
cdx
rdx
eε/4
ε/16
since
bernoulli
applying
bernstein
inequality
upper
bounded
l=k+1
l=k+1
exp
exp
cdx
rdxfx
ε/16
cid:0
n−1
cid:88
cid:0
n−1
cid:88
cdx
rdx
cid:1
cdx
rdx
ε/16
cid:1
cdx
rdxfx
ε/16
cid:1
cid:0
cid:0
cdx
rdx
ε/8
cid:0
cdx
rdx
e−ε/4
cid:12
cid:12
cid:12
cid:1
cid:0
n−1
cid:88
cid:0
n−1
cid:88
cdx
rdx
e−ε/4
cid:1
cdx
rdx
e−ε/4
cid:1
cdxrdx
ε/16
cid:1
cdx
rdx
ε/16
cdx
rdx
exp
512
7ε/48
l=k+1
similarly
tail
bound
direction
given
l=k+1
right
hand
side
negative
upper
bounded
cdxrdx
e−ε/4
cdx
rdx
e−ε/4
cdx
rdx
ε/8
cdx
rdx
e−ε/4
ε/8
cdx
rdx
ε/16
small
enough
cdx
rdx
e−ε/4
small
enough
e−ε/4
3ε/16
similarly
upper
bounded
cdx
rdx
e−ε/4
cid:1
cid:0
n−1
cid:88
l=k+1
exp
cdx
rdx
therefore
upper
bounded
cid:0
cid:12
cid:12
log
ˆfx
log
cid:12
cid:12
cid:12
cid:12
cid:1
fρk
512
7ε/48
r=r2
cid:90
cid:90
log
cdx+dy
cid:90
log
cdx+dy
log
cdx
dx+dy
dx+dy
log
cdx
cid:0
cid:12
cid:12
log
ˆfx
log
cid:12
cid:12
cid:12
cid:12
cid:1
fρk
exp
cdx
rdxfx
fρk
exp
1024
exp
1024
cdxfx
log
cdx
log
2dx
512
7ε/48
suﬃciently
large
n/2
given
recall
ˆfu
n−1
cdy
rdy
cid:0
cid:12
cid:12
log
ˆfu
log
cid:12
cid:12
ε/4
cid:12
cid:12
cid:1
cid:0
cid:12
cid:12
log
log
log
cdy
log
log
cid:12
cid:12
ε/4
cid:12
cid:12
cid:1
cid:0
cid:12
cid:12
log
log
cdy
rdy
cid:12
cid:12
ε/4
cid:12
cid:12
cid:1
cid:0
cdy
rdy
eε/4
cid:12
cid:12
cid:1
cid:0
cdy
rdy
e−ε/4
cid:12
cid:12
cid:1
recall
cid:88
cid:54
cid:48
cid:107
cid:107
cid:88
cid:54
cid:107
cid:107
write
cid:88
cid:88
cid:107
zj−z
cid:107
cid:107
zj−z
cid:107
cid:107
cid:107
since
c1/µ
c2/µ
k/c2
r2,2
probability
distribution
given
following
lemma
k/c1
given
distributed
cid:80
n−1
lemma
given
deterministic
sequence
limn→∞
positive
distribution
l=k+1
i.i.d
random
variables
1/c1
mean
cdy
rdy
ε/8
cdy
rdy
ε/8
suﬃciently
large
given
lemma
fact
cid:0
cdy
rdy
eε/4
cid:12
cid:12
cid:12
cid:1
k/c2
obtain
cid:0
n−1
cid:88
cid:1
cid:0
n−1
cid:88
cdy
rdy
eε/4
k/c2
l=k+1
cdy
rdy
eε/4
k/c2
cid:1
l=k+1
right
hand
side
lower
bounded
cdy
rdy
eε/4
k/c2
cdy
rdy
eε/4
k/c2
cdy
rdy
ε/8
cdy
rdy
eε/4
ε/8
k/c2
cdy
rdy
ε/16
suﬃciently
large
cdy
rdy
eε/4
ε/16
k/c2
since
upper
bounded
1/c1
l=k+1
cid:0
n−1
cid:88
cid:0
n−1
cid:88
/c1
applying
bernstein
inequality
upper
bounded
cdy
rdy
cid:1
cdy
rdy
ε/16
cid:1
cdy
rdy
ε/8
cid:1
cid:0
cid:0
cdy
rdy
ε/8
/c1
cdy
rdy
ε/16
cid:1
cdy
rdy
ε/16
cdy
rdy
ε/16
3c1
l=k+1
3c1
exp
exp
exp
similarly
since
k/c1
tail
bound
way
given
c1ε2
cdy
rdy
512
7ε/48
cid:0
cdy
rdy
e−ε/4
cid:12
cid:12
cid:12
cid:1
cid:0
n−1
cid:88
cid:1
cid:0
n−1
cid:88
cdy
rdy
e−ε/4
k/c1
l=k+1
cdy
rdy
e−ε/4
k/c1
cid:1
l=k+1
right
hand
side
negative
upper
bounded
cdy
rdy
e−ε/4
k/c1
cdy
rdy
e−ε/4
k/c1
cdy
rdy
ε/8
cdy
rdy
e−ε/4
ε/8
cdy
rdy
ε/16
small
enough
cdy
rdy
e−ε/4
1/c1
small
enough
e−ε/4
3ε/16
similarly
upper
bounded
cdy
rdy
e−ε/4
cid:1
cid:0
n−1
cid:88
l=k+1
exp
c1ε2
cdy
rdy
therefore
upper
bounded
cid:0
cid:12
cid:12
log
ˆfu
log
cid:12
cid:12
cid:12
cid:12
cid:1
fρk
512
7ε/48
r=r2
cid:90
cid:90
log
cdx+dy
cid:90
log
cdx+dy
log
cdy
dx+dy
dx+dy
log
cdy
cid:0
cid:12
cid:12
log
ˆfu
log
cid:12
cid:12
cid:12
cid:12
cid:1
fρk
exp
c1ε2
cdy
rdy
fρk
exp
c1ε2
1024
exp
c1ε2
1024
cdy
log
cdy
log
2dy
512
7ε/48
suﬃciently
large
7ε/48
n/2
combining
obtain
cid:12
cid:12
cid:0
log
ˆfx
log
ˆfu
cid:1
cid:0
log
log
cid:1
cid:12
cid:12
cid:1
cid:0
cid:88
cid:90
cid:90
i=1
cid:48
exp
log
dx+dy
cid:48
cdx+dy
min
cdx
cdy
log
dx+dy
1−k
min
exp
1024
log
2dx
exp
1024
log
2dy
max
dy/dx
dx/dy
min
dx+dy
conclude
cid:48
cid:0
cid:88
cid:12
cid:12
cid:0
log
ˆfx
log
ˆfu
cid:1
cid:0
log
log
cid:1
cid:12
cid:12
cid:1
lim
n→∞
dx+dy
four
terms
goes
i=1
therefore
combining
convergence
properties
error
kernel
density
estimation
error
self-
normalized
importance
sampling
error
density
estimation
obtain
converges
probability
a.1.4
proof
lemma
given
let
partition
indexes
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
deﬁne
event
associated
partition
cid:8
cid:107
cid:107
cid:107
cid:107
cid:107
cid:107
cid:107
cid:107
cid:9
n−k−1
k−1
thus
cid:0
100
since
i.i.d
random
variables
event
identical
probability
number
partitions
cid:1
n−k−1
k−1
cdf
given
n−1
cid:0
cid:12
cid:12
cid:1
cid:88
cid:0
cid:1
cid:0
cid:12
cid:12
cid:1
n−1
cid:88
cid:0
cid:12
cid:12
cid:1
101
condition
event
namely
k-nearest
neighbor
distance
set
samples
distance
smaller
set
samples
distance
greater
recall
number
samples
cid:107
cid:107
index
cid:107
cid:107
satisﬁed
therefore
means
samples
x-distance
smaller
let
cid:107
cid:107
cid:12
cid:12
cid:107
cid:107
.therefore
cid:0
cid:12
cid:12
cid:1
cid:0
cid:88
cid:0
cid:88
cid:0
n−1
cid:88
cid:107
cid:107
cid:12
cid:12
cid:107
cid:107
cid:107
cid:107
cid:107
cid:107
cid:1
cid:107
cid:107
cid:12
cid:12
cid:107
cid:107
cid:1
cid:1
102
t∈t
t∈t
l=k+1
drop
conditions
cid:54
since
independent
therefore
given
cid:107
cid:107
variables
cid:107
cid:107
i.i.d
distribution
therefore
cid:0
cid:12
cid:12
cid:1
cid:0
cid:12
cid:12
cid:1
cid:0
cid:88
cid:88
cid:88
cid:1
l=k+1
cid:0
n−1
cid:88
cid:1
l=k+1
given
distribution
cid:80
n−1
cid:107
u−x
cid:107
cid:82
cid:82
cid:0
cid:107
cid:107
cid:12
cid:12
cid:107
cid:107
cid:1
cid:82
cid:82
cid:82
cid:107
cid:107
dudv
cid:107
cid:107
dudv
l=k+1
given
mean
cid:0
cid:107
cid:107
cid:107
cid:107
cid:1
cid:0
cid:107
cid:107
cid:1
104
103
since
cid:107
cid:107
almost
everywhere
decays
goes
inﬁnity
suﬃciently
large
following
bound
cid:107
u−x
cid:107
cid:107
u−x
cid:107
∇fx
cid:107
cid:107
cid:90
cid:90
cdx
rdx
cr2
cdx
rdx
ε/8
cid:90
cid:90
cdxrdx
cr2
cdx+dy
rdx+dy
cr2
cdxrdx
ε/8
cid:107
cid:107
dudv
cid:107
u−x
cid:107
a.1.5
proof
lemma
given
deﬁne
lemma
let
given
cid:107
cid:107
cid:12
cid:12
cid:107
cid:107
cid:107
cid:107
cid:107
cid:107
cid:1
cid:107
cid:107
cid:12
cid:12
cid:107
cid:107
cid:1
cid:12
cid:12
cid:107
cid:107
condition
event
cdf
cid:12
cid:12
cid:1
cid:0
cid:0
cid:88
cid:0
cid:88
cid:0
n−1
cid:88
cid:1
t∈t
t∈t
l=k+1
107
105
106
cid:107
cid:107
cid:90
cid:90
cid:90
cid:90
similarly
drop
conditions
cid:54
therefore
given
cid:107
cid:107
cid:107
cid:107
i.i.d
distribution
therefore
variables
cid:12
cid:12
cid:1
cid:0
cid:88
cid:12
cid:12
cid:1
cid:0
cid:0
n−1
cid:88
cid:88
cid:1
l=k+1
cid:0
n−1
cid:88
cid:1
distribution
cid:80
n−1
l=k+1
mean
given
l=k+1
given
supx
108
1/c1
cid:2
cid:82
cid:82
cid:107
v−y
cid:107
cid:107
cid:107
cid:12
cid:12
cid:107
cid:107
cid:3
dudv
cid:82
cid:82
cid:82
cid:82
cid:107
cid:107
cid:107
cid:107
dudv
dudv
109
since
cid:107
cid:107
almost
everywhere
decays
goes
inﬁnity
suﬃciently
large
following
bound
cid:107
v−y
cid:107
dudv
∇fu
cid:107
cid:107
cid:107
v−y
cid:107
cid:107
v−y
cid:107
cdy
rdy
cr2
cdy
rdy
ε/8
cid:107
v−y
cid:107
cid:90
cid:90
dudv
cid:90
cid:90
cid:90
cdy
rdy
cr2
cdx+dy
rdx+dy
cr2
cdy
rdy
ε/8
cid:107
cid:107
cid:107
cid:107
dudv
cid:90
cid:90
cid:107
v−y
cid:107
dudv
110
111
a.2
case
discrete
assumption
prove
general
version
theorem
let
i.i.d
samples
drawn
unknown
prior
anb
let
known
distribution
/px
deﬁne
cid:88
cid:54
cid:107
cid:107
112
113
115
116
117
118
119
120
121
114
proposed
estimator
cid:98
claim
cid:98
converges
true
value
probability
i.e
cid:16
cid:1
cid:17
log
cid:0
log
nxi
log
i=1
cid:88
cid:0
cid:12
cid:12
cid:98
cid:12
cid:12
cid:1
cid:88
y|x
log
lim
n→∞
cid:90
y|x
x∈x
cid:88
cid:48
y|x
cid:48
notice
theorem
special
case
uniform
deﬁne
log
cid:0
log
log
cid:1
cid:80
i=1
deﬁne
quantity
true
prior
apply
triangular
inequality
show
term
converges
zero
probability
i=1
i=1
cid:54
y|x
log
cid:107
cid:107
x∈x
cid:48
cid:48
cid:48
cid:48
cid:90
cid:48
cid:1
cid:12
cid:12
cid:12
cid:90
cid:88
cid:48
log
cid:0
log
nxi
log
cid:48
cid:1
cid:12
cid:12
cid:12
cid:12
cid:98
cid:12
cid:12
cid:12
cid:88
cid:88
cid:12
cid:12
cid:12
cid:88
cid:0
cid:12
cid:12
cid:12
cid:48
cid:88
cid:88
cid:12
cid:12
cid:88
cid:0
cid:16
cid:12
cid:12
cid:12
cid:16
x∈x
cid:16
cid:54
cid:16
cid:54
log
cid:48
cid:1
cid:12
cid:12
cid:17
cid:12
cid:12
cid:12
log
cid:17
cid:16
y|x
log
εpx
x∈x
cid:48
cid:48
max
x∈x
cid:17
cid:17
x∈x
max
max
cid:48
log
log
i=1
i=1
log
deviates
upper
bounded
cid:48
cid:16
suﬃciently
large
recall
/nx
124
bounded
cid:80
cid:48
y|x
cid:48
y|x
cid:12
cid:12
cid:12
cid:80
cid:48
y|x
cid:48
y|x
cid:12
cid:12
cid:12
122
123
cid:17
log
124
cid:17
125
εpx
log
ﬁrst
term
122
captures
error
estimating
similar
188
probability
εpx
cid:0
|nx
εp2
suﬃciently
large
therefore
binomial
random
variable
parameter
therefore
hoeﬀding
inequality
log
1/3
recall
cid:80
cid:1
exp
cid:16
cid:54
log
126
last
inequality
comes
assumption
c1/|x|
/px
cid:17
union
bound
125
upper
bounded
exp
72|x|2c
log
log
ε2c
εpx
εpx
εp2
i=1
log
cid:16
|nx
εp2
log
log
cid:17
|x|
max
x∈x
2|x|
exp
ε2c
72|x|2c
log
127
combining
124
know
122
converges
probability
i=1
i=1
cid:48
cid:48
cid:88
second
term
error
123
comes
sample
noise
density
estimation
decompose
estimator
three
terms
cid:48
cid:98
log
log
log
nxi
nxi
cid:88
cid:0
nxi
log
cdy
log
cid:1
cid:0
log
log
log
cdy
log
cid:98
cid:88
cid:98
cid:88
cid:98
cid:48
log
log
log
nxi
nxi
converges
probability
goes
inﬁnity
desired
claim
follows
directly
following
two
lemmas
showing
convergence
entropy
estimates
corresponding
conditional
entropy
entropy
desired
claim
immediately
follows
two
lemmas
notice
cid:80
cid:48
cid:48
cid:1
128
129
i=1
i=1
i=1
lemma
hypotheses
theorem
lim
n→∞
cid:32
cid:12
cid:12
cid:12
cid:98
cid:0
cid:88
cid:32
cid:12
cid:12
cid:12
cid:98
cid:0
cid:88
x∈x
cid:33
cid:90
cid:90
y|x
log
y|x
cid:1
cid:12
cid:12
cid:12
cid:33
y|x
log
cid:1
cid:1
cid:12
cid:12
cid:12
lemma
hypotheses
theorem
lim
n→∞
cid:80
x∈x
y|x
x∈x
130
a.2.1
proof
lemma
deﬁne
ˆfy
yi|xi
exp
nxi
cdy
ρdy
131
132
cid:88
i=1
cid:48
log
ˆfy
133
notice
ˆfy
yi|xi
k-nearest
neighbour
density
estimator
conditional
pdf
y|x
therefore
theorem
smh+03
cid:2
log
ˆfy
yi|xi
cid:12
cid:12
cid:3
log
y|x
lim
n→∞
notice
cid:48
log
yi|xi
identically
distributed
therefore
use
technique
proof
lemma
equation
switch
order
limit
integration
therefore
cid:48
log
ˆfx
lim
n→∞
lim
n→∞
lim
n→∞
lim
n→∞
cid:88
cid:88
x∈x
x∈x
cid:16
cid:90
cid:2
log
ˆfx
cid:12
cid:12
cid:3
y|x
cid:17
cid:16
cid:90
cid:2
log
ˆfx
cid:12
cid:12
cid:3
y|x
cid:16
cid:90
cid:2
log
ˆfx
cid:12
cid:12
cid:3
y|x
cid:2
log
ˆfx
cid:12
cid:12
cid:3
y|x
cid:17
cid:17
cid:88
x∈x
cid:16
cid:90
cid:90
lim
n→∞
cid:88
cid:88
x∈x
x∈x
lim
n→∞
lim
n→∞
logy
y|x
y|x
cid:90
cid:88
x∈x
y|x
log
y|x
therefore
134
135
cid:17
moreover
theorem11
smh+03
n→∞
var
ˆfy
yi|xi
lim
cid:48
cid:48
var
log
y|x
cid:54
n→∞
cov
ˆfy
yi|xi
ˆfy
yj|yi
lim
since
cid:48
similarly
lemma
obtain
n→∞
var
cid:2
lim
cid:3
cid:90
logy
y|x
y|x
cid:1
cid:12
cid:12
cid:17
cid:16
cid:12
cid:12
cid:98
cid:0
cid:88
lim
n→∞
x∈x
combining
137
140
know
converges
mean
hence
probability
i.e.
141
136
137
138
139
140
a.2.2
proof
lemma
deﬁne
triangle
inequality
write
formula
lemma
y|x
log
cid:1
cid:1
cid:12
cid:12
cid:90
y|x
log
cid:1
cid:12
cid:12
cid:90
y|x
log
cid:1
cid:12
cid:12
ˆfq
cdy
ρdy
cid:48
log
ˆfq
i=1
i=1
x∈x
x∈x
cid:90
cid:88
cid:0
cid:88
cid:12
cid:12
cid:98
cid:12
cid:12
cid:88
cid:12
cid:12
cid:88
cid:88
cid:48
cid:48
cid:48
log
ˆfq
cid:88
log
cid:88
cid:12
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
cid:12
log
cid:16
cid:90
cid:17
cid:88
cid:88
cid:16
cid:48
x∈x
i=1
i=1
i=1
log
x∈x
cid:17
log
y|x
log
ﬁrst
term
comes
144
sampling
recall
cid:48
/px
therefore
strong
law
large
numbers
almost
surely
mean
given
therefore
144
converges
almost
surely
second
term
145
comes
density
estimation
ﬁxed
union
bound
obtain
second
term
converges
zero
law
large
numbers
ﬁrst
term
bounded
i=1
i=1
cid:48
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
cid:1
cid:8
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
cid:12
cid:12
cid:12
ε/2
cid:9
cid:1
cid:88
cid:0
cid:88
cid:0
cid:91
cid:8
cid:12
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
cid:12
cid:12
cid:12
ε/2
cid:9
cid:1
cid:0
cid:91
cid:0
cid:12
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
cid:12
cid:12
cid:12
ε/2
cid:1
cid:88
cid:0
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
cid:12
cid:12
ε/2
cid:124
cid:123
cid:122
cid:90
x∈x
i=1
i=1
≤i1
+i2
+i3
cid:12
cid:12
cid:12
cid:1
cid:125
cid:48
y|x
150
142
143
144
145
146
147
148
149
cid:0
1/2px
y|x
cdy
−1/dy
cid:12
cid:12
cid:1
cid:0
log
1+δ/2
cdy
−1/dy
cid:12
cid:12
cid:1
cid:0
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
ε/2
cid:90
cid:12
cid:12
cid:12
cid:1
fρk
r=r2
151
152
153
fρk
pdf
given
1/2px
y|x
cdy
log
1+δ/2
cdy
consider
three
terms
separately
let
cid:107
cid:107
dy-dimensional
ball
centered
radius
since
hessian
matrix
exists
cid:107
cid:107
almost
everywhere
suﬃciently
small
probability
mass
within
given
cid:90
cid:0
cid:1
cid:2
y|x
cdy
rdy
cr2
y|x
cdy
rdy
cr2
cid:3
cid:107
v−y
cid:107
cid:107
v−y
cid:107
cid:90
t∇fy
cid:107
cid:107
suﬃciently
large
probability
mass
within
lower
bounded
cid:0
1/2px
y|x
cdy
−1/dy
cid:1
y|x
cdy
n−1/2
cid:0
1/2px
y|x
cdy
−1/dy
cid:1
cid:0
1/2px
y|x
cdy
cid:0
cid:18
cid:12
cid:12
cid:1
cid:19
n−1−m
k−1
cid:88
k−1
cid:88
m=0
probability
samples
fall
upper
bounded
n−1−m
m=0
k−1
n−k−1
k−1
exp
let
log
1+δ/2
cdy
−1/dy
suﬃciently
large
probability
mass
within
cid:0
log
1+δ/2
cdy
−1/dy
cid:1
cid:0
log
1+δ/2
cdy
−1/dy
cid:1
given
cid:0
log
1+δ/2
cdy
−1/dy
cid:1
y|x
cdy
2px
y|x
cid:80
2px
y|x
x∈x
y|x
log
1+δ/2
log
1+δ/2
n−1
log
1+δ/2
n−1
c3|x|n
154
cid:1
155
156
157
last
equation
comes
assumption
/px
probability
least
samples
lying
therefore
upper
bounded
cid:12
cid:12
cid:1
cid:0
cid:18
cid:19
n−1−m
m=k
m=k
n−1
cid:88
n−1
cid:88
n−1
cid:88
n−1
cid:88
n−1
cid:88
m=k
m=k
m=k
mpm
mpm
m/e
ep2
c3|x|
log
1+δ/2
158
use
fact
m/e
since
log
1+δ
assumption
log
1+δ/2
decreasing
increases
suﬃciently
large
c1|x|
log
1+δ/2
1/2
obtain
c3|x|
log
1+δ/2
c3|x|
log
1+δ
log
logn
1+δ
159
given
recall
ˆfq
cid:0
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
ε/2
cid:12
cid:12
cid:1
cid:0
cid:12
cid:12
log
log
log
cdy
log
log
cid:12
cid:12
ε/2
cid:12
cid:12
cid:1
cid:0
cid:12
cid:12
log
log
cdy
rdy
cid:12
cid:12
ε/2
cid:12
cid:12
cid:1
cid:0
cdy
rdy
eε/2
cid:12
cid:12
cid:1
cid:0
cdy
rdy
e−ε/2
cid:12
cid:12
cid:1
n−1
cdy
rdy
160
following
similar
technique
analysis
proof
lemma
obtain
cid:0
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
ε/2
cid:12
cid:12
cid:1
exp
c3ε2
128
7ε/24
cdy
rdy
161
lower
bound
/px
therefore
upper
bounded
cid:0
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
ε/2
cid:12
cid:12
cid:1
fρk
cid:90
cid:90
1/2px
y|x
cdy
−1/dy
cid:90
1/2px
y|x
cdy
−1/dy
log
1+δ/2
cdy
−1/dy
r=r2
log
1+δ/2
cdy
−1/dy
cid:0
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
ε/2
cid:12
cid:12
cid:1
fρk
exp
c3ε2
cdy
rdy
fρk
exp
c3ε2
256
exp
c3ε2
256
cdy
log
1+δ/2
cdy
log
1+δ/2
suﬃciently
large
7ε/24
n/2
128
7ε/24
162
combine
156
159
162
obtain
cid:0
cid:88
cid:88
i=1
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
cid:1
cid:90
cid:48
exp
x∈x
exp
c3ε2
256
log
1+δ/2
c3|x|
log
1+δ
log
logn
1+δ
163
one
easily
see
ﬁrst
second
terms
converges
goes
inﬁnity
given
log
see
last
term
converges
show
logarithm
goes
goes
inﬁnity
log
c3|x|
log
1+δ
log
logn
1+δ
negative
term
larger
exponent
logarithm
goes
log
log
log
log
cid:0
cid:88
i=1
cid:48
lim
n→∞
c3|x|
log
1+δ
log
log
1+δ
c3|x|
log
1+δ
cid:12
cid:12
log
ˆfq
log
cid:12
cid:12
cid:1
log
1+δ
dy+1
cid:98
therefore
combining
convergence
error
sampling
error
density
estimation
obtain
converges
probability
164
165
proof
cmi
estimator
convergence
assumption
make
following
assumptions
cid:82
y|x
cid:12
cid:12
log
y|x
cid:12
cid:12
cid:82
y|x
cid:0
log
y|x
cid:1
almost
everywhere
exists
ﬁnite
constant
hessian
matrix
exists
cid:107
cid:107
exists
ﬁnite
constant
cid:48
conditional
pdf
cid:12
cid:12
cid:48
almost
everywhere
exists
ﬁnite
constants
ratio
optimal
prior
/px
maximizer
deﬁnition
true
prior
satisﬁes
every
exists
ﬁnite
constants
c5/|x|
c6/|x|
deﬁne
cid:88
x∈x
cid:90
y|x
log
cid:80
cid:48
cid:48
y|x
cid:48
y|x
ˆik
cid:16
log
cid:0
log
nxi
log
cid:1
cid:17
wxi
cid:88
i=1
166
167
maxqx∈q
quantity
maxw∈t∆
ˆik
first
consider
max
qx∈t∆
168
cid:88
x∈x
constraint
set
deﬁned
r|x|
/px
|x|∆
|x|∆
169
rewrite
error
term
theorem
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:98
cid:12
cid:12
cid:98
cid:12
cid:12
170
ﬁrst
error
comes
quantization
let
maximizer
assumption
/px
since
quantization
simplex
exists
|q0
bound
diﬀerence
following
lemma
lemma
assumptions
theorem
cid:48
cid:12
cid:12
cid:48
cid:12
cid:12
max
x∈x
cid:48
171
positive
constant
max
max
x∈x
|q0
q∈t∆
172
similarly
let
q∗∗
maximizer
also
ﬁnd
|q1
−q∗∗
using
lemma
obtain
therefore
ﬁrst
term
170
bounded
consider
second
term
upper
bound
second
term
relies
convergence
dicrete
umi
estimation
theorem
recall
proof
theorem
shown
certain
conditions
bounded
/px
/px
since
set
ﬁnite
union
bound
cid:0
cid:12
cid:12
ˆik
cid:12
cid:12
ε/2
cid:1
n→∞−→
cid:0
cid:12
cid:12
ˆik
cid:12
cid:12
ε/2
cid:1
cid:0
cid:12
cid:12
cid:12
cid:12
ε/2
cid:1
lim
n→∞
|t∆
lim
n→∞
cid:0
|px
/n|
∆/c2|x|
cid:1
also
strong
law
large
numbers
lim
n→∞
claim
events
inside
probability
174
175
happen
simultaneously
cid:12
cid:12
cid:98
cid:12
cid:12
implies
desired
claim
175
xpx
since
/px
ˆik
deﬁne
let
arg
maxw∈t∆
cid:12
cid:12
cid:88
x∈x
cid:12
cid:12
cid:12
cid:12
cid:88
x∈x
nx/n
∆/2
|x|
cid:12
cid:12
cid:12
cid:12
nx/n
cid:12
cid:12
173
174
176
therefore
|x|
∆/2
max
x∈x
|x|/2
cid:98
ˆik
177
hand
consider
q∗∗
arg
maxqx∈t∆
deﬁne
q∗∗
/px
i=1
claim
sum
closed
know
follows
|x|
necessarily
cid:80
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:88
i=1
nxq∗∗
cid:8
q∗∗
cid:12
cid:12
cid:12
cid:12
cid:9
x∈x
max
x∈x
c2|x|
178
ﬁnd
let
xpx
similar
176
know
moreover
cid:12
cid:12
q∗∗
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
q∗∗
x∈x
|q∗∗
max
ˆik
179
therefore
thus
proof
complete
b.1
proof
lemma
show
cid:12
cid:12
cid:48
cid:12
cid:12
cid:88
∂qx
l/|x|
therefore
x∈x
x∈x
|qx
cid:48
∂qx
max
||qx
cid:48
180
x∈x
y|x
since
c1px
c2px
c1c5/|x|
c2c6/|x|
let
cid:80
know
c1c5
min
x∈x
y|x
c2c6
max
x∈x
y|x
therefore
absolute
value
gradient
bounded
cid:90
∂qx
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:16
cid:88
cid:12
cid:12
cid:12
cid:90
cid:12
cid:12
cid:12
max
∂qx
x∈x
log
y|x
log
y|x
y|x
log
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:48
y|x
cid:12
cid:12
cid:12
y|x
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
max
max
log
c1c5|
log
c2c6|
c1c5
cid:1
cid:12
cid:12
cid:12
cid:90
y|x
cid:48
y|x
y|x
cid:48
181
182
cid:12
cid:12
cid:12
|x|
max
log
c1c5|
log
c2c6|
proof
lemma
term
equation
upper
bounded
cid:48
cid:1
cid:12
cid:12
cid:12
cid:48
cid:12
cid:12
i||g
cid:48
i||g
cid:48
i=1
i=1
i=1
cid:12
cid:12
cid:12
cid:88
cid:0
wig
cid:48
cid:88
cid:12
cid:12
wig
cid:48
cid:16
|wi
cid:48
cid:88
cid:16
|wi
cid:48
cid:88
cid:16
|wi
cid:48
cid:88
cid:88
cid:88
|wi
cid:48
i=1
i=1
i=1
|wi
cid:48
i=1
max
1≤i≤n
|wi
cid:48
i||g
cid:48
cid:88
cid:88
i||g
cid:48
i||g
cid:48
i=1
cid:0
max
1≤i≤n
cid:48
i=1
cid:12
cid:12
cid:48
cid:12
cid:12
cid:17
cid:12
cid:12
cid:17
cid:12
cid:12
log
log
cid:48
cid:12
cid:12
cid:48
cid:12
cid:12
cid:16
cid:80
cid:80
cid:0
max1≤j≤n
cid:48
2ny
cid:54
cid:48
wj|
min1≤j≤n
cid:48
cid:17
cid:107
yi|
cid:48
cid:54
cid:107
yi|
cid:48
wj|
cid:107
yi|
cid:48
cid:54
cid:107
yi|
wj|
cid:17
cid:80
cid:80
cid:54
cid:1
max1≤j≤n
cid:48
wj|
min1≤j≤n
cid:48
min1≤j≤n
cid:48
min1≤j≤n
183
last
inequality
follows
fact
cid:80
cid:80
1/c1
implies
cid:48
similarly
cid:48
suﬃciently
large
cid:48
1/µ
c2/µ
cid:54
cid:48
cid:48
log
log
log
log
log
cdxcdy
cdx+dy
cdx
cdy
cdx+dy
similarly
using
fact
cid:80
i=1
upper
bound
term
follows
1/c2
184
cid:107
cid:107
k/c2
ﬁnite
cid:0
log
log
cid:1
cid:0
log
log
k/c2
cid:1
185
cid:54
cid:48
cid:107
cid:107
n/c1
cid:0
log
log
n/c1
cid:1
cid:48
log
log
log
cdx
cdy
cdx+dy
claim
suﬃciently
large
log
max
c2ε/3
3c2/2
|wi
cid:48
183
upper
bounded
186
log
cid:48
min1≤j≤n
cid:48
min1≤j≤n
cid:1
2/c2
log
log
max
1≤i≤n
1≤i≤n
|wi
cid:48
cid:0
max
cid:0
log
cid:0
log
cid:0
wig
cid:48
log
cid:1
cid:48
cid:1
cid:12
cid:12
cid:17
cid:16
cid:16
cid:12
cid:12
cid:88
deﬁne
i=1
cid:48
cid:48
putting
bounds
together
suﬃciently
large
|wi
cid:48
log
max
1≤i≤n
applying
triangle
inequality
union
bound
188
j=1
n/fx
cid:80
cid:0
1/fx
cid:1
cid:17
cid:17
wi|
cid:17
log
cid:48
cid:48
|wi
cid:48
log
max
cid:48
cid:48
cid:48
1≤i≤n
cid:48
cid:48
cid:48
wi|
cid:48
cid:48
log
cid:16
cid:16
cid:16
cid:16
max
1≤i≤n
max
1≤i≤n
max
1≤i≤n
max
1≤i≤n
cid:17
log
cid:17
187
188
189
190
191
190
recall
cid:48
/fx
since
/fx
1/c2
1/c1
therefore
max
1≤i≤n
max
1≤i≤n
cid:16
cid:16
cid:16
cid:0
max
cid:16
cid:12
cid:12
cid:88
cid:16
cid:12
cid:12
cid:88
1≤i≤n
j=1
cid:48
cid:48
cid:48
j=1
j=1
j=1
cid:17
log
log
log
log
n/fx
cid:17
cid:17
cid:0
1/fx
cid:1
cid:80
cid:12
cid:12
cid:0
/fx
cid:1
cid:12
cid:12
cid:1
cid:80
cid:88
cid:12
cid:12
cid:1
cid:17
cid:12
cid:12
cid:82
cid:48
cid:17
cid:12
cid:12

cid:1
1/c1
1/c2
log
log
log
j=1
cid:27
cid:16
cid:12
cid:12
cid:88
−
cid:0
cid:26
j=1
exp
exp
ε2n
log
note
cid:48
therefore
hoeﬀding
inequality
obtain
i.i.d
random
variables
cid:48
192
1/c2
1/c1
shows
probability
190
goes
goes
inﬁnity
probability
191
recall
use
following
lemma
shows
upper
bound
error
kernel
density
estimator
wi|
cid:12
cid:12
cid:48
cid:48
n/fx
cid:80
cid:0
1/fx
cid:1
cid:88
lemma
10.
assume
cid:82
cid:82
rdx
choosing
hdx
˜fx
cid:54
j=1
cid:80
cid:0
˜fx
cid:1
cid:12
cid:12
˜fx
j=1
rdx
cid:107
cid:107
positive
integer
n−1/
2dx+3
given
cid:16
cid:12
cid:12
˜fx
cid:12
cid:12
n−1/
2dx+3
cid:17
exp
2dx+3
16a2
196
applying
union
bound
get
probability
least
exp
2dx+3
16a2
˜fx
n−1/
2dx+3
197
bound
holds
claim
event
inside
probability
191
holds
suﬃciently
large
together
193
proves
desired
claim
given
large
enough
cid:12
cid:12
cid:88
cid:0
wig
cid:48
cid:48
cid:1
cid:12
cid:12
i=1
193
194
195
198
probability
least
exp
2dx+3
16a2
exp
log
c2−c1
ε2n
left
show
197
implies
event
inside
probability
191
given
197
˜fx
n−1/
2dx+3
n−1/
2dx+3
therefore
suﬃciently
large
cid:48
cid:48
cid:48
cid:80
cid:80
cid:80
j=1
j=1
n/fx
lower
bounded
j=1
˜fx
cid:0
˜fx
cid:1
cid:0
1/fx
cid:1
cid:80
cid:0
1/fx
cid:1
cid:0
n−1/
2dx+3
cid:0
1/fx
cid:1
cid:0
n−1/
2dx+3
n/fx
cid:1
n−1/
2dx+3
n/fx
3c2µ
n−1/
2dx+3
j=1
199
200
201
cid:1
200
follows
fact
1/3
201
follows
fact
c1/µ
c2/µ
similarly
upper
bounded
cid:48
cid:48
n/fx
cid:80
cid:80
cid:80
j=1
j=1
j=1
˜fx
cid:80
cid:0
1/fx
cid:1
cid:0
˜fx
cid:1
cid:1
cid:0
1/fx
cid:1
cid:0
n−1/
2dx+3
cid:0
1/fx
cid:1
cid:0
n−1/
2dx+3
n−1/
2dx+3
n/fx
cid:1
n/fx
3c2µ
n−1/
2dx+3
j=1
202
202
comes
fact
therefore
|wi
cid:48
cid:48
3c2µ
n−1/
2dx+3
given
suﬃciently
large
log
3c2µ
n−1/
2dx+3
cid:17
cid:48
cid:48
max
1≤i≤n
cid:16
cid:16
cid:16
˜fx
n−1/
2dx+3
cid:17
wi|
log
3c2µ
n−1/
2dx+3
|wi
cid:48
cid:48
max
1≤i≤n
cid:17
together
188
193
proves
desired
convergence
ﬁrst
term
proof
proposition
proof
steps
similar
proposition
requiring
citations
properties
r´enyi
divergence
asymmetric
information
clearly
axiom
holds
follows
standard
result
almost
everywhere
csi95
axiom
suppose
cmiλ
pz|x
achieved
pz|y
uti-
lizing
data-processing
inequality
asymmetric
mutual
information
equation
pv10
get
consider
joint
distribution
cmiλ
max
pz|x
cmiλ
pz|x
203
thus
axiom
satisﬁed
consider
axiom
joint
distribution
let
marginal
cmiλ
pz|y
max
pz|y
pz|x
cmiλ
pz|x
pz|y
204
axiom
asymmetric
mutual
information
additivity
property
traditional
mutual
information
theorem
veh14
corresponding
additivity
cmiλ
follows
axiom
information-centroid
representation
cmiλ
states
see
csi95
equation
pv10
cmiλ
min
max
|x=x
cid:107
205
|x=x
cid:48
cid:80
characterization
allows
make
observation
cmiλ
function
con-
vex
hull
probability
distributions
|x=x
earlier
given
conditional
probability
distribution
augment
input
alphabet
one
input
symbol
cid:48
αxpy
|x=x
convex
combination
conditional
distributions
claim
cmiλ
new
channel
unchanged
one
direction
obvious
i.e.
new
channel
capacity
greater
equal
original
channel
since
adding
new
symbol
decrease
capacity
show
direction
use
observe
due
quasi
convexity
r´enyi
divergence
arguments
theorem
veh14
get
|x=x
cid:48
cid:107
αxpy
|x=x
cid:107
max
|x=x
cid:107
cid:88
thus
cmiλ
function
convex
hull
range
map
satisfying
axiom
function
monotonic
directly
205
thus
satisfying
axiom
axiom
ﬁxed
output
alphabet
clear
maxx
cmiλ
log
|y|
suppose
conditional
distribution
cmiλ
log
|y|
implies
optimizing
input
distribution
log
|y|
implies
log
|y|
thus
deterministic
function
essential
support
since
log
|y|
schur
concavity
r´enyi
entropy
theorem
hv15
implies
uniform
distribution
deterministic
function
onto
