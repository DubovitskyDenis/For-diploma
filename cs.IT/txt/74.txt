diffusion
kernel
lms
algorithm
nonlinear
adaptive
networks
symeon
chouvardas
moez
draief
mathematical
algorithmic
sciences
lab
huawei
france
paris
france
symeon.chouvardas
moez.draief
huawei.com
abstract
work
presents
distributed
algorithm
nonlinear
adaptive
learning
particular
set
nodes
obtain
mea-
surements
sequentially
one
per
time
step
related
via
nonlinear
function
goal
collectively
minimize
cost
function
employing
diffusion
based
kernel
least
mean
squares
klms
algorithm
follows
adapt
combine
mode
cooperation
moreover
theoret-
ical
properties
algorithm
studied
proved
certain
assumptions
algorithm
suffers
re-
gret
bound
finally
comparative
experiments
verify
proposed
scheme
outperforms
variants
lms
index
terms—
adaptive
networks
diffusion
rkhs
kernel
lms
introduction
recent
years
interest
topic
distributed
learn-
ing
inference
grown
rapidly
mainly
due
constantly
increasing
requirements
memory
com-
putational
resources
demanded
modern
applications
cope
huge
amount
available
data
data
spring
several
sources/applications
com-
munication
imaging
medical
platforms
well
social-
networking
sites
e.g.
natural
way
deal
large
number
data
need
processed
split
problem
subproblems
resort
distributed
oper-
ations
thus
development
algorithms
dealing
scenarios
data
available
sin-
gle
location
instead
spread
multiple
locations
becomes
essential
important
application
within
distributed
learning
context
one
distributed
adaptive
learning
nutshell
problem
considers
decentralized
network
consisting
nodes
interested
performing
speciﬁc
task
instance
parameter
estimation
classiﬁca-
tion
etc
nodes
constantly
obtain
new
measurements
continuously
adapt
learn
gives
capa-
bility
track
adapt
changes
environment
top
assumed
central
node
could
perform
necessary
operations
nodes
act
independent
learners
perform
computations
finally
task
interest
considered
common
similar
across
nodes
direction
cooperate
cooperation
proved
beneﬁcial
learning
process
since
improves
learning
performance
paper
concerned
problem
distributed
adaptive
learning
reproducing
kernel
hilbert
spaces
rhks
speciﬁc
consider
ad–hoc
net-
work
nodes
obtain
input/output
measurements
sequentially
one
per
time
step
related
via
nonlinear
un-
known
function
cope
nonlinearity
resort
family
kernel–based
algorithms
nonlinear
adap-
tive
ﬁltering
particular
proposed
algorithm
belongs
kernel
lms
klms
algorithmic
family
follows
diffusion
rationale
cooperation
among
nodes
related
work
several
studies
distributed
adaptive
estimation
linear
systems
proposed
litera-
ture
include
diffusion
based
algorithms
e.g.
consensus
ones
e.g.
well
algorithms
mul-
titask
learning
problem
non–linear
adaptive
estimation
rkhs
studied
e.g.
recent
study
considers
problem
nonlinear
adap-
tive
ﬁltering
distributed
networks
found
major
differences
paper
work
sum-
marized
sequel
first
authors
consider
prede-
ﬁned
dictionary
essentially
makes
dimension
problem
ﬁnite
equal
number
elements
dictionary
contrary
consider
gen-
eral
case
dictionary
allowed
grow
time
in-
creases
present
general
form
algorithm
furthermore
study
ﬁrst
time
theoretical
properties
diffusion
kernel
lms
dklms
derive
regret
bounds
proposed
scheme
contributions
paper
propose
novel
nonlin-
ear
distributed
algorithm
adaptive
learning
particular
propose
klms
algorithm
follows
diffusion
rationale
adapt
combine
mode
cooperation
among
nodes
followed
speciﬁc
assume
nodes
obtain
measurements
arrive
sequentially
fig
ad–hoc
network
related
via
nonlinear
system
goal
min-
imization
expected
value
networkwise
discrep-
ancy
desired
output
estimated
one
direction
step
nodes
perform
local
up-
date
step
exploiting
recent
measurements
co-
operate
order
enhance
estimates
comparative
experiments
illustrate
proposed
scheme
outperforms
lms
variants
theoretical
properties
proposed
scheme
discussed
notation
lowercase
uppercase
boldfaced
letters
stand
vectors
matrices
respectively
symbol
stands
set
real
numbers
set
nonnegative
integers
denotes
inﬁnite
dimensional
hilbert
space
equipped
inner
product
denoted
hf1
f2i
∀f1
induced
norm
given
kfk
phf
given
set
term
|s|
de-
note
cardinality
problem
statement
consider
ad–hoc
network
illustrated
fig
consist-
ing
nodes
node
discrete
time
instance
access
scalar
vector
related
via
unknown
yet
common
nodes
function
belonging
hilbert
space
term
stands
additive
noise
process
overall
goal
estimation
function
minimizes
cost
k∈n
distributed
collaborative
fashion
nodes
want
minimize
cost
relying
solely
local
pro-
cessing
well
interactions
neighbors
2.1.
linear
diffusion
lms
order
help
reader
grasp
concept
diffusion
lms
section
describe
linear
scenario
i.e.
one
function
estimated
vector
say
essentially
becomes
cost
function
minimized
case
written
follows
k∈n
cid:0
cid:1
cost
includes
information
coming
whole
net-
work
order
minimize
global
knowledge
re-
quired
nevertheless
distributed
decentralized
learn-
ing
node
interact
exchange
information
neighborhood
denoted
fully
distributed
algorithm
employed
estimation
diffusion
lms
see
example
starting
point
scheme
modiﬁcation
steepest–descent
method
properly
reformulated
enable
distributed
operations
avoid
global
computation
interested
reader
referred
ad-
dition
instantaneous
approximation
adopted
according
statistical
values
substituted
instanta-
neous
ones
e.g.
node
updates
estimate
time
step
according
following
iterative
scheme
µkek
lw′
l∈nk
step
size
furthermore
stand
combination
coefﬁcients
following
properties
pl∈n
common
choice
among
others
choosing
coefﬁcients
metropolis
rule
weights
equal
max
|nk|
|nl|
pl∈nk\k
otherwise
intuition
behind
scheme
presented
summarized
follows
ﬁrst
step
node
updates
estimate
using
lms
based
update
adaptation
step
ex-
ploiting
local
information
sequel
cooperates
neighborhood
combining
intermediate
estimates
obtain
updated
estimate
weights
assign
non–negative
weight
estimates
received
neigh-
borhood
whereas
equal
zero
rest
nodes
hence
node
aggregates
information
received
neighborhood
scheme
also
known
adapt
combine
atc
diffusion
strategy
2.2.
centralized
kernel
lms
2.2.1.
preliminaries
let
provide
elementary
properties
rkhs
used
sequel
throughout
section
node
subscript
suppressed
since
describe
properties
centralized
learning
consider
real
hilbert
space
comprising
functions
deﬁned
function
rm×
called
reproducing
kernel
following
properties
hold
function
belongs
holds
properties
hold
called
reproducing
kernel
hilbert
space
typical
example
gaussian
ker-
nel
deﬁnition
exp
−βkxi−xjk2
important
property
exploited
se-
quel
states
points
rkhs
written
follows
n=0
αnκ
finally
reproducing
kernel
continuous
symmetric
positive-deﬁnite
2.2.2.
kernel
lms
kernel
lms
originally
proposed
gen-
eralization
original
lms
algorithm
utilizes
transformed
input
i.e.
iteration
step
put
mathematical
terms
recursion
klms
given
hfn−1
fn−1
since
space
may
inﬁnite
dimensional
may
dif-
ﬁcult
direct
access
transformed
input
data
function
however
back
forget
distributed
aspect
see
quantity
in-
terest
computed
exploiting
particular
following
similar
steps
shown
klms
recursion
equivalently
written
employ
non–linear
transformation
input
similarly
resulting
recursion
n−1
µkek
l∈nk
deﬁned
similarly
despite
fact
seems
trivial
generalization
al-
ready
discussed
previously
one
resort
directly
form
iterations
since
access
transformed
data
may
possible
exploiting
lemma
presented
shortly
bypass
aforementioned
problem
deriving
inner
product
obtained
function
trans-
formed
input
vector
closed
form
proceed
let
introduce
notation
networkwise
function
time
denoted
cartesian
product
similarly
deﬁne
networkwise
input
µ1e1
µkek
finally
gather
combination
coefﬁcients
matrix
l–th
entry
contains
readily
shown
written
whole
network
following
compact
form
times
cid:16
n−1
cid:17
lemma
assume
fk,0
equation
equivalently
written
i=1
an−i+1g
n−1
i=1
i=1
hence
vector
responses
˜d1
˜dk
time
instance
given
i=1
an−i+1g
proof
follows
mathematical
induction
omitted
due
lack
space
presented
elsewhere
cid:4
note
reformulation
convenient
computes
response
estimated
function
input
without
need
estimate
function
diffusion
kernel
lms
section
describe
proposed
algorithm
together
theoretical
properties
recall
problem
con-
sideration
discussed
section
goal
bring
together
tools
described
sections
2.1
2.2
derive
kernel
based
lms
algorithm
suitable
distributed
op-
eration
starting
point
atc–lms
described
remark
coefﬁcient
reduction
time
take
closer
look
seen
number
coefﬁcients
one
store
well
required
num-
ber
operations
grow
time
evolves
several
sophisti-
cated
techniques
set
coefﬁcients
zero
avoiding
performance
degradation
pro-
posed
literature
e.g.
become
ap-
parent
simulations
section
adopt
simple
method
apply
buffer
size
case
store
recent
coefﬁcients
becomes
i=max
n−l+1
an−i+1g
-10
-15
-20
-25
linear
dlms
kdlms
proposed
non-cooperative
klms
100
200
150
250
iteration
number
300
350
400
-10
-15
-20
100
linear
dlms
kdlms
proposed
non
cooperative
klms
300
350
400
150
200
250
iteration
number
fig
average
mse
ﬁrst
experiment
fig
average
mse
second
experiment
equals
remark
coefﬁcient
reduction
space
shown
see
appendix
l–th
entry
i–th
power
matrix
.pk
ji−1
akj1
akj2
aji−1l
last
relation
difﬁcult
obtain
node
exploits
information
nodes
belong
neighor-
hood
however
break
rules
decentralized
learning
since
also
holds
nonzero
iff
distance
measured
hops
smaller
equal
hops
hence
nodes
send
input
vectors
neighbors
turn
forward
neighbors
increases
network
load
one
avoid
setting
weights
zero
discussed
remark
simple
strategy
set
zero
coefﬁcients
belong
nodes
belong
neighborhood
3.1.
theoretical
properties
sequel
present
regret
bound
pro-
posed
scheme
particular
show
grows
sublinearly
time
theorem
certain
assumptions
boundness
input
step-size
combination
weights
networkwise
regret
bounded
i=1
i−1
γ√n
k∈n
constants
positive
cid:4
proof
proof
omitted
due
lack
space
presented
elsewhere
simulations
section
performance
proposed
algorithm
validated
within
distributed
nonlinear
adaptive
ﬁlter-
ing
framework
consider
network
comprising
nodes
distributed
version
problem
studied
input
output
related
via
gaussian
variance
10−3
input
also
gaussian
variance
0.1χk
0.5
respect
uniform
distribution
compare
proposed
algorithm
linear
dif-
fusion
lms
non–cooperative
klms
i.e.
klms
nodes
cooperate
kernel
based
algorithms
employ
gaussian
kernel
1.1
choose
step–size
equal
0.6
algorithms
furthermore
combination
weights
chosen
respect
metropolis
rule
buffer
size
node
equals
100
take
consideration
infor-
mation
coming
single
hop
neighbors
finally
adopted
performance
metric
average
mse
def-
inition
1/k
pk∈n
seen
fig
kdlms
outperforms
lms
variants
since
converges
faster
lower
error
ﬂoor
compared
second
experiment
setup
sim-
ilar
previous
one
albeit
increase
variance
noise
equals
10−1
fig
illustrates
enhanced
performance
kdlms
compared
algorithms
retained
scenario
well
conclusions
future
research
paper
novel
kernel
based
diffusion
lms
suitable
non–linear
distributed
adaptive
ﬁltering
proposed
theoretical
properties
algorithm
discussed
performance
scheme
tested
adaptive
strategies
future
research
focuses
accelerating
convergence
speed
utilizing
data
per
iteration
well
investigating
sophisticated
strategies
reduce
number
coefﬁcients
storing
informative
ones
references
konstantinos
slavakis
georgios
giannakis
gon-
zalo
mateos
modeling
optimization
big
data
analytics
statistical
learning
tools
era
data
deluge
ieee
signal
processing
magazine
vol
18–31
2014
cheng
chu
sang
kyun
kim
yi-an
lin
yuanyuan
gary
bradski
andrew
kunle
olukotun
map-reduce
machine
learning
multicore
ad-
vances
neural
information
processing
systems
vol
281
2007
paul
zikopoulos
chris
eaton
al.
understanding
big
data
analytics
enterprise
class
hadoop
stream-
ing
data
mcgraw-hill
osborne
media
2011
ali
sayed
diffusion
adaptation
networks
academic
press
library
signal
processing
vol
323–454
2013
symeon
chouvardas
konstantinos
slavakis
ser-
gios
theodoridis
adaptive
robust
distributed
learn-
ing
diffusion
sensor
networks
ieee
transactions
signal
processing
vol
4692–4707
2011
cassio
lopes
ali
sayed
diffusion
least-mean
squares
adaptive
networks
formulation
per-
formance
analysis
ieee
transactions
signal
pro-
cessing
vol
3122–3136
2008
federico
cattivelli
ali
sayed
diffusion
lms
strategies
distributed
estimation
ieee
transactions
signal
processing
vol
1035–1048
2010
ioannis
schizas
gonzalo
mateos
georgios
giannakis
distributed
lms
consensus-based
in-
network
adaptive
processing
ieee
transactions
signal
processing
vol
2365–2382
2009
gonzalo
mateos
ioannis
schizas
georgios
giannakis
distributed
recursive
least-squares
consensus-based
in-network
adaptive
estimation
sig-
nal
processing
ieee
transactions
vol
4583–4588
2009
jie
chen
c´edric
richard
ali
sayed
multitask
diffusion
adaptation
networks
ieee
transactions
signal
processing
vol
4129–4144
2014
jorge
plata-chaves
nikola
bogdanovic
kostas
berberidis
distributed
diffusion-based
lms
node-
speciﬁc
adaptive
parameter
estimation
ieee
transac-
tions
signal
processing
vol
3448–
3460
2015
pantelis
bouboulis
sergios
theodoridis
exten-
sion
wirtinger
calculus
reproducing
kernel
hilbert
spaces
complex
kernel
lms
ieee
transac-
tions
signal
processing
vol
964–978
2011
pantelis
bouboulis
sergios
theodoridis
michael
mavroforakis
augmented
complex
kernel
lms
ieee
transactions
signal
processing
vol
4962–4967
2012
konstantinos
slavakis
sergios
theodoridis
isao
yamada
online
kernel-based
classiﬁcation
using
adaptive
projection
algorithms
ieee
transactions
signal
processing
vol
2781–2796
2008
konstantinos
slavakis
sergios
theodoridis
isao
yamada
adaptive
constrained
learning
reproduc-
ing
kernel
hilbert
spaces
robust
beamforming
case
ieee
transactions
signal
processing
vol
4744–4764
2009
wei
gao
jie
chen
c´edric
richard
jianguo
huang
diffusion
adaptation
networks
kernel
least-
mean-square
computational
advances
multi-
sensor
adaptive
processing
camsap
2015
ieee
in-
ternational
workshop
2015
submitted
simon
haykin
adaptive
ﬁlter
theory
pearson
edu-
cation
india
2008
alex
smola
bernhard
sch¨olkopf
learning
kernels
citeseer
1998
sergios
theodoridis
machine
learning
bayesian
optimization
perspective
academic
press
2015
weifeng
liu
puskal
pokharel
jose
principe
kernel
least-mean-square
algorithm
ieee
trans-
actions
signal
processing
vol
543–
554
2008
wei
gao
jie
chen
cedric
richard
jianguo
huang
online
dictionary
learning
kernel
lms
ieee
transactions
signal
processing
vol
2765–2777
2014
kumpati
narendra
kannan
parthasarathy
iden-
tiﬁcation
control
dynamical
systems
using
neural
networks
ieee
transactions
neural
networks
vol
4–27
1990
danilo
mandic
generalized
normalized
gradient
ieee
signal
processing
letters
descent
algorithm
vol
115–118
2004
