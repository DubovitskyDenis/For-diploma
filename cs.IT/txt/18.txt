cluster-seeking
james-stein
estimators
pavan
srinath
ramji
venkataramanan
abstract—this
paper
considers
problem
estimating
high-dimensional
vector
parameters
noisy
observation
noise
vector
i.i.d
gaussian
known
variance
squared-error
loss
function
james-stein
estimator
known
dominate
simple
maximum-likelihood
estimator
dimension
exceeds
two
js-
estimator
shrinks
observed
vector
towards
origin
risk
reduction
ml-estimator
greatest
lie
close
origin
js-estimators
generalized
shrink
data
towards
target
subspace
estimators
also
dominate
ml-estimator
risk
reduction
signiﬁcant
lies
close
subspace
leads
question
absence
prior
information
design
estimators
give
signiﬁcant
risk
reduction
ml-
estimator
wide
range
paper
propose
shrinkage
estimators
attempt
infer
structure
observed
data
order
construct
good
attracting
subspace
particular
components
observed
vector
separated
clusters
elements
cluster
shrunk
towards
common
attractor
number
clusters
attractor
cluster
determined
observed
vector
provide
concentration
results
squared-error
loss
convergence
results
risk
proposed
estimators
results
show
estimators
give
signiﬁcant
risk
reduction
ml-estimator
wide
range
particularly
large
simulation
results
provided
support
theoretical
claims
index
terms—high-dimensional
estimation
large
deviations
bounds
loss
function
estimates
risk
estimates
shrinkage
esti-
mators
introduction
onsider
problem
estimating
vector
parameters
noisy
observation
form
noise
vector
distributed
σ2i
i.e.
components
i.i.d
gaussian
random
variables
mean
zero
variance
emphasize
deterministic
joint
probability
density
function
given
2πσ2
cid:107
y−θ
cid:107
2σ2
performance
estimator
measured
using
squared-error
loss
function
given
cid:107
cid:107
work
supported
part
marie
curie
career
integration
grant
grant
agreement
631489
early
career
grant
isaac
newton
trust
paper
presented
part
2016
ieee
international
symposium
information
theory
srinath
venkataramanan
department
engineer-
ing
university
cambridge
cambridge
cb2
1pz
e-mail
pk423
rv285
cam.ac.uk
cid:107
cid:107
denotes
euclidean
norm
risk
estimator
given
expected
value
loss
function
cid:104
cid:107
cid:107
cid:105
expectation
computed
using
density
normalized
risk
applying
maximum-likelihood
criterion
yields
ml-estimator
ˆθm
ml-estimator
unbiased
estimator
risk
ˆθm
nσ2
goal
paper
design
estimators
give
signiﬁcant
risk
reduction
ˆθm
wide
range
without
prior
assumptions
structure
1961
james
stein
published
surprising
result
proposing
estimator
uniformly
achieves
lower
risk
ˆθm
estimator
ˆθjs
given
cid:20
cid:107
cid:107
risk
chapter
thm
5.1
ˆθjs
cid:21
cid:21
cid:20
cid:107
cid:107
nσ2
2σ4e
cid:16
cid:17
ˆθjs
hence
ˆθjs
ˆθm
nσ2
estimator
ˆθ1
said
dominate
another
estimator
ˆθ2
ˆθ1
ˆθ2
inequality
strict
least
one
thus
im-
plies
james-stein
estimator
js-estimator
dominates
ml-estimator
unlike
ml-estimator
js-estimator
non-linear
biased
however
risk
reduction
ml-estimator
signiﬁcant
making
attractive
option
many
situations
see
example
evaluating
expression
shown
risk
js-estimator
depends
via
cid:107
cid:107
risk
decreases
cid:107
cid:107
decreases
intuition
note
large
cid:107
cid:107
nσ2
cid:107
cid:107
dependence
risk
cid:107
cid:107
illustrated
fig
average
loss
js-estimator
plotted
versus
cid:107
cid:107
two
different
choices
js-estimator
shrinks
element
towards
origin
extending
idea
js-like
estimators
deﬁned
shrinking
towards
vector
generally
towards
target
subspace
let
denote
pro-
jection
onto
cid:107
y−pv
cid:107
minv∈v
cid:107
y−v
cid:107
js-estimator
shrinks
towards
subspace
attracting
vector
would
give
signiﬁcant
risk
reduction
ˆθm
one
motivation
lindley
estimator
comes
guess
components
close
empirical
mean
since
know
approximate
use
attracting
vector
¯y1
fig
shows
performance
ˆθjs
ˆθl
depends
structure
left
panel
ﬁgure
empiri-
cal
mean
always
risks
estimators
increase
monotonically
cid:107
cid:107
right
panel
components
equal
case
distance
i=1
2/n
risk
vary
cid:107
cid:107
contrast
risk
ˆθjs
increases
cid:107
cid:107
attracting
vector
attracting
vector
ˆθl
cid:107
¯y1
cid:107
cid:112
cid:80
risk
reduction
obtained
using
js-like
shrinkage
es-
timator
ˆθm
crucially
depends
choice
attracting
vector
achieve
signiﬁcant
risk
reduction
wide
range
paper
infer
structure
data
choose
attracting
vectors
tailored
structure
idea
partition
clusters
shrink
components
cluster
towards
common
element
attractor
number
clusters
attractor
cluster
determined
based
data
motivating
example
consider
half
components
equal
cid:107
cid:107
half
equal
cid:107
cid:107
fig
shows
risk
reduction
ˆθjs
ˆθl
diminish
cid:107
cid:107
gets
larger
empirical
mean
close
zero
hence
ˆθjs
ˆθl
shrink
towards
ideal
js-estimator
would
shrink
towards
attractor
cid:107
cid:107
corresponding
cid:107
cid:107
remaining
observations
towards
cid:107
cid:107
estimator
would
give
handsome
gains
ˆθm
structure
hand
components
equal
lindley
estimator
ˆθl
excellent
choice
signiﬁcantly
smaller
risk
ˆθm
values
cid:107
cid:107
fig
would
like
intelligent
estimator
correctly
distinguish
different
structures
two
choose
appropriate
attracting
vector
based
propose
estimators
sections
iii
reasonably
large
estimators
choose
good
attracting
subspace
tailored
structure
use
approximation
best
attracting
vector
within
subspace
main
contributions
paper
follows
construct
two-cluster
js-estimator
provide
con-
centration
results
squared-error
loss
asymp-
totic
convergence
results
risk
though
estima-
tor
dominate
ml-estimator
shown
provide
signiﬁcant
risk
reduction
lindley
estimator
regular
js-estimator
components
approximately
separated
two
clusters
present
hybrid
js-estimator
large
risk
close
minimum
lindley
estimator
proposed
two-cluster
js-estimator
thus
hybrid
estimator
asymptotically
dominates
ml-estimator
lindley
estimator
gives
signif-
icant
risk
reduction
ml-estimator
wide
range
fig
comparison
average
normalized
loss
regular
js-estimator
lindley
estimator
positive-part
versions
function
cid:107
cid:107
loss
ml-estimator
cid:107
cid:107
···
10.
cid:107
cid:107
···
cid:107
cid:107
cid:34
cid:35
cid:107
cid:107
dimension
v.1
classic
example
estimator
lindley
estimator
shrinks
towards
one-dimensional
subspace
deﬁned
all-ones
vector
given
cid:20
cid:107
¯y1
cid:107
cid:21
ˆθl
¯y1
cid:80
i=1
empirical
mean
shown
different
variants
js-estimator
dominate
ml-estimator.2
js-estimators
share
following
key
property
smaller
euclidean
distance
attracting
vector
smaller
risk
throughout
paper
term
attracting
vector
refers
vector
shrunk
towards
ˆθjs
attracting
vector
risk
reduction
ˆθm
larger
cid:107
cid:107
close
zero
similarly
components
clustered
around
value
js-estimator
1the
dimension
greater
estimator
achieve
lower
risk
ˆθm
2the
risks
js-estimators
form
usually
computed
using
stein
lemma
states
cid:48
standard
normal
random
variable
weakly
differentiable
function
¯y1
0123456700.20.40.60.811.21.41.6∥θ∥˜r
regular
js−estimatorjs−estimator
positive
partlindley
estimatorlindley
estimator
positive
partml−estimator0123456700.20.40.60.811.21.41.6∥θ∥˜r
regular
js−estimatorjs−estimator
positive
partlindley
estimatorlindley
estimator
positive
partml−estimator
generalize
idea
deﬁne
general
multiple-
cluster
hybrid
js-estimators
provide
concentration
convergence
results
squared-error
loss
risk
respectively
provide
simulation
results
support
theoretical
results
loss
function
simulations
indicate
hybrid
estimator
gives
signiﬁcant
risk
reduction
ml-estimator
wide
range
even
modest
values
e.g
50.
empirical
risk
hybrid
estimator
converges
rapidly
theoretical
value
growing
related
work
george
proposed
multiple
shrinkage
estimator
convex
combination
multiple
subspace-based
js-
estimators
form
coefﬁcients
deﬁning
convex
combination
give
larger
weight
estimators
whose
target
subspaces
closer
leung
barron
also
studied
similar
ways
combining
estimators
risk
properties
proposed
estimators
also
seek
emulate
best
among
class
subspace-based
estimators
key
differences
target
subspaces
ﬁxed
priori
possibly
based
prior
knowledge
might
lie
absence
prior
knowledge
may
possible
choose
good
target
subspaces
motivates
estimators
proposed
paper
use
target
subspace
constructed
data
nature
clustering
inferred
used
deﬁne
suitable
subspace
another
difference
earlier
work
attracting
vector
determined
given
target
subspace
rather
choosing
attracting
vector
projection
onto
use
approximation
projection
onto
approximation
computed
concentration
inequalities
provided
guarantee
goodness
approximation
risk
js-like
estimator
typically
computed
using
stein
lemma
however
data-dependent
subspaces
use
result
estimators
hard
analyze
using
technique
therefore
use
concentration
inequalities
bound
loss
function
proposed
estimators
conse-
quently
theoretical
bounds
get
sharper
dimension
increases
may
accurate
small
however
even
relatively
small
simulations
indicate
risk
reduction
ml-estimator
signiﬁcant
wide
range
noting
shrinkage
factor
multiplying
could
negative
stein
proposed
following
positive-part
js-
estimator
cid:20
cid:107
cid:107
cid:21
ˆθjs+
denotes
max
similarly
deﬁne
positive-part
versions
js-like
estimators
positive-part
lindley
estimator
given
¯y1
cid:20
cid:107
¯y1
cid:107
ˆθl+
¯y1
cid:21
baranchik
proved
ˆθjs+
dominates
ˆθjs
result
also
proves
ˆθl+
dominates
ˆθl
estimators
dominate
ˆθjs+
discussed
fig
shows
positive-
part
versions
give
noticeably
lower
loss
regular
lindley
estimators
however
large
shrinkage
factor
positive
high
probability
hence
positive-part
estimator
nearly
always
identical
regular
js-estimator
indeed
large
cid:107
cid:107
+σ2
shrinkage
factor
cid:107
cid:107
cid:107
cid:107
nσ2
cid:19
cid:18
cid:107
cid:107
cid:18
cid:19
analyze
positive-part
version
proposed
hybrid
estimator
using
concentration
inequalities
though
guarantee
hybrid
estimator
dominates
positive-
part
lindley
estimators
ﬁnite
show
large
loss
hybrid
estimator
equal
minimum
positive-part
lindley
estimator
cluster-based
estimator
high
probability
theorems
rest
paper
organized
follows
section
two-cluster
js-estimator
proposed
performance
analyzed
section
iii
presents
hybrid
js-estimator
along
performance
analysis
general
multiple-attractor
js-
estimators
discussed
section
simulation
results
corroborate
theoretical
analysis
provided
section
proofs
main
results
given
section
concluding
remarks
possible
directions
future
research
constitute
section
vii
notation
cid:82
bold
lowercase
letters
used
denote
vectors
plain
lowercase
letters
entries
example
entries
···
vectors
length
column
vectors
unless
otherwise
mentioned
vectors
cid:104
cid:105
denotes
euclidean
inner
product
all-zero
vector
all-one
vector
length
denoted
respectively
complement
set
denoted
ﬁnite
set
real-valued
elements
min
denotes
minimum
elements
use
denote
indicator
function
event
central
chi-squared
distributed
random
variable
degrees
freedom
denoted
q-function
given
random
variable
denotes
max
real-valued
functions
notation
means
limx→0
means
limx→∞
positive
constant
p−→
n=1
l1−→
respectively
denote
convergence
probability
almost
sure
convergence
convergence
norm
random
variable
ities
let
variables
notation
random
variable
constant
means
use
following
shorthand
concentration
inequal-
n=1
sequence
random
either
sequence
random
variables
a.s.−→
exp
|xn
min
2,1
max
cid:107
cid:107
2/n,1
cid:34
cid:35
positive
constants
depend
exact
values
speciﬁed
shrinkage
estimators
propose
general
form
nσ2
cid:107
cid:107
ith
component
attracting
vector
attractor
point
towards
shrunk
two-cluster
james-stein
estimator
half
equal
cid:107
cid:107
recall
example
section
half
compo-
nents
equal
cid:107
cid:107
ideally
would
like
shrink
corresponding
ﬁrst
group
towards
cid:107
cid:107
remaining
points
towards
cid:107
cid:107
however
without
oracle
accurately
guess
point
shrunk
towards
would
like
obtain
estimator
identiﬁes
separable
clusters
constructs
suitable
attractor
cluster
shrinks
cluster
towards
attractor
start
dividing
observed
data
two
clusters
based
separating
point
obtained
natural
choice
would
empirical
mean
since
unknown
use
deﬁne
clusters
points
shrunk
towards
attractors
respectively
deﬁned
later
section
brevity
henceforth
indicate
dependence
attractors
thus
attracting
vector
yn≤¯y
deﬁned
proposed
estimator

...


cid:35
cid:16
cid:107
cid:107
2/n
nσ2
cid:107
cid:107
cid:34
1
ˆθjs2
function
deﬁned
max

y1≤¯y
y2≤¯y
...
cid:17
lies
attracting
vector
two-
dimensional
subspace
deﬁned
orthogonal
vectors
···
y1≤¯y
···
yn≤¯y
de-
rive
values
useful
compare
attracting
vector
lindley
estimator
recall
lindley
attracting
vector
lies
one-dimensional
subspace
spanned
vector
lying
subspace
closest
euclidean
distance
projection
¯θ1
since
unknown
use
approximation
deﬁne
attracting
vector
¯y1
analogously
vector
two-dimensional
subspace
deﬁned
closest
projection
onto
subspace
computing
projection
desired
values
found
i=1
θi1
i=1
i=1
θi1
yi≤¯y
i=1
yi≤¯y
cid:80
cid:80
cid:80
cid:80
ades
ades
ades
available
deﬁne
attractors
approximations
ades
obtained
using
following
concentration
results
lemma
yi1
θi1
¯θ−θi
2σ2
i=1
i=1
i=1
i=1
i=1
θi1
yi1
yi≤¯y
cid:88
cid:88
cid:88
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:32
cid:16
¯θ−θi
cid:88
cid:88
cid:88
cid:88
cid:88
yi≤¯y
cid:88
cid:17
θi1
yi≤¯y
i=1
i=1
i=1
i=1
i=1
i=1
i=1
θiq
θiqc
θi1
yi≤¯y
cid:18
cid:19
cid:18
cid:19
cid:19
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:18
cid:19
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:18
cid:17
cid:16
¯θ−θi
cid:88
cid:88
i=1
i=1
¯θ−θi
2σ2
ke−nk2
ke−nk2
recall
section
i-b
shorthand
concentration
inequality
symbol
form
cid:80
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
2nδ
cid:88
i=0
proof
given
appendix
using
lemma
obtain
estimates
ades
ades
term
achieved
via
following
provided
estimate
¯θ−θi
2σ2
concentration
result
lemma
fix
i=1
cid:32
cid:88
i=0
|yi−¯y|≤δ
¯θ−θi
2σ2
κnδ
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
10e−nk2
positive
constant
|κn|
2πe
proof
given
appendix
note
henceforth
paper
used
denote
generic
bounded
constant
whose
exact
value
needed
coefﬁcient
expressions
form
κnδ
constant
example
a+bδ
illustrate
usage
let
using
lemmas
two
attractors
deﬁned
|bδ|
κnδ
cid:80
i=1
yi1
cid:80
i=1
yi1
yi≤¯y
cid:80
cid:80
cid:80
cid:80
i=1
i=1
yi≤¯y
1+bδ/a
i=0
|yi−¯y|≤δ
i=0
|yi−¯y|≤δ
chosen
small
positive
number
completes
speciﬁcation
attracting
vector
hence
two-cluster
js-estimator
closest
note
deﬁned
approxima-
tion
projection
onto
two-dimensional
sub-
space
spanned
vectors
1y1
···
1yn
1y1≤¯y
···
1yn≤¯y
remark
approximates
vector
distinct
projection
onto
analysis
easier
would
terms
involving
chosen
projection
instead
onto
numerical
simulations
suggest
choice
yields
signiﬁcantly
higher
risk
intuition
behind
choosing
projection
onto
group
attracted
common
point
without
prior
information
natural
choice
would
mean
within
group
mean
i=1
θi1
yi≥¯y
different
determined
term
cid:80
cid:80
cid:33
cid:32
cid:88
cid:80
i=1
yi1
yi≥¯y
yi≥¯y
i=1
wi1
yi≥¯y
involving
i=1
term
cid:32
cid:88
wi1
yi≥¯y
cid:54
i=1
approximates
cid:33
note
attracting
vector
dependent
also
two
attractors
lemma
deviation
probability
fall
exponentially
needs
held
constant
independent
practical
design
point
view
needed
nδ2
cid:29
indeed
i=0
|yi−¯y|≤δ
reliable
¯θ−θi
approximation
term
shown
appendix
speciﬁcally
100
need
nδ2
cid:29
numerical
experiments
suggest
value
large
enough
good
approximation
cid:80
cid:80
i=1
2nδ
2σ2
present
ﬁrst
main
result
paper
theorem
loss
function
two-cluster
js-estimator
satisﬁes
following
ﬁxed
independent
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:19
cid:18
cid:20
cid:21
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:107
ˆθjs2
cid:107
min
βnσ2
κnδ
min
2,1
max
cid:107
cid:107
2/n,1
given
positive
constant
independent
another
positive
constant
independent
ﬁxed
sequence
increasing
dimension
lim
supn→∞
cid:107
cid:107
2/n
βnσ2
ˆθjs2
min
cid:18
cid:20
cid:19
lim
n→∞
κnδ
cid:12
cid:12
cid:12
cid:12
constants
given
cid:19
i=1
cid:18
cid:88
cid:19
cid:32
cid:88
cid:18
cid:16
¯θ−θi
cid:17
cid:17
cid:16
¯θ−θi
i=1
cid:18
i=1
cid:88
cid:33
i=1
θiqc
cid:16
¯θ−θi
cid:80
cid:16
¯θ−θi
cid:80
2σ2
i=1
¯θ−θi
cid:17
cid:17
cid:107
cid:107
cid:80
cid:80
i=1
θiq
i=1
cid:21
cid:12
cid:12
cid:12
cid:12
cid:19
proof
theorem
given
section
vi-b
remark
theorem
represents
concentrating
value
distance
attracting
vector
shown
sec
vi-b
cid:107
θ−ν2
cid:107
2/n
concentrates
around
βn+κnδ
therefore
closer
attracting
subspace
lower
normalized
asymptotic
risk
ˆθjs2
term
represents
concentrating
value
distance
shown
sec
vi-b
cid:107
cid:107
2/n
concentrates
around
κnδ
remark
comparing
note
cid:80
cid:16
θi−¯θ
i=1
cid:17
cid:17
cid:16
cid:80
cid:17
cid:16
θi−¯θ
cid:16
θi−¯θ
i=1
i=1
cid:16
cid:80
cid:17
cid:17
see
observe
sum
numerator
function
assigns
larger
weight
terms
terms
furthermore
large
either
|θi
¯θ|
|θi
¯θ|
ﬁrst
case
···
get
cid:107
¯θ1
cid:107
2/n
second
case
suppose
values
equal
remaining
values
equal
−p2
veriﬁed
cid:107
cid:107
n1p2
therefore
asymptotic
normalized
risk
ˆθjs2
converges
cases
proof
theorem
leads
following
corollaries
corollary
loss
function
estimator
satisﬁes
following
positive-part
js-
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
γnσ2
ke−nk
min
2,1
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:13
cid:13
¯θ1
cid:13
cid:13
cid:17
ρnσ2
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:107
ˆθjs+
cid:107
cid:12
cid:12
cid:12
cid:12
lim
n→∞
sequence
increasing
dimension
cid:107
cid:107
2/n
positive
constants
lim
supn→∞
cid:107
cid:107
2/n
ˆθjs+
γnσ2
cid:12
cid:12
cid:12
cid:12
note
positive-part
lindley
estimator
essentially
single-cluster
estimator
shrinks
points
towards
henceforth
denote
ˆθjs1
corollary
loss
function
positive-part
lindley
estimator
satisﬁes
following
ρnσ2
ke−nk
min
2,1
positive
constants
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:107
ˆθjs1
cid:107
cid:16
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
sequence
increasing
dimension
lim
supn→∞
cid:107
cid:107
2/n
lim
n→∞
ˆθjs1
remark
statement
corollary
known
literature
implies
ˆθjs+
asymptotically
minimax
euclidean
balls
indeed
denotes
set
cid:107
cid:107
2/n
pinsker
theorem
implies
minimax
risk
asymptotically
equal
σ2c2
c2+σ2
statement
corollary
statements
corollary
new
best
knowledge
comparing
corollaries
observe
since
cid:107
θ−¯θ1
cid:107
cid:107
cid:107
strict
inequality
whenever
cid:54
therefore
positive-part
lindley
estimator
asymptotically
dominates
positive
part
js-estimator
well
known
ˆθjs+
ˆθjs1
dominate
ml-
estimator
corollary
clear
asymptotically
normalized
risk
ˆθjs+
small
small
i.e.
close
origin
similarly
corollary
asymptotic
normalized
risk
ˆθjs1
small
small
occurs
components
close
mean
natural
ask
two-cluster
estimator
ˆθjs2
dominates
ˆθm
asymptotic
normalized
risk
close
answer
questions
use
following
example
shown
fig
consider
whose
components
take
one
two
values
−ρτ
close
zero
possible
hence
number
components
taking
value
cid:98
nρ/
cid:99
choosing
1000
key
asymptotic
risk
term
min
βn/
theorem
plotted
function
fig
various
values
fig
asymptotic
risk
term
min
βn/
two-cluster
estimator
plotted
1000
different
values
components
take
two
values
−ρτ
number
components
taking
value
cid:98
nρ/
cid:99
two
important
observations
made
plots
firstly
min
βn/
exceeds
certain
values
hence
ˆθjs2
dominate
ˆθm
secondly
normalized
risk
ˆθjs2
goes
zero
large
enough
note
large
cid:107
cid:107
2/n
cid:107
¯θ1
cid:107
2/n
large
hence
normalized
risks
ˆθjs+
ˆθjs1
close
although
ˆθjs2
dominate
ˆθm
ˆθjs+
ˆθjs2
range
ˆθjs2
much
lower
ˆθjs+
ˆθjs1
serves
motivation
designing
hybrid
estimator
attempts
pick
better
ˆθjs1
ˆθjs2
context
described
next
section
example
fig
worth
examining
two-cluster
estimator
performs
poorly
certain
range
giving
signiﬁcantly
risk
reduction
large
enough
first
consider
ideal
case
known
components
theta
equal
ones
equal
−ρτ
although
values
may
known
case
could
use
james-stein
estimator
ˆθjsv
form
target
subspace
two-dimensional
subspace
basis
vectors

θ1=τ
θ2=τ
...
θn=τ


θ1=−ρτ
θ2=−ρτ
...
θn=−ρτ

since
ﬁxed
subspace
depend
data
shown
ˆθjsv
dominates
ml-estimator
actual
problem
access
ideal
basis
vectors
use
ˆθjsv
two-cluster
estimator
ˆθjs2
attempts
approximate
ˆθjsv
choosing
target
subspace
data
shown
done
using
basis
vectors



cid:98
y1≥¯y
y2≥¯y
...
yn≥¯y
cid:98
...

05101520253035404550−0.200.20.40.60.811.2τmin
βn/
αn+1
ρ=0.15ρ=0.1ρ=0.25ρ=0.5ρ=1
since
good
approximation
separation
large
enough
noise
term
unlikely
pull
wrong
region
hence
estimated
basis
vectors
cid:98
cid:98
close
ideal
4.5σ
cid:98
cid:98
approximate
ideal
basis
vectors
ones
indeed
fig
indicates
minimum
separation
equal
least
well
normalized
risk
close
hand
approximation
ideal
basis
vectors
turns
poor
components
neither
close
far
evident
remark
iii
hybrid
james-stein
estimator
two
clusters
depending
underlying
either
positive-part
lindley
estimator
ˆθjs1
two-cluster
estimator
ˆθjs2
could
smaller
loss
theorem
corollary
would
like
estimator
selects
better
among
ˆθjs1
ˆθjs2
context
end
estimate
loss
ˆθjs1
ˆθjs2
based
based
loss
estimates
denoted
ˆθjs1
ˆθjs2
respectively
deﬁne
hybrid
estimator
ˆθjs1
ˆθjs2
ˆθjsh
ˆθjs1
ˆθjs2
respectively
given
given
otherwise
ˆθjs1
ˆθjs2
loss
function
estimates
ˆθjs1
ˆθjs2
obtained
follows
based
corollary
loss
function
ˆθjs1
estimated
via
estimate
ρnσ2/
given
straightforward
check
along
lines
proof
theorem
cid:26
cid:32
cid:107
¯y1
cid:107
ˆθjs1
cid:33
cid:0
cid:1
1
cid:16
cid:107
¯y1
cid:107
2/n
cid:17
therefore
estimate
normalized
loss
ˆθjs1
loss
function
two-cluster
estimator
ˆθjs2
estimated
using
theorem
estimating
deﬁned
respectively
lemma
section
vi-b
cid:107
cid:107
κnδ
using
concentration
inequalities
lemmas
section
deduce
cid:107
cid:107
κnδ
|yi−¯y|≤δ
cid:32
cid:88
i=0
cid:33
deﬁned
use
estimate
concentrating
value
noting
cid:18
cid:19
min
βnσ2
βnσ2
cid:17
max
yields
following
estimate
ˆθjs2
ˆθjs2
cid:80
cid:107
cid:107
cid:16
i=0
|yi−¯y|≤δ
cid:107
cid:107
2/n
loss
function
estimates
complete
speciﬁcation
hybrid
estimator
following
theorem
characterizes
loss
function
hybrid
estimator
showing
loss
estimates
concentrate
around
values
speciﬁed
corollary
theorem
respectively
theorem
loss
function
hybrid
js-estimator
satisﬁes
following
cid:32
cid:107
ˆθjsh
cid:107
cid:33
cid:32
cid:107
ˆθjs1
cid:107
min
cid:107
ˆθjs2
cid:107
cid:33
min
2,1
max
cid:107
cid:107
2/n,1
sequence
increasing
dimension
positive
constants
lim
supn→∞
cid:107
cid:107
2/n
cid:17
cid:17
cid:16
lim
sup
n→∞
ˆθjsh
cid:16
cid:16
cid:104
cid:16
ˆθjs1
min
cid:17
cid:105
cid:19
ˆθjs2
proof
theorem
given
section
vi-c.
theorem
implies
hybrid
estimator
chooses
better
ˆθjs1
ˆθjs2
high
probability
probability
choosing
worse
estimator
decreasing
exponentially
also
implies
asymptotically
ˆθjsh
dominates
ˆθjs1
ˆθjs2
hence
ˆθm
well
remark
instead
picking
one
among
two
several
candidate
estimators
one
could
consider
hybrid
estimator
weighted
combination
candidate
estimators
indeed
george
leung
barron
proposed
combining
estimators
using
exponential
mixture
weights
based
stein
unbiased
risk
estimates
sure
due
presence
indicator
functions
deﬁnition
attracting
vector
challenging
obtain
sure
ˆθjs2
therefore
use
loss
estimates
choose
better
estimator
furthermore
instead
choosing
one
estimator
based
loss
estimate
follow
approach
employ
combination
estimators
using
exponential
mixture
weights
based
un-normalized
loss
estimates
weight
assigned
estimator
smallest
loss
estimate
exponentially
larger
therefore
dimension
high
effectively
equivalent
loss
estimate
picking
estimator
smallest
general
multiple-cluster
james-stein
estimator
section
generalize
two-cluster
estimator
section
l-cluster
estimator
deﬁned
arbitrary
partition
real
line
partition
deﬁned
functions
···
···
constants
···
µl−1
words
partition
deﬁned
via
functions
concentrates
around
deterministic
value
increases
two-cluster
estimator
one
function
concentrates
around
points
partition
real
line
sl−1
sl−1
sl−2
∪···∪
clusters
deﬁned
sj−1
section
iv-b
discuss
one
choice
partitioning
points
deﬁne
clusters
ﬁrst
construct
analyse
estimator
based
general
partition
satisfying
points
shrunk
towards
point
deﬁned
later
section
attracting
vector
cid:88

y1∈cj
...
yn∈cj
proposed
l-cluster
js-estimator
j=1
cid:20
ˆθjsl
cid:107
cid:107
2/n
max
deﬁned
attracting
vector
lies
l-dimensional
subspace
vectors
y1∈cl
···
yn≤cl
y1∈c1
···
yn∈c1
desired
values
attracting
vector
projection
onto
l-dimensional
subspace
computing
projection
ﬁnd
desired
values
means
cluster
orthogonal
i=1
θi1
yi∈c1
i=1
yi∈c1
i=1
θi1
yi∈cl
i=1
yi∈cl
ades
ades
unavailable
set
approxi-
obtained
using
concentration
results
mations
ades
similar
lemmas
section
attractors
given
ades
cid:80
cid:80
cid:80
cid:80

cid:21
i=1
yi1
yi∈cj
i=1
yi∈cj
cid:80
cid:2
|yi−sj
|≤δ
|yi−sj−1
|≤δ
cid:3
i=0
cid:80
i=1
yi∈cj
cid:80
cid:80
cid:19
cid:21
cid:21
cid:19
cid:21
cid:18
cid:12
cid:12
cid:12
cid:12
cid:18
cid:20
cid:88
chosen
small
positive
number
completes
speciﬁcation
attracting
vector
hence
l-cluster
js-estimator
theorem
loss
function
l-cluster
js-estimator
satisﬁes
following
cid:19
cid:33
cid:107
ˆθjsl
cid:107
min
κnδ
lσ2
min
2,1
max
cid:107
cid:107
/n,1
cid:18
cid:12
cid:12
cid:12
cid:12
positive
constants
cid:88
cid:107
cid:107
j=0
cid:88
cid:88
i=1
cid:20
j=1
i=1
cid:19
cid:18
µj−1
µj−θi
2σ2
µj−1−θi
2σ2
i=1
j=0
cid:104
cid:20
cid:19
cid:107
cid:107
cid:88
cid:18
cid:17
cid:16
µj−θi
cid:17
cid:16
µj−θi
cid:18
lim
supn→∞
cid:107
cid:107
2/n
cid:88
cid:80
cid:80
cid:12
cid:12
cid:12
cid:12
lim
n→∞
i=1
cid:16
cid:104
i=1
ˆθjsl
κnδ
cid:17
min
cid:12
cid:12
cid:12
cid:12
cid:18
µj−1
cid:16
µj−1−θi
cid:17
cid:105
cid:17
cid:105
cid:16
µj−1−θi
cid:19
σ2βn
sequence
increasing
dimension
proof
similar
theorem
provide
sketch
section
vi-d.
get
intuition
asymptotic
normalized
risk
ˆθjsl
depends
consider
four-cluster
esti-
mator
setup
fig
i.e.
components
take
one
two
values
−ρτ
fig
plots
asymptotic
risk
term
min
βn,4/
αn,4
versus
four-cluster
estimator
comparing
fig
fig
observe
four-cluster
estimator
risk
min
βn,4/
αn,4
behaves
similarly
two-cluster
estimator
risk
min
βn,2/
αn,2
notable
difference
magnitude
0.5
peak
value
βn,4/
αn,4
smaller
βn,2/
αn,2
however
smaller
values
reverse
true
means
ˆθjs4
better
ˆθjs2
even
certain
deﬁne
hybrid
estimator
cid:88
cid:96
cid:96
ˆθjs
cid:96
ˆθjsh
cid:96
cid:26
ˆθjs
cid:96
min1≤k≤l
otherwise
ˆθjsk
ˆθjs
cid:96
denoting
loss
function
estimate
ˆθjs
cid:96
cid:96
estimate
loss
ˆθjs
cid:96
using
theorem
estimating
cid:96
cid:96
deﬁned
respectively
section
vi-d
obtain
cid:107
cid:96
cid:107
cid:96
κnδ
using
concentration
inequalities
similar
lemmas
section
deduce
cid:96
cid:88
cid:88
cid:2
|yi−sj
|≤δ
j=1
i=0
cid:107
cid:96
cid:107
|yi−sj−1
|≤δ
cid:3
cid:33
cid:96
κnδ
cid:96
deﬁned
use
estimate
concentrating
value
theorem
thus
obtain
following
estimate
ˆθjs
cid:96
cid:107
cid:96
cid:107
ˆθjs
cid:96
cid:107
cid:107
2/n
cid:2
|yi−sj
|≤δ
|yi−sj−1
|≤δ
cid:3
cid:96
cid:88
cid:88
cid:32
j=1
i=0
loss
function
estimator
cid:96
together
loss
function
estimator
cid:96
completes
speciﬁcation
l-hybrid
estimator
using
steps
similar
theorem
show
σ2βn
cid:96
cid:96
min
cid:96
κnδ
ˆθjs
cid:96
cid:96
theorem
loss
function
l-hybrid
js-estimator
satisﬁes
following
cid:17
cid:18
cid:19
cid:16
cid:32
cid:107
ˆθjsh
cid:107
min
cid:96
cid:32
cid:107
ˆθjs
cid:96
cid:107
cid:33
cid:33
min
2,1
max
cid:107
cid:107
2/n,1
sequence
increasing
dimension
positive
constants
lim
supn→∞
cid:107
cid:107
2/n
lim
sup
n→∞
ˆθjsh
ˆθjs
cid:96
proof
theorem
omitted
along
cid:17
min
cid:96
cid:20
cid:16
cid:17
cid:21
cid:16
fig
asymptotic
risk
term
min
βn,4/
αn,4
four-cluster
estimator
plotted
1000
different
values
components
take
two
values
−ρτ
cid:98
1000ρ/
cid:99
components
taking
value
take
values
set
−ρτ
equal
probability
next
consider
example
scenarios
take
two
values
two-
value
example
ˆθjs4
typically
better
two
four
attractors
ˆθjs4
closer
values
two
attracting
points
ˆθjs2
closer
respective
values
take
values
−ρτ
equal
probability
sce-
nario
favorable
ˆθjs4
figure
shows
plot
min
βn,4/
αn,4
function
different
values
clear
separation
points
large
enough
asymptotic
normalized
risk
approaches
l-hybrid
james-stein
estimator
suppose
estimators
ˆθjs1
ˆθjsl
ˆθjs
cid:96
cid:96
-cluster
js-estimator
constructed
described
cid:96
recall
cid:96
corresponds
lindley
positive-part
estimator
depending
one
estimators
could
achieve
smallest
loss
would
like
design
hybrid
estimator
picks
best
estimators
context
section
iii
construct
loss
estimates
estimators
05101520253035404550−0.200.20.40.60.811.21.4τmin
βn,4
βn,4/
αn,4+1
ρ=0.25ρ=0.15ρ=0.1ρ=0.5ρ=1051015202530354000.20.40.60.81τmin
βn,4
βn,4/
αn,4+1
ρ=0.5ρ=0.25ρ=0.15ρ=0.1
lines
proof
theorem
thus
high
probability
l-hybrid
estimator
chooses
best
ˆθjs1
ˆθjsl
probability
choosing
worse
estimator
decreasing
exponentially
obtaining
clusters
subsection
present
simple
method
obtain
partitioning
points
l-cluster
js-estimator
integer
recursively
assuming
already
2a−1-
cluster
estimator
associated
partitioning
points
cid:48
···
2a−1
means
2a−1-cluster
estimator
real
line
partitioned
cid:0
cid:48
2a−1−1
cid:3
cid:0
cid:48
2a−1−2
cid:3
···
2a−1−1
cid:48
cid:48
recall
section
considered
case
single
partitioning
point
new
partitioning
points
···
2a−
obtained
follows
···
2a−1
deﬁne
i=1
yi1
cid:48
yi≤s
cid:48
i=1
cid:48
yi≤s
cid:48
cid:111
cid:111
cid:80
cid:80
cid:80
i=1
yi1
cid:110
yi≤s
cid:48
cid:80
i=1
cid:110
yi≤s
cid:48
j−1
j−1
s2j−1
s2j
cid:48
s2a−1
2a−1−1
2a−1−1
hence
partition
l-cluster
cid:48
estimator
s2a−1
s2a−1
s2a−2
···
use
partition
construct
4-cluster
estimator
simulations
next
section
simulation
results
section
present
simulation
plots
compare
average
normalized
loss
proposed
estimators
regular
js-estimator
lindley
estimator
various
choices
plot
normalized
loss
labelled
-axis
computed
averaging
1000
realizations
use
i.e.
noise
variance
regular
js-estimator
ˆθjs
lindley
estimator
ˆθjs1
used
positive-part
versions
respectively
proposed
given
choose
estimators
figs
4–7
consider
three
different
structures
representing
varying
degrees
clustering
ﬁrst
structure
components
i=1
arranged
two
clusters
second
structure
i=1
uniformly
distributed
within
interval
whose
length
varied
third
structure
i=1
arranged
four
clusters
clustered
struc-
tures
locations
widths
clusters
well
number
points
within
cluster
varied
locations
points
within
cluster
chosen
uniformly
random
captions
ﬁgures
explain
details
structure
fig
average
normalized
loss
various
estimators
different
values
i=1
placed
two
clusters
one
centred
another
cluster
width
0.5τ
n/2
points
01234567891000.20.40.60.81τ˜r
/nn
regular
js−estimatorlindley
estimatortwo−cluster
js−estimatorhybrid
js−estimatorml−estimator01234567891000.20.40.60.81τ˜r
/nn
regular
js−estimatorlindley
estimatortwo−cluster
js−estimatorhybrid
js−estimatorml−estimator01234567891000.20.40.60.81τ˜r
/nn
100
regular
js−estimatorlindley
estimatortwo−cluster
js−estimatorhybrid
js−estimatorml−estimator01234567891000.20.40.60.81τ˜r
/nn
1000
regular
js−estimatorlindley
estimatortwo−cluster
js−estimatorhybrid
js−estimatorml−estimator
i=1
fig
average
normalized
loss
various
estimators
versus
ρn+σ2
αn+σ2
min
αn+σ2
function
different
ρn+σ2
arrangements
i=1
placed
two
clusters
width
one
around
around
containing
equal
number
points
i=1
placed
two
clusters
width
1.25
one
around
around
containing
equal
number
points
i=1
placed
two
clusters
width
0.25
one
around
0.5
around
−0.5
containing
equal
number
points
i=1
placed
uniformly
fig
average
normalized
loss
various
estimators
different
arrangements
samples
i=1
1000.
clusters
width
0.5τ
one
around
0.25τ
containing
300
points
around
containing
700
points
consists
200
components
taking
value
remaining
800
taking
value
−0.25τ
two
clusters
width
0.125τ
one
around
containing
300
points
another
around
containing
700
points
i=1
arranged
uniformly
01234567891000.20.40.60.81τ˜r
regular
js−estimatorlindley
estimatortwo−cluster
js−estimatorhybrid
js−estimatorml−estimator01234567891000.20.40.60.811.21.4τ˜r
regular
js−estimatorlindley
estimatortwo−cluster
js−estimatorhybrid
js−estimatorml−estimator01234567891000.20.40.60.81τ˜r
regular
js−estimatorlindley
estimatortwo−cluster
js−estimatorhybrid
js−estimatorml−estimator01234567891000.20.40.60.81τ˜r
regular
js−estimatorlindley
estimatortwo−cluster
js−estimatorhybrid
js−estimatorml−estimator10110210310400.511.5n
ρnσ2ρn+σ2˜r
/nβnσ2αn+σ2=min
ρnσ2ρn+σ2
βnσ2αn+σ2
lindley
estimatortwo−cluster
js−estimatorhybrid
js−estimator
10110210310400.511.5n˜r
ρnσ2ρn+σ2βnσ2αn+σ2=min
ρnσ2ρn+σ2
βnσ2αn+σ2
lindley
js−estimatortwo−cluster
js−estimatorhybrid
js−estimator10110210310400.10.20.30.40.5n˜r
βnσ2αn+σ2ρnσ2ρn+σ2=min
ρnσ2ρn+σ2
βnσ2αn+σ2
lindley
estimatortwo−cluster
js−estimatorhybrid
js−estimator
10110210310400.511.5n˜r
ρnσ2ρn+σ2=min
ρnσ2ρn+σ2
βnσ2αn+σ2
βnσ2αn+σ2lindley
estimatortwo−cluster
js−estimatorhybrid
js−estimator
proofs
mathematical
preliminaries
list
concentration
results
used
proofs
theorems
lemma
let
variables
n=1
sequence
random
i.e.
min
2,1
max
cid:107
cid:107
2/n,1
|xn
positive
lim
supn→∞
cid:107
cid:107
2/n
proof
exists
positive
integer
cid:107
cid:107
2/n
hence
|xn
constants
a.s.−→
min
max
cid:107
cid:107
/n,1
cid:88
cid:88
n=1
n=1
ke−
min
2,1
c+τ
cid:88
therefore
use
borel-cantelli
lemma
conclude
a.s.−→
n=m
n=1
lemma
sequences
random
variables
proof
|xn|
k1e
|yn|
k2e
triangle
inequality
max
cid:107
cid:107
2/n,1
max
cid:107
cid:107
2/n,1
positive
constants
n=1
follows
nk2
min
nk1
min
2,1
|xn
yn|
cid:16
|xn|
cid:17
min
cid:0
min
2,1
max
cid:107
cid:107
2/n,1
cid:16
|yn|
cid:1
cid:17
lemma
let
random
variables
ax|
k1e−nk1
min
2,1
k2e−nk2
min
2,1
positive
constants
positive
integer
constants
|xy
ke−nk
min
2,1
positive
constant
depending
fig
average
normalized
loss
various
estimators
1000
i=1
different
arrangements
samples
i=1
placed
four
equal-sized
clusters
width
0.5τ
clusters
width
0.25τ
cases
clusters
centred
1.5τ
0.9τ
−0.5τ
−1.25τ
fig
i=1
arranged
two
clusters
one
centred
plots
show
average
normalized
loss
function
different
values
four
estimators
ˆθjs
ˆθjs1
two-attractor
js-estimator
ˆθjs2
given
hybrid
js-estimator
ˆθjsh
given
observe
increases
average
loss
ˆθjsh
gets
closer
minimum
ˆθjs1
ˆθjs2
fig
shows
average
normalized
loss
different
arrangements
ﬁxed
1000.
plots
illustrate
cases
ˆθjs2
signiﬁcantly
lower
risk
ˆθjs1
also
strength
ˆθjsh
large
fig
compares
average
normalized
losses
ˆθjs1
ˆθjs2
ˆθjsh
asymptotic
risk
values
obtained
corollary
theorem
theorem
respectively
subﬁgure
considers
different
arrangement
i=1
shows
average
losses
converge
respective
theoretical
values
growing
fig
demonstrates
effect
choosing
four
attractors
i=1
form
four
clusters
four-hybrid
estimator
ˆθjsh,4
attempts
choose
best
among
ˆθjs1
ˆθjs2
ˆθjs4
based
data
clear
depending
values
ˆθjsh,4
reliably
tracks
best
signiﬁcantly
lower
loss
ˆθjs1
ˆθjs2
especially
large
values
01234567891000.20.40.60.81τ˜r
lindley
estimatortwo−cluster
js−estimatorfour−cluster
js−estimatorfour−hybrid
js−estimatorml−estimator01234567891000.20.40.60.81τ˜r
lindley
estimatortwo−cluster
js−estimatorfour−cluster
js−estimatorfour−hybrid
js−estimatorml−estimator
cid:17
proof
|xy
xay
2ax
cid:17
cid:16
cid:16
|xay
2ax
cid:16
cid:17
cid:16
cid:17
cid:16
ax|
cid:114
cid:114
cid:18
cid:18
cid:19
cid:19
cid:17
cid:16
ax|
cid:16
cid:17
cid:17
k1e−nk
cid:48
min
,1
k2e−nk
cid:48
min
,1
k1e−nk
cid:48
cid:48
min
2,1
min
2,1
ke−nk
min
2,1
cid:48
cid:48
cid:48
cid:48
cid:48
k2e−nk
cid:48
cid:48
cid:48
min
positive
constant
lemma
let
non-negative
random
variable
exists
k1e−nk1
min
2,1
k2e−nk2
min
2,1
positive
constants
positive
integer
constants
cid:18
cid:12
cid:12
cid:12
cid:12
proof
cid:18
cid:18
cid:18
cid:19
cid:18
cid:18
1/ay
−ay
−nk1
min
k1e
cid:48
min
cid:18
cid:19
cid:12
cid:12
cid:12
cid:12
cid:18
cid:19
cid:18
cid:18
ay
cid:19
cid:19
cid:18
cid:16
cid:17
cid:19
cid:16
1+ay
1+ay
ay
cid:19
cid:19
cid:19
cid:18
ke−nk
min
2,1
cid:19
cid:19
1/ay
k1e−nk
cid:48
min
2,1
cid:19
similarly
cid:17
note
cid:33
therefore
cid:18
cid:32
cid:18
cid:18
cid:18
−nk2
min
cid:32
cid:19
cid:18
ay
ay
cid:19
cid:19
1−ay
cid:33
cid:19
cid:19
k2e
k2e−nk
cid:48
k2e−nk2
min
4,1
cid:19
min
2,1
cid:12
cid:12
cid:12
cid:12
min
cid:0
cid:1
using
cid:18
cid:12
cid:12
cid:12
cid:12
cid:48
obtain
min
cid:48
cid:48
lemma
let
n=1
sequence
random
variables
another
random
variable
constant
|xn
ke−nk
min
2,1
positive
constants
function
max
ke−nk
min
2,1
ke−nk
min
2,1
proof
let
cid:12
cid:12
|xn
cid:0
cid:12
cid:12
cid:1
cid:0
cid:12
cid:12
ke−nk
min
2,1
cid:0
cid:12
cid:12
cid:1
cid:1
cid:1
finally
cid:1
proves
follows
second
term
rhs
equals
also
let
consider
case
condition
fact
|xn
hence
hence
case
also
case
cid:0
cid:12
cid:12
cid:0
cid:12
cid:12
almost
surely
let
cid:80
cid:16
|sn|
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
lemma
lemma
hoeffding
inequality
thm
2.8
let
···
independent
random
variables
i=1
2n22
bi−ai
lemma
chi-squared
concentration
i.i.d
gaus-
sian
random
variables
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
min
cid:0
cid:17
2e−nk
min
cid:1
cid:80
i=1
i=1
4σ4
2σ2
lemma
10.
···
let
indepen-
dent
real-valued
ﬁnite
constants
cid:32
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
i=1
i=1
2e−nk1
min
wi1
wi1
wi≤ai
cid:88
cid:88
i=1
i=1
2σ2
2σ2
cid:33
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
2e−nk2
min
positive
constants
proof
given
appendix
lemma
11.
let
cid:0
σ2i
cid:1
let
function
2e−nk2
constants
cid:32
cid:32
cid:32
i=1
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
i=1
i=1
i=1
cid:88
θi1
cid:88
wi1
cid:88
i=1
i=1
4e−nk1
min
positive
constant
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
wi1
proof
given
appendix
lemma
12.
assumptions
lemma
let
function
2e−nl2
≥yi
cid:88
b≥yi
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
i=1
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
8e−nk2
i=1
proof
result
follows
lemma
noting
≥yi
b≥yi
proof
theorem
cid:20
cid:20
cid:107
ˆθjs2
cid:107
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:18
cid:107
cid:107
2/n
cid:18
cid:107
cid:107
2/n
cid:107
cid:107
2/n
cid:107
cid:107
2/n
cid:19
cid:107
cid:107
2/n
cid:107
cid:107
2/n
cid:107
cid:107
cid:104
cid:105
cid:19
cid:13
cid:13
cid:13
cid:13
cid:21
cid:21
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
also
cid:107
cid:107
cid:107
cid:107
cid:107
cid:107
cid:107
cid:107
cid:104
cid:105
cid:107
cid:107
cid:19
cid:107
ˆθjs2
cid:107
cid:107
cid:107
2/n
cid:107
cid:107
2/n
cid:107
cid:107
cid:18
cid:18
cid:107
cid:107
cid:107
cid:107
2/n
cid:19
cid:107
cid:107
cid:107
cid:107
use
following
results
whose
proofs
given
appendix
appendix
lemma
cid:107
cid:107
given
κnδ
lemma
cid:107
cid:107
κnδ
given
using
lemma
together
cid:32
cid:107
cid:107
cid:33
κnδ
using
together
lemmas
4e−nk12
cid:104
cid:105
cid:107
cid:107
cid:107
cid:107
θi1
nk12
cid:107
cid:107
2/n
using
obtain
obtain
cid:107
ˆθjs2
cid:107
cid:18
cid:0
cid:1
cid:19
cid:0
cid:0
cid:1
cid:1
κnδ
cid:40
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:107
ˆθjs2
cid:107
cid:12
cid:12
cid:12
cid:12
κnδ
βnσ2
αn+σ2
κnδ
min
otherwise
κnδ
min
cid:18
cid:19
cid:19
βnσ2
cid:107
cid:107
2/n
therefore
cid:18
proves
hence
ﬁrst
part
theorem
prove
second
part
theorem
use
following
deﬁnition
result
deﬁnition
vi.1
uniform
integrability
se-
quence
n=1
said
uniformly
integrable
cid:2
|xn|1
|xn|≥k
cid:3
cid:19
lim
sup
n→∞
lim
k→∞
sec
13.7
let
n=1
sequence
fact
l1−→
equivalently
e|xn|
also
let
i.e.
|xn
following
two
conditions
satisﬁed
p−→
sequence
consider
individual
terms
rhs
n=1
using
lemmas
obtain
cid:0
cid:1
κnδ
cid:107
cid:107
2/n
cid:107
cid:107
2/n
lemma
cid:34
cid:0
cid:1
cid:107
cid:107
2/n
cid:107
cid:107
2/n
cid:35
κnδ
a.s.−→
similarly
obtain
βnσ2
cid:107
cid:107
2/n
cid:107
cid:107
2/n
cid:107
cid:107
2/n
cid:107
cid:107
2/n
cid:20
cid:20
cid:20
cid:107
cid:107
2/n
cid:107
cid:107
2/n
cid:21
a.s.−→
cid:21
cid:21
a.s.−→
κnδ
κnδ
a.s.−→
κnδ
cid:21
cid:12
cid:12
cid:12
cid:12
cid:12
using
write
cid:18
cid:18
cid:107
ˆθjs2
cid:107
cid:19
cid:33
βnσ2
cid:32
cid:107
cid:107
min
κnδ
cid:19
cid:21
cid:12
cid:12
cid:12
cid:12
note
jensen
inequality
ex|
|xn
therefore
cid:19
cid:12
cid:12
cid:12
cid:12
ˆθjs2
min
cid:20
cid:18
cid:12
cid:12
cid:12
cid:12
cid:12
cid:107
ˆθjs2
cid:107
cid:18
cid:20
cid:12
cid:12
cid:12
cid:12
min
βnσ2
βnσ2
cid:12
cid:12
cid:12
cid:12
cid:107
cid:107
e|sn|
e|tn|
e|un|
e|vn|
cid:19
cid:12
cid:12
cid:12
cid:12
cid:107
cid:107
cid:12
cid:12
cid:12
cid:12
κnδ
κnδ
ﬁrst
show
cid:107
cid:107
cid:12
cid:12
cid:12
cid:12
cid:21
l1−→
i.e.
cid:20
cid:12
cid:12
cid:12
cid:12
cid:107
cid:107
cid:90
cid:90
cid:90
cid:12
cid:12
cid:12
cid:12
cid:21
cid:18
cid:12
cid:12
cid:12
cid:12
cid:107
cid:107
cid:90
2e−nkxdx
2e−nkxdx
e−t2
lim
n→∞
holds
cid:20
cid:12
cid:12
cid:12
cid:12
cid:107
cid:107
cid:90
cid:90
cid:90
2e−nkx2
2e−nkx2
inequality
due
lemma
cid:12
cid:12
cid:12
cid:12
cid:19
e−tdt
n→∞−→
thus
prove
sufﬁcient
show
e|sn|
e|tn|
e|un|
e|vn|
converge
fact
implies
need
show
n=1
considering
n=1
n=1
n=1
cid:0
cid:1
cid:107
cid:107
2/n
cid:107
cid:107
2/n
since
sum
terms
involve
bounded
absolute
value
chosen
ﬁxed
see
note
exists
|sn|
2σ2
hence
deﬁnition
vi.1
n=1
similar
argument
n=1
next
considering
cid:107
cid:107
2/n
cid:107
cid:107
cid:107
cid:107
2/n
hence
|vn|
cid:107
cid:107
note
fact
cid:107
cid:107
2/n
n=1
complete
proof
use
following
result
whose
proof
provided
appendix
lemma
15.
let
n=1
sequence
positive-valued
random
variables
let
n=1
sequence
random
variables
|xn|
cyn
positive
constants
hence
n=1
finally
considering
see
n=1
also
cid:16
cid:107
y−ν2
cid:107
cid:107
θ−ν2
cid:107
cid:17
cid:16
cid:107
y−ν2
cid:107
cid:107
y−ν2−w
cid:107
cid:17
cid:107
cid:107
cid:17
cid:17
2σ2
cid:16
cid:107
y−ν2
cid:107
cid:16
cid:107
y−ν2
cid:107
cid:19
cid:18
cid:107
cid:107
βnσ2
note
last
inequality
due
assumption
lim
supn→∞
cid:107
cid:107
2/n
therefore
|tn|
cid:107
cid:107
2/n
2σ2
ﬁnite
constant
thus
lemma
therefore
terms
rhs
goes
completes
proof
theorem
proof
theorem
let
cid:110
cid:107
ˆθjs1
cid:107
cid:107
ˆθjs2
cid:107
cid:111
cid:107
ˆθjs2
cid:107
cid:107
ˆθjs1
cid:107
cid:32
cid:107
ˆθjs1
cid:107
without
loss
generality
given
assume
|∆n|
clear
cid:107
ˆθjsh
cid:107
cid:107
ˆθjs2
cid:107
cid:33
min
lemma
obtain
following
concen-
tration
inequality
loss
estimate
ˆθjs1
ρnσ2
using
together
corollary
obtain
cid:107
ˆθjs1
cid:107
ˆθjs1
following
steps
similar
proof
lemma
obtain
following
loss
estimate
ˆθjs2
βnσ2
κnδ
combining
theorem
ˆθjs2
cid:107
ˆθjs2
cid:107
cid:18
lemma
−∆n
therefore
ˆθjs2
ˆθjs1
ˆθjs2
−∆n
min
2,1
max
cid:107
cid:107
2/n,1
ˆθjs1
cid:19
cid:19
cid:19
positive
constants
let
denote
probability
chosen
therefore
last
inequality
obtained
cid:18
cid:18
ˆθjs1
ˆθjs1
min
2,1
max
cid:107
cid:107
2/n,1
cid:32
cid:107
ˆθjsh
cid:107
cid:107
ˆθjs1
cid:107
cid:32
cid:107
ˆθjsh
cid:107
cid:107
ˆθjs2
cid:107
ˆθjs2
ˆθjs2
cid:33
cid:33
min
max
cid:107
cid:107
/n,1
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:33
cid:32
cid:107
ˆθjsi
cid:107
max
cid:107
cid:107
2/n,1
min
2,1
−∆n
therefore
arrive
cid:32
cid:107
ˆθjsh
cid:107
min
i=1,2
similar
manner
obtain
min
max
cid:107
cid:107
2/n,1
proves
ﬁrst
part
theorem
second
part
first
suppose
ˆθjs1
lower
risk
given
let
ajs1
ajs2a
ajs2b
rn\
ajs1
ajs2a
denoting
2πσ2
exp
ˆθjsh
cid:16
cid:107
y−θ
cid:107
cid:90
cid:17
cid:107
ˆθjs1
cid:107
2σ2
ajs1
cid:107
ˆθjs2
cid:107
cid:90
cid:90
cid:90
cid:90
cid:32
cid:90
ajs2a
ajs2b
ajs1
cid:107
ˆθjs1
cid:107
cid:107
ˆθjs1
cid:107
cid:107
ˆθjs2
cid:107
ajs2a
ajs2b
cid:33
1/2
ˆθjs1
1/2
cid:16
cid:107
ˆθjs2
cid:107
cid:17
1/2
ˆθjs1
min
max
cid:107
cid:107
2/n,1
cid:107
ˆθjs2
cid:107
ajs2b
step
uses
deﬁnition
ajs2a
step
last
term
obtained
using
cauchy-schwarz
inequality
product
functions
cid:112
cid:112
cid:107
ˆθjs2−θ
cid:107
step
cid:20
cid:17
min
i=1,2
ˆθjsi
hence
obtain
ˆθjsh
min
2,1
similarly
ˆθjs2
lower
risk
get
cid:16
cid:107
ˆθjs1
cid:107
cid:17
1/2
ˆθjsh
ˆθjs2
min
max
cid:107
cid:107
2/n,1
cid:16
cid:18
cid:16
cid:107
ˆθjsi
cid:107
cid:17
1/2
cid:19
cid:21
cid:21
cid:17
cid:16
cid:17
cid:21
cid:16
cid:16
cid:107
ˆθjsi
cid:107
cid:17
1/2
cid:20
ˆθjsh
min
cid:20
max
cid:107
cid:107
2/n,1
max
i=1,2
ﬁnite
get
since
true
every
therefore
lim
supn→∞
ˆθjsi
lim
sup
n→∞
noting
i=1,2
lim
sup
n→∞
ˆθjsh
min
i=1,2
ˆθjsi
assumption
completes
proof
theorem
cid:16
cid:107
ˆθjs1
cid:107
cid:107
ˆθjs2
cid:107
cid:17
note
note
best
case
scenario
cid:107
ˆθjsh
cid:107
occurs
min
realization
hybrid
estimator
picks
better
two
rival
estimators
ˆθjs1
ˆθjs2
case
inequality
strict
provided
realizations
non-zero
probability
measure
one
estima-
tor
strictly
better
proof
theorem
i=1
i=1
i=1
2σ2
cid:18
2σ2
cid:19
cid:21
cid:19
cid:17
cid:16
cid:18
cid:20
cid:88
cid:88
proof
similar
theorem
provide
sketch
note
real-valued
ﬁnite
···
cid:88
cid:2
wi≤bi
cid:3
cid:2
wi1
wi≤bi
cid:3
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:19
cid:88
since
wi≤bi
follows
wi1
wi≤bi
cid:19
cid:21
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:32
min
max
hoeffding
inequality
obtain
cid:19
cid:12
cid:12
cid:12
cid:12
cid:12
cid:32
wi1
wi≤bi
cid:17
cid:16
cid:18
cid:88
wi≤bi
cid:88
2e−2n2
2σ2
cid:18
2σ2
cid:20
2n2
i=1
i=1
i=1
i=1
i=1
cid:80
i=1
ni−mi
subsequently
steps
lemma
used
obtain
cid:107
νyl
cid:107
cid:107
cid:107
cid:88
cid:19
cid:18
j=0
cid:18
cid:20
cid:88
cid:19
cid:88
i=1
cid:19
cid:18
cid:20
cid:88
j=1
i=1
cid:18
µj−1
cid:19
cid:21
cid:21
µj−1−θi
2σ2
µj−θi
2σ2
κnδ
finally
employing
steps
lemma
get
cid:107
νyl
cid:107
cid:107
cid:107
cid:88
j=0
cid:20
cid:88
i=1
cid:18
cid:19
cid:18
µj−1
cid:19
cid:21
κnδ
subsequent
steps
proof
along
lines
theorem
vii
concluding
remarks
paper
presented
class
shrinkage
estimators
take
advantage
large
dimensionality
infer
clustering
structure
parameter
values
data
structure
used
construct
attracting
vector
shrinkage
estimator
good
cluster-based
attracting
vector
enables
signiﬁcant
risk
reduction
ml-estimator
even
composed
several
inhomogeneous
quantities
obtained
concentration
bounds
squared-error
loss
constructed
estimators
convergence
results
risk
estimators
signiﬁcantly
smaller
risks
regular
js-estimator
wide
range
even
though
cid:105
clearly
since
cid:18
cid:18
cid:18
cid:18
cid:104
cid:19
cid:16
cid:19
cid:16
cid:19
cid:16
cid:19
cid:16
cid:17
cid:17
cid:18
cid:17
cid:18
cid:17
cid:18
cid:90
cid:19
cid:19
cid:19
x−b
ﬁrst
mean
value
theorem
integrals
e−x2
x−b
therefore
sup
consider
−∞,0
x+b
x−b
cid:21
cid:20
cid:2
e−bx
ebx
cid:3
2e−
cid:2
e−bx
ebx
cid:3
−x2
−x2
−x2
cosh
2e−
cosh
establishes
monotone
non-decreasing
hence
sup
finally
follows
applying
chernoff
trick
using
obtain
cid:2
eλx
cid:3
nλ2σ2
cid:0
eλx
eλ
cid:1
cid:2
eλx
cid:3
cid:16
eλ
cid:16
λ−
nλ2σ2
choosing
nσ2
minimizes
cid:17
hence
λ−
nλ2
get
cid:17
dominate
regular
positive-part
js-estimator
ﬁnite
important
next
step
test
performance
proposed
estimators
real
data
sets
would
interesting
adapt
estimators
analyze
risks
sample
values
bounded
known
value
i.e.
|θi|
···
known
another
open
question
one
decide
maximum
number
clusters
considered
hybrid
estimator
interesting
direction
future
research
study
conﬁdence
sets
centered
estimators
paper
compare
conﬁdence
sets
centered
positive-part
js-estimator
studied
james-stein
estimator
colored
gaussian
noise
i.e.
known
studied
variants
proposed
would
interesting
extend
ideas
paper
case
colored
gaussian
noise
noise
general
sub-
gaussian
distribution
yet
another
research
direction
construct
multi-dimensional
target
subspaces
data
general
cluster-based
subspaces
proposed
goal
obtain
greater
risk
savings
wider
range
cost
complex
attractor
appendix
proof
lemma
note
cid:2
wi1
cid:3
2σ2
wi1
2σ2
cid:88
i=1
let
moment
generating
function
mgf
cid:16
cid:90
2σ2
consider
λwi1
2σ2
dwi
eλwie
2σ2
dwi
2σ2
dwi
cid:88
i=1
cid:17
cid:90
i=1
cid:89
cid:2
eλx
cid:3
cid:89
e−λmi√
cid:20
cid:89
cid:104
cid:89
2πσ2
e−λmi
e−λmi
i=1
i=1
e−λmi√
cid:20
cid:90
2πσ2
2πσ2
λ2σ2
i=1
eλwie
2σ2
dwi
cid:17
cid:16
cid:17
cid:105
positive
real
number
consider
function
note
rhs
written
cid:81
i=1
bound
mgf
bounding
cid:21
cid:16
cid:105
cid:17
cid:21
cid:90
cid:16
cid:104
cid:18
cid:32
cid:19
cid:32
cid:88
n2
2σ2
i=1
2nσ2
wi1
cid:88
i=1
cid:33
cid:33
2σ2
obtain
lower
tail
inequality
use
following
result
fact
thm
3.7
independent
random
variables
satisfying
cid:80
i=1
i=1
cid:3
···
clearly
wi1
min
take
min
···
therefore
i=1
cid:32
cid:88
cid:88
···
cid:2
cid:88
cid:32
cid:88
i=1
i=1
cid:80
i=1
cid:33
cid:33
nσ2+
hence
cid:32
cid:32
cid:88
wi1
cid:88
i=1
2σ2
cid:33
cid:33
σ2+
cid:32
i=1
n2
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
i=1
cid:88
i=1
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
using
upper
lower
tail
inequalities
obtained
respectively
get
wi1
2σ2
n2
σ2+
2e−nk
min
positive
constant
due
ﬁnite
proves
concentration
inequality
similarly
proven
detailed
proof
lemma
ﬁrst
prove
immediately
follows
setting
let
denote
event
whose
probability
want
bound
case
cid:40
cid:12
cid:12
cid:12
cid:12
cid:88
i=1
θi1
cid:88
θi1
i=1
cid:41
cid:12
cid:12
cid:12
cid:12
i=1
cid:32
θi1
yi≤f
cid:12
cid:12
cid:12
cid:12
cid:88
cid:32
cid:12
cid:12
cid:12
cid:12
cid:88
cid:32
cid:20
cid:88
|θi|1
yi≤a+t
cid:32
cid:20
cid:88
|θi|1
a−t
yi≤a
θi1
yi≤a
cid:33
cid:12
cid:12
cid:12
cid:12
cid:33
cid:12
cid:12
cid:12
cid:12
cid:33
cid:21
cid:33
cid:21
2e−nkt2
i=1
i=1
i=1
cid:90
a+t
cid:88
yi≤a+t
2πσ2
yi−θi
2σ2
dyi
cid:80
used
|θi|1
yi≤a+t
2σ2
let
yi−θi
2πσ2
i=1
i=1
2πσ2
|θi|
yi≤a+t
cid:88
|θi|
i=1
since
|θi|
hoeffding
inequality
exp
2n2
cid:107
cid:107
2/n
implies
cid:18
cid:107
cid:107
cid:80
2n2
cid:107
cid:107
2/n
i=1|θi|
set
/2
cid:107
cid:107
2πσ2
cid:19
cid:112
πσ2/2
cid:107
cid:107
1/n
n2
cid:107
cid:107
2/n
n2
cid:107
cid:107
2/n
obtain
cid:32
cid:32
cid:88
cid:88
i=1
i=1
similar
analysis
yields
cid:33
|θi|1
yi≤a+t
cid:33
|θi|1
a−t
yi≤a
n2
cid:107
cid:107
using
recalling
given
obtain
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:18
θi1
cid:88
i=1
n2kπσ2
cid:107
cid:107
/n2
i=1
n2
cid:107
cid:107
2/n
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
θi1
cid:19
nk2
cid:107
cid:107
positive
constant
last
inequality
holds
1/n2
cid:107
cid:107
2/n
cauchy-schwarz
inequal-
cid:107
cid:107
ity
lim
supn→∞
cid:107
cid:107
2/n
assumption
proves
obtain
cid:32
cid:32
cid:88
i=1
cid:88
i=1
cid:33
|wi|1
yi≤a+t
2e−nk
min
positive
constant
using
similar
steps
shown
third
term
rhs
also
bounded
|wi|1
a−t
yi≤a
2e−nk
min
cid:33
completes
proof
proof
lemma
limk→∞
cid:0
lim
supn→∞
cid:2
yn1
yn≥k
cid:3
cid:1
therefore
n=1
deﬁnition
vi.1
since
cid:105
cid:105
cid:2
|xn|1
|xn|≥k
cid:3
cid:2
c|yn|1
|xn|≥k
cid:3
cid:2
|xn|≥k
cid:3
cid:2
yn1
cyn+a≥k
cid:3
cid:2
cyn+a≥k
cid:3
cid:104
cid:105
cid:104
cid:19
cid:18
cid:104
yn≥
k−a
cid:2
|xn|1
|xn|≥k
cid:3
cid:19
cid:18
cid:104
yn1
yn≥
k−a
yn1
yn≥
k−a
cid:105
cid:19
cid:19
cid:19
yn1
yn≥
k−a
lim
k→∞
lim
k→∞
lim
sup
n→∞
lim
sup
n→∞
cid:18
cid:18
cid:18
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
θie
cid:104
ﬁrst
prove
immedi-
ately
follow
setting
lemma
cid:107
cid:107
since
θi1
independent
hoeffding
inequality
θi1
i=1
nk2
cid:32
i=1
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
θi1
cid:88
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:104
2n2
cid:107
cid:107
cid:105
i=1
θi1
also
i=1
cid:88
cid:105
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:0
cid:1
cid:0
cid:1
cid:19
cid:18
cid:88
cid:18
cid:19
therefore
obtain
θi1
i=1
θiq
cid:88
i=1
lim
k→∞
lim
sup
n→∞
proof
lemma
next
prove
using
steps
similar
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:32
i=1
2e−nkt2
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:33
|wi|1
yi≤a+t
wi1
i=1
cid:33
cid:88
wi1
cid:88
cid:32
cid:88
cid:80
i=1
i=1
i=1
|wi|1
a−t
yi≤a
let
|wi|1
yi≤a+t
|wi|1
a−θi
wi≤a−θi+t
noting
|wi|
θi|
cid:90
a−θi+t
cid:18
|c|√
a−θi
e−w2/2σ2
|w|√
e−c2/2σ2
cid:19
2πσ2
2πσ2
2πe
note
mean
value
theorem
integrals
xe−x2
hence
cid:88
i=1
2πe
takes
values
interval
length
hoeffding
inequality
2e−2n2
2e−2n2
2πe
cid:18
cid:19
using
value
rhs
1/t2
1/t2
set
2πe
obtain
cid:32
cid:88
i=1
|wi|1
yi≤a+t
2e−nk11
setting
get
using
following
inequality
4+1−1
cid:26
x2/32
cid:0
cid:1
3x/4
cid:33
cid:19
cid:88
¯θ−θi
2σ2
2nδ
concentration
result
immediately
follows
writ-
ing
yi≤¯y
cid:88
prove
write
cid:88
cid:88
θi1
wi1
yi1
i=1
i=1
i=1
hence
show
i=1
lemma
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
wi1
nk2
cid:107
cid:107
2/n
cid:32
cid:104
wi1
i=1
wi1
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
wi1
cid:88
cid:90
cid:105
cid:90
i=1
i=1
¯θ−θi
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
i=1
2e−nk
min
wi1
wi√
2πσ2
2πσ2
2σ2
dwi
2σ2
using
lemma
get
wi1
¯θ−θi
2σ2
cid:90
¯θ+δ
¯θ−δ
ﬁrst
mean
value
theorem
integrals
∃εi
2πσ2
yi−θi
2σ2
dyi
¯θ+εi−θi
2σ2
rhs
102
written
cid:18
2πσ2
let
since
|εi|
cid:88
i=0
i=0
¯θ−δ
cid:90
¯θ+δ
cid:88
cid:12
cid:12
cid:12
cid:12
cid:18
cid:90
¯θ+δ
cid:88
i=0
2πσ2
max
2πσ2
yi−θi
2σ2
dyi
¯θ+εi−θi
2σ2
cid:12
cid:12
cid:12
cid:12
cid:19
2σ2
xi+εi
2σ2
2σ2
2πe
2πσ2
yi−θi
2σ2
dyi
2πσ2
cid:88
i=0
¯θ−δ
¯θ−θi
2σ2
κnδ
103
therefore
2nδ
cid:88
i=0
|κn|
using
103
102
obtained
result
101
100
proof
lemma
complete
2πe
cid:19
dwi
¯θ−θi
2σ2
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
obtain
combining
similarly
shown
using
lemma
lemma
establish
cid:88
i=1
wi1
wi≤¯y
¯θ−θi
2σ2
i=1
proof
lemma
cid:18
cid:88
i=1
cid:88
lemma
|yi−¯y|≤δ
cid:88
i=0
hoeffding
inequality
|yi−¯θ|≤δ
proof
lemma
cid:32
i=0
nk2δ2
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:88
i=0
8nδ2
cid:12
cid:12
cid:12
cid:12
cid:12
cid:104
cid:88
cid:88
i=0
cid:32
also
cid:105
cid:88
i=0
yi−θi
2σ2
|yi−¯θ|≤δ
cid:90
¯θ+δ
2πσ2
¯θ−δ
i=0
|yi−¯θ|≤δ
100
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
cid:105
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:0
cid:12
cid:12
cid:12
cid:12
cid:1
|yi−¯θ|≤δ
101
cid:88
cid:104
i=0
cid:107
cid:107
cid:88
cid:88
i=1
i=1
yi≤¯y
104
cid:35
a1yi1
cid:88
i=1
i=1
cid:88
cid:34
cid:88
cid:34
cid:88
cid:88
cid:88
i=1
i=1
i=1
i=1
cid:88
i=1
¯y−θi
cid:88
cid:88
i=1
i=1
cid:21
θiwi1
¯y−θi
dyi
102
a1yi1
a2yi1
yi≤¯y
105
¯θ−θi
2σ2
κnδ
completes
proof
lemma
n2
cid:107
cid:107
2/n
106
proof
lemma
similarly
i=1
cid:88
yi≤¯y
cid:88
cid:88
cid:88
cid:88
yi≤¯y
i=1
i=1
cid:20
cid:88
i=1
wi≤¯y−θi
θiwi1
wi≤¯y−θi
cid:21
a2yi1
yi≤¯y
i=1
i=1
yi≤¯y
cid:88
therefore
104
cid:107
cid:107
cid:107
cid:107
cid:88
i=1
cid:88
i=1
θiwi
a1yi1
cid:19
since
i=1
i=1
i=1
i=1
yi≤¯y
cid:32
cid:88
cid:88
i=1
θiwi
cid:16
cid:80
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:32
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:88
cid:17
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:12
cid:12
cid:12
cid:12
cid:12
cid:33
cid:107
cid:107
θiwi
i=1
i=1
lemma
2e−nk
min
positive
constant
next
claim
κnδ
κnδ
107
deﬁned
concentration
107
follows
lemmas
together
results
concentration
products
reciprocals
lemmas
respectively
using
107
lemma
obtain
κnδ
cid:33
cid:18
cid:19
cid:88
i=1
κnδ
i=1
θiq
yi1
cid:33
cid:88
cid:32
cid:88
cid:19
cid:18
cid:32
cid:19
cid:18
cid:88
cid:19
cid:18
cid:88
i=1
κnδ
2c1
i=1
κnδ
2c2
i=1
κnδ
cid:33
cid:33
¯θ−θi
2σ2
¯θ−θi
2σ2
i=1
cid:88
cid:88
cid:88
i=1
i=1
2c1σ
¯θ−θi
2σ2
similarly
cid:32
cid:88
cid:32
i=1
2c1
yi≤¯y
κnδ
employing
steps
get
cid:32
cid:32
cid:88
cid:88
i=1
yi1
yi≤¯y
i=1
2c2σ
cid:88
i=1
cid:33
cid:88
cid:88
i=1
i=1
cid:18
cid:19
cid:18
cid:19
cid:33
2c2
¯θ−θi
2σ2
κnδ
110
111
therefore
using
106
111
105
ﬁnally
obtain
cid:18
cid:18
cid:88
cid:107
cid:107
cid:107
cid:107
cid:19
cid:88
cid:19
cid:32
cid:88
i=1
i=1
cid:19
cid:18
cid:18
cid:19
cid:33
i=1
proof
along
lines
lemma
13.
cid:107
cid:107
cid:34
cid:88
i=1
cid:107
cid:107
cid:88
i=1
cid:107
cid:107
cid:32
cid:32
cid:88
i=1
i=1
yi≤¯y
cid:88
cid:88
cid:88
cid:88
cid:19
cid:18
cid:19
cid:18
cid:19
cid:18
i=1
cid:35
i=1
i=1
a1θi1
cid:88
yi≤¯y
cid:88
cid:33
cid:88
cid:88
cid:88
cid:19
cid:18
cid:19
cid:33
cid:18
cid:18
cid:19
θiqc
θiq
i=1
i=1
2c1
2c2
i=1
a2θi1
yi≤¯y
i=1
κnδ
cid:107
cid:107
i=1
κnδ
acknowledgement
authors
thank
samworth
useful
discussions
james-stein
estimators
barron
anonymous
referee
comments
led
much
improved
manuscript
108
109
james
stein
estimation
quadratic
loss
proc
fourth
berkeley
symp
math
stat
probab.
361–380
1961.
references
lehmann
casella
theory
point
estimation
springer
new
york
1998
efron
morris
data
analysis
using
stein
estimator
generalizations
amer
statist
assoc.
vol
311–319
1975
lindley
discussion
professor
stein
paper
stat
soc.
vol
285–287
1962
stein
estimation
mean
multivariate
normal
distribution
ann
stat.
vol
1135–1151
1981
efron
morris
stein
estimation
rule
competitors—an
empirical
bayes
approach
amer
statist
assoc.
vol
117–
130
1973
george
minimax
multiple
shrinkage
estimation
ann
stat.
vol
188–205
1986
george
combining
minimax
shrinkage
estimators
amer
statist
assoc.
vol
437–445
1986
leung
barron
information
theory
mixing
least-
squares
regressions
ieee
trans
inf
theory
vol
3396–
3410
2006
leung
improving
regression
model
mixing
phd
thesis
yale
university
2004
baranchik
multiple
regression
estimation
mean
multivariate
normal
distribution
tech
report
stanford
university
1964
shao
strawderman
improving
james-stein
positive
part
estimator
ann
stat.
vol
1517–1538
1994
maruyama
strawderman
necessary
conditions
dominating
james-stein
estimator
ann
inst
stat
math.
vol
157–165
2005
beran
unbearable
transparency
stein
estimation
nonpara-
metrics
robustness
modern
statistical
inference
time
series
analysis
festschrift
honor
professor
jana
jureˇckov´a
25–34
institute
mathematical
statistics
2010
johnstone
gaussian
estimation
sequence
wavelet
models
online
http
//statweb.stanford.edu/∼imj/ge09-08-15.pdf
2015
boucheron
lugosi
massart
concentration
inequalities
nonasymptotic
theory
independence
oxford
university
press
2013
wasserman
statistics
concise
course
statistical
infer-
ence
springer
new
york
2nd
ed.
2005
williams
probability
martingales
cambridge
university
press
1991
hwang
casella
minimax
conﬁdence
sets
mean
multivariate
normal
distribution
annals
statistics
vol
868–881
1982
samworth
small
conﬁdence
sets
mean
spherically
symmetric
distribution
journal
royal
statistical
society
series
statistical
methodology
vol
343–361
2005
bock
minimax
estimators
mean
multivariate
normal
distribution
ann
stat.
vol
209–218
1975
manton
krishnamurthy
poor
james-stein
state
filtering
algorithms
ieee
trans
sig
process.
vol
2431–
2447
sep.
1998
ben-haim
eldar
blind
minimax
estimation
ieee
trans
inf
theory
vol
3145–3157
sep.
2007
chung
concentration
inequalities
martingale
inequal-
ities
survey
internet
mathematics
vol
79–127
2006
