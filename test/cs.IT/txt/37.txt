gaussianity
kolmogorov
complexity
mixing
sequences
morgane
austern
arian
maleki
abstract
let
kpx1
xnq
hpxn|xn´1
x1q
denote
kolmogorov
complexity
proved
shannon
entropy
rate
stationary
ergodic
process
txiu8
kpx1
xnq
hpxn|xn´1
x1q
almost
surely
paper
studies
convergence
rate
asymptotic
result
partic-
ular
show
process
satisﬁes
certain
mixing
conditions
exists
kpx1
hpx0|x1
x´8q˙
np0
σ2q
furthermore
show
slightly
stronger
mixing
conditions
one
may
obtain
non-
asymptotic
concentration
bounds
kolmogorov
complexity
introduction
1.1
motivation
objective
kolmogorov
complexity
binary
sequence
deﬁned
length
shortest
program
fed
universal
turing
machine
would
print
sequence
halt
formally
let
denote
universal
turing
machine
given
program
sequence
printed
denoted
uppq
deﬁnition
1.1.
let
denote
set
binary
programs
generate
ﬁnite
length
binary
sequence
halt
kolmogorov
complexity
denoted
kpxq
deﬁned
kpxq
inf
pppx
lengthppq
lengthppq
denotes
length
sequence
furthermore
kolmogorov
complexity
ﬁnite-length
ﬁnite-alphabet
sequence
kolmogorov
complexity
binary
representa-
tion
apart
mathematical
elegance
kolmogorov
complexity
exhibited
promising
the-
oretical
results
areas
research
including
inductive
inference
denoising
linear
regression
density
estimation
etc
however
theoretical
results
overshadowed
fact
kolmogorov
complexity
computable
see
theorem
1.5
usefulness
kolmogorov
complexity
incomputability
motivated
researchers
ﬁnd
approximations
quantity
one
main
approaches
restrict
class
sequences
stationary
ergodic
sequences
use
properties
sequences
ﬁnd
good
approximations
following
theorem
due
levin
clariﬁes
assumptions
might
useful
theorem
let
txiu8i
denote
binary
stationary
ergodic
process
left
shift
whose
law
computable
let
shannon
conditional
entropy
rate
hpx1|x0
x´8q
then,1
kpx1
a.s.ñ
hpx1|x0
x´8q
denotes
vector
px1
xnq
according
theorem
shannon
entropy
seen
approximation
kol-
mogorov
complexity
process
result
asymptotic
clear
accurate
approximations
even
large
paper
establishes
accuracy
approxima-
tion
certain
mixing
assumptions
process
clariﬁed
later
1.2
related
work
komogorov
complexity
evolved
seminal
papers
solomonoﬀ
kolmogorov
chaitin
author
developed
used
quantity
diﬀerent
pur-
poses
instance
inspired
shannon
theory
information
kolmogorov
developed
notion
complexity
quantify
amount
information
present
sequence
bits
kolmogorov
also
conjectured
binary
sequences
maximal
complexity
e.g
kpx1
xnq
ﬁxed
random
intuitive
sense.this
conjecture
later
established
martin-lof
intuitively
speaking
proved
sequence
satisﬁes
kpx1
xnq
test
implemented
turing
machine
accept
randomness
sequence
possibly
use
diﬀerent
signiﬁcance
level
intellectual
value
test
randomness
overshadowed
incomputability
kolmogorov
complexity
many
researchers
explored
new
ways
improve
applicability
kolmogorov
complexity
instance
explored
computable
approximations
kolmogorov
complexity
another
popular
direction
research
pursued
connections
kolmogorov
complexity
shannon
entropy
levin
result
i.e
theorem
one
general
connections
kolmogorov
complexity
shannon
entropy
paper
push
connections
one
step
providing
convergence
rate
concentration
results
kolmogorov
complexity
main
result
according
theorem
every
stationary
ergodic
sequence
txiu8i
probability
space
kpx1
a.s.ñ
hpx1|x0
x´8q
discussed
paper
would
like
characterize
rate
convergence
asymptotic
result
ﬁrst
goal
would
like
show
general
conditions
convergence
rate
speciﬁcally
would
like
show
kpx1
hpx1|x0
x´8q˙
1this
theorem
5.1
mentioned
levin
even
ergodicity
necessary
careful
deﬁning
entropy
information
refer
converges
distribution
non-degenerate
random
variable
discuss
main
theorem
would
like
show
impose
extra
conditions
process
convergence
rate
could
slower
example
2.1.
let
denote
probability
measure
set
positive
natural
numbers
probability
mass
function
ν1ptq
2´ǫ
normalizing
constant
let
tτiu8i
denote
iid
samples
distribution
furthermore
let
tyiu8i
˜yiu8i
denote
random
variables
independent
tτiu8i
given
two
independent
sequences
iid
bernp
natural
number
let
denote
vector
size
elements
zero
construct
binary
sequence
˜xiu8i
following
way
pick
ﬁrst
sequence
set
˜x1
y10τ1
y1q
˜y1
construct
ith
block
repeat
speciﬁcally
draw
set
˜xτ1
...
τi´1
...
τi´1
yi0τi
yiq
˜yτ1
...
τi´1
...
τi´1
make
˜xu8i
stationary
draw
θ|τ1
unifp0
uniform
integers
generate
new
process
left
shift
opera-
a.s.ñ
tor
straightforward
see
process
stationary
ergodic
hence
kpx1
nqn
hpx1|x0
x´8q
however
convergence
rate
slower
proof
claim
found
section
3.6.1.
note
major
issue
example
fact
elements
sequence
far
apart
still
strong
dependencies
hence
intuitively
speaking
expect
dependency
process
weaker
may
able
obtain
convergence
rate
mixing
conditions
deﬁned
capture
dependancies
stochastic
process
start
mixing
conditions
used
paper
let
txiu8i
denote
stationary
ergodic
process
let
σpxi
denote
σ-ﬁled
events
generated
random
variables
xn´2
xn´1
f8n
σpxi
denote
σ-ﬁled
events
generated
deﬁnition
2.1.
α-mixing
coeﬃcients
process
txiu8i
deﬁne
αpnq
sup
sup
apfj
bpf8
ˇˇˇ
´ač
ppaqppbqˇˇˇ
process
α-mixing
αpnq
α-mixing
condition
ensures
parts
process
far
apart
almost
inde-
pendent
hence
hope
αpnq
decays
fast
enough
avoid
dependency
issue
raised
example
2.1.
results
need
slightly
stronger
notion
mixing
deﬁne
deﬁnition
2.2.
φ-mixing
coeﬃcient
process
txiu8i
deﬁned
φpnq
sup
sup
apfj
bpf8
pb|aq
ppbq|
furthermore
process
called
φ-mixing
φpnq
remark
straightforward
see
φpnq
αpnq
hence
process
φ-mixing
α-mixing
addition
mixing
proof
requires
another
condition
described
deﬁnition
2.3.
let
txiu8
denote
stationary
ergodic
process
consider
deﬁne
νδpnq
ep|
logpppx0|x´1
x´8qq
logpppx0|x´1
x´nqq|
note
paper
logarithms
base
since
νδpnq
standard
notion
probability
theory
explain
interesting
features
deﬁnition
νδpnq
close
deﬁnition
kullback-leiber
divergence
ppx0|x´1
x´8q
ppx0
x´1
x´nq
hence
measures
discrepancy
process
markov
process
b-markov
source
txiu8i
lpx1|x0
lpx1|x0
´8q2
hence
νδpnq
every
sequences
generated
hidden
markov
model
also
fast
decaying
νδpnq
following
lemma
justiﬁes
claim
lemma
consider
hidden
markov
model
p0,8q
denoting
transition
kernel
underlying
markov
process
gp¨
denoting
distribution
observed
variables
given
value
hidden
variable
also
suppose
process
satisﬁes
following
conditions
essinf
qpx
x1q
esssup
qpx
x1q
esssupxgpy|xq
essinf
xgpy|xq
supy
sequence
observations
generated
process
exists
value
depending
νδpnq
e˜ˇˇˇˇ
ppy0|y´1
´8q˙ˇˇˇˇ
logˆ
ppy0|y´1
´nq
constant
depends
proof
lemma
presented
section
3.3.
addition
mixing
conditions
require
notion
stability
likelihood
process
ﬁnite
sample
concentration
results
understand
notion
ﬁrst
deﬁne
hamming
distance
two
vectors
deﬁnition
2.4.
hamming
distance
two
sequences
deﬁned
denotes
indicator
function
dnpx1
ixi‰yi
lpx1|x0
conditional
distribution
knowing
lpx1|x0
´8q
conditional
distribution
knowing
notion
enables
deﬁne
notion
m´stability
deﬁnition
2.5.
-stability
coeﬃcient
ﬁnite
state
m-markov
process
txiu8i
deﬁned
sup
nqď1|
logpppx1
logpppx11
nqq|
nqpan2
s.t
dnpx1
px1
sup
say
txiu8i
-stable
m´stability
coeﬃcient
ﬁnite
remark
consider
ﬁnite-state
m´markov
chain
txiu8i
minx1
´mpam
ppx1|x´m:0q
m-stability
coeﬃcient
satisﬁes
log´
proof
claim
presented
section
3.4.
notion
m´stability
used
obtain
ﬁnite-sample
concentration
results
notion
seen
relation
vast
majority
concentration
inequalities
azuma
hoefding
mcmiarmid
require
boundedness
conditions
using
notions
developed
state
ﬁrst
main
result
conﬁrms
asymptotic
gaussianity
kolmogorov
complexity
ergodic
sequences
theorem
let
txiu8
ta1
alu
furthermore
suppose
kolmogorov
complexity
ajs
ﬁnite
i.e.
maxipt1
...
kpaiq
assume
ﬁxed
numbers
denote
stationary
ergodic
process
assume
αpnq
kn´β
δqp1
νδpnq
deﬁne
2´cn
logplq˘
varplogpppx0|x´1
x´8qqq
2ÿk
covplogpppx0|x´1
x´8qq
logpppxk|xk´1
x´8qqq
kpx1
hpx0|x´1
x´8q˙
np0
σ2q
notation
used
convergence
distribution
proof
theorem
presented
section
3.5.
note
theorem
implies
theorem
however
result
provides
rate
convergence
well
theorem
theorem
concerned
asymptotic
behavior
kolmogorov
complexity
provide
information
ﬁnite
sample
behavior
quantity
following
corollary
simpliﬁes
statement
theorem
independent
identically
distributed
sequence
corollary
3.1.
let
txiu8
sume
ta1
alu
furthermore
assume
maxipt1
...
kpaiq
denote
independent
identically
distributed
process
as-
kpx1
hpx0q˙
np0
σ2q
varplogpppx0qqq
next
goal
derive
probabilistic
upper
bounds
discrepancy
kolmogorov
complexity
shannon
entropy
ﬁnite
sample
sizes
next
theorem
shows
bounds
obtained
slightly
stronger
mixing
conditions
theorem
integer
number
deﬁne
log˚pnq
denote
stationary
m-markov
process
assume
log˚plogpnqq
theorem
let
txiu8
ta1
alu
furthermore
assume
kolmogorov
complexity
ajs
ﬁnite
i.e.
maxipt1
...
kpaiq
-stability
coeﬃcient
process
ﬁnite
φ-mixing
coeﬃcients
process
satisfy
24ř8k
φpkq
let
0.5q
ﬁxed
number
constant
depends
universal
mahcine
deﬁne
c1pnqn
2´ηq
c1pnq
log˚pmq
maxjďl
kpajq
lpm
log˚
log˚
moreover
γ1pnq
log˚pmq
maxjďl
kpajq
log˚
log˚
mhpx1q
log˚pnqn
2∆2
k11pnq
2∆2rc1
log˚pnq
maxi
kpaiqs2
finally
let
constant
less
equal
maxiďl
kpaiq
hpx1|x0
2´η
opn´
γ1pnq
furthermore
p´|
kpx1
hpx1|x0
1q|
2e´
npt´γ1pnqq2
hpx1|x0
1qˇˇˇ
2e´
npt´γn
kpx1
p´ˇˇˇ
pnq
2e´
nt2
2∆2
log˚pnq2
nζ2´n
theorem
formulated
following
slightly
diﬀerent
way
corollary
4.1.
let
txiu8
fix
value
deﬁned
theorem
every
m´markov
process
satisﬁes
conditions
theorem
kpx1
nqn
hpx1|x0
1q|
2e´
proof
straightforward
application
theorem
proof
3.1
background
kolmogorov
complexity
two
simple
results
komogorov
complexity
employ
proofs
mention
two
simple
lemmas
refer
later
proofs
main
results
proof
results
reader
may
refer
chapter
example
14.2.7
theorem
14.2.4
lemma
let
denote
integer
number
following
upper-bound
kolmogorov
complexity
kpnq
log˚pnq
log˚pxq
log˚plogpxqq
constant
depends
universal
machine
straightforward
show
log˚pnq
logpnq
another
result
used
komogorov
complexity
paper
following
lemma
let
1u8
ť8i
1t0
1ui
1u8
kpxq
|cv|
3.2
background
information
mixing
sequences
proofs
also
use
well-known
results
central
limit
theorem
empirical
average
weakly
dependent
sequences
summarize
results
section
theorem
let
txiu8i
denote
stationary
process
epx1q
ep|x1|2
let
deﬁne
varp
xiq
suppose
function
p0,8q
let
denote
cdf
cumulative
distribution
α-mixing
coeﬃcients
satisfy
1pαpiqq
exist
following
conditions
hold
c.1
logpnq
logp16q
c.2
4kpαpm
1qq
c.3
2km
constant
depend
process
satisﬁes
|fnptq
φptq|
crx2
1qδ
1q2
sup
x2p1
bnpαpm
1qq
x2ppm
2qpαpm
1qq
2p2
xppm
2ep|x1|2
2σn
proof
theorem
approximate
kolmogorov
complexity
using
triangular
arrays
would
like
show
distribution
sum
ﬁrst
elements
n-th
row
triangular
array
converges
normal
distribution
obtain
use
following
corollary
theorem
corollary
7.1.
let
α-mixing
coeﬃcients
suppose
exists
value
ep|x
ep|x
p0,8q
let
denote
cdf
cumulative
distribution
function
exist
u8i
double-index
process
furthermore
let
αkpnq
denote
1|2
opkζq
suppose
suppose
u8i
stationary
process
epx
varpřn
u8i
assume
let
deﬁne
1qpδ
minp
αjpnq
minpc1pn
jǫq´β
δqp1
β´1
ﬁxed
number
sup
δqn´
δpβ´1q
|fnptq
φptq|
onrep|x
2pβ
onpn´ηq
δpβ
2pβ
proof
would
like
use
theorem
consider
nth
sequence
note
α-mixing
coeﬃcient
sequence
αnpkq
minppk
nǫq´β
δqp1
without
loss
generality
notational
simplicity
assume
hence
straightforward
see
ř8k
αnpkq
ﬁxed
number
derive
inequality
used
upper-bound
αnpkq
furthermore
choose
theorem
following
way
logpnq
1qpδ
straightforward
show
suﬃciently
large
conditions
c.1
c.2
c.3
required
theorem
hold
furthermore
straightforward
check
required
theorem
hence
obtain
nσ2
sup
|fnptq
φptq|
crx2
pmm
1qδ
npmn
1q2
qpαpmn
1qq
np1
nppmn
xnppmn
bnpαpmn
1qq
2p2
2ep|x
onrep|x
δqn´
δpβ´1q
2pβ
hence
proof
complete
straightforward
check
dominant
term
pmn
1qδ
following
lemma
enables
connect
correlation
two
random
variables
respectively
measurable
mixing
coeﬃcients
lemma
let
random
variables
measurable
respect
respectively
suppose
f8t
ep|ξ|2
ep|η|2
|epξηq
epξqepηq|
αpτq1´
3pcβ
c1´β
c1´β
2qq
concentration
inequalities
random
processes
assume
independence
however
want
make
assumptions
proof
theorem
use
following
result
kontorovich
ramanan
generalizes
martingale
method
dependent
variables
lemma
suppose
countable
space
let
txiu8i
stationary
process
furthermore
let
1-lipschitz
function
respect
hamming
metric
deﬁne
supx0
ppxj
¨|x0
ppxj
¨|x0
let
matrix
deﬁned
following
way
i=j
otherwise
ppgpx1
epgpx1
nqq
ppgpx1
epgpx1
nqq
´tq
2n∆2
2n∆2
maxiďnp1
¨¨¨
remark
note
lemma
proposed
two-sided
bound
use
one-sided
version
furthermore
note
conditions
bit
stronger
one
proposed
simplicity
use
condition
3.3
proof
lemma
according
proposition
exits
ppy0
¨|y´1
´mq
ppy0
¨|y´1
´8q
also
note
following
three
facts
straightforward
prove
hpxq
logpxq|
|x´1|
hpxq
dppy0|y´1
´8q
dppy0|y´1
´mq
şx0
şx0
increasing
function
p1,8q
dppx0|y´1
´8qgpy0|x0qdx0
dppx0|y´1
´mqgpy0|x0qdx0
gpy0|x0q
esssupx0
essinf
gpy0|x0q
employing
facts
obtain
ppy0|y´1
´8q
ppy0|y´1
´mqq|
ep|
logp
epż
logp
epż
logp
maxp2
dppy0|y´1
´8q
dppy0|y´1
´mqq|
dppy0|y´1
´8q
dppy0|y´1
´mqq|
logpηq|
qepż
logpηq|
qep
ppy0
¨|y´1
´mq
ppy0
¨|y´1
´8q
dppy0|y´1
´8qq
dppy0|y´1
´8q
dppy0|y´1
´mq
dppy0|y´1
´8q
dppy0|y´1
´mq|dppy0|y´1
´mqq
dppy0|y´1
´mqq
maxp2
c1τ
note
similar
ideas
used
3.4
proof
remark
consider
two
vectors
dnpx
x1q
dnpx
x1q
easily
see
logpppxq
logpppx1qq|
hence
assume
dnpx
x1q
suppose
x1i
1|s
logpppx1
logpppx11
nqq|
logpppx1
i´1q
logpppx11
i´1qq|
logpppxm
n|xi
iqq
logpppx1m
n|x1i
iqq|
logpppxj|xj´1
j´mqq
logpppx1j|x1j´1
j´mqq|
´pm
logpρq
logpppxj|xj´1
j´mqq
logpppx1j|x1j´1
j´mqq|
comes
following
facts
every
x11
hence
logpppx1
i´1q
logpppx11
i´1qq|
every
x1j
hence
logpppxm
n|xi
iqq´
logpppx1m
n|x1i
iqq|
iii
finally
r|i
i|s
logpppxj|xj´1
j´mqq
logpppx1j|x1j´1
j´mqq|
logpρq
proof
1|s
similar
hence
skipped
3.5
proof
theorem
3.5.1
lower
bound
proof
discuss
details
proof
give
brief
overview
proof
strategy
help
reader
navigate
proof
easily
consider
sequence
assume
|a|
section
ﬁrst
present
simple
program
universal
computer
use
generate
sequence
deﬁne
note
constant
one
used
condition
statement
theorem
program
ﬁrst
tells
universal
logpnq
logplq
computer
ﬁrst
bits
sequence
counts
number
times
pmn
1q´tuple
present
remaining
sequence
reports
it.3
words
deﬁne
xk´mn
amn
amn
amn
numbers
vector
empirical
counts
i.e.
jth
element
speciﬁc
order
described
universal
computer
described
universal
computer
let
denote
lmn
deﬁne
operator
1slmn
output
deﬁne
type
sequence
following
set
tz1
ofpx1
ofpz1
mnu
tx1
takes
input
returns
given
information
known
universal
computer
far
already
access
tx1
remaining
piece
information
universal
computer
reconstruct
entire
sequence
index
sequence
among
sequences
type
let
count
number
bits
used
far
describe
sequence
description
requires
bits
specify
following
quantities
iii
ﬁrst
bits
frequency
observing
possible
block
length
pmn
systematic
way
build
sequences
length
tx1
index
tx1
kpmnq
log˚pmnq
describe
maxjďl
kpajq
required
iii
describe
ﬁrst
symbols
require
mnplog˚plq
describe
frequency
block
require
lmn
log˚pnq
bits
reason
clear
lmn
diﬀerent
l-ary
blocks
length
elements
far
universal
computer
detected
tx1
describe
element
tx1
ﬁrst
step
write
constant
size
program
universal
computer
realizes
ordering
sequences
using
next
step
specify
index
sequence
list
evaluate
number
bits
required
describing
index
count
number
elements
tx1
deﬁne
new
measure
following
properties
mn-markov
property
i.e.
mnpx1
xnq
mnpx1
xmnq
mnpxj
xj´1
xj´mnq
1th-dimension
transition
probabilities
original
dis-
tribution
i.e.
mnpxj
xj´1
xj´mnq
ppxj
xj´1
xj´mnq
3for
instance
sequence
01001
couple
present
twice
couple
notational
simplicity
consider
notation
qmn
mnpxmn
amn
mn|x0
amn
j,0
xmn´1
amn
mn´1q
j,0
amn
pamn
mnq
jth
element
amn
new
notation
count
number
elements
tx1
note
ﬁrst
symbols
already
known
let
call
xmn
since
ÿx1
nptx1
mnpxmn
xmn
xmnq
hence
ÿx1
nptx1
mnpxj|xj´1
xj´mnq
|tx1
ℓmn
pqmn
qpn´mnqf
|tx1
2´pn´mnqřlpmn
log
qmn
implies
code
index
element
tx1
require
less
´pn´mnqřlmn
bits
combining
pieces
obtain
following
upper
bound
length
program
log
qmn
lmn
kpx1
log˚pmnq
max
jďl
kpajq
lpmn
log˚
log˚
l´pn´mnq
nhpx1|x0
´8q
converges
distribution
normal
ran-
goal
show
kpx1
...
xnq
dom
variable
note
ﬁrst
ﬁve
terms
deterministic
divided
converge
zero
hence
focus
remaining
term
i.e.
pn´mnqřlmn
log
qmn
hpx0|x´1
x´8qq
log
qmn
log
qmn
lmn
npn
mnq
nphpx0|x´1
x´8q
hpx0|x´1
x´mnqq
log
qmn
lmn
hpx0|x´1
x´mnqq
ﬁrst
claim
nphpx0|x´1
x´8q
hpx0|x´1
x´mnqq
see
holds
note
n|hpx0|x´1
x´8q
hpx0|x´1
x´mnqq|
ne|
log
ppx0
x´1
x8q
log
ppx0
x´1
x´mnq|
npe|
log
ppx0
x´1
x8q
log
ppx0
x´1
x´mnq|
npνδpmnqq
paq
2´ǫ
remind
reader
picked
log
log
satisfying
note
obtain
used
holder
inequality
last
step
derived
form
condition
theorem
regarding
decay
combining
conclude
remaining
step
show
n´mn
hpx0|x´1
x´mnqq
gaussian
toward
goal
ﬁrst
deﬁne
log
ppxj|xj´1
xj´2
xj´mnq
řlmn
log
qmn
note
řlmn
log
qmn
deﬁne
n´mn
smn
prove
gaussianity
smn
theorem
employ
corollary
7.1.
first
let
check
conditions
e|y
boundedness
e|y
ÿxjpa
ÿxjpa
first
note
ppxj|xj´1
xj´2
xj´mnq|
log
ppxj|xj´1
xj´2
xj´mnq|2
gpppxj|xj´1
xj´2
xj´mnqq
function
deﬁned
following
way
gptq
logptq|2
also
gp0q
straightforward
check
following
properties
continuous
zero
exists
g1pcδq
iii
g1ptq
g1ptq
automatically
implies
gptq
gpcδq
combing
fact
implies
gpppxj|xj´1
xj´2
xj´mnq
lgpcδq
e|y
ÿxjpa
note
upper
bound
depend
either
mixing
coeﬃcient
first
let
mnpiq
denote
α-mixing
coeﬃcient
sequence
let
αpiq
denote
mixing
coeﬃcient
original
process
straightforward
check
every
mnpiq
αpi
mnq
kpi
mnq´β
δqp1
otherwise
last
step
due
condition
statement
theorem
reminder
oplogpnqq
covplogpppx0|x´1
x´8qq
logpppxk|xk´1
x´8qq
wkq
deﬁne
˜σ2
notational
simplicity
rest
proof
use
notation
řn´mn
n´mnq
later
prove
˜σ2
varpymn
varplogpppx0|x´1
x´8qqq
2ÿk
covplogpppx0|x´1
x´8qq
logpppxk|xk´1
x´8qqq
first
see
goal
deﬁne
instead
logpppxj|xj
j´1qq
paq
covplogpppx0|x´1
x´8qq
logpppxk|xk´1
x´8qqq
covplogpppx0|x´1
x´8qq
wkq
kp4
6lgpcδqqÿk
6lgpcδqq
pνδpk
2qq
2´c
logpℓq
βp1
obtain
ﬁrst
term
inequality
employed
lemma
obtain
second
term
inequality
used
holder
inequality
deﬁnition
2.3.
last
inequality
result
condition
statement
theorem
prove
˜σ2
varpřn´mn
varpy
varpy
covpy
covpy
obtain
last
equality
used
stationarity
process
goal
show
quantity
converges
simplify
expression
following
two
steps
simplifying
varpy
first
note
|eplogpppx1|x0
´mn
1qqq
eplogpppx1|x0
´8qqq|
pe|plogpppx1|x0
´mn
1qqq
logpppx1|x0
´8qq|
obtain
last
inequality
used
holder
obtain
last
convergence
used
condition
statement
theorem
furthermore
note
pνδpmnqq
|eplog2pppx1|x0
´mnqqq
eplog2pppx1|x0
´8qqq|
pe|plogpppx1|x0
´mn
1qqq
logpppx1|x0
´8qq|
ˆpe|plogpppx1|x0
´mn
1qqq
logpppx1|x0
´8qq|2
prove
last
convergence
note
ﬁrst
term
goes
zero
according
condition
statement
theorem
furthermore
similar
proof
show
last
expectation
bounded
hence
straightforward
combine
two
equations
obtain
varpy
varplogpppx1|x0
´mn
1qqq
varplogpppx1|x0
´8qqq
second
step
discuss
covariance
terms
deﬁne
covpy
covplogpppx1|x´1
´8qq
logpppxj|xj´1
´8qqqq
note
goal
bound
1psi
sq|
2mn
|si
2mn
1|si
prove
later
supi
|si
bounded
hence
since
conclude
ﬁrst
term
goes
zero
hence
focus
second
term
deﬁne
logpppxj|xj´1
´8qq
covpz1
zjq|
2mn
2mn
1|si
2mn
2mn
2mn
|covpy
2mn
1|covpy
|covpz1
zjq|
covpz1
zjq|
show
three
terms
right
converge
zero
proceed
note
z1|
ep|y
furthermore
similar
proof
straightforward
show
logpppx1|x0
´mn
1qq
logpppx1|x0
´8qq|
νδpmnq
ep|zj|q
pe|zj|2
e|y
pe|y
suptpr0,1s
|g2
δptq|
δptq
logptq|2
turn
attention
bounding
terms
|covpy
covpz1
zjq|
|covpy
zjq|
|covpy
pzj
z1qezj|
e|y
|epy
qepzj
zj|
pe|y
pe|y
zj|
e|zj|
e|y
e|py
z1qzj|
|epy
pe|y
z1|
pe|y
zj|
pe|zj|2
pe|y
4mpνδpmnqq
hence
conclude
2mn
2mn
|covpy
covpz1
zjq|
2mn
2mn4mpνδpmnqq
note
last
convergence
theorem
derived
condition
statement
theorem
ﬁnd
bound
second
term
deﬁne
logpppxj|xj
j´1qq
|covpy
|covpy
|covpy
|covpy
zjq|
|covpy
covpz1
zjq|
|covpy
wjq|
|covpy
wjq|
|covpy
wjq|
|covpy
wjq|
|covpy
wjq|
|covpy
wjq|
strategy
use
bound
terms
|covpy
wjq|
|covpy
´z1
wjq|
|covpy
wjq|
also
strategy
use
bound
|covpy
hence
derive
bounds
following
three
terms
|covpy
wjq|
|covpy
|covpy
wjq|
iii
|covpy
wjq|
using
holder
inequality
conclude
|covpy
wjq|
e|y
wjq|
zj´
pzj
wjq|
e|y
ep|y
|e|zj
wj|
2pe|zj
wj|
2pνδpj
2qq
mea-
|covpy
surable
respect
wjq|
note
measurable
respect
hence
employing
lemma
conclude
|covpy
wjq|
αpj
˜mq
lgpcδq
note
obtain
last
inequality
used
|covpy
similar
argument
previous
case
conclude
|covpy
q||
αpj
mnq
2p4
˜mq
combining
three
cases
conclude
2mn
2mn
2mn
2mn
1|covpy
2mn
4pνδpj
2qq
4pνδpj
2qq
covpz1
zjq|
2αpj
2p4
2mq
αpj
mnq
2p4
2mq
2αpj
2mq
αpj
mnq
2mq
last
term
bounded
exactly
similar
fashion
i.e.
use
upper
bound
|covpz1
zjq|
|covpz1
wjq|
|covpz1
wjq|
employ
lemma
deﬁnition
bound
error
since
proof
similar
skip
combining
steps
conclude
equations
together
prove
1psi
sq|
varpřn´mn
therefore
proved
n´mn
hpx1|x0
´mn
1qs
l2ýýñ
hence
nrc1
log˚pmnq
max
jďl
kpajq
lpmn
log˚
n´mn
log˚
l´pn´mnq
lmn
log
qmn
dýñ
apply
corollary
7.1
employing
triangle
inequality
denoting
cdf
řmn
´khpx1|x0
...
x´mnq
bvarpřmn
sup
sup
ˇˇˇˇˇˇ
ˇˇˇ
φptq
φpt
lpmn
mnσ
bvarpřn
according
corollary
7.1
supt
proved
log
qmn
n´mnptq
φptq|
hpx0|x´1
x´mnqq
qˇˇˇ
sup
n´mnptq´φptq|
oppn´mnq´
δpβ´1q
mnσ
bvarpřn´mn
employing
mean
value
theorem
show
φptqˇˇˇˇˇˇ
4pβ
opn´
δpβ´1q
4pβ
q.moreover
sup
mnσ
qˇˇˇ
bvarpřn´mn
use
conclude
lim
inf
nñ8
φptσq
one
side
prove
φptq
φpt
ˇˇˇ
kpx1
nq´hpx0|x´1
x´8qq
3.5.2
upper
bound
proof
deﬁne
kpx1
kpx1
logpppx1
nqq
δn¯
logpppx1
nqq
kpx1
kpx1
kpx1
logpppx1
nqq
kpx1
goal
show
proper
choice
probabilities
right
converge
zero
first
note
kpvq
kpvq
ppvqă2´pi
nδn
furthermore
choose
kpx1
2i2´pi
nδnq
nx2´nδn
hpx0|x´1
x´8q˙
kpvq
hpx0|x´1
x´8q
ppx1
2´pi
nδnq
ppvqă2´pi
nδn
ppvqă2´pi
nδnq
2log
ppvq
hence
combining
kpx1
logpppx1
nqq
δn¯
kpx1
hpx0|x´1
x´8qq
δn¯
np´
hand
kpx1
hpx0|x´1
x´8qq
note
two
main
points
last
expression
according
ﬁrst
term
goes
zero
would
like
characterize
limiting
distribution
logpppx1
nqq
nδn
nhpx0|x´1
x´8qq
rewrite
expression
following
way
logpppx1
nqq
logpppx1
nqq
nδn
nhpx0|x´1
x´8qq
logpppxj|xj´1
j´mnqq
logpppx1
nqq
logpppx1
nqq
´řn
nδn
logpppxj|xj´1
j´mnqq
nhpx0|x´1
x´8qq
2´ǫ
log
log
logpppx1
nqq
note
prove
nδn
logpppxj|xj´1
j´mnqq
logpppxj|xj´1
j´mnqq
nhpx0|x´1
x´8qq
np0
σ2q
slutsky
theorem
conclude
np´
logpppx1
nqq
hpx0|x´1
x´8qq
φpσtq
proof
proof
presented
last
section
prove
ﬁrst
note
nδn
furthermore
2mn
logpppx1
nqq
e˜ˇˇˇˇˇ
e˜ˇˇˇˇˇ
logpppx1
mnqq
´eˆ
logpppx1
mnqq
´eˆ
logpppx1
mnqq
logpppxj|x1
jqq
logpppxj|xj´1
j´mn´1qq
ˇˇˇˇˇ
logpppxj|xj´1
j´mnqq
ˇˇˇˇˇ
ep|
logpppxj|x1
j´1qq
logpppxj|xj´mn
j´1qq|q
ep|
logpppxj|x1
j´1qq
logpppxj|x´8
j´1qq|q
ep|
logpppxj|x´8
j´1qq
logpppxj|xj´mn
j´1qq|q
pνδpmnqq
´eˆ
logpppx1
mnqq
hence
lim
supnñ8
1pνδpjqq
kpx1
hpx0|x´1
x´8qq
φptσq
3.6
proof
theorem
details
proof
review
main
ideas
going
use
upper
lower
bounds
kolmogorov
complexity
derived
proof
theorem
get
inequality
bound
obtain
concentration-inequalities
combine
obtain
concentration
result
kolmogorov
complexity
use
concentration
inequality
presented
lemma
note
use
notations
deﬁned
deﬁne
gpx1
log
would
like
use
lemma
show
gpx1
concentrates
toward
goal
need
following
two
steps
calculate
upper
bound
matrix
elements
i=j
otherwise
gpx1
1-lipschitz
hamming-distance
following
two
steps
prove
kpx1
nqn
distance
calculate
upper-bound
kpx1
nqn
discuss
details
proof
show
inequality
would
also
use
lemma
toward
goal
also
need
lipschitz
function
hamming-
hpx1|x0
summary
first
bound
every
deﬁne
npx0
txj
an´j
ppxj
n|x0
ppxj
ppxj
¨|x0
ppxj
sup
ÿxj
npan´j
|ppxj
n|x0
ppxj
nq|
sup
ÿxj
npan
jpx0
sup
ppxj
n|x0
ppxj
ÿxj
npac
ppxj
ppxj
n|x0
iqs
jpx0
sup
irppxj
jpx0
iq|x0
ppxj
jpx0
iqq
ppxj
sup
bpf8
apfi
jpx0
iq|x0
ppxj
jpx0
iqqs
|ppb|aq
ppaq|
2φpj
hence
according
deﬁnition
sup
sup
ppxj
¨|x0
ppxj
¨|x0
ppxj
¨|x0
ppxj
sup
bpf8
apfi
|ppb|aq
ppaq|
4φpj
using
notations
introduced
4ř8k
φpkq
moreover
according
proof
theorem
kpx1
log˚pmq
max
kpajq
lpm
log˚
log˚
log
jďl
log
function
1-lipschitz
use
lemma
note
let
c1pnq
log˚pmq
maxjďl
kpajq
lpm
log˚
log˚
goal
ﬁnd
concentration
inequality
mqřlm
toward
goal
prove
logpqm
ipxj´m
log
kth
element
let
denote
two
vectors
diﬀer
jth-coordinate
i.e
x1i
-stability
assumption
theorem
|gpxq´
gpx1q|
note
log-likelihood
hence
-lipschitz
hamming
metric
lemma
implies
every
p´pn
log
mqhpx1|x0
´t¯
2e´
2nm
2∆2
p´pn
log
mqhpx1|x0
2e´
2nm
2∆2
straightforward
conﬁrm
every
c1pnqn
hpx1|x0
kpx1
n´t´
hpx1|x0
log
hpx1
|x0
1q¯2
log
2∆2
pnq
ppn
rpn
mqhpx1|x0
mhpx1|x0
1qsq
ppn
mqhpx1|x0
1qq
c1pnq
hpx1|x0
1q¯
c1pnq
prove
upper
bound
ﬁrst
set
3.5.2
prove
similar
proof
presented
section
hence
satisﬁes
logpppx1
nqq
kpx1
hpx1|x0
δn¯
nζe´n
hpx1|x0
hpx1|x0
1q¯
rpn
log
mqhpx1|x0
1qs
hpx1|x0
1q¯
hpx1|x0
´t˙
logpp
px1
nqq
logpp
px1
nqq
δn¯
kpx1
kpx1
kpx1
logpp
px1
nqq
hpx1
|x0
1q´δnq2
npt´
2∆2
δn¯
nζe´n
obtain
second
inequality
used
fact
logpppx1
nqq
logpppx1
mqq
logpppxm
n|x1
mqq
logpppxm
n|x1
mqq
finally
ﬁrst
term
last
line
similar
hence
combining
obtain
nˆt´
hpx1
|x0
1q´δn˙2
pnq
nζe´n
finally
note
c1pnq
onpn´1
log˚pnqq
hence
deﬁne
c1pnqn
hpx1|x0
2∆2
kpx1
pˆˇˇˇˇ
2e´
hpx1|x0
1qˇˇˇˇ
pˆˇˇˇˇ
kpx1
hpx1|x´m:0qˇˇˇˇ
2e´
npt´γn
nζ2´n
opn´p
2´ηqq
want
discuss
details
proof
inequality
consider
two
vectors
an2
dnpx
x1q
dnpx
x1q
easily
see
|kpx1
kpx11
nq|
hence
assume
dnpx
x1q
suppose
x1i
note
universal
machine
knows
know
x11
need
know
x1i
therefore
kpx11
kpx1
log˚pnq
max
kpaiq
constant
depends
universal
machine
lipschitz
previous
inequality
symetric
obtain
lemma
implies
every
kpx1
log˚pnq
maxi
kpaiq
moreover
thanks
kraft
inequality
positivity
kullback-leiller
divergence
kpx1
p´|
2´kpx1
qqq
hence
hpx1
epkpx1
nqq
kpx1
nqq|
2e´
nt2
2pc1
log˚pnq
maxi
kpai
qq2
moreover
use
upper-bound
kolmogorov
complexity
obtained
equation
eplogp
ppx1
get
epkpx1
nqq
log˚pmq
max
jďl
kpajq
log˚
log˚
mqhpx1|x0
hence
γ1pnq
epkpx1
nqq´hpx1|x0
1q|
therefore
deﬁning
γ1pnq
log˚pmq
maxjďl
kpajq
log˚
log˚
mhpx1q
get
log˚pmq
maxjďl
kpajq
log˚
log˚
mhpx1q
p´|
kpx1
kpx1
nqq|
2e´
npt´γ1pnqq2
2pc1
log˚pnq
maxi
kpai
qq2
3.6.1
proof
example
2.1
ﬁrst
mention
following
central-limit
theorem
triangular
arrays
martingales
later
used
proof
theorem
let
psn
zero-mean
square
integrable
martingale
array
diﬀerences
let
a.s.
ﬁnite
random
variable
suppose
ÿiďkn
ÿiďkn
epx
epx
ii|xn
i|ąǫ|fi´1q
pýñ
i|fi´1q
pýñ
dýñ
characteristic
function
epe´
η2t2
fnptτku8k
review
roadmap
proof
first
ﬁnd
upper
bound
lower-bound
complexity
terms
pτkqk
pykqk
using
upper
lower
bound
prove
function
kpx1
nqn
tyku8k
´8qq
almost
surely
implies
central-limit
theorem
holds
asymptotic
dis-
tribution
nphpx1|x0
´8q
fnptτku8k
tyku8k
´8qq
would
also
gaussian
2´ηphpx1|x0
´8q
prove
happen
since
fnptτku8k
first
understand
proof
notice
process
txiu8i
constituted
diﬀerent
segments
random
variables
comes
diﬀerent
distributions
seg-
ments
diﬀerent
lengths
example
τ1´θ|τ1
comes
certain
distribution
xτ1´θ
τ1´θ
τ2|τ1
may
come
another
distribution
let
tliui
denote
ith
segments
e.g
τ1´θ
deﬁne
length
ﬁrst
segment
every
deﬁne
tyku8k
´8qq
bounded
probability
maxtk
¨¨¨
maximum
number
segments
tlkuk
including
ﬁrst
one
entirely
finally
deﬁne
llef
tpiq
´řni
number
elements
diﬀerent
describe
may
describe
segment
xl1
xl1
řnn
straightforward
conﬁrm
following
two
facts
ith
segment
described
length
segment
constant
cost
indicate
machine
produce
array
ith
segment
described
describing
element
segment
since
segments
straightforward
conﬁrm
kpx1
τnn
llef
tpnq
ynnq
cpnn
minpl1
nqiy1
ÿiďnn
τiiyi
llef
tpnqiynn
note
full-description
also
describe
following
universal
machine
pl1
llef
tpnqq
py1
hence
straightforward
check
following
upper
bound
kolmogorov
complexity
kpx1
pnn
1qp1
log˚pnqq
minpl1
nqiy1
ÿiďnn
τiiyi
llef
tpnqiynn
proceed
simplify
upper
bound
let
ﬁnd
lower
bound
kolmogorov
complexity
well
deﬁne
vector
following
way
take
segments
n´llef
tpnq
coming
concatenate
obtain
vector
note
universal
computer
access
requires
following
information
construct
values
ynn
τnn
hence
straightforward
show
kpvnq
kpx1
pnn
1qp1
log˚
intuitively
clear
since
iid
bernp1
elements
kolmogorov
complexity
concentrated
around
length
prove
intuition
lemma
11.
let
denote
length
n´2
ppkpvnq
nδn|lnq
proof
first
certain
describe
describe
length
sequence
cost
log˚plnq
describe
elements
sequence
cost
iii
telling
build
sequence
cost
constant
depends
universal
machine
hence
ppkpvnq
2ln|lnq
kpvnq
log˚plnq
ppkpvnq
logpppvn|lnqq
nδn|lnq
ppkpvnq
2ln|lnq
ppkpvnq
2ln
kpvnq
logpppvn|lnqq
nδn|lnq
ppkpvnq
2ln|lnq
2logpppv
|lnqq
ppkpvnq
2ln|lnq
2ln2nδn
2ln
kpvq
please
note
pass
second-line
third
used
lemma
finally
ppkpvnq
nδn|lnq
indeed
knowing
sequence
iid
bernouillip
note
pl1
nqiy1
řiďnn
kpx1
logpppvnq|lnq
τiiyi
llef
tpnqiynn
combing
lemma
obtain
following
upper
lower
bounds
kpx1
pnn
1qp1
log˚pnqq
kpx1
pnn
1qp1
log˚pnqq
nδn
lower
bound
holds
probability
converging
next
goal
show
probability
converging
one
n˜kpx1
minpl1
nqiy1
ÿiďnn
τiiyi
llef
tpnqiynn
straightforward
conﬁrm
nδn
hence
prove
nnplog˚pnq
going
true
log˚pnq
toward
goal
deﬁne
since
sum
iid
variables
straightforward
conﬁrm
a.sýýñ
2´ǫ
moreover
also
obtain
convergence
exchangeability
pτiqiďn|sn
dominated
convergence
theorem
|snqq
nτ1
pýñ
epepřn
therefore
nτ1
psn´τ1q1
l1ýýñ
implies
psn
τ1q1
npp
nτ1
psn
τ1q1
ppτ1
psn
τ1q1
u|sn
τ1q˘
k1neps´p1
uqp
used
fact
constant
ﬁxed
pp|τ1|
k1b´p
2´ǫq
hence
eps´p
2´ǫqp1
note
employing
markov
inequality
obtain
s´1
oppn´p1
uq´1p
´ǫq´1
ppnn
2´ǫ
pps
ppsm
pp1
s´1
last
equation
comes
equation
straightforward
conclude
pmp
q´1
4q´1
4q´1
log˚pnq
´ǫqp1
hence
completes
proof
straightforward
prove
entropy
rate
process
hence
would
like
show
kpx1
xnq
ωp1q
suppose
case
using
prohorov
theorem
sequence
tight
sequence
kpx1
...
xnq
subsequence
converges
almost
surely
simplify
notation
instead
working
convergent
subsequence
assume
entire
sequence
converges
distribution
since
minpl1
nqiy1
´řiďnn
kpx1
xnq
τiiyi
llef
tpnqiynn
n˜kpx1
minpl1
nqiy1
ÿiďnn
τiiyi
llef
tpnqiynn
according
n˜kpx1
minpl1
nqiy1
ÿiďnn
τiiyi
llef
tpnqiynn
pýñ
assumed
kpx1
...
xnq
sky
theorem
claim
minpl1
nqiy1
0´řiďnn
τiiyi
llef
tpnqiynn
distribution
note
l1iy1
llef
tpnqiynn
τnn
therefore
converges
distribution
use
slut-
converges
minpl1
nqiy1
llef
tpnqiynn
a.s.ýýñ
hence
analyses
reduces
analysis
n´řiďnn
minpl1
nqiy1
llef
tpnqiynn
τiiyi
note
τnn˙
0.5˙
τiiyi
τnn˙
0.5˙
řiďnn
llef
tpnq
τiiyi
nˆeˆřiďnn
nˆeˆřiďnn
2řiďnn
a.s.ýýñ
hence
discuss
limiting
distribution
following
quantity
nˆřiďnn
τiiyi
eˆřiďnn
τiiyi
τnn˙˙
toward
goal
ﬁrst
introduce
following
sigma-ﬁelds
σpτi
iiďnl
iyi
processes
aři
straightforward
see
ppy
sponding
martingale
diﬀerences
given
τiiiďnlpiyi
iiďnn
flqlqn
triangular
array
martingales
corre-
aři
iiďnn
ini´1ăjďnipiyi
would
like
use
theorem
10.
straightforward
check
τjini´1ăjďnipiyi
2qq2|fi´1q
furthermore
prove
following
claim
iiďnn
eppÿj
epˇˇřj
τjini´1ăjďnipiyi
aři
iiďnn
ÿiďn
|řj
ini´1ăjďni
iiďnn
piyi
|ąǫ|fi´1q
pýñ
toward
goal
note
˝ˇˇˇ
ÿiďn
paq
ÿiďn
ÿiďnn
ÿiďnn
pbq
iiďnn
iiďnn
iiďnn
τjini´1ăjďnipiyi
ˇˇˇ
iˇˇˇ
piyi
iiďnn
iiďnn
ini´1ăjďni
aři
epˇˇÿj
2qˇˇ
|řj
ini´1ăjďni
pp|řj
τjini´1ăjďnipiyi
ǫ|fi´1q
ini´1ăjďnipiyi
iiďnn
ˇˇˇąǫ|fi´1˛
piyi
iiďnn
aři
iiďnn
ěǫ2
max
iiďnn
ěǫ2
maxi
iiďnn
ěǫ2
|ąǫ|fi´1q
note
obtain
equality
used
fact
measurable
hence
obtain
inequality
used
fact
ﬁxed
diﬀerence
ni´1
one
also
|iyi
finally
straightforward
see
iiďnn
ǫ2q
proves
ppmaxi
according
theorem
theorem
iiďnn
τiiiďnnpiyi
dýñ
np0
hence
aři
nˆřiďnn
τiiyi
converges
non-degenerate
distribution
need
swartz
easily
see
přiďnn
slower
pn´l1´lef
tpnqq2
iiďnn
τiiyi
e´řiďnn
iiďnn
τnn¯˙
iiďnn
θpnq
however
cauchy-
τiq2
lef
tpnqq2
therefore
contradiction
proves
speed
convergence
nnˆn
references
ray
solomonoﬀ
complexity-based
induction
systems
comparisons
convergence
theorems
information
theory
ieee
transactions
:422–432
1978
david
leigh
donoho
kolmogorov
sampler
department
statistics
stanford
univer-
sity
2002
shirin
jalali
arian
maleki
minimum
complexity
pursuit
communication
control
computing
allerton
2011
49th
annual
allerton
conference
pages
1764–1770
ieee
2011
andrew
barron
thomas
cover
minimum
complexity
density
estimation
infor-
mation
theory
ieee
transactions
:1034–1054
1991
alexander
zvonkin
leonid
levin
complexity
ﬁnite
objects
develop-
ment
concepts
information
randomness
means
theory
algorithms
russian
mathematical
surveys
:83–124
1970
ray
solomonoﬀ
formal
theory
inductive
inference
part
information
control
:1–22
1964
ray
solomonoﬀ
formal
theory
inductive
inference
part
information
control
:224–254
1964
andrei
kolmogorov
three
approaches
quantitative
deﬁnition
oﬁnformation
prob-
lems
information
transmission
:1–7
1965
gregory
chaitin
simplicity
speed
programs
computing
inﬁnite
sets
natural
numbers
journal
acm
jacm
:407–422
1969
gregory
chaitin
length
programs
computing
ﬁnite
binary
sequences
journal
acm
jacm
:547–569
1966
gregory
chaitin
theory
program
size
formally
identical
information
theory
journal
acm
jacm
:329–340
1975
gregory
chaitin
information-theoretic
limitations
formal
systems
journal
acm
jacm
:403–424
1974
david
willis
computational
complexity
probability
constructions
journal
acm
jacm
:241–259
1970
juris
hartmanis
generalized
kolmogorov
complexity
structure
feasible
compu-
foundations
computer
science
1983.
24th
annual
symposium
pages
tations
439–445
ieee
1983
michael
sipser
complexity
theoretic
approach
randomness
proceedings
ﬁfteenth
annual
acm
symposium
theory
computing
pages
330–335
acm
1983
vladimir
yugin
ergodic
theorems
individual
random
sequences
theoretical
com-
puter
science
207
:343–361
1998
brudno
entropy
complexity
trajectories
dynamic
system
trudy
moskovskogo
matematicheskogo
obshchestva
44:124–149
1982
thomas
cover
thomas
joy
elements
information
theory
john
wiley
sons
2012
sunklodas
approximation
distributions
sums
weakly
dependent
random
variables
normal
distribution
lithuanian
mathematical
journal
:359–368
1987
i.a
ibragimov
independent
stationary
sequences
random
variables
1971
l.kontorovich
ramanan
concentration
inequalities
dependent
random
variables
via
martingale
method
annals
probility
2008
randal
douc
catherine
matias
asymptotics
maximum
likelihood
estimator
general
hidden
markov
models
bernouilli
:381–420
2001
igal
sason
reverse
pinkser
inequalities
peter
hall
christopher
heyde
martingale
limit
theory
application
academic
press.
2014
