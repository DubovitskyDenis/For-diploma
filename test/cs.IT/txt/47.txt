complexity
estimating
renyi
divergences
maciej
skorski
ist
austria
email
mskorski
ist.ac.at
abstract—this
paper
studies
complexity
estimating
renyi
divergences
discrete
distributions
observed
samples
baseline
distribution
known
priori
extending
results
acharya
soda
estimating
renyi
entropy
present
improved
estimation
techniques
together
upper
lower
bounds
sample
complexity
show
contrarily
estimating
renyi
entropy
sublinear
alphabet
size
number
samples
sufﬁces
sample
complexity
heavily
dependent
events
occurring
unlikely
unbounded
general
matter
estimation
technique
used
divergence
order
bigger
provide
upper
lower
bounds
number
samples
dependent
probabilities
conclude
worst-case
sample
complexity
polynomial
alphabet
size
probabilities
non-negligible
gives
theoretical
insights
heuristics
used
applied
papers
handle
numerical
instability
occurs
small
probabilities
result
explains
small
probabilities
handled
care
numerical
issues
also
blow
sample
complexity
keywords—renyi
divergence
sampling
complexity
anomaly
detection
introduction
renyi
divergences
anomaly
detection
popular
statistical
approach
detect
anomalies
real-
time
data
compare
empirical
distribution
certain
features
updated
stored
proﬁle
learned
past
observations
computed
off-line
used
refer-
ence
distribution
signiﬁcant
deviations
observed
distri-
bution
assumed
proﬁle
trigger
alarm
gmt05
technique
among
many
applications
often
used
detect
ddos
attacks
network
trafﬁc
gcfjp+15
pky15
quantify
deviation
actual
data
reference
distribution
one
needs
employ
suitable
dissimilarity
metric
context
based
empirical
stud-
ies
renyi
divergences
suggested
good
dissimilarity
measures
lzy09
xlz11
bbk15
gcfjp+15
pky15
divergence
evaluated
based
theoretical
models1
much
important
especially
real-time
de-
tection
estimation
basis
samples
related
literature
focused
mainly
tunning
performance
speciﬁc
implementations
choosing
appropriate
parameters
suitable
deﬁnition
sampling
frequency
based
empirical
evidence
hand
much
supported
european
research
council
consolidator
grant
682815-
tocnet
example
one
uses
fractional
brownian
motions
simulate
real
network
trafﬁc
poisson
distributions
model
ddos
trafﬁc
xlz11
algorithm
estimation
renyi
divergence
refer-
ence
distribution
known
a-priori
input
divergence
parameter
alphabet
reference
distribution
samples
unknown
output
number
approximating
α-divergence
initialize2ni
end
let
compute
empirical
frequencies
estimation
stands
falling
α-power
divergences
power
sums
bias-corrected
power
sum
←pi
q1−α
α−1
log
return
known
theoretical
performance
estimating
renyi
divergences
general
discrete
distributions
continu-
ous
distributions
need
extra
smoothness
assumptions
sp14
limited
case
estimating
renyi
entropy
aost15
corresponds
uniform
reference
distribution
paper
attempt
ﬁll
gap
providing
better
estimators
renyi
divergence
together
theoretical
guarantees
performance
approach
motivated
mentioned
applications
anomaly
detection
assume
reference
distribution
explicitly
known
distribution
observed
i.i.d
samples
contribution
related
works
better
estimators
a-priori
known
reference
dis-
tributions
literature
renyi
divergences
typically
estimated
straightforward
plug-in
estimators
see
lzy09
bbk15
lzy09
xlz11
bbk15
gcfjp+15
pky15
approach
one
puts
empirical
distribution
estimated
samples
divergence
formula
place
true
distribution
unfortunately
worse
statistical
properties
e.g
heavily
biased
affects
number
samples
required
get
reliable
estimate
obtain
reliable
estimates
within
possible
small
number
samples
extend
techniques
aost15
key
idea
use
falling
powers
estimate
power
sums
distribution
trick
fact
bias
correction
method
estimator
illustrated
algorithm
certain
cases
reference
distribution
close
uniform
estimate
divergence
number
samples
sublinear
alphabet
size
whereas
plug-in
estimators
need
superlinear
number
samples
particular
uniform
reference
distribution
recover
upper
bounds
estimating
renyi
entropy
aost15
upper
lower
bounds
sample
complexity
show
sample
complexity
estimating
divergence
unknown
observed
samples
explicitly
known
dependent
reference
distribution
take
small
probabilities
non-trivial
estimation
possible
even
sublinear
alphabet
size
however
takes
arbitrarily
small
values
complexity
dependent
inverse
powers
probability
masses
unbounded
ﬁxed
alphabet
without
extra
assumptions
stress
lower
bounds
no-
results
independent
estimation
technique
quantitative
comparison
see
table
assumption
mini
k−1
maxi
cid:16
k1−
cid:17
assumptions
complexity
cid:17
mini
k−ω
mini
k−o
cid:16
comment
almost
uniform
complexity
sublinear
complexity
least
square
root
negligible
masses
super-polynomial
complexity
non-negligible
mass
polynomial
complexity
reference
corollary
corollary
corollary
corollary
table
brief
summary
results
problem
estimating
renyi
divergence
divergence
parameter
ﬁxed
constant
known
baseline
distribution
distribution
learned
samples
alphabet
size
complexity
number
samples
needed
estimate
divergence
constant
error
success
probability
least
complexity
instability
numerical
instability
results
provide
theoretical
insights
heuristic
patches
renyi
divergence
formula
suggested
applied
literature
since
formula
numerically
unstable
one
probability
masses
becomes
arbitrarily
small
see
deﬁnition
authors
suggested
omit
round
small
probabilities
see
example
lzy09
pky15
accordance
shown
table
sample
complexity
also
unstable
unlike
events
occur
reference
distribution
moreover
case
even
distribution
perfectly
known
therefore
conclude
small
probabilities
subtle
numerical
instability
importantly
sample
complexity
unstable
techniques
upper
bounds
merely
borrow
extend
techniques
aost15
lower
bounds
approach
however
different
ﬁnd
pair
distributions
close
total
variation
yet
much
different
divergences
variational
approach
writing
explicit
optimization
program
result
obtain
lower
bounds
2storing
updating
empirical
frequencies
implemented
efﬁciently
matters
almost
uniform
distributions
sublinear
time
memory
complexity
general
case
accuracy
turn
argument
aost15
even
extended
renyi
divergence
inherit
limitations
make
work
sufﬁciently
small
accuracies
thus
say
lower
bound
technique
comparison
aost15
offers
lower
bounds
valid
regimes
accuracy
parameter
particular
constant
values
used
applied
literature
fact
technique
strictly
improves
known
lower
bounds
estimating
collision
entropy
taking
special
case
uniform
obtain
sample
complexity
esti-
even
constant
accuracy
mating
collision
entropy
results
aost15
guarantees
small
exact
threshold
given
hidden
constants
may
dependent
captured
notation
˜˜ω
organization
section
introduce
necessary
notions
notations
upper
bounds
sample
complexity
discussed
section
iii
lower
bounds
section
conclude
work
section
preliminaries
distribution
alphabet
denote
logarithms
base
deﬁnition
total
variation
total
variation
two
distri-
butions
ﬁnite
alphabet
equals
2pi
|pi
p′i|
recall
deﬁnition
renyi
divergence
refer
reader
eh14
survey
properties
deﬁnition
renyi
divergence
renyi
divergence
order
short
renyi
α-divergence
two
distributions
support
deﬁned
qα−1
logxi
setting
uniform
get
relation
renyi
entropy
remark
renyi
entropy
renyi
divergence
renyi
entropy
order
equals
logxi
−dα
log
|a|
uniform
distribution
deﬁnition
renyi
divergence
estimation
fix
alphabet
size
two
distributions
let
estq
algorithm
receives
independent
samples
input
say
estq
provides
additive
-approximation
renyi
α-divergence
|estq
xi←p
deﬁnition
renyi
divergence
estimation
complexity
sample
complexity
estimating
renyi
divergence
given
probability
error
additive
accuracy
minimal
number
exists
algorithm
satisfying
equation
turns
convenient
work
directly
estimators
renyi
divergence
rather
estima-
tors
weighted
power
sums
deﬁnition
divergence
power
sums
power
sum
cor-
responding
divergence
deﬁned
def
1−α
pkq
=xi
qα−1
following
lemma
shows
estimating
divergences
equation
absolute
relative
error
corresponding
power
sums
equation
relative
error
equivalent
lemma
equivalence
additive
multiplicative
esti-
mations
suppose
number
|δ|
α−1
log
satisﬁes
way
around
|δ|
1−α
satisﬁes
proof
straightforward
consequence
ﬁrst
order
taylor
approximation
appear
full
version
iii
upper
bounds
sample
complexity
state
upper
bounds
sample
complexity
result
similar
formula
aost15
simpliﬁcations
except
fact
statement
additional
weights
coming
possibly
non-uniform
simpliﬁed
theorem
generalizing
aost15
distributions
alphabet
size
number
satisﬁes
α−1
xr=0
cid:18
nα−r
cid:19
cid:16
α+r
q2α−2
α−1
cid:17
ǫδ2
complexity
estimating
renyi
α-divergence
given
proof
deferred
appendix
discuss
corollaries
ﬁrst
corollary
shows
complexity
sublinear
reference
distribution
close
uniform
corollary
sublinear
complexity
almost
uniform
refer-
ence
probabilities
extending
aost15
let
distribu-
tions
alphabet
size
constant
suppose
maxi
k−1
mini
k−1
complexity
estimating
renyi
α-divergence
respect
constant
accuracy
probability
error
α−1
shown
next
corollary
complexity
polynomial
reference
probabilities
negligible
corollary
polynomial
complexity
non-negligible
refer-
ence
probabilities
let
distributions
alphabet
cid:16
cid:17
size
suppose
mini
k−o
let
constant
complexity
estimating
renyi
divergence
respect
constant
accuracy
probability
error
0.3
sense
deﬁnition
α+r
q2α−2
α−1
α−1
α−1
α+r
q2α−2
therefore
need
chose
theorem
conclude
sufﬁcient
condition
proof
corollary
assumptions
pα+r
since
get
cid:17
pα+r
cid:16
nα−r
pα+r
xr=0
cid:18
cid:19
α−1
α−r
cid:19
r−α
discussion
aost15
know
thus
need
ﬁnd
satisﬁes
α−1
0.3.
xr=0
cid:18
inequality
cid:0
cid:19
cid:18
cid:1
follows
0.3
taylor
expansion
positive
real
number
symmetry
binomial
coefﬁcients
need
0.3
α+r
α−1
α−1
taylor
expansion
valid
sufﬁces
α−1
0.3.
ﬁnishes
proof
proof
corollary
corollary
concluded
in-
specting
proof
corollary
bounds
except
factor
replaced
constant
ﬁnal
condition
reduces
cid:16
sample
complexity
lower
bounds
cid:17
α−1
following
theorem
provides
lower
bounds
sample
complexity
distribution
since
statement
somewhat
technical
discuss
corollaries
refer
appendix
proof
theorem
sample
complexity
lower
bounds
let
two
ﬁxed
distributions
0.5
numbers
given
δ−1
q1−α
q1−α
αpi
δipα
q1−α
q1−α
δ−2
satisfying
ﬁxed
estimating
renyi
divergence
sense
deﬁnition
error
probability
constant
accuracy
requires
least
δipi
andpi
pi|δi|
polynomial
alphabet
size
reference
distribution
take
negligible
probability
masses
ex-
plained
numerical
properties
divergence
formula
references
cid:16
max
pc2
cid:17
samples
choosing
appropriate
numbers
theorem
obtain
bounds
different
settings
corollary
lower
bounds
general
case
estimating
renyi
divergence
requires
always
cid:16
cid:17
samples
proof
corollary
theorem
chose
uniform
index
minimizing
k−1
elsewhere
gives
q1−α
constant
dependent
q1−α
bigger
q1−α
q1−α
k−1
choice
corollary
polynomial
complexity
requires
non-negligible
probability
masses
sufﬁciently
large
mini
k−ω
exists
distribution
dependent
estimation
least
proof
corollary
fix
one
alphabet
symbol
ai0
real
positive
numbers
let
put
probability
uniform
elsewhere
also
let
put
probability
uniform
elsewhere
qα−1
cid:26
k−1
α−1
−dα
max
pα−2
qα−1
max
α−1
α−2
choose
satisﬁes
example
α−1
works
obtain
theorem
take
constant
elsewhere
conditions
ensure
k2d
respectively
sufﬁciently
large
minimal
number
samples
note
choice
implies
also
thus
corollary
follows
cid:0
cid:1
conclusion
extended
techniques
recently
used
analyze
complexity
entropy
estimation
problem
estimating
renyi
divergence
showed
general
uniform
bounds
sample
complexity
complexity
eh14
bbk15
aost15
cdgr16
acharya
orlitsky
suresh
tyagi
complexity
estimating
r´enyi
entropy
proceedings
twenty-sixth
annual
acm-siam
symposium
discrete
al-
gorithms
soda
2015
san
diego
usa
january
4-6
2015
2015.
bhuyan
bhattacharyya
kalita
empirical
evaluation
information
metrics
low-rate
high-rate
ddos
attack
detection
pattern
recognition
letters
2015
canonne
diakonikolas
gouleakis
rubinfeld
testing
shape
restrictions
discrete
distributions
33rd
symposium
theoretical
aspects
computer
science
stacs
2016
february
17-20
2016
orl´eans
france
2016.
van
erven
harremo¨es
r´enyi
diver-
gence
kullback-leibler
divergence
ieee
trans
information
theory
60.7
2014
gcfjp+15
gulisano
callau-zori
jim´enez-
peris
papatriantaﬁlou
pati˜no-
mart´ınez
stone
streaming
ddos
de-
fense
framework
expert
syst
appl
42.24
2015
mccallum
towsley
de-
tecting
anomalies
network
trafﬁc
using
maximum
entropy
estimation
proceed-
ings
5th
acm
sigcomm
conference
internet
measurement
imc
berkeley
usa
usenix
association
2005.
zhou
effective
met-
ric
detecting
distributed
denial-of-service
attacks
based
information
divergence
iet
communications
3.12
2009
pukkawanna
kadobayashi
yam-
aguchi
network-based
mimicry
anomaly
de-
tection
using
divergence
measures
inter-
national
symposium
networks
computers
communications
isncc
2015
yasmine
hammamet
tunisia
may
13-15
2015
2015.
singh
p´oczos
generalized
ex-
ponential
concentration
inequality
renyi
divergence
estimation
proceedings
31th
international
conference
machine
learning
icml
2014
beijing
china
21-26
june
2014
2014.
xiang
zhou
low-rate
ddos
attacks
detection
traceback
using
new
information
metrics
ieee
gmt05
pky15
xlz11
lzy09
sp14
trans
information
forensics
security
6.2
2011
appendix
proof
theorem
assume
max
otherwise
estimate
trivial
start
following
lemma
similar
technique
used
aost15
exposition
different
proof
theorem
sketch
follow
essentially
proof
strategy
aost15
difference
corresponding
corresponding
entropy
let
ˆpi
empirical
frequency
i-th
symbol
stream
consider
following
estimator
α−1
estimate
weighted
power
sums
q1−α
divergence
instead
sumspi
est
ˆpi
αq1−α
note
precisely
power
sum
deﬁned
algorithm
lemma
sufﬁces
consider
estimator
multiplicative
error
constant
particular
use
fact
randomize
make
sample
poisson
distribution
mean
transformation
hurt
estimator
convergence
hand
makes
empirical
frequencies
independent
see
aost15
details
poisson
sampling
notations
algorithm
arrive
formula
var
q1−α
=xi
6xi
xr=0
cid:18
α−1
var
cid:20
cid:21
q2α−2
n2α
q2α−2
n2α
cid:19
nα−r
pα+r
q2α−2
next
reduction
observation
sufﬁces
construct
estimator
fails
probability
success
probability
ampliﬁed
median
trick
aost15
general
pretty
standard
literature
simply
present
estimators
constant
error
probability
cdgr16
let
deﬁne
success
event
ailure
δpi
q1−α
chebyszev
inequality
obtain
following
bound
ailure
δ−2
qα−1
δ−2
qα−1
α−1
xr=0
cid:18
xr=0
cid:18
nα−r
cid:19
nα−r
cid:19
α−1
qα−1
α−r
α−r
qα−1
consistent
aost15
uniform
ﬁnishes
proof
lemma
suppose
exists
-estimator
renyi
divergence
deﬁnition
uses
samples
following
true
two
distribu-
tions
1−2ǫ
-close
total
variation
must
satisfy
|dα
proof
lemma
lemma
follows
following
ob-
servation
estimator
fails
probability
distributions
one
build
distinguisher
n-fold
products
p′n
comparing
algo-
rithm
outputs
threshold
distinguisher
works
advantage
total
variation
complete
proof
standard
hybrid
argument
n-fold
products
p′n
away
total
variation
distributions
must
1−2ǫ
away
q1−α
combining
lemma
sufﬁces
prove
close
total
variation
max
√c2
consider
vector
probability
distribution
ﬁrst
order
taylor
approximation
recall
−pi
particular
αxi
min
α−2q1−α
q1−α
δipα−1
assuming
obtain
αxi
δipα−1
q1−α
pα−2
q1−α
changing
variables
δ′ipi
denoting
p′i
piδ′i
|δi|
gives
away
total
variation
c1δ
c2δ2
consider
two
cases
assume
ﬁrst
inequality
c1δ
implies
additive
error
c1δ
estimation
note
scaled
factor
smaller
c1δ
distance
least
cid:16
similarly
scaling
possible
arrive
c2δ2
inequality
c1δ2
yields
additive
error
estimation
distance
cid:17
suppose
cid:16
cid:17
bounds
follow
lemma
must
cid:16
cid:17
cid:17
cid:16
