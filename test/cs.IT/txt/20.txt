introduction
deep
learning
physical
layer
tim
shea
senior
member
ieee
jakob
hoydis
member
ieee
abstract—we
present
discuss
several
novel
applications
deep
learning
physical
layer
interpreting
communications
system
autoencoder
develop
fundamental
new
way
think
communications
system
design
end-to-end
reconstruction
task
seeks
jointly
optimize
transmitter
receiver
components
single
process
show
idea
extended
networks
multiple
transmitters
receivers
present
concept
radio
transformer
networks
rtns
means
incorporate
expert
domain
knowledge
machine
learning
model
lastly
demonstrate
application
convolutional
neural
networks
cnns
raw
samples
modulation
classiﬁcation
achieves
competitive
accuracy
respect
traditional
schemes
relying
expert
features
paper
concluded
discussion
open
challenges
areas
future
investigation
introduction
communications
ﬁeld
rich
expert
knowledge
model
channels
different
types
compensate
various
hardware
imperfections
design
optimal
signaling
detection
schemes
ensure
reliable
transfer
data
complex
mature
engineering
ﬁeld
many
distinct
areas
investigation
seen
diminishing
returns
regards
performance
improvements
particular
physical
layer
high
bar
performance
machine
learning
deep
learning
based
approach
must
pass
order
provide
tangible
new
beneﬁts
domains
computer
vision
natural
language
processing
shines
difﬁcult
characterize
real
world
images
language
rigid
mathematical
models
example
almost
impossible
task
write
robust
algorithm
detection
handwritten
digits
objects
images
almost
trivial
today
implement
algorithms
learn
accomplish
task
beyond
human
levels
accuracy
communications
hand
design
transmit
signals
enable
straightforward
algorithms
symbol
detection
variety
channel
system
models
e.g.
detection
constellation
symbol
additive
white
gaussian
noise
awgn
thus
long
models
sufﬁciently
capture
real
effects
expect
yield
signiﬁcant
improvements
physical
layer
nevertheless
believe
applications
explore
paper
useful
insightful
way
fundamentally
rethinking
communications
system
design
shea
bradley
department
electrical
computer
engineering
virginia
tech
deepsig
arlington
oshea
vt.edu
hoydis
nokia
bell
labs
route
villejust
91620
nozay
france
jakob.hoydis
nokia-bell-labs.com
problem
hold
promise
performance
improvements
complex
communications
scenarios
difﬁcult
describe
tractable
mathematical
models
main
con-
tributions
follows
demonstrate
possible
learn
full
transmitter
receiver
implementations
given
channel
model
optimized
chosen
loss
function
e.g.
minimizing
block
error
rate
bler
interestingly
learned
systems
competitive
respect
current
state-of-the-art
key
idea
represent
transmitter
channel
receiver
one
deep
neural
network
trained
autoencoder
beauty
approach
even
applied
channel
models
loss
functions
optimal
solutions
unknown
extend
concept
adversarial
network
multiple
transmitter-receiver
pairs
competing
capacity
leads
interference
channel
ﬁnding
best
signaling
scheme
long-standing
research
problem
demonstrate
setup
also
represented
multiple
inputs
outputs
transmitter
receiver
implementations
jointly
optimized
respect
common
individual
performance
metric
introduce
radio
transformer
networks
rtns
way
integrate
expert
knowledge
model
rtns
allow
example
carry
predeﬁned
cor-
rection
algorithms
transformers
receiver
e.g.
multiplication
complex-valued
number
convolution
vector
may
fed
parameters
learned
another
integrated
end-to-end
training
process
task
performed
transformed
signal
e.g.
symbol
detection
study
use
nns
complex-valued
samples
problem
modulation
classiﬁcation
show
convolutional
neural
networks
cnns
cornerstone
systems
computer
vision
outperform
traditional
classiﬁcation
techniques
based
expert
features
result
mirrors
relentless
trend
various
domains
learned
features
ulti-
mately
outperform
displace
long-used
expert
features
scale-invariant
feature
transform
sift
bag-of-words
ideas
presented
paper
provide
multitude
interesting
avenues
future
research
discussed
detail
hope
stimulate
wide
interest
within
research
community
rest
article
structured
follows
section
i-a
discusses
potential
beneﬁts
physical
layer
sec-
tion
i-b
presents
related
work
background
deep
learning
presented
section
section
iii
several
applications
communications
presented
section
contains
overview
discussion
open
problems
key
areas
future
investigation
section
concludes
article
ability
algorithms
higher
level
programming
languages
make
efﬁcient
use
inherently
concurrent
nature
computation
memory
access
across
wide
deep
nns
demonstrated
surprising
ability
readily
achieve
high
resource
utilization
architectures
minimal
application
speciﬁc
tuning
optimization
required
historical
context
related
work
potential
physical
layer
apart
intellectual
beauty
fully
learned
communications
system
reasons
could
provide
gains
existing
physical
layer
algorithms
first
signal
processing
algorithms
communications
solid
foundations
statistics
information
theory
often
provably
optimal
tractable
mathematically
mod-
els
generally
linear
stationary
gaussian
statistics
practical
system
however
many
imperfections
non-linearities
e.g.
non-linear
power
ampliﬁers
pas
ﬁnite
resolution
quantization
approximately
captured
models
reason
dl-based
commu-
nications
system
processing
block
require
mathematically
tractable
model
optimized
speciﬁc
hardware
conﬁguration
channel
might
able
better
optimize
imperfections
second
one
guiding
principles
communications
systems
design
split
signal
processing
chain
multiple
independent
blocks
executing
well
deﬁned
isolated
function
e.g.
source/channel
coding
modulation
channel
estimation
equalization
although
approach
led
efﬁcient
versatile
controllable
systems
today
clear
individually
optimized
processing
blocks
achieve
best
possible
end-to-end
performance
example
separation
source
channel
coding
many
practical
channels
short
block
lengths
see
references
therein
well
separate
coding
modulation
known
sub-optimal
attempts
jointly
optimize
components
e.g.
based
factor
graphs
provide
gains
lead
unwieldy
computationally
com-
plex
systems
learned
end-to-end
communications
system
hand
unlikely
rigid
modular
structure
optimized
end-to-end
performance
third
shown
nns
universal
function
approximators
recent
work
shown
remarkable
capacity
algorithmic
learning
recurrent
nns
known
turing-complete
since
execution
nns
highly
parallelized
concurrent
architectures
easily
implemented
low-precision
data
types
evidence
learned
algorithms
taking
form
could
executed
faster
lower
energy
cost
manually
programmed
counterparts
fourth
massively
parallel
processing
architectures
distributed
memory
architectures
graphic
processing
units
gpus
also
increasingly
specialized
chips
inference
e.g.
shown
energy
efﬁcient
capable
impressive
computational
throughput
fully
utilized
concurrent
algorithms
performance
architectures
however
largely
limited
applications
communications
long
history
covering
wide
range
applications
comprise
channel
modeling
prediction
localization
equalization
decoding
quantization
compression
demodulation
modulation
recog-
nition
spectrum
sensing
name
references
therein
however
best
knowledge
due
reasons
mentioned
applications
commonly
adopted
led
wide
commercial
success
also
interesting
essentially
applications
focus
individual
receiver
processing
tasks
alone
consideration
transmitter
full
end-
to-end
system
entirely
missing
literature
advent
open-source
libraries
see
section
ii-b
readily
available
specialized
hardware
along
astonishing
progress
computer
vision
stimulated
renewed
interest
application
communications
networking
currently
essentially
two
different
main
approaches
applying
physical
layer
goal
either
improve/augment
parts
existing
algorithms
completely
replace
consider
among
papers
falling
ﬁrst
category
improved
belief
propagation
channel
decoding
multiple-input
multiple-output
mimo
detection
respectively
works
inspired
idea
deep
unfolding
existing
iterative
algorithms
essentially
interpreting
iteration
set
layers
similar
manner
aims
improving
solution
sparse
linear
inverse
problems
second
category
papers
include
dealing
blind
detection
mimo
systems
low-resolution
quan-
tization
detection
molecular
commu-
nications
mathematical
channel
model
exists
studied
idea
learning
solve
complex
optimization
tasks
wireless
resource
allocation
power
control
investigated
also
demonstrated
initial
results
area
learned
end-to-end
communications
systems
well
considered
problems
modula-
tion
recognition
signal
compression
channel
decoding
state-of-the
art
tools
notations
use
boldface
upper-
lower-case
letters
denote
matrices
column
vectors
respectively
vector
denotes
ith
element
cid:107
cid:107
euclidean
norm
transpose
cid:12
element-wise
product
matrix
xij
denotes
-element
denote
sets
real
complex
numbers
respectively
multivariate
gaussian
complex
gaussian
distributions
mean
vector
covariance
matrix
respectively
bern
bernoulli
distribution
success
probability
gradient
operator
cid:96
cid:96
cid:96
none
none
none
deep
learning
basics
table
list
layer
types
feedforward
multilayer
perceptron
mlp
layers
describes
mapping
rn0
cid:55
rnl
input
vector
rn0
output
vector
rnl
iterative
processing
steps
name
dense
noise
dropout
normalization
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
βin
cid:96
cid:12
cid:96
bern
cid:96
−1r
cid:96
cid:107
cid:96
cid:107
e.g.
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
cid:55
cid:96
mapping
carried
cid:96
layer
mapping
depends
output
vector
cid:96
previous
layer
also
set
parameters
cid:96
moreover
mapping
stochastic
i.e.
cid:96
function
random
variables
use
denote
set
parameters
network
cid:96
layer
called
dense
fully-connected
cid:96
cid:96
cid:96
form
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
activation
function
deﬁned
shortly
set
param-
eters
layer
cid:96
cid:96
cid:96
table
lists
several
layer
types
together
mapping
functions
parameters
used
manuscript
layers
stochastic
mappings
generate
new
random
mapping
time
called
example
noise
layer
simply
adds
gaussian
noise
vector
zero
mean
covariance
matrix
βin
cid:96
input
thus
generates
different
output
input
time
called
activation
function
introduces
non-linearity
important
so-
called
expressive
power
without
non-linearity
would
much
advantage
stacking
multiple
layers
top
generally
activation
function
applied
individually
element
input
vector
i.e.
commonly
used
activation
functions
listed
table
ii.1
nns
generally
trained
using
labeled
training
data
i.e.
set
input-output
vector
pairs
cid:63
desired
output
neural
cid:63
network
used
input
goal
training
process
minimize
loss
cid:88
i=1
cid:63
respect
parameters
rnl
rnl
cid:55
loss
function
output
used
input
several
relevant
loss
functions
provided
table
iii
different
norms
e.g.
parameters
activations
added
loss
function
favor
solutions
small
sparse
values
form
regularization
popular
algorithm
ﬁnd
good
sets
parameters
stochastic
gradient
descent
sgd
starts
random
initial
values
updates
iteratively
θt+1
1the
linear
activation
function
typically
used
output
layer
context
regression
tasks
i.e.
estimation
real-valued
vector
table
list
activation
functions
name
linear
tanh
relu
max
tanh
1+e−ui
euj
sigmoid
softmax
eui
cid:80
range
table
iii
list
loss
functions
categorical
cross-entropy
cid:80
name
mse
cid:107
cid:107
log
learning
rate
approximation
loss
function
computed
random
mini-
batch
training
examples
size
iteration
i.e.
cid:63
cid:88
i∈st
choosing
small
compared
gradient
computation
complexity
signiﬁcantly
reduced
still
reducing
weight
update
variance
note
many
variants
sgd
algorithm
dynamically
adapt
learning
rate
improve
convergence
8.5
gradient
efﬁciently
computed
backpropagation
algorithm
6.5
deﬁnition
training
nns
almost
arbitrary
shape
easily
done
one
many
existing
libraries
presented
section
ii-b
convolutional
layers
convolutional
neural
network
cnn
layers
introduced
provide
efﬁcient
learning
method
images
tying
adjacent
shifts
weights
together
way
similar
ﬁlter
sliding
across
input
vector
convolutional
layers
able
force
learning
features
invariance
shifts
input
vector
also
greatly
reduce
model
complexity
measured
number
free
parameters
layer
weight
matrix
required
represent
equivalent
shift-invariant
features
using
fully
connected
layers
reducing
sgd
optimization
complexity
improving
generalization
appropriate
datasets
general
convolutional
layer
consists
set
ﬁlter
weights
ra×b
called
depth
generate
so-called
feature
map
cid:48
cid:48
input
matrix
rn×m
according
following
2in
image
processing
commonly
three-dimensional
tensor
third
dimension
corresponding
color
channels
ﬁlters
weights
also
three-dimensional
work
input
channels
simultaneously
convolution
a−1
cid:88
b−1
cid:88
a−k
cid:96
x1+s
i−1
−k,1+s
j−1
cid:96
k=0
cid:96
cid:99
cid:48
cid:98
m+b−2
integer
parameter
called
stride
cid:48
cid:99
assumed
cid:98
n+a−2
padded
zeros
i.e.
output
dimensions
reduced
either
increasing
stride
adding
pooling
layer
pooling
layer
partitions
regions
computes
single
output
value
e.g.
maximum
average
value
l2-norm
example
taking
vectorized
grayscale
image
input
consisting
28×
pixels
connecting
dense
layer
number
activations
results
single
weight
matrix
784
784
614
656
free
parameters
hand
use
convolutional
feature
map
containing
six
ﬁlters
sized
pixels
obtain
much
reduced
number
free
parameters
150.
right
kind
dataset
technique
extremely
effective
see
application
convolutional
layers
section
iii-d.
details
cnns
refer
machine
learning
libraries
recent
times
numerous
tools
algorithms
emerged
make
easy
build
train
large
nns
tools
deploy
training
routines
high
level
language
massively
parallel
gpu
architectures
key
enablers
among
caffe
mxnet
tensorflow
theano
torch
name
allow
high
level
algorithm
deﬁnition
various
programming
languages
conﬁguration
ﬁles
automatic
differentiation
training
loss
functions
arbitrarily
large
networks
compilation
network
forwards
backwards
passes
hardware
optimized
concurrent
dense
matrix
algebra
ker-
nels
keras
provides
additional
layer
primitives
theano
tensorflow
back-end
highly
customizable
interface
quickly
experiment
deploy
deep
nns
become
primary
tool
used
generate
numerical
results
manuscript
network
dimensions
training
term
deep
become
common
recent
liter-
ature
referring
number
sequential
layers
within
network
also
generally
methods
commonly
used
train
networks
depth
relates
directly
number
iterative
operations
performed
input
data
sequential
layers
transfer
functions
deep
networks
allow
numerous
iterative
transforms
data
min-
imum
latency
network
would
likely
shallow
possible
width
used
describe
number
output
activations
per
layer
layers
average
relates
directly
memory
required
layer
best
practice
training
methods
varied
years
direct
solution
techniques
gradient
descent
ge-
netic
algorithms
favored
considered
one
time
see
1.2
short
history
layer-
by-layer
pre-training
also
recently
popular
method
scaling
training
larger
networks
backpropagation
struggled
however
systems
today
able
train
networks
wide
deep
directly
using
back-
propagation
sgd
methods
adaptive
learning
rates
e.g.
adam
regularization
methods
prevent
overﬁtting
e.g.
dropout
activations
functions
reduce
gradient
issues
e.g.
relu
iii
examples
machine
learning
applications
physical
layer
section
show
represent
end-to-
end
communications
system
autoencoder
train
via
sgd
idea
extended
multiple
transmitters
receivers
study
example
two-user
interference
channel
introduce
concept
rtns
improve
performance
fading
channels
demonstrate
application
cnns
raw
radio
frequency
time-series
data
task
modulation
classiﬁcation
autoencoders
end-to-end
communications
systems
figure
simple
communications
system
consisting
transmitter
receiver
connected
channel
simplest
form
communications
system
consists
transmitter
channel
receiver
shown
fig
transmitter
wants
communicate
one
possible
messages
...
receiver
making
discrete
uses
channel
end
applies
transformation
cid:55
message
generate
transmitted
signal
rn.3
generally
hardware
transmitter
imposes
certain
constraints
e.g.
energy
constraint
cid:107
cid:107
amplitude
constraint
|xi|
1∀i
average
power
constraint
cid:2
|xi|2
cid:3
1∀i
communication
rate
communications
system
k/n
bit/channel
use
log2
sequel
notation
means
communications
system
sends
one
messages
i.e.
bits
channel
uses
channel
described
conditional
probability
density
function
y|x
denotes
received
signal
upon
reception
receiver
applies
transformation
cid:55
produce
estimate
transmitted
message
point
view
simple
communications
system
seen
particular
type
autoencoder
typically
goal
autoencoder
ﬁnd
low-dimensional
representation
input
intermediate
layer
allows
reconstruction
output
minimal
error
way
autoencoder
learns
3we
focus
real-valued
signals
extensions
complex-valued
signals
discussed
section
alternatively
one
consider
mapping
r2n
interpreted
concatenation
real
imaginary
parts
approach
adopted
sections
iii-b
iii-c
iii-d.
figure
communications
system
awgn
channel
represented
autoencoder
input
encoded
one-hot
vector
output
probability
distribution
possible
messages
likely
picked
output
non-linearly
compress
reconstruct
input
case
purpose
autoencoder
different
seeks
learn
representations
messages
robust
respect
channel
impairments
mapping
i.e.
noise
fading
distortion
etc
transmitted
message
recovered
small
probability
error
words
autoencoders
remove
redundancy
input
data
compression
autoencoder
channel
autoencoder
often
adds
redundancy
learning
intermediate
representation
robust
channel
perturbations
example
autoencoder
shown
fig
transmitter
consists
feedforward
multiple
dense
layers
followed
normalization
layer
ensures
physical
constraints
met
note
input
transmitter
encoded
one-hot
vector
i.e.
m-dimensional
vector
sth
element
equal
one
zero
otherwise
channel
represented
additive
noise
layer
ﬁxed
variance
2reb/n0
eb/n0
denotes
energy
per
bit
noise
power
spectral
density
ratio
receiver
also
implemented
feedforward
last
layer
uses
softmax
activation
whose
output
probability
vector
possible
messages
decoded
message
corresponds
index
element
highest
probability
autoencoder
trained
end-to-end
using
sgd
set
possible
messages
using
well
suited
categorical
cross-entropy
loss
function
p.4
i.e.
cid:54
communications
system
employing
binary
phase-shift
keying
bpsk
modulation
hamming
7,4
code
either
binary
hard-decision
decoding
maximum
likelihood
decoding
mld
bler
achieved
trained
autoencoder
7,4
ﬁxed
energy
constraint
cid:107
cid:107
systems
operate
rate
4/7
com-
parison
also
provide
bler
uncoded
bpsk
4,4
result
shows
autoencoder
learned
without
fig
compares
block
error
rate
bler
memory-efﬁcient
approach
implement
architecture
replacing
one-hot
encoded
input
ﬁrst
dense
layer
embedding
turns
message
indices
vectors
loss
function
replaced
sparse
categorical
cross-entropy
accepts
message
indices
rather
one-hot
vectors
labels
done
experiments
prior
knowledge
encoder
decoder
function
together
achieve
performance
hamming
7,4
code
mld
layout
autoencoder
provided
table
although
single
layer
represent
mapping
message
index
corresponding
transmit
vector
experiments
shown
sgd
converges
better
global
solution
using
two
transmit
layers
instead
one
increased
dimension
parameter
search
space
may
actually
help
reduce
likelihood
convergence
sub-optimal
minima
making
solutions
likely
emerge
saddle
points
optimization
training
done
ﬁxed
value
eb/n0
section
iv-b
using
adam
learning
rate
0.001.
observed
increasing
batch
size
training
helps
improve
accuracy
implementation
details
refer
source
code
fig
shows
similar
comparison
8,8
2,2
communications
system
i.e.
surprisingly
autoencoder
achieves
bler
uncoded
bpsk
2,2
outperforms
latter
8,8
full
range
eb/n0
implies
learned
joint
coding
modulation
scheme
coding
gain
achieved
truly
fair
comparison
result
compared
higher-order
modulation
scheme
using
channel
code
optimal
sphere
packing
eight
dimensions
detailed
performance
comparison
various
channel
types
param-
eters
different
baselines
scope
paper
left
future
investigations
fig
shows
learned
representations
messages
different
values
complex
constellation
points
i.e.
y-axes
correspond
ﬁrst
second
transmitted
symbols
respectively
fig
depict
seven-dimensional
message
representations
using
two-dimensional
t-distributed
stochastic
neighbor
embedding
t-sne
noisy
observations
instead
fig
shows
simple
system
converges
rapidly
classical
quadrature
phase
shift
keying
qpsk
constellation
arbitrary
rotation
similarly
fig
shows
system
leads
rotated
16-psk
constellation
impact
chosen
normalization
becomes
clear
fig
parameters
average
power
normalization
100
10−1
10−2
10−3
10−4
10−5
uncoded
bpsk
4,4
hamming
7,4
hard
decision
autoencoder
7,4
hamming
7,4
mld
eb/n0
100
10−1
10−2
10−3
10−4
10−5
uncoded
bpsk
8,8
autoencoder
8,8
uncoded
bpsk
2,2
autoencoder
2,2
eb/n0
figure
bler
versus
eb/n0
autoencoder
several
baseline
communication
schemes
table
layout
autoencoder
used
figs
trainable
parameters
resulting
791
135,944
parameters
2,2
7,4
8,8
autoencoder
respectively
layer
input
dense
relu
dense
linear
normalization
noise
dense
relu
dense
softmax
output
dimensions
instead
ﬁxed
energy
constraint
forces
symbols
lie
unit
circle
results
interesting
mixed
pentagonal/hexagonal
grid
arrangement
indistinguish-
able
bler
performance
16-qam
constellation
symbol
origin
surrounded
ﬁve
equally
spaced
nearest
neighbors
six
almost
equally
spaced
neighbors
fig
shows
t-sne
embedding
leads
similarly
shaped
arrangement
clusters
examples
section
treat
communication
task
classiﬁcation
problem
representation
m-dimensional
vector
becomes
quickly
impractical
large
circumvent
problem
possible
use
compact
representations
binary
vector
log2
dimensions
case
output
activation
functions
sigmoid
loss
functions
mse
binary
cross-entropy
appropriate
nevertheless
scaling
architecture
large
values
remains
challenging
due
size
training
set
model
recall
important
property
autoencoder
also
learn
communicate
channel
even
information-theoretically
optimal
scheme
known
figure
constellations
produced
autoencoders
using
pa-
rameters
average
power
constraint
2-dimensional
t-sne
embedding
received
symbols
autoencoders
multiple
transmitters
receivers
autoencoder
concept
section
iii-a
readily
extended
multiple
transmitters
receivers
share
common
channel
example
consider
two-user
awgn
interference
channel
shown
fig
transmitter
wants
communicate
message
receiver
transmitter
wants
communicate
message
receiver
2.5
transmitter-receiver
pairs
5extensions
users
possibly
different
rates
i.e.
well
channel
types
straightforward
ts/ae
ts/ae
figure
two-user
interference
channel
seen
combi-
nation
two
interfering
autoencoders
try
reconstruct
respective
messages
implemented
nns
difference
respect
autoencoder
last
section
transmitted
messages
interfere
receivers
resulting
noisy
observations
100
10−1
10−2
10−3
10−4
10−5
βin
gaussian
noise
simplicity
adopted
complex-valued
notation
rather
considering
real-valued
vectors
size
notation
means
messages
transmitted
complex-valued
channel
uses
denote
log
cid:0
ˆs1
cid:1
log
cid:0
ˆs2
cid:1
individual
cross-entropy
loss
functions
ﬁrst
second
transmitter-receiver
pair
respectively
˜l1
˜l2
associated
losses
mini-batch
context
less
clear
one
train
two
coupled
autoencoders
conﬂicting
goals
one
approach
consists
minimizing
weighted
sum
losses
i.e.
˜l1
˜l2
one
would
minimize
˜l1
alone
i.e.
transmitter
would
learn
transmit
constant
signal
independent
receiver
could
simply
subtract
opposite
true
however
giving
equal
weight
losses
i.e.
0.5
necessarily
result
equal
performance
observed
experiments
generally
leads
highly
unfair
suboptimal
solutions
reason
adopted
dynamic
weights
mini-batch
αt+1
˜l1
˜l1
˜l2
0.5.
thus
smaller
˜l1
compared
˜l2
smaller
weight
αt+1
next
mini-batch
many
possibilities
train
system
claim
optimality
approach
however
led
experiments
desired
result
identical
blers
transmitter-receiver
pairs
fig
shows
bler
one
autoencoders
denoted
function
eb/n0
sets
parameters
nn-layout
autoencoders
provided
table
letting
used
average
power
constraint
competitive
eb/n0
figure
bler
versus
eb/n0
two-user
interference
channel
achieved
autoencoder
22k/n-qam
time-sharing
different
parameters
interesting
look
higher-order
modulation
schemes
fig
baseline
comparison
provide
bler
uncoded
22k/n-qam
rate
used
together
time-sharing
transmitters.6
au-
toencoder
time-sharing
identical
blers
former
achieves
substantial
gains
around
0.7
bler
10−3
reasons
similar
explained
section
iii-a
learned
message
representations
shown
fig
transmitters
learned
use
binary
phase
shift
keying
bpsk
-like
constellations
orthogonal
directions
arbitrary
rotation
around
origin
achieves
performance
qpsk
time-sharing
however
learned
constellations
orthogonal
anymore
interpreted
form
super-position
coding
ﬁrst
symbol
transmitter
uses
high
power
transmitter
low
power
second
symbol
roles
changed
constellations
difﬁcult
interpret
see
constellations
transmitters
resemble
ellipses
orthogonal
major
axes
varying
focal
distances
effect
visible
increased
number
constellation
points
in-depth
study
learned
constellations
impacted
chosen
normalization
weight
initializations
scope
paper
interesting
topic
future
investigations
would
like
point
one
easily
consider
types
multi-transmitter/receiver
communications
systems
approach
comprise
general
multiple
access
channel
mac
broadcast
channel
well
6for
transmitter
sends
4-qam
i.e.
qpsk
symbol
every
channel
use
16-qam
used
instead
cid:88
bler
example
describes
rtn
receiver-side
processing
similarly
used
wherever
parametric
transformations
seeded
estimated
parameters
needed
rtns
form
learned
feed-forward
attention
inspired
spatial
transformer
networks
stns
worked
well
computer
vision
problems
basic
functioning
rtn
best
understood
simple
example
problem
phase
offset
estimation
compensation
let
ejϕ
˜yc
vector
samples
undergone
phase
rotation
phase
offset
let
cid:60
cid:61
r2n
goal
estimate
scalar
close
phase
offset
used
parametric
transform
compute
¯yc
e−j
ˆϕyc
canonicalized
signal
cid:60
¯yc
cid:61
¯yc
thus
given
cid:20
cos
cid:60
¯yc
sin
cid:61
¯yc
cid:21
cos
cid:61
¯yc
sin
cid:60
¯yc
fed
discriminative
network
processing
classiﬁcation
compelling
example
demonstrating
advantages
rtns
shown
fig
compares
bler
autoencoder
8,4
without
rtn
multipath
fading
channel
channel
taps
received
signal
cid:60
cid:61
r2n
given
cid:96
cid:96
cid:96
l−1il
i.i.d
rayleigh
fading
channel
taps
reb/n0
−1in
receiver
noise
transmitted
signal
assume
goal
parameter
estimator
predict
complex-valued
vector
represented
real
values
used
transformation
layer
compute
complex
convolution
thus
rtn
tries
equalize
channel
output
inverse
ﬁltering
order
simplify
task
discriminative
network
implemented
estimator
two
dense
layers
tanh
activations
followed
dense
output
layer
linear
activations
plain
autoencoder
struggles
meet
perfor-
mance
differential
bpsk
dbpsk
maximum
likeli-
hood
sequence
estimation
mle
hamming
7,4
code
autoencoder
rtn
outperforms
another
advantage
rtns
faster
training
convergence
seen
fig
compares
validation
loss
autoencoder
without
rtn
function
training
epochs
observed
experiments
autoencoder
rtn
consistently
outperforms
plain
autoencoder
independently
chosen
hyper-parameters
however
performance
differences
diminish
encoder
de-
coder
networks
made
wider
trained
iterations
although
theoretically
nothing
rtn-augmented
plain
rtn
helps
incorporating
domain
knowledge
simplify
target
manifold
similar
7we
assume
complex-valued
channel
uses
transmitter
receiver
real-valued
inputs
outputs
figure
learned
constellations
two-user
interference
channel
parameters
constellation
points
transmitter
represented
red
dots
black
crosses
respectively
systems
jammers
eavesdroppers
soon
transmitters
receivers
non-cooperative
adversarial
training
strategies
could
adopted
see
radio
transformer
networks
augmented
signal
process-
ing
algorithms
many
physical
phenomena
undergone
communi-
cations
channel
transceiver
hardware
inverted
us-
ing
compact
parametric
models/transformations
widely
used
transformations
include
re-sampling
estimated
symbol/clock
timing
mixing
estimated
carrier
tone
convolving
inverse
channel
impulse
response
estimation
processes
parameters
seed
transformations
e.g.
frequency
offset
symbol
timing
impulse
response
often
involved
specialized
based
signal
speciﬁc
proper-
ties
and/or
information
pilot
tones
see
e.g.
one
way
augmenting
models
expert
propagation
domain
knowledge
signal
speciﬁc
assumptions
use
rtn
shown
fig
rtn
consists
three
parts
learned
parameter
estimator
cid:55
computes
parameter
vector
input
parametric
transform
rn×rp
cid:55
cid:48
applies
deterministic
differentiable
function
parametrized
suited
propagation
phenomena
iii
learned
discriminative
network
cid:48
cid:55
produces
estimate
transmitted
message
label
information
canonicalized
input
cid:48
allowing
parameter
estimator
take
form
train
system
end-to-end
optimize
given
loss
function
importantly
training
process
rtn
seek
directly
improve
parameter
estimation
rather
optimizes
way
parameters
estimated
obtain
best
end-to-end
performance
e.g.
figure
radio
receiver
represented
rtn
input
ﬁrst
runs
parameter
estimation
network
known
transform
applied
generate
canonicalized
signal
classiﬁed
discriminative
network
produce
output
100
10−1
10−2
10−3
10−4
0.8
0.6
0.4
0.2
autoencoder
autoencoder
rtn
100
training
epoch
autoencoder
8,4
dbpsk
8,7
mle
hamming
7,4
autoencoder
8,4
rtn
eb/n0
figure
bler
versus
eb/n0
various
communication
schemes
channel
rayleigh
fading
taps
figure
autoencoder
training
loss
without
rtn
role
convolutional
layers
imparting
translation
invariance
appropriate
leads
simpler
search
space
improved
generalization
autoencoder
rtn
presented
easily
extended
operate
directly
samples
rather
symbols
effectively
deal
problems
pulse
shaping
timing-
frequency-
phase-offset
compensation
exciting
promising
area
research
leave
future
investigations
interesting
applications
approach
could
also
arise
optical
communications
dealing
highly
non-linear
channel
impairments
notoriously
difﬁcult
model
compensate
cnns
classiﬁcation
tasks
many
signal
processing
functions
within
physical
layer
learned
either
regression
classiﬁcation
tasks
look
well-known
problem
modulation
classiﬁca-
tion
single
carrier
modulation
schemes
based
sampled
radio
frequency
time-series
data
i.e.
samples
task
accomplished
years
approach
expert
feature
engineering
either
analytic
decision
trees
single
trees
widely
used
practice
trained
discrimination
methods
operating
compact
feature
space
support
vector
machines
random
forests
small
feedforward
nns
recent
methods
take
step
beyond
using
pattern
recognition
expert
feature
maps
spectral
coherence
function
α-proﬁle
combined
nn-based
classiﬁcation
however
approaches
point
sought
use
feature
learning
raw
time-series
data
radio
domain
however
norm
computer
vision
motivates
approach
widely
done
image
classiﬁcation
leverage
se-
ries
narrowing
convolutional
layers
followed
dense/fully-
connected
layers
terminated
dense
softmax
layer
classiﬁer
similar
vgg
architecture
layout
provided
table
refer
source
code
table
layout
cnn
modulation
classiﬁcation
324,330
trainable
parameters
layer
input
convolution
128
ﬁlters
size
relu
max
pooling
size
strides
convolution
ﬁlters
size
relu
max
pooling
size
strides
flatten
dense
relu
dense
relu
dense
relu
dense
softmax
output
dimensions
128
128
121
128
1408
128
implementation
details
dataset8
benchmark
consists
1.2m
sequences
128
complex-valued
basedband
samples
corresponding
ten
different
digital
analog
single-carrier
modulation
schemes
psk
qam
etc
gone
wireless
channel
harsh
realistic
effects
including
multipath
fading
sample
rate
center
frequency
offset
samples
taken
different
signal-to-noise
ratios
snrs
within
range
−20
fig
compare
classiﬁcation
accuracy
cnn
extreme
gradient
boosting9
1000
estimators
well
single
scikit-learn
tree
operating
mix
analog
cumulant
expert
features
pro-
posed
short-time
nature
examples
places
task
difﬁcult
end
modulation
classiﬁcation
spectrum
since
compute
expert
features
high
stability
long
periods
time
see
cnn
outperforms
boosted
feature-based
classiﬁer
around
low
medium
snr
range
performance
high
snr
almost
identical
performance
single
tree
case
worse
cnn
medium
snr
3.5
worse
high
snr
fig
shows
confusion
matrix
cnn
snr
revealing
confusing
cases
cnn
qam16
qam64
analog
modulations
wideband
wbfm
double-sideband
am-dsb
even
high
snr
confusion
arises
times
underlying
voice
signal
idle
cary
much
information
distinction
qam16
qam64
hard
short-time
observation
symbols
share
constellation
points
accuracy
feature-based
classiﬁer
saturates
high
snr
reasons
authors
report
successful
application
similar
cnn
detection
black
hole
mergers
astrophysics
noisy
time-series
data
discussion
open
research
challenges
data
sets
challenges
order
compare
performance
models
algorithms
crucial
common
benchmarks
open
datasets
rule
computer
vision
8rml2016.10b—https
//radioml.com/datasets/radioml-2016-10-dataset/
9at
time
writing
document
xgb
http
//xgboost.readthedocs
io/
together
cnns
model
consistently
competions
data-science
platform
kaggle
https
//www.kaggle.com/
0.8
0.6
0.4
0.2
cnn
boosted
tree
single
tree
random
guessing
−20
−10
snr
figure
classiﬁer
performance
comparison
versus
snr
figure
confusion
matrix
cnn
snr
voice
recognition
natural
language
processing
domains
e.g.
mnist10
imagenet11
nothing
comparable
exists
communications
domain
somewhat
different
deals
inherently
man-made
signals
accurately
generated
synthetically
allowing
possibility
standard-
izing
data
generation
routines
rather
data
cases
would
also
desirable
establish
set
common
problems
corresponding
datasets
data-generating
software
researchers
benchmark
compare
algorithms
one
example
task
modulation
clas-
siﬁcation
section
iii-d
others
could
include
mapping
impaired
received
samples
symbols
codewords
bits
even
autoencoder
competitions
could
held
standardized
set
benchmark
impairments
taking
form
canonical
impairment
layers
would
need
made
available
major
libraries
see
section
ii-b
10http
//yann.lecun.com/exdb/mnist/
11http
//www.image-net.org/
8pskam-dsbbpskcpfskgfskpam4qam16qam64qpskwbfmprediction8pskam-dsbbpskcpfskgfskpam4qam16qam64qpskwbfmground
truth0.00.20.40.60.81.0
data
representation
loss
functions
training
snr
communications
new
ﬁeld
little
known
optimal
data
representations
loss-functions
training
strategies
example
binary
signals
represented
binary
one-hot
vectors
modulated
complex
symbols
integers
optimal
representation
might
depend
among
factors
architecture
learning
objective
loss
function
decoding
problems
instance
one
would
choice
plain
channel
observations
clipped
log-likelihood
ratios
general
seems
representation
suited
solve
particular
task
via
similarly
obvious
snr
processing
blocks
trained
clearly
desirable
learned
system
operates
snr
regardless
snr
snr-range
trained
however
observed
generally
case
training
low
snr
instance
allow
discovery
structure
important
higher
snr
scenarios
training
across
wide
ranges
snr
also
severely
effect
training
time
authors
observed
starting
training
high
snr
gradually
lowering
epoch
led
signiﬁcant
performance
improvements
application
related
question
optimal
choice
loss
function
sections
iii-a
iii-c
treated
communications
classiﬁcation
problem
categorical
cross-
entropy
common
choice
however
alternate
output
data
representations
choice
less
obvious
applying
inappropriate
loss
function
lead
poor
results
choosing
right
architecture
training
parameters
sgd
mini-batch
size
learning
rate
also
important
practical
questions
satisfying
hard
rules
exist
guidelines
found
methods
select
hyper-parameters
currently
active
area
research
investigation
world
examples
include
architecture
search
guided
hyper-gradients
differential
hyper-parameters
well
genetic
algorithm
particle
swarm
style
optimization
complex-valued
neural
networks
owing
widely
used
complex
baseband
representation
typically
deal
complex
numbers
communications
related
signal
processing
algorithms
rely
phase
ro-
tations
complex
conjugates
absolute
values
etc
reason
would
desirable
nns
operate
complex
rather
real
numbers
however
none
previously
described
libraries
see
section
ii-b
currently
support
due
several
reasons
first
possible
represent
mathematical
operations
complex
domain
purely
real-valued
twice
size
i.e.
complex
number
simply
represented
two
real
values
example
scalar
complex
input
output
connected
single
complex
weight
i.e.
represented
real-valued
vectors
contain
real
imaginary
parts
dimension
r2×2
weight
matrix
note
real-valued
version
four
parameters
complex-valued
version
two
second
complication
arises
complex-valued
nns
since
traditional
loss
activation
functions
generally
holomorphic
gradients
deﬁned
solution
problem
wirtinger
calculus
although
complex-
valued
nns
might
easier
train
consume
less
memory
currently
believe
provide
signiﬁcant
advantage
terms
expressive
power
nevertheless
keep
interesting
topic
future
research
ml-augmented
signal
processing
biggest
challenge
learned
end-to-end
communica-
tions
systems
scalability
large
message
sets
already
100
bits
i.e.
2100
possible
messages
training
complexity
prohibitive
since
autoencoder
must
see
least
every
message
also
naive
neural
channel
decoders
studied
suffer
curse
dimensionality
since
need
trained
possible
codewords
thus
rather
switching
immediately
learned
end-to-end
communications
systems
fully
replacing
certain
algorithms
nns
one
gradual
approach
might
augmenting
speciﬁc
sub-tasks
deep
unfolding
interesting
approach
context
existing
iterative
algorithms
outlined
approach
offers
potential
leverage
additional
side
information
training
data
improve
existing
signal
processing
al-
gorithm
recently
applied
context
channel
decoding
mimo
detection
instance
shown
training
single
codeword
sufﬁcient
since
structure
code
embedded
tanner
graph
concept
rtns
presented
section
iii-c
another
way
incorporating
side
information
existing
models
along
information
derived
rich
dataset
algorithm
improve
performance
reducing
model
training
complexity
system
identiﬁcation
end-to-end
learning
sections
iii-a
iii-c
tacitly
assumed
transfer
function
channel
known
back-
propagation
algorithm
compute
gradient
example
rayleigh
fading
channel
autoencoder
needs
know
training
phase
exact
realization
channel
coefﬁcients
compute
slight
change
transmitted
signal
impacts
received
signal
easily
possible
simulated
systems
poses
major
challenge
end-to-end
learning
real
channels
hardware
essence
hardware
channel
together
form
black-box
whose
input
output
observed
exact
analytic
expression
known
priori
constructing
model
black
box
data
called
system
identiﬁcation
widely
used
control
theory
transfer
learning
one
appealing
candidate
adapting
end-to-end
communications
system
trained
statistical
model
real-
world
implementation
worked
well
domains
e.g.
computer
vision
important
related
question
one
learn
general
model
wide
range
communication
scenarios
tasks
would
avoid
retraining
scratch
every
individual
setting
learning
csi
beyond
accurate
channel
state
information
csi
fundamental
requirement
multi-user
mimo
communications
reason
current
cellular
communication
systems
invest
signif-
icant
resources
energy
time
acquisition
csi
base
station
user
equipment
information
generally
used
anything
apart
precoding/detection
tasks
directly
related
processing
current
data
frame
storing
analyzing
large
amounts
csi
radio
data
—possibly
enriched
location
information—
poses
signiﬁcant
potential
revealing
novel
big-data-driven
physical-layer
understanding
algorithms
beyond
immidiate
ra-
dio
environment
needs
new
applications
beyond
tradi-
tional
scope
communications
tracking
iden-
tiﬁcation
humans
walls
well
gesture
emotion
recognition
could
achieved
using
radio
signals
conclusion
discussed
several
promising
new
applications
physical
layer
importantly
introduced
new
way
thinking
communications
end-to-end
reconstruction
optimization
task
using
autoencoders
jointly
learn
transmitter
receiver
implementations
well
signal
encodings
without
prior
knowledge
comparisons
traditional
baselines
various
scenarios
reveal
extremely
competitive
bler
performance
although
scalability
long
block
lengths
remains
challenge
apart
potential
performance
improvements
terms
reliability
latency
approach
provide
interesting
insight
opti-
mal
communication
schemes
e.g.
constellations
scenarios
optimal
schemes
unknown
e.g.
interference
channel
believe
beginning
wide
range
studies
communications
excited
possibilities
could
lend
towards
future
wireless
communications
systems
ﬁeld
matures
great
number
open
problems
solve
practical
gains
identiﬁed
important
key
areas
future
investigation
highlighted
need
benchmark
problems
data
sets
used
compare
performance
different
models
algorithms
references
rappaport
wireless
communications
principles
practice
2nd
prentice
hall
2002
gagliardi
karp
optical
communications
2nd
wiley
1995
meyr
moeneclaey
fechtel
digital
communication
receivers
synchronization
channel
estimation
signal
processing
john
wiley
sons
inc.
1998
schenk
imperfections
high-rate
wireless
systems
impact
digital
compensation
springer
science
business
media
2008
proakis
salehi
digital
communications
5th
mcgraw-
hill
education
2007
lecun
al.
generalization
network
design
strategies
con-
nectionism
perspective
143–155
1989
zhang
ren
sun
delving
deep
rectiﬁers
surpassing
human-level
performance
imagenet
classiﬁcation
proc
ieee
int
conf
computer
vision
2015
1026–1034
lowe
object
recognition
local
scale-invariant
features
proc
ieee
int
conf
computer
vision
1999
1150–1157
harris
distributional
structure
word
vol
2-3
146–
162
1954
goldsmith
joint
source/channel
coding
wireless
channels
proc
ieee
vehicular
technol
conf.
vol
1995
614–618
zehavi
8-psk
trellis
codes
rayleigh
channel
ieee
trans
commun.
vol
873–884
1992
wymeersch
iterative
receiver
design
cambridge
university
press
2007
vol
234
hornik
stinchcombe
white
multilayer
feedforward
networks
universal
approximators
neural
networks
vol
359–366
1989
reed
freitas
neural
programmer-interpreters
arxiv
preprint
arxiv:1511.06279
2015
siegelmann
sontag
computational
power
neural
nets
proc
5th
annu
workshop
computational
learning
theory
acm
1992
440–449
vanhoucke
senior
mao
improving
speed
neural
networks
cpus
proc
deep
learning
unsupervised
feature
learning
nips
workshop
2011
y.-h.
chen
krishna
emer
sze
eyeriss
energy-
efﬁcient
reconﬁgurable
accelerator
deep
convolutional
neural
net-
works
ieee
solid-state
circuits
vol
127–138
2017
raina
madhavan
large-scale
deep
unsupervised
learning
using
graphics
processors
proc
int
conf
mach
learn
icml
acm
2009
873–880
digital
communications–a
survey
elsevier
signal
processing
vol
1185–1215
2000
applications
ibnkahla
networks
neural
bkassiny
jayaweera
survey
machine-learning
techniques
cognitive
radios
ieee
commun
surveys
tuts.
vol
1136–1159
2013
qadir
k.-l.
yau
imran
vasilakos
ieee
access
special
section
editorial
artiﬁcial
intelligence
enabled
networking
ieee
access
vol
3079–3082
2015
nachmani
ery
burshtein
learning
decode
linear
codes
using
deep
learning
proc
ieee
annu
allerton
conf
com-
mun.
control
computing
allerton
2016
341–346
nachmani
marciano
burshtein
ery
rnn
decoding
linear
block
codes
arxiv
preprint
arxiv:1702.07560
2017
samuel
diskin
wiesel
deep
mimo
detection
arxiv
preprint
arxiv:1706.01151
2017
hershey
roux
weninger
deep
unfolding
model-based
inspiration
novel
deep
architectures
arxiv
preprint
arxiv:1409.2574
2014
borgerding
schniter
onsager-corrected
deep
learning
sparse
linear
inverse
problems
arxiv
preprint
arxiv:1607.05966
2016
y.-s.
jeon
s.-n.
hong
lee
blind
detection
mimo
systems
low-resolution
adcs
using
supervised
learning
arxiv
preprint
arxiv:1610.07693
2016
farsad
goldsmith
detection
algorithms
communication
systems
using
deep
learning
arxiv
preprint
arxiv:1705.08044
2017
sun
chen
shi
hong
sidiropoulos
learning
optimize
training
deep
neural
networks
wireless
resource
management
arxiv
preprint
arxiv:1705.09412
2017
shea
karra
clancy
learning
communicate
channel
auto-encoders
domain
speciﬁc
regularizers
attention
proc
ieee
int
symp
signal
process
inf
technol
isspit
2016
223–228
shea
corgan
clancy
convolutional
radio
mod-
ulation
recognition
networks
proc
int
conf
eng
applications
neural
networks
springer
2016
213–226
shea
corgan
clancy
unsupervised
representation
learning
structured
radio
communication
signals
proc
ieee
int
workshop
sensing
processing
learning
intelligent
machines
spline
2016
1–5
gruber
cammerer
hoydis
ten
brink
deep
learning-
based
channel
decoding
proc
ieee
51st
annu
conf
inf
sciences
syst
ciss
2017
1–6
cammerer
gruber
hoydis
brink
scaling
deep
learning-based
decoding
polar
codes
via
partitioning
arxiv
preprint
arxiv:1702.06901
2017
goodfellow
bengio
courville
deep
learning
mit
press
2016
srivastava
hinton
krizhevsky
sutskever
salakhutdinov
dropout
simple
way
prevent
neural
networks
overﬁtting.
mach
learn
res.
vol
1929–1958
2014
nair
hinton
rectiﬁed
linear
units
improve
restricted
boltzmann
machines
proc
int
conf
mach
learn
icml
2010
807–814
jia
shelhamer
donahue
karayev
long
girshick
guadarrama
darrell
caffe
convolutional
architecture
fast
feature
embedding
arxiv
preprint
arxiv:1408.5093
2014
chen
lin
wang
wang
xiao
zhang
zhang
mxnet
ﬂexible
efﬁcient
machine
learning
library
heterogeneous
distributed
systems
arxiv
preprint
arxiv:1512.01274
2015
abadi
al.
tensorflow
large-scale
machine
learning
heterogeneous
systems
2015
software
available
tensorﬂow.org
online
available
http
//tensorﬂow.org/
al-rfou
alain
almahairi
al.
theano
python
frame-
work
fast
computation
mathematical
expressions
arxiv
preprint
arxiv:1605.02688
2016
collobert
kavukcuoglu
farabet
torch7
matlab-like
environment
machine
learning
biglearn
nips
workshop
2011
chollet
keras
https
//github.com/fchollet/keras
2015
shea
hoydis
source
code
https
//github.com/
-available-after-review
2017
hinton
osindero
y.-w.
teh
fast
learning
algorithm
deep
belief
nets
neural
computation
vol
1527–1554
2006
kingma
adam
method
stochastic
optimization
arxiv
preprint
arxiv:1412.6980
2014
dauphin
pascanu
gulcehre
cho
ganguli
bengio
identifying
attacking
saddle
point
problem
high-dimensional
non-convex
optimization
advances
neural
information
processing
systems
nips
2014
2933–2941
maaten
hinton
visualizing
data
using
t-sne
mach
learn
res.
vol
nov
2579–2605
2008
goodfellow
pouget-abadie
mirza
warde-farley
ozair
courville
bengio
generative
adversarial
nets
advances
neural
information
processing
systems
nips
2014
2672–2680
abadi
andersen
learning
protect
communications
adversarial
neural
cryptography
arxiv
preprint
arxiv:1610.06918
2016
jaderberg
simonyan
zisserman
al.
spatial
transformer
networks
advances
neural
information
processing
systems
nips
2015
2017–2025
estaran
al.
artiﬁcial
neural
networks
linear
non-linear
impairment
mitigation
high-baudrate
im/dd
systems
proc
42nd
european
conf
optical
commun
ecoc
vde
2016
1–3
nandi
azzouz
algorithms
automatic
modulation
recognition
communication
signals
ieee
trans
commun.
vol
431–436
1998
fehske
gaeddert
reed
new
approach
signal
clas-
siﬁcation
using
spectral
correlation
neural
networks
ieee
int
symp
new
frontiers
dynamic
spectrum
access
networks
dyspan
2005
144–150
simonyan
zisserman
deep
convolutional
networks
large-scale
image
recognition
arxiv
preprint
arxiv:1409.1556
2014
pedregosa
al.
scikit-learn
machine
learning
python
mach
learn
res.
vol
2825–2830
2011
abdelmutalab
assaleh
el-tarhuni
automatic
mod-
ulation
classiﬁcation
based
high
order
cumulants
hierarchical
polynomial
classiﬁers
physical
communication
vol
10–18
2016
george
huerta
deep
neural
networks
enable
real-time
multimessenger
astrophysics
arxiv
preprint
arxiv:1701.00008
2016
maclaurin
duvenaud
adams
gradient-based
hyper-
parameter
optimization
reversible
learning
proc
32nd
int
conf
mach
learn
icml
2015
bergstra
bengio
random
search
hyper-parameter
opti-
mization
mach
learn
res.
vol
281–305
feb.
2012
hirose
complex-valued
neural
networks
springer
science
business
media
2006
amin
amin
al-nuaimi
murase
wirtinger
calculus
based
gradient
descent
levenberg-marquardt
learning
al-
gorithms
complex-valued
neural
networks
int
conf
neural
information
processing
springer
2011
550–559
goodwin
payne
dynamic
system
identiﬁcation
exper-
iment
design
data
analysis
academic
press
1977
pan
yang
survey
transfer
learning
ieee
trans
knowl
data
eng.
vol
1345–1359
2010
adib
c.-y
hsu
mao
katabi
durand
capturing
human
ﬁgure
wall
acm
trans
graphics
tog
vol
219
2015
zhao
adib
katabi
emotion
recognition
using
wireless
signals
proc
acm
annu
int
conf
mobile
computing
net-
working
2016
95–108
