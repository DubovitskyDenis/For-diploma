information
dimension
stochastic
processes
bernhard
geiger
senior
member
ieee
tobias
koch
senior
member
ieee
abstract—in
1959
r´enyi
proposed
information
dimension
d-dimensional
entropy
measure
information
content
general
random
variables
paper
proposes
generalization
information
dimension
stochastic
processes
deﬁning
information
dimension
rate
entropy
rate
uniformly-quantized
stochastic
process
divided
minus
logarithm
quantizer
step
size
1/m
limit
demonstrated
information
dimension
rate
coincides
rate-distortion
dimension
deﬁned
twice
rate-distortion
function
stochastic
process
divided
log
limit
shown
among
multivariate
stationary
processes
given
matrix-valued
spectral
distribution
function
sdf
gaussian
process
largest
information
dimension
rate
information
dimension
rate
multivariate
stationary
gaussian
processes
given
average
rank
derivative
sdf
presented
results
reveal
fundamental
limits
almost
zero-distortion
recovery
via
compressible
signal
pursuit
almost
lossless
analog
compression
different
general
index
terms—entropy
gaussian
process
information
dimen-
sion
rate-distortion
dimension
introduction
1959
r´enyi
proposed
information
dimension
d-dimensional
entropy
measure
information
content
general
random
variables
rvs
idea
quantize
uniform
quantizer
step
size
1/m
analyze
entropy
quantized
work
bernhard
geiger
partly
funded
erwin
schr¨odinger
fellowship
3765
austrian
science
fund
german
ministry
education
research
framework
alexander
von
humboldt
professorship
know-center
funded
within
austrian
comet
program
competence
centers
excellent
technologies
auspices
austrian
federal
ministry
transport
innovation
technology
austrian
federal
ministry
digital
economic
affairs
state
styria
comet
managed
austrian
research
promotion
agency
ffg
work
tobias
koch
received
funding
european
research
council
erc
european
union
horizon
2020
research
innovation
programme
grant
agreement
number
714161
7th
european
union
framework
programme
grant
333680
ministerio
econom´ıa
competitividad
spain
grants
tec2013-41718-r
ryc-2014-16332
tec2016-78434-c3-
3-r
aei/feder
comunidad
madrid
grant
s2103/ice-2845
work
presented
part
2017
ieee
international
symposium
information
theory
aachen
germany
june
2017
2018
international
zurich
seminar
information
communication
zurich
switzerland
february
2018.
bernhard
geiger
know-center
gmbh
8010
graz
austria
e-mail
geiger
ieee.org
tobias
koch
signal
theory
communications
department
universidad
carlos
iii
madrid
28911
legan´es
spain
also
gregorio
mara˜n´on
health
research
institute
28007
madrid
spain
e-mail
koch
tsc.uc3m.es
copyright
2019
ieee
personal
use
material
permitted
however
permission
use
material
purposes
must
obtained
ieee
sending
request
pubs-permissions
ieee.org
limit
tends
inﬁnity
assuming
entropy
exists
asymptotic
expansion
log
holds
refers
remainder
terms
vanish
r´enyi
referred
information
dimension
d-dimensional
entropy
recent
years
shown
information
dimension
relevance
various
areas
information
theory
including
rate-distortion
theory
almost
lossless
analog
compression
analysis
interference
channels
example
kawabata
dembo
showed
information
dimension
equal
rate-distortion
dimension
deﬁned
twice
rate-distortion
function
divided
log
limit
koch
demonstrated
rate-distortion
func-
tion
source
inﬁnite
information
dimension
inﬁnite
source
ﬁnite
information
dimension
ﬁnite
differential
entropy
shannon
lower
bound
rate-
distortion
function
asymptotically
tight
verd´u
analyzed
linear
encoding
lipschitz
decoding
discrete-
time
independent
identically
distributed
i.i.d
stochastic
processes
showed
information
dimension
plays
fundamental
role
achievability
converse
results
showed
degrees
freedom
k-user
gaussian
interference
channel
characterized
sum
information
dimensions
stotz
b¨olcskei
generalized
result
vector
interference
channels
jalali
poor
proposed
generalization
information
dimension
stationary
discrete-time
stochastic
processes
deﬁning
information
dimension
stochastic
process
information
dimension
divided
limit
∞.1
showed
ψ∗-mixing
processes
information
dimension
achievable
rate
universal
compressed
sensing
linear
encoding
decoding
via
lagrangian
minimum
entropy
pursuit
rezagah
showed
coincides
certain
conditions
rate-distortion
dimension
dimr
thus
generalizing
result
kawa-
bata
dembo
stochastic
processes
notions
information
dimensions
stochastic
processes
discussed
paper
propose
different
deﬁnition
information
dimension
stationary
discrete-time
stochas-
tic
processes
speciﬁcally
let
denote
stochastic
1more
precisely
jalali
poor
deﬁne
information
dimension
stochastic
process
via
conditional
entropy
uniformly-quantized
process
stationary
processes
deﬁnition
coincides
above-
mentioned
deﬁnition
lemma
process
uniformly
quantized
step
size
1/m
deﬁne
information
dimension
rate
entropy
rate
divided
log
limit
i.i.d
processes
deﬁnition
coincides
jalali
poor
fact
evaluates
r´enyi
in-
formation
dimension
marginal
generally
show
deﬁnitions
equivalent
ψ∗-mixing
processes
nevertheless
stochastic
processes
two
deﬁnitions
disagree
particular
derive
closed-form
expression
information
dimension
rate
stationary
multivariate
gaussian
processes
power
spectral
density
psd
specialized
univariate
case
yields
equal
lebesgue
measure
set
harmonics
−1/2
1/2
positive
gaussian
processes
bandlimited
psd
implies
information
dimension
rate
equal
twice
psd
bandwidth
consistent
intuition
processes
samples
contain
information
example
bandwidth
psd
1/4
expect
half
samples
expressed
linear
combinations
samples
hence
contain
information
contrast
show
information
dimension
positive
set
positive
lebesgue
measure
words
capture
dependence
information
dimension
support
size
emulating
proof
lemma
3.2
show
stochastic
process
information
dimension
rate
coincides
rate-distortion
dimension
dimr
implies
coincides
dimr
stochastic
processes
rest
paper
organized
follows
section
introduce
notation
used
paper
section
iii
present
preliminary
results
r´enyi
information
dimen-
sion
rvs
random
vectors
section
present
deﬁnition
information
dimension
rate
stochastic
process
discuss
connection
rate-distortion
dimension
compute
information
dimension
rate
stationary
gaussian
processes
section
review
information
dimension
proposed
jalali
poor
discuss
relation
section
brieﬂy
discuss
operational
meanings
information
dimension
compressed
sensing
zero-distortion
recovery
section
vii
concludes
paper
discussion
obtained
results
proofs
deferred
appendices
notation
preliminaries
denote
set
real
numbers
set
complex
numbers
set
integers
respectively
denote
set
nonnegative
real
numbers
set
positive
integers
respectively
use
calligraphic
font
denote
sets
denote
complements
set
difference
two
sets
written
real
imaginary
parts
complex
number
denoted
respectively
i.e.
ıim
√−1
complex
conjugate
denoted
xk−1
x∞ℓ
use
uppercase
letters
denote
deterministic
matrices
boldface
lowercase
letters
denote
deterministic
vectors
transpose
vector
matrix
denoted
hermitian
transpose
determinant
rank
matrix
det
rank
respectively
denote
identity
matrix
denote
rvs
uppercase
letters
e.g.
nite
countably
inﬁnite
collection
rvs
abbreviate
xℓ+1
xk−1
random
vectors
denoted
boldface
uppercase
letters
e.g.
univariate
discrete-time
stochastic
processes
denoted
short
l-variate
stochastic
processes
use
notation
replaced
call
component
denote
probability
measure
absolutely
continuous
respect
w.r.t
lebesgue
measure
denote
probability
density
function
pdf
denote
gaussian
mean
variance
denote
corresponding
gaussian
density
process
deﬁne
quantization
real-valued
precision
⌊mx⌋
⌊a⌋
largest
integer
less
equal
likewise
⌈a⌉
denotes
smallest
integer
greater
equal
denote
component-wise
quantization
similarly
ﬁnite
countably
inﬁnite
collections
rvs
random
vectors
complex
rvs
real
part
imaginary
part
quantization
equal
deﬁne
···
k-dimensional
hypercube
bottom-left
corner
sidelength
example
let
·k·
denote
entropy
differential
entropy
relative
entropy
respectively
let
de-
note
mutual
information
take
logarithms
base
2.718
mutual
informations
entropies
dimen-
sion
nats
entropy
rate
discrete-valued
stationary
l-variate
process
sec
4.2
1/m
lim
k→∞
note
stationarity
guarantees
limit
exists
equal
4.2.1
lim
k→∞
lim
k→∞
x1|x0
say
stationary
process
ψ∗-mixing
lim
k→∞
sup
px0
px0
px∞
2if
empty
set
supremum
f∞k
f∞k
satisfying
px0
σ-ﬁelds
generated
x∞k
respectively
ψ∗-mixing
property
implies
information
regular
px∞
i.e.
111-112
lim
k→∞
x∞k
iii
´enyi
information
dimension
r´enyi
information
dimension
collection
rvs
deﬁned
lim
m→∞
log
limit
exists
limit
exist
say
information
dimension
exist
case
one
may
replace
limit
either
limit
superior
limit
inferior
denoted
lim
lim
respectively
lim
m→∞
lim
m→∞
log
log
upper
lower
information
respectively
clearly
call
dimension
limit
exists
shall
follow
notation
throughout
document
speciﬁcally
reporting
results
connection
limits
overline
indicates
quantity
brackets
computed
using
limit
superior
underline
indicates
computed
using
limit
inferior
overline
underline
indicates
result
holds
irrespective
whether
limit
superior
limit
inferior
taken
write
lines
limit
exists
deﬁnition
two
rvs
joint
probabil-
ity
measure
conditional
information
dimension
deﬁned
x|w
lim
m→∞
m|w
log
provided
limit
exists
limit
exist
deﬁne
upper
lower
conditional
information
dimension
x|w
x|w
replacing
limit
limit
superior
limit
inferior
respectively
properties
information
dimension
information
dimension
collection
bounded
number
rvs
collection
given
integer
part
collection
ﬁnite
entropy
lemma
prop
let
collection
real-valued
rvs
trivially
collection
discrete
rvs
satisfying
moreover
joint
distribution
absolutely
continuous
w.r.t
lebesgue
measure
generally
r´enyi
claims
information
dimension
equals
joint
distribution
absolutely
continuous
sufﬁciently
smooth
n-dimensional
manifold
209
furthermore
real-valued
satisfying
probability
measure
ρpc
discrete
measure
absolutely-continuous
measure
two
well-known
properties
entropy
reduced
conditioning
2.6.5
obeys
chain
rule
furthermore
conditional
entropy
given
computed
ﬁrst
calculating
entropy
conditioned
event
averaging
corresponding
results
information
dimension
presented
following
three
lemmas
two
rvs
lemma
suppose
x|y
dpy
x|y
x|y
x|y
dpy
consequently
x|y
exists
-almost
surely
limit
exists
x|y
x|y
dpy
proof
see
appendix
a-a
lemma
two
rvs
x|y
equality
independent
proof
since
conditioning
reduces
entropy
m|y
equality
independent
lemma
follows
dividing
sides
inequality
log
taking
limits
lemma
collection
rvs
xt=1
xt=1
xt|x
t−1
proof
see
appendix
a-b
left-most
inequality
holds
equality
information
dimensions
exist
rvs
independent
examples
right-most
inequality
strict
example
let
uniformly
distributed
let
bijective
function
constructed
see
also
discussion
section
iv.b
since
bijective
|x1
x2|y
moreover
since
uniformly
distributed
finally
lemma
lemma
get
|x1
however
also
x2|y
follows
x2|y
chain
rule
x2|y
holds
strict
inequality
example
demonstrates
chain
rule
information
dimension
may
hold
strict
inequality
also
shows
order
chain
rule
expanded
crucial
information
dimension
finite-variance
rvs
rvs
ﬁnite
variance
upper
bound
presented
lemma
tightened
end
introduce
notation
denote
covariance
matrix
vector
furthermore
cross-covariance
matrix
denoted
covariance
matrix
vector
denoted
clearly
one
show
information
dimension
collection
exceed
rank
covariance
real-valued
rvs
matrix
i.e.
rank
agrees
intuition
linearly-dependent
compo-
nents
contribute
information
dimension
one
show
collections
gaussian
rvs
achieve
upper
bound
equality
thus
among
rvs
given
covariance
structure
gaussian
maximizes
information
dimension
results
follow
directly
general
results
stochastic
processes
theorem
section
next
theorem
evaluates
conditional
information
collection
real-valued
jointly
gaussian
rvs
conditional
information
dimension
jointly
gaussian
rvs
theorem
let
dimension
given
given
equal
1|y
schur
complement
since
rank
ℓ+1|x
denoted
ℓ+1|x
written
sum
ranks
7.1.p28
claim
follows
information
dimension
rate
next
propose
information
dimension
rate
gen-
eralization
information
dimension
stochastic
processes
deﬁne
information
dimension
rate
general
possibly
non-stationary
processes
however
sake
simplicity
results
presented
stationary
processes
deﬁnition
information
dimension
rate
l-variate
stochastic
process
deﬁned
lim
m→∞
lim
k→∞
log
provided
limits
exist
limits
exist
deﬁne
upper
lower
information
dimension
rate
replacing
limits
limits
superior
limits
inferior
respectively
properties
information
dimension
rate
information
dimension
rate
satisﬁes
properties
similar
presented
lemma
information
dimension
summarize
following
lemma
lemma
let
stationary
l-variate
real-valued
process
proof
suppose
ﬁrst
rightmost
inequality
follows
left-most
inequality
follows
nonnegativity
entropy
finally
center
inequality
follows
since
conditioning
reduces
en-
suppose
since
function
tropy
hence
every
implies
claim
follows
deﬁnition
affect
information
dimension
rate
next
result
discusses
lipschitz
transformations
lemma
let
stationary
l-variate
real-valued
process
let
sequence
lipschitz
func-
tions
lipschitz
constants
satisfying
rank
sup
t∈z
proof
see
appendix
a-c.
generalized
schur
complement
theorem
implies
chain
rule
lemma
holds
equality
gaussian
rvs
indeed
col-
lection
real-valued
jointly
gaussian
rvs
moreover
theorem
equals
rank
generalized
rank
rank
ℓ+1|x
proof
see
appendix
sequence
bi-lipschitz
functions
uniformly-bounded
lipschitz
constants
lemma
im-
plies
corollary
thus
obtain
information
dimension
rate
invariant
scaling
translation
generally
follows
sequences
l-variate
vectors
dimensional
matrices
latter
satisfying
supt∈z
kwtk
supt∈z
induced
matrix
norm
k·k
wtxt
since
information
dimension
rate
i.i.d
process
equals
information
dimension
marginal
rvs
recover
well-known
result
information
dimension
collections
rvs
invariant
scaling
translation
lemma
next
lemma
shows
information
dimension
rate
collection
stochastic
processes
unaffected
zero
information
dimension
rate
lemma
let
two
jointly
stationary
variate
real-valued
processes
assume
moreover
discrete
lim
m→∞
lim
k→∞
proof
see
appendix
m|z
log
inter
alia
lemma
used
compute
information
dimension
rate
countable
mixture
stochastic
processes
example
specialized
i.i.d
processes
together
lemma
recovers
choosing
information
dimension
rate
vs.
rate-distortion
dimension
denote
rate-distortion
function
let
source
i.e.
inf
|xk
ˆxk
1−xk
1k2
ˆxk
inﬁmum
conditional
distributions
ˆxk
given
ˆxk
1k2
denotes
euclidean
norm
following
deﬁnition
deﬁnition
rate-distortion
dimension
l-variate
stochastic
process
deﬁned
lim
k→∞
dimr
lim
d↓0
log
provided
limits
exist
pro-
cess
stationary
limit
always
exists
9.8.1
limits
exist
deﬁne
upper
lower
rate-distortion
dimension
dimr
replacing
limits
limits
superior
limits
inferior
respectively
dim
intuitively
rate-distortion
function
lim
k→∞
corresponds
minimum
number
nats
per
source
symbol
required
compress
stationary
ergodic
source
vector
quantizer
average
per-symbol
distortion
exceeding
sec
9.8
rate-distortion
dimension
characterizes
growth
vanishes
example
i.i.d
gaussian
source
variance
13.3.2
log
cid:18
cid:19
cid:8
cid:9
denotes
indicator
function
observe
case
grows
like
1/2
log
1/d
rate-
distortion
dimension
corresponds
twice
pre-log
factor
rate-distortion
function
case
contrast
information
dimension
rate
characterizes
growth
entropy
rate
increases
turn
corresponds
essentially
entropy
rate
number
nats
per
source
symbol
required
compress
symbol
stationary
ergodic
source
uniform
quantizer
step
size
1/m
since
symbol-wise
uniform
quantizer
outperform
best
vector
quantizer
follows
information
dimension
rate
lower-bounded
rate-distortion
dimension
rvs
kawabata
dembo
showed
rate-
distortion
dimension
actually
equal
information
dimen-
sion
prop
3.3
thus
symbol-wise
uniform
quantizer
achieves
information
dimension
best
vector
quantizer
following
theorem
generalizes
result
stochastic
processes
theorem
l-variate
real-valued
process
dim
information
dimension
rate
finite-variance
processes
let
stationary
l-variate
real-valued
process
mean
vector
matrix-valued
spectral
distribution
function
sdf
thus
bounded
non-
decreasing
right-continuous
function
−1/2
1/2
autocovariance
function
given
lebesgue-stieltjes
integral
7.3
141
cid:2
xt+τ
cid:3
1/2
e−ı2πτ
θdfx
−1/2
follows
-th
element
cross
sdf
fxixj
component
processes
i.e.
kxixj
1/2
−1/2
e−ı2πτ
θdfxixj
ˆxk
note
theorem
also
holds
non-stationary
processes
proof
see
appendix
kxixj
t+τ
denotes
cross-covariance
function
follows
diagonal
elements
real
non-decreasing
satisfy
fxi
1/2
fxi
−1/2
denotes
standard
deviation
shown
derivative
almost
everywhere
positive
semi-
deﬁnite
hermitian
values
7.4
141
shall
denote
derivative
f′x
absolutely
continuous
w.r.t
lebesgue
measure
derivative
f′x
coincides
psd
following
theorem
shows
among
processes
given
sdf
gaussian
process
maximizes
informa-
tion
dimension
rate
characterizes
information
dimension
rate
processes
terms
sdf
theorem
let
stationary
l-variate
real-valued
process
sdf
1/2
equality
gaussian
proof
see
appendix
−1/2
rank
f′x
order
prove
theorem
invoke
bussgang
theorem
obtain
expression
sdf
quantized
gaussian
process
function
sdf
original
process
since
believe
result
interesting
present
lemma
let
stationary
l-variate
real-valued
gaussian
process
mean
vector
sdf
-th
entry
sdf
satisﬁes
fxixj
fninj
denote
mean
standard
deviation
every
ai|
πσ2
1/2
−1/2
dfni
moreover
component
processes
zero
mean
unit
variance
2a1
proof
see
appendix
corrolary
theorem
obtain
univariate
stationary
gaussian
processes
psd
information
dimension
rate
equal
lebesgue
measure
set
harmonics
−1/2
1/2
positive
i.e.
denotes
lebesgue
measure
pointed
one
reviewers
also
obtained
directly
using
equivalence
information
dimension
rate
rate-
distortion
dimension
theorem
together
parametric
representation
rate-distortion
function
eqs
9.7.42
9.7.43
2zbβ
1/2
−1/2
cid:19
log
cid:18
min
−1/2
1/2
indeed
zero
since
case
process
zero
variance
hence
entropy
rate
quantized
process
zero
strictly
positive
distortion
bounded
follows
continuity
lebesgue
measure
dβ/β
consequently
rate-distortion
dimension
written
dimr
lim
cid:17
log
cid:16
β↓0
rbβ
log
β↓0
rbβ
β↓0
rbβ
lim
lim
log
log
log
log
continuity
lebesgue
measure
every
exists
bβ′
since
follows
bβ′
bβ′
thus
every
zbβ
log
=zbβ\bβ′
log
+zbβ′
log
bβ′
log
bβ′
log
log
bβ′
log
dividing
sides
log
letting
ﬁrst
tend
zero
obtain
second
term
rhs
nonnegative
however
assumption
process
ﬁnite
variance
psd
integrable
−1/2
1/2
consequently
using
inequality
log
nonnegativity
obtain
zbβ
log
1/2
−1/2
dividing
sides
log
letting
tend
zero
obtain
second
term
rhs
also
nonpositive
conclude
term
zero
follows
theorem
observe
theorem
information
dimen-
sion
rate
gaussian
process
depends
derivative
sdf
coincides
almost
everywhere
derivative
absolutely-continuous
part
indeed
sdf
decomposed
4.3
124
absolutely
continuous
w.r.t
lebesgue
mea-
sure
discrete
singular
furthermore
f′x
f′x
almost
everywhere
sec
conse-
quently
information
dimension
rate
gaussian
process
depends
absolutely-continuous
part
sdf
combining
theorem
lemma
show
true
non-gaussian
processes
corollary
let
stationary
l-variate
real-
valued
process
sdf
let
stationary
l-variate
real-valued
process
sdf
absolutely-continuous
part
proof
combining
decomposition
spec-
tral
representation
stationary
processes
sec
4.11
shown
every
stationary
process
written
stationary
mutually
uncorrelated
stochastic
processes
respective
sdfs
see
758
references
therein
since
f′x
f′x
zero
almost
everywhere
sec
obtain
theorem
nonnegativity
information
dimension
rate
lemma
corollary
follows
applying
lemma
ﬁrst
together
show
autocovariance
function
say
stationary
variate
complex-valued
process
proper
ﬁnite
variance
mean
vector
zero
vector
pseudo-
autocovariance
function
satisﬁes
zt+τ
following
result
generalizes
theorem
complex-
valued
stochastic
processes
theorem
let
stationary
l-variate
complex-
valued
process
matrix-valued
sdf
1/2
−1/2
rank
f′z
equality
gaussian
proper
proof
see
appendix
note
neither
gaussianity
properness
sufﬁcient
equality
theorem
13.
conversely
gaussianity
proper-
ness
necessary
equality
example
univariate
stationary
gaussian
process
achieves
equality
real
imaginary
components
independent
derivatives
sdfs
matching
support
another
definition
information
dimension
jalali
poor
proposed
different
deﬁnition
information
dimension
univariate
stochastic
process
shall
refer
information
dimension
block-average
information
dimension
denote
sec-
tion
discuss
scenarios
information
dimension
rate
deﬁnition
coincides
differs
block-
average
information
dimension
ease
exposition
section
follow
restrict
attention
univariate
real-valued
processes
together
show
following
deﬁnition
information
dimension
stochastic
processes
proposed
deﬁnition
block-average
information
dimension
stochastic
process
deﬁned
information
dimension
rate
complex-valued
processes
far
considered
real-valued
stochastic
processes
however
every
complex-valued
written
two-dimensional
real-valued
random
vector
previous
results
directly
generalize
complex
case
particular
one
deﬁne
information
dimension
rate
l-variate
complex-valued
process
information
dimension
rate
-variate
real-valued
process
ˆxt
follows
stacking
real
part
top
imaginary
part
let
stationary
l-variate
complex-valued
process
mean
vector
matrix-valued
sdf
i.e.
1/2
−1/2
e−ı2πτ
θdfz
zt+τ
lim
k→∞
lim
m→∞
k−1
log
provided
limits
exist
limits
exist
one
deﬁne
upper
lower
block-average
information
dimension
replacing
limits
limits
superior
limits
inferior
respectively
following
restrict
stationary
pro-
cesses
case
limit
guaranteed
exist
refer
block-average
information
dimension
shown
lemma
stationary
information
dimension
exists
every
lim
k→∞
exist
proof
lemma
reveals
lim
k→∞
lim
k→∞
since
conditioning
reduces
entropy
follows
immediately
thus
like
information
dimension
rate
block-average
information
dimension
stochastic
process
exceed
information
dimension
marginal
entropy
rate
stationary
process
alternatively
written
conditional
entropy
given
block-average
information
dimension
general
permit
similar
expression
fact
let
x1|x
lim
m→∞
lim
k→∞
m|x
log
provided
limit
exists
since
conditioning
reduces
entropy
limit
always
exists
upper
lower
information
dimensions
x1|x
deﬁned
analogously
replacing
limit
limit
superior
limit
inferior
respectively
x1|x
x1|x
inequality
strict
see
theorem
example
thus
satisﬁed
processes
allow
change
order
taking
limits
tend
inﬁnity
however
general
difﬁcult
check
next
present
sufﬁcient
condition
easier
verify
corollary
let
stationary
process
assume
exists
nonnegative
integer
x−n
proof
see
appendix
h-b
condition
holds
ψ∗-mixing
processes
indeed
since
every
ψ∗-mixing
process
satisﬁes
follows
one
ﬁnd
x∞1
x−n
condition
holds
data
processing
inequality
holds
even
x1|x
thus
case
presented
generalizations
information
dimension
stochastic
processes
coincide
informa-
tion
dimension
marginal
prove
note
gives
follows
data
processing
inequality
block-average
information
dimension
vs.
information
dimension
rate
consequently
next
demonstrate
ψ∗-mixing
processes
information
dimension
rate
coincides
block-
average
information
dimension
however
general
two
deﬁnitions
coincide
exists
ordering
theorem
let
stationary
process
x1|x
moreover
k−1
log
lim
k→∞
lim
m→∞
lim
k→∞
lim
m→∞
k−1
log
limits
exist
stationarity
mutual
information
monotonically
decreasing
proof
see
appendix
h-a
k−1
inequalities
imply
limits
exist
lim
k→∞
lim
m→∞
k−1
log
necessary
sufﬁcient
condition
equality
note
every
8.9
x1|x
lim
m→∞
log
claim
follows
limit
exists
general
x1|x
condition
satisﬁed
example
sequence
i.i.d
rvs
discrete-valued
stochastic
process
ﬁnite
marginal
entropy
continuous-valued
stochastic
process
ﬁnite
marginal
differential
entropy
ﬁnite
differential
entropy
rate
following
present
two
examples
processes
x1|x
shall
argue
neither
examples
satisﬁes
hence
sufﬁcient
necessary
example
let
sequence
i.i.d
bernoulli-ρ
rvs
i.e.
pbt
pbt
let
sequence
i.i.d
rvs
pdf
supported
ﬁnite
differential
entropy
thus
every
deﬁne
stochastic
process
btxt−1
assume
marginal
distribution
note
ﬁrst-order
markov
x1|x
x1|x0
furthermore
demonstrates
thus
together
yields
lim
k→∞
k−1
x1|x
stochastic
process
deﬁned
satisﬁes
indeed
every
nonnegative
integer
x−n
since
ﬁnite
differential
entropy
event
x−n
positive
probability
follows
every
violated
contrast
x−n
lim
m→∞
k−1
log
lim
m→∞
k−1
log
since
conditioning
binary
random
variable
changes
mutual
information
one
bit
xk−1
independent
k−1
cases
conditional
mutual
information
zero
satisﬁed
given
k−1
example
let
process
˜xt
periodic
period
ﬁnite
marginal
differential
entropy
let
uniformly
distributed
shifted
process
deﬁned
˜xt+∆
stationary
10-5
ﬁnite
marginal
differential
entropy
every
m|x
−k+1
hence
x1|x
stochastic
process
previous
example
satisﬁes
indeed
every
nonnegative
integer
since
ﬁnite
differential
entropy
process
periodic
contrast
xk−p
conditional
mutual
information
zero
given
k−1
x−n
many
cases
inequalities
theorem
strict
following
example
shows
strict
inequality
class
stationary
gaussian
processes
psd
supported
set
positive
lebesgue
measure.3
example
let
stationary
gaussian
process
zero
mean
variance
psd
support
follows
theorem
next
argue
x1|x
consequently
x1|x
show
note
lim
k→∞
lim
k→∞
lim
m→∞
lim
m→∞
k−1
log
m|x−1
log
3the
assumption
psd
made
notational
convenience
essential
steps
example
continue
hold
replace
derivative
sdf
independent
x−1
inequality
follows
stationarity
be-
cause
conditioning
reduces
entropy
conditioned
x−1
since
gaussian
follows
conditioned
x−1
gaussian
mean
x0|x−1
independent
x−1
shown
every
ﬁnite
see
lemma
follows
conditioned
x−1
pdf
variance
x−1
x−1
together
fatou
lemma
shows
rhs
demonstrate
x1|x
note
log
lim
m→∞
cid:12
cid:12
x−1
hence
implies
1/2
−1/2
log
necessary
sufﬁcient
condition
see
e.g.
sec
10.6
intuitively
fact
implies
conditional
distribution
given
almost
surely
degenerate
hence
x1|x
prove
rigorously
apply
lemma
together
fact
conditioning
reduces
entropy
upper-bound
m|x
x1|x
log
expressing
x1−e
x1|x
⌊mσk+1z⌋
log
since
σk+1z
zero-mean
unit-variance
gaussian
rhs
written
obtain
lemma
lim
k→∞
m|x
log
claim
follows
deﬁnition
x1|x
lemma
let
stationary
univariate
real-valued
gaussian
process
zero
mean
variance
sdf
suppose
ﬁnite
consequently
f′x
proof
see
appendix
h-c.
block-average
information
dimension
vs.
rate-distortion
dimension
connection
block-average
information
di-
mension
rate-distortion
dimension
stochastic
process
studied
equivalence
rate-distortion
dimension
information
dimension
prop
3.3
directly
implies
lim
k→∞
lim
d→0
log
rezagah
demonstrated
order
limits
rhs
exchanged
precisely
states
limd→0
log
exists
dimr
may
appear
contradiction
results
since
demonstrate
theorem
dim
example
demonstrates
stochastic
processes
however
proof
relies
fact
sec
vi-e
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
lim
κ→∞
rhs
vanishes
indeed
case
see
eqs
8.6
case
also
holds
8.10
shown
corollary
fact
discussed
case
presented
generalizations
corollary
information
dimension
stochastic
processes
coincide
information
dimension
marginal
contrast
hold
data
processing
inequality
rhs
inﬁnite
example
case
stationary
process
positive
variance
psd
zero
set
positive
lebesgue
measure
since
processes
differential
entropy
x1|x
proof
theorem
rely
thus
every
stochastic
conclude
dim
process
dim
processes
operational
characterizations
information
dimension
recently
given
operational
characterization
almost
lossless
data
compression
speciﬁcally
verd´u
deﬁned
minimum
ǫ-achievable
rate
minimum
exists
sequence
encoders
r⌊rk⌋
decoders
r⌊rk⌋
satisfying
def
sufﬁciently
large
argued
sec
iv-b
impose
restrictions
zero
rate
achievable
even
since
cardinality
however
restrict
either
encoders
linear
decoders
lipschitz
continuous
minimum
ǫ-achievable
rate
collections
i.i.d
rvs
discrete-continuous
mixed
distribution
i.e.
distribution
form
given
100
thus
rvs
information
dimension
operational
characterization
stochastic
processes
verd´u
demonstrated
minimum
ǫ-achievable
rate
achievable
lipschitz-continuous
decoders
lower-bounded
remark
101
best
knowledge
non-i.i.d
processes
matching
achievability
result
exists
almost
lossless
data
compression
contrast
universal
compressed
sensing
linear
encoding
decoding
via
lagrangian
minimum
entropy
pursuit
shown
jalali
poor
achievable
rate
ψ∗-mixing
theorem
consider
ψ∗-mixing
stationary
process
taking
value
upper
block-average
information
dimension
let
entries
measurement
matrix
rℓ×k
drawn
i.i.d
according
zero-mean
unit-variance
gaussian
distribution
given
generated
let
102
arg
min
ˆhj
conditional
empirical
entropy
def
log
log
log
log
log
log
number
measurements
satisﬁes
kau
tk2
ˆhj
u∈x
arbitrary
103
√kk
tk2
probability
104
words
theorem
states
rate
random
linear
measurements
slightly
larger
block-
average
information
dimension
lagrangian
relax-
ation
minimum
entropy
pursuit
provides
asymptotically
distortion-free
estimate
terms
euclidean
norm
thus
ψ∗-mixing
processes
block-average
information
dimension
achievable
rate
almost
zero-distortion
recovery
next
discuss
operational
characterization
rate-
distortion
dimension
theorem
also
operational
characterization
information
dimension
rate
rezagah
considered
almost
zero-distortion
recovery
stationary
processes
decoder
employs
compress-
ible
signal
pursuit
csp
optimization
theorem
cor
consider
stationary
real-valued
process
system
random
linear
observations
measurement
matrix
rℓ×k
composed
i.i.d
zero-mean
unit-variance
gaus-
sian
rvs
number
measurements
satisﬁes
lim
k→∞
dimr
105
exists
family
compression
codes
√kk
tk2
probability
106
solution
csp
optimization
arg
min
u∈ck
auk2
107
denotes
codebook
compression
code
words
rate
random
linear
measurements
slightly
larger
rate-distortion
dimension
exists
family
compression
codes
csp
optimization
yields
asymptotically
distortion-free
estimate
terms
euclidean
norm
thus
rate-distortion
dimension
achievable
rate
almost
zero-distortion
recovery
summarize
101
demonstrates
yields
lower
bound
sampling
rate
required
almost
loss-
less
recovery
lipschitz-continuous
decoders
contrast
theorem
demonstrates
dimr
hence
also
achievable
rate
almost
zero-distortion
recovery
furthermore
illustrated
example
processes
dimr
108
results
thus
demonstrate
exist
stationary
pro-
cesses
sampling
rate
required
almost
zero-
distortion
recovery
strictly
smaller
sampling
rate
re-
quired
almost
lossless
recovery
lipschitz-continuous
decoders
words
fundamental
limits
almost
zero-distortion
recovery
almost
lossless
recovery
dif-
ferent
general
comparing
lower
bound
101
almost
lossless
re-
covery
theorem
almost
zero-distortion
recovery
observe
two
main
differences
setup
101
obtained
lipschitz-continuous
decoder
whereas
theorem
based
csp
optimzation
almost
lossless
recovery
required
exactly
equal
high
probability
whereas
almost
zero-distortion
recovery
sufﬁces
1√kk
tk2
small
following
example
presents
class
stationary
processes
almost
zero-distortion
recovery
rate
may
also
achieved
linear
encoders
decoders
suggests
second
difference
greater
impact
example
let
stationary
univariate
real-
valued
gaussian
process
possessing
psd
support
−1/4
1/4
theorem
1/2
next
invoke
sampling
theorem
demonstrate
linear
encoders
rℓk
decoders
exist
rℓk
lim
k→∞
√kk
tk2
109
probability
110
describe
divide
indices
three
groups
111
113
arbitrary
sequence
even
integers
tends
inﬁnity
sublinearly
encoder
re-
produces
values
indices
i.e.
112
consequently
cid:23
2∆k
cid:22
2∆k
114
rate
ℓk/k
converges
next
show
ﬁnd
decoder
110
holds
clearly
values
∪i2
directly
observed
therefore
remains
estimate
missing
values
done
via
interpolation
formula
ˆxt
∆k/2
xi=−∆k/2
follows
x2i+t−1
sin
cid:0
cid:2
cid:3
cid:1
cid:0
cid:1
115
ehk
ˆx1
ˆxk
tk2
xt∈i3
cid:24
2∆k
cid:25
ˆx∆k+1
x∆k+1
ˆxt
116
last
step
due
stationarity
sampling
theorem
stochastic
processes
expected
value
rhs
116
vanishes
thus
dividing
sides
116
letting
gives
117
ehk
ˆx1
ˆxk
tk2
together
chebyshev
inequality
4.10.7
yields
110
lim
k→∞
vii
conclusions
r´enyi
proposed
information
dimension
dimensional
entropy
measure
information
content
general
rvs
idea
quantize
real-valued
uniform
quantizer
step
size
1/m
analyze
entropy
quantized
limit
tends
inﬁnity
results
demonstrate
positive
information
dimension
inﬁnite
information
content
e.g.
case
rvs
whose
probability
measure
absolutely-continuous
part
problem
becomes
even
interesting
stochastic
processes
since
information
content
determined
distribution
marginals
also
temporal
dependence
example
consider
stationary
gaussian
process
bandlimited
psd
one
hand
gaussian
processes
absolutely-continuous
marginals
one
would
expect
information
content
inﬁnite
hand
processes
bandlimited
psd
present
sample
perfectly
predicted
inﬁnite
past
x−1
x−2
see
example
suggests
information
content
zero
shed
light
questions
proposed
gen-
eralization
information
dimension
stochastic
processes
deﬁning
information
dimension
rate
entropy
rate
divided
log
limit
demonstrated
information
dimension
rate
coincides
rate-distortion
dimension
deﬁned
twice
pre-log
factor
rate-distortion
function
showed
among
stationary
process
psd
gaussian
process
largest
information
dimension
rate
consistent
observation
gaussian
processes
hardest
predict
hence
expected
largest
information
content
showed
information
dimension
rate
stationary
gaussian
processes
given
average
rank
i.e.
1/2
−1/2
rank
118
specialized
univariate
case
yields
informa-
tion
dimension
rate
given
lebesgue
measure
support
i.e.
119
agrees
intuition
psd
zero
set
positive
lebesgue
measure
samples
expressed
terms
remaining
samples
therefore
information
content
answers
question
whether
stationary
gaussian
processes
bandlimited
psd
inﬁnite
information
content
positive
unless
psd
zero
almost
everywhere
alternative
deﬁnition
information
dimension
stochastic
process
proposed
jalali
poor
information
dimension
divided
limit
referred
quantity
block-
average
information
dimension
coincide
ψ∗-mixing
processes
general
inequality
strict
particular
illustrated
example
support
gaussian
process
positive
lebesgue
measure
thus
contrast
information
dimension
rate
block-average
information
dimension
capture
dependence
information
dimension
support
size
essential
difference
deﬁnitions
order
limits
quantization
bin
size
1/m
block
size
taken
rezagah
showed
limits
exchanged
process
satisﬁes
120
case
dimr
however
case
information
dimension
stochastic
process
coincides
information
dimension
marginal
words
processes
generalization
information
dimension
stochastic
processes
redundant
contrast
showed
theorem
stochastic
process
information
dimension
rate
co-
incides
rate-distortion
dimension
dimr
implies
coincides
dimr
stochastic
processes
equivalence
information
dimension
rate
rate-distortion
dimension
dimr
im-
plies
inherits
operational
characterizations
dimr
example
demonstrated
dimr
achievable
rate
almost
zero-distortion
recovery
contrast
shows
lower
bound
minimum
ǫ-achievable
rate
achievable
lipschitz-continuous
decoders
demonstrating
processes
dimr
121
results
show
fundamental
limits
almost
zero-
distortion
recovery
almost
lossless
recovery
different
general
jalali
poor
showed
achievable
rate
universal
lossless
compressed
sensing
linear
encoding
decoding
via
lagrangian
minimum
entropy
pursuit
ψ∗-mixing
since
ψ∗-mixing
processes
deﬁnition
also
inherits
operational
characterization
appendix
appendix
section
iii
proof
lemma
ﬁrst
inequality
namely
lim
m→∞
m|y
log
dpy
m→∞z
m|y
lim
log
dpy
122
follows
directly
fatou
lemma
1.6.8
second
inequality
follows
limit
inferior
upper-
bounded
limit
superior
third
inequality
note
every
m|y
log
1|y
log
123
furthermore
since
conditioning
reduces
entropy
cid:18
1|y
log
cid:19
dpy
1|y
log
log
124
every
hence
rhs
123
integrable
third
inequality
follows
fatou
lemma
proof
lemma
right-
inequality
holds
trivially
moreover
case
least
one
also
thus
also
left-most
inequality
holds
1|x
t−1
125
hence
upper
information
dimensions
ﬁnite
follows
chain
rule
entropy
conditioning
reduces
entropy
lim
m→∞
xt=1
t−1
log
log
126
likewise
lim
m→∞
xt=1
t−1
log
m|x
t−1
log
lim
m→∞
xt=1
xi=t
lim
m→∞
xt=1
xt=1
x|y
rank
remains
show
gen-
eralized
schur
complement
indeed
7.1.p28
exists
matrix
cyx
cyw
generalized
schur
complement
given
cx|y
hcyw
130
comparing
130
expression
given
lemma
observe
cx|y
matrix
lemma
satisﬁes
cyx
cyt
indeed
case
since
since
uncorrelated
cyx
yxt
cyt
yyt
131
proves
theorem
xt|x
t−1
127
appendix
proof
lemma
inequality
follows
conditioning
reduces
en-
tropy
conditioned
t−1
independent
t−1
proof
theorem
simplify
notation
shall
write
collections
rvs
vectors
namely
proof
theorem
based
following
lemma
lemma
let
ℓ-dimensional
jointly
gaussian
vectors
mean
vectors
joint
covariance
matrix
exists
matrix
length-k
vector
x|y
moreover
zero
mean
uncorrelated
satisﬁes
cyt
proof
jointly
gaussian
written
linear
transformation
uncorrelated
error
follows
fact
jointly
gaussian
linear
minimum
mean-square
error
lmmse
estimator
given
always
exists
given
x|y
result
zero
mean
uncorrelated
satisﬁes
cx−t
cyt
follows
direct
calculation
since
information
dimension
translation
invariant
fol-
lows
x|y
e|y
e|y
128
furthermore
since
jointly
gaussian
fact
uncorrelated
follows
independent
thus
e|y
rank
129
covariance
matrix
identities
128
129
hold
every
follows
lemma
prove
lemma
shall
need
following
auxiliary
result
lemma
let
collection
real-valued
rvs
let
lipschitz
continuous
lipschitz
constant
132
log⌈k√k
proof
note
1/m
cube
diameter
√k/m
image
cube
lipschitz
function
diam-
eter
greater
k√k/m
computing
induces
partition
ℓ-dimensional
cubes
partition
⌈k√k
1⌉ℓ
elements
nonempty
intersection
image
1/m
therefore
log⌈k√k
lemma
follows
averaging
next
prove
lemma
let
prove
every
133
right-most
relation
use
every
134
second
summand
upper-bounded
135
xt=1
since
every
function
lipschitz
lipschitz
constant
supt∈z
use
lemma
bound
rhs
135
log⌈k√l
since
term
independent
contribution
second
summand
rhs
134
vanishes
thus
obtain
dividing
sides
134
log
letting
tend
inﬁnity
prove
left-most
relation
use
every
appendix
proof
theorem
136
claim
follows
dividing
sides
136
log
letting
tend
inﬁnity
proof
theorem
essentially
identical
proof
lemma
3.2
sake
completeness
reproduce
full
proof
indeed
choosing
appendix
proof
lemma
every
yields
ˆxk
142
137
dividing
log
letting
ﬁrst
tend
inﬁnity
yields
prove
note
lemma
yield
reverse
inequality
use
lemma
fact
conditioning
reduces
entropy
obtain
log
log
138
dividing
sides
138
log
letting
ﬁrst
tend
inﬁnity
yields
proves
finally
discrete
m|z
since
m|z
139
second
entropy
ﬁnite
assumption
ﬁrst
entropy
satisﬁes
conversely
discrete
since
choice
142
kxk
hence
satisﬁes
consequently
dividing
log
taking
limits
obtain
ˆxk
1k2
143
lim
d↓0
lim
k→∞
log
lim
d↓0
lim
k→∞
lim
m→∞
lim
k→∞
log
log
144
limits
exist
limits
exist
obtain
upper
bound
limits
replaced
limits
superior
limits
inferior.4
next
derive
lower
bound
rate-distortion
dimen-
sion
simplify
notation
treat
collection
l-variate
random
vectors
collection
rvs
show
upper
bound
144
holds
equality
use
following
lower
bound
given
a.1
sup
s≤0
λsnsd
ehlog
cid:16
arbitrary
nonnegative
measur-
cid:17
145
rk′
able
function
satisfying
m|z
m|z
140
sup
yk′
∈rk′
ehλs
cid:16
cid:17
eskyk′
k2i
146
dividing
terms
letting
tend
inﬁnity
thus
yields
lim
k→∞
m|z
m|z
lim
k→∞
since
second
term
rhs
141
tends
zero
tends
inﬁnity
thus
dividing
141
log
letting
tend
inﬁnity
yields
141
lim
k→∞
following
proof
lemma
3.2
apply
145
−m2
xk′
∈zk′
xik′
xi=0
e−i2
cid:26
xk′
cid:27
ik′
ik′
147
148
149
4since
l/d
taking
limit
tantamount
taking
limit
ﬁrst
show
choice
satisﬁes
146
indeed
cid:17
eskyk′
k2i
sup
yk′
∈rk′
sup
yk′
∈rk′
ehλs
cid:16
xik′
∈zk′
sup
xk′
xk′
ik′
yℓ=1
sup
0≤˜xℓ
sup
jk′
∈zk′
yℓ=1
sup
jℓ∈z
˜yk′
∈zk′
sup
0,1
xik′
0≤˜yℓ
1xiℓ∈z
sup
sup
0≤˜xℓ
e−m2kyk′
−xk′
˜yℓ+jℓ−˜xℓ−iℓ
˜yℓ+jℓ−˜xℓ−iℓ
150
second
step
follows
substituting
˜xℓ
mxℓ
˜yℓ
myℓ
since
sum
depend
follows
limits
exist
limits
exist
obtain
lower
bound
limits
replaced
limits
superior
limits
inferior
combining
156
144
proves
theorem
appendix
proof
theorem
proof
consists
two
parts
ﬁrst
part
show
processes
given
sdf
gaussian
process
largest
information
dimension
rate
section
e-a
second
part
demonstrate
information
dimension
rate
gaussian
processes
given
average
rank
derivative
sdf
section
e-b
gaussian
processes
maximize
information
dimension
theorem
upper
information
dimension
rate
sup
jℓ∈z
sup
0≤˜yℓ
1xiℓ∈z
sup
˜yℓ+jℓ−˜xℓ−iℓ
given
0≤˜xℓ
sup
0≤˜yℓ
1xiℓ∈z
˜yℓ−˜xℓ−iℓ
151
sup
0≤˜xℓ
upper-bounded
sup
0≤˜yℓ
1xiℓ∈z
hence
sup
0≤˜xℓ
˜yℓ−˜xℓ−iℓ
e−i2
152
xi=0
sup
yk′
∈rk′
ehλs
cid:16
cid:17
eskyk′
k2i
e−i2
153
xi=0
149
equal
follows
chosen
147
148
satisfy
146
next
evaluate
145
choice
distortion
yields
−km2d
log
log

xik′

cid:27
ik′
ik′
cid:17
m2d
log
cid:26
cid:16
∈zk′


154
155
l/d
becomes
log
next
replace
collection
rvs
equivalent
collection
random
vectors
dividing
sides
155
log
taking
limits
yields
lim
lim
d↓0
k→∞
lim
d↓0
log
lim
k→∞
log
log
lim
m→∞
lim
k→∞
log
156
lim
d↓0
lim
k→∞
log
157
claim
information
dimension
maximized
gaussian
process
follows
well-known
fact
random
vectors
given
covariance
matrix
cxk
gaussian
random
vector
largest
rate-distortion
function
prove
claim
multivariate
sources
shall
write
collection
l-variate
vectors
collection
rvs
since
information
dimension
rate
translation
invariant
lemma
assume
without
loss
optimality
rvs
zero
mean
furthermore
eigenvalue
decomposition
exists
orthogonal
matrix
random
variables
given
yk′
xk′
uncorrelated
variances
eigenvalues
shall
denote
λk′
since
mutual
information
invariant
bijections
euclidean
norm
invariant
multiplications
orthogonal
matrices
follows
case
independent
zero-mean
gaus-
sian
random
variables
variances
λk′
rate-
distortion
function
given
13.3.3
xt=1
log
158
159
chosen
dk′
rhs
158
also
achieved
non-gaussian
rvs
choosing
following
possibly
suboptimal
distri-
bution
reconstruction
values
ˆyt
λt−dt
160
variances
dtλt
λt−dt
independent
zero-mean
gaussian
rvs
159
indeed
easy
check
160
satisﬁes
distortion
constraint
furthermore
using
conditioning
reduces
entropy
gaussian
rvs
maximize
differential
entropy
shown
log
161
lim
k→∞
xt=1
comparing
161
158
conclude
uncorre-
lated
rvs
cid:0
cid:1
ance
matrix
jointly
gaussian
covari-
since
cid:1
cid:0
cid:0
cid:1
also
true
general
rvs
together
157
proves
processes
given
sdf
gaussian
process
largest
information
dimension
rate
162
information
dimension
gaussian
processes
deﬁne
assume
gaussian
every
163
furthermore
let
i.i.d
uniformly
distributed
1/m
let
deﬁne
corresponding
mul-
tivariate
processes
since
independent
every
matrix-valued
sdfs
satisfy
164
moreover
matrix-valued
psd
exists
equals
165
12m2
since
information
dimension
rate
translation
invariant
lemma
since
sdf
depend
mean
vector
assume
without
loss
generality
zero
mean
show
lemma
appendix
e-c
assume
without
loss
generality
every
component
process
unit
variance
lemma
thus
follows
2a1
166
continue
writing
entropy
terms
differential
entropy
i.e.
log
167
denoting
covariance
matrix
gwk
expressed
gaussian
vector
mean
respectively
denoting
fwk
pdfs
fwk
1kgwk
+kl
log
168
dividing
log
letting
ﬁrst
tend
inﬁnity
yields
information
dimension
rate
lemma
appendix
e-c
shows
1kgwk
fwk
169
constant
independent
moreover
differential
entropy
rate
stationary
l-variate
gaus-
sian
process
given
7.10
log
2πe
1/2
−1/2
log
det
f′w
170
thus
follows
information
dimension
rate
equals
lim
m→∞
log
1/2
−1/2
log
det
f′w
171
remains
show
rhs
171
equal
rhs
ﬁrst
show
integral
rhs
171
restricted
subset
−1/2
1/2
entries
f′n
bounded
υ/m2
show
set
det
f′w
bounded
products
afﬁne
transforms
eigenvalues
f′x
bounds
asymptotically
tight
i.e.
equal
limit
tends
inﬁnity
complete
proof
showing
order
limit
integration
exchanged
restriction
−1/2
1/2
choose
let
max
cid:8
f′ni
υ/m2
cid:9
i=1
...
f′ni
υ/m2
lemma
every
cid:16
cid:17
since
set
union
follows
union
bound
175
prove
174
note
lebesgue
decomposition
theorem
2.2.6
fact
radon-nikodym
derivative
dfni
/dλ
coincides
f′ni
almost
every-
sec
2.3
1/2
−1/2
f′ni
−1/2
dfni
1/2
≥zf
cid:16
cid:17
f′ni
second
inequality
follows
f′ni
nonnegative
third
inequality
follows
deﬁnition
lemma
integral
left-hand
side
lhs
176
upper-bounded
1/m2
hence
174
follows
164
165
f′w
12m2
177
172
173
174
176
since
derivatives
matrix-valued
sdfs
positive
semidef-
inite
follows
det
f′w
det
12m2
178
hence
lim
m→∞rfυ
log
det
f′w
log
log
12m2
log
179
lim
m→∞
2l2
last
step
follows
175
applying
hadamard
jensen
inequality
get
log
det
f′w
zfυ
log
f′wi
xi=1zfυ
log
rfυ
f′wi
xi=1
log
cid:18
2a1
12m2
cid:19
180
last
step
follows
164
166
assumption
every
component
process
zero
mean
unit
variance
since
lemma
180
yields
m→∞zfυ
lim
log
log
det
f′w
lim
m→∞
181
consequently
log
det
f′w
2l2
lim
m→∞rfυ
m→∞rfυ
lim
log
log
det
f′w
log
every
follows
integral
contribute
information
dimension
rate
let
tend
inﬁnity
view
171
thus
obtain
information
dimension
rate
evaluating
log
det
f′w
183
log
mzf
limit
ﬁrst
tends
inﬁnity
bounding
det
f′w
eigenvalues
f′x
lemma
177
yield
f′w
2a1
f′x
f′n
12m2
184
let
denote
eigenvalues
f′x
since
f′n
positive
semideﬁnite
obtain
det
f′w
det
cid:18
2a1
f′x
yi=1
cid:18
2a1
12m2
cid:19
12m2
cid:19
next
derive
upper
bound
det
f′w
let
j=1
|f′ninj
denote
ℓ1-matrix
norm
f′n
since
f′n
positive
semideﬁnite
element
maximum
modulus
main
diagonal
kf′n
problem
7.1.p1
furthermore
assumption
diagonal
elements
bounded
hence
obtain
kf′n
186
known
matrix
norms
bound
largest
eigenvalue
matrix
5.6.9
thus
upper
bound
186
also
upper
bound
largest
eigenvalue
f′n
let
denote
eigenvalues
f′w
8/π
2a1
cor
4.3.15
det
f′w
yi=1
yi=1
cid:18
2a1
l2υ
12m2
cid:19
187
combining
185
187
183
obtain
lim
m→∞
xi=1
m→∞rf
lim
m→∞rf
lim
xi=1
lim
m→∞
log
cid:0
2a1
12m2
cid:1
log
log
det
f′w
log
log
det
f′w
log
log
cid:16
2a1
log
+l2υ
cid:17
188
xi=1
lim
m→∞zf
log
cid:0
2a1
cid:1
log
1/m2
189
either
1/12
left-most
inequality
188
1/12
l2υ
right-most
inequality
188
exchanging
limit
integration
evaluate
189
continue
along
lines
sec
viii
speciﬁcally
split
integral
rhs
189
three
parts
fii
fiii
190
191
192
arbitrary
185
5this
bound
holds
without
multiplicative
constant
since
spectral
radius
matrix
inﬁmum
matrix
norms
lemma
5.6.10
182
compute
limit
183
thus
need
evaluate
ﬁrst
part
obtain
log
1/m2
log
log
1/m2
log
cid:0
2a1
cid:1
zfi
=zfi
cid:18
log
1/m2
cid:19
log
1/m2
log
193
evaluates
limit
next
show
integrals
fii
fiii
contribute
189
end
sufﬁces
consider
integral
function
log
cid:16
2a1
log
1/m2
cid:17
log
cid:0
cid:1
log
1/m2
194
remainder
proof
shall
assume
without
loss
generality
8/π
case
fii
∪fiii
clearly
whenever
function
194
converges
zero
moreover
function
nonpositive
fii
2a1
hence
ﬁnd
sufﬁciently
large
lemma
since
result
also
2a1
8/π
follows
max
p8/π
log
cid:16
log
1/m2
cid:17
log
cid:0
cid:1
log
1/m2
195
lhs
195
nonpositive
monotonically
increases
zero
thus
apply
monotone
convergence
theorem
1.6.7
get
lim
m→∞zfii
m→∞zfii
lim
m→∞zfii
lim
=zfii
lim
m→∞
log
1/m2
log
1/m2
log
cid:0
cid:1
log
cid:0
cid:1
log
cid:16
cid:17
log
cid:16
cid:17
log
1/m2
log
1/m2
196
next
turn
case
fiii
shown
443
function
194
bounded
furthermore
nonnegative
nonpositive
monotonically
increasing
restricting
case
8/π
thus
obtain
fiii
log
cid:0
cid:1
197
made
use
fact
2a1−1
1−ε
fiii
lemma
2a1−
8/π
log
1−ε
log
π/8
log
1/m2
otherwise
hence
fiii
magnitude
function
194
bounded
cid:12
cid:12
cid:12
cid:12
cid:12
log
cid:0
cid:1
log
1/m2
max
cid:12
cid:12
cid:12
cid:12
cid:12
cid:17
log
cid:16
log
8/π
1−ε
198
thus
apply
dominated
convergence
theorem
1.6.9
get
lim
m→∞zfiii
=zfiii
lim
m→∞
log
1/m2
log
cid:0
2a1
cid:1
log
cid:16
2a1
cid:17
log
1/m2
199
combining
193
196
199
evaluate
189
log
cid:0
2a1
cid:1
log
1/m2
lim
m→∞zf
xi=1
xi=1
xi=1
200
wrapping
compute
limit
183
ﬁrst
tends
inﬁnity
remains
let
rhs
200
continuity
lebesgue
measure
yields
1/2
−1/2
rank
f′x
201
summarize
combining
171
182
200
obtain
xi=1
lim
υ→∞
1/2
−1/2
lim
m→∞
log
1/2
−1/2
rank
f′x
log
det
f′w
202
proves
theorem
10.
auxiliary
results
lemma
suppose
stationary
l-variate
real-valued
gaussian
process
mean
vector
sdf
suppose
component
processes
ordered
variances
i.e.
···
l′+1
···
t/σ1
xl′
t/σl′
almost
every
203
204
rank
f′x
rank
x1/σ1
...
xl′
/σl′
proof
normalizing
component
processes
positive
information
variance
unit
variance
affect
dimension
rate
follows
lemma
205
component
process
almost
surely
constant
follows
i,1
every
every
1,1
l′,1
206
dividing
log
letting
tend
inﬁnity
shows
let
diagonal
matrix
values
main
diagonal
component
processes
zero
variance
corresponding
row
column
f′x
zero
almost
everywhere
hence
almost
every
σl′
xl′
f′x
cid:20
πf′
x1/σ1
...
xl′
/σl′
cid:21
207
denotes
all-zero
matrix
appropriate
size
thus
rank
f′x
rank
x1/σ1
...
xl′
/σl′
almost
every
lemma
let
ℓ-variate
real-valued
gaussian
vector
mean
vector
covariance
matrix
let
ℓ-variate
vector
indepen-
dent
components
independently
uniformly
distributed
1/m
fwkgw
log
cid:18
cid:18
cid:19
cid:19
208
consequently
denoting
ax′
fw|x′=x′kgw|x′=x′
log
cid:16
mℓq
det
cw|x′
cid:17
w|x′
µw|x′=x′
214
µw|x′=x′
tc−1
z,1/m
µw|x′=x′
cw|x′
denote
conditional
mean
conditional
covariance
matrix
given
computed
23.7.4
µw|x′=x′
cwx′
c−1
cw|x′
cwx′
c−1
wx′
215
216
cwx′
denotes
cross-covariance
matrix
cx′
denote
covariance
matrices
respectively
deﬁning
since
independent
cross-covariance
matrix
equal
cross-covariance
matrix
bussgang
theorem
yields
kzj
ajkxj
deﬁned
hence
diagonal
matrix
main
diagonal
czx
λacx
209
get
acx′
cwx′
cwxa
hence
proof
23.6.14
cwx′
cwxa
czxa
λacxa
λaacx′
217
written
ax′
209
together
215
216
yields
ℓ′-dimensional
zero-mean
gaussian
vector
independent
components
whose
variances
nonzero
eigenvalues
matrix
satisfying
ata
iℓ′
use
data
processing
inequality
chain
rule
relative
entropy
fact
gaussian
obtain
fwkgw
x′kgw
fx′kgx′
fw|x′=x′kgw|x′=x′
fx′
dx′
fw|x′=x′kgw|x′=x′
fx′
dx′
210
denotes
pdf
gaussian
vector
mean
vector
covariance
matrix
fw|x′=x′
gw|x′=x′
fx′
gx′
211
212
evaluate
relative
entropy
rhs
210
ﬁrst
note
given
random
vector
uniformly
distributed
ℓ-dimensional
cube
length
since
obtained
via
209
conditional
pdf
given
fw|x′=x′
mℓ1
ax′
213
µw|x′=x′
λaax′
cw|x′
λacxλa
218
219
combining
218
209
using
triangle
inequal-
ity
upper-bound
component
µw|x′=x′
|wj
|zj
xj|
|uj
220
µj|
aj||xj
µj|
ﬁrst
third
term
rhs
220
upper-bounded
second
term
upper-bounded
lemma
get
term
|1−
aj|
upper-bounded
1/mq2/πσ2
variance
thus
obtain
µw|x′=x′k2
25ℓ
xj=1
221
next
note
since
since
independent
i.i.d
1/m
cw|x′
λacxλa
12m2
222
shown
λacxλa
conditional
covari-
ance
matrix
given
hence
positive
semideﬁnite.6
6indeed
czx
cwx
209
czx′
czxa
replacing
216
repeating
steps
leading
219
obtain
desired
result
follows
smallest
eigenvalue
cw|x′
bounded
second
term
rhs
214
lower-
12m2
together
221
yields
z,1/m
6mℓ+2
µw|x′=x′
tc−1
w|x′
µw|x′=x′
xj=1
25ℓ
223
75ℓ
xj=1
upper-bound
ﬁrst
term
rhs
214
use
222
combined
lemma
implies
every
diagonal
element
cw|x′
given
cid:2
cid:3
12m2
2σ2
12m2
cid:2
cid:3
term
rhs
224
negative
ﬁrst
224
second
term
upper-bounded
cid:2
cid:3
1/m2
hence
every
element
main
diagonal
cw|x′
upper-bounded
1+1/12
inequality
thus
follows
hadamard
log
cid:16
mℓq
det
cw|x′
cid:17
log
cid:18
cid:18
cid:19
cid:19
225
combining
223
225
214
210
yields
fwℓ
1kgwℓ
log
cid:18
cid:18
log
cid:18
cid:18
cid:18
75ℓ
cid:19
cid:19
cid:19
cid:19
xj=1
cid:19
cid:2
cid:3
226
completes
proof
appendix
spectral
distribution
function
let
stationary
l-variate
gaussian
process
mean
vector
sdf
let
deﬁned
respectively
every
pair
kninj
=kxixj
kzizj
kxizj
kzixj
227
theorem
bussgang
kxizj
kzj
ajkxixj
deﬁned
consequently
yields
kninj
kxixj
kzizj
ajkxixj
aikxj
kxixj
kzizj
228
since
sdf
fully
determined
covariance
structure
process
206
obtain
prove
namely
1/2
−1/2
dfni
note
229
1/2
since
|xi
t−zi
dfni
cid:2
cid:3
µi−
230
µi−e
claim
−1/2
follows
remains
prove
namely
ai|
πσ2
e−α2/2σ2
set
231
√2πσ2
furthermore
follows
√2πσ2
√2πσ2
ai|
√2πσ2
√2πσ2
πσ2
i+1
232
233
i+1
cid:18
cid:12
cid:12
cid:12
cid:12
i+1
cid:19
cid:12
cid:12
cid:12
cid:12
|dα
|dα
since
i/m|
1/m
i/m
yields
proves
concludes
proof
lemma
11.
appendix
proof
theorem
let
stationary
l-variate
complex-valued
process
matrix-valued
sdf
let
real
composite
process
ˆxt
deﬁned
ˆxt
ˆxt
obtained
stacking
real
part
top
imaginary
part
let
augmented
process
ˆzt
deﬁned
ˆzt
clearly
ˆxt
ˆzt
satisfy
ˆzt
ˆxt
cid:20
−ıil
cid:21
ıil
unitary
factor
i.e.
2il
matrix-valued
autocovariance
function
ˆzt
reads
cid:20
k∗z
k∗z
cid:21
234
235
236
237
proves
next
bound
difference
245
238
k−1
k−1
247
e−ı2πτ
θdfz
239
dividing
247
log
taking
ﬁrst
limit
limit
yields
denotes
pseudo-autocovariance
function
corresponding
matrix-valued
sdf
given
−f∗z
cid:21
−f∗
fˆz
cid:20
satisﬁes
1/2
−1/2
autocovariance
functions
sdfs
ˆxt
ˆzt
related
via
fˆz
240
241
theorem
deﬁnition
ˆxt
thus
follows
ˆxt
1/2
rank
f′ˆx
−1/2
242
since
left
right
multiplication
nonsingular
matrix
leaves
rank
unchanged
obtain
241
rank
f′ˆx
equal
rank
f′ˆz
furthermore
238
rank
f′ˆz
upper-bounded
rank
f′z
plus
rank
f′z
consequently
1/2
1/2
rank
f′z
rank
f′z
rank
f′z
−1/2
−1/2
243
second
step
follows
complex
conjugation
affect
rank
gaussian
242
holds
equality
theorem
10.
addition
proper
derivative
zero
almost
everywhere
hence
derivative
fˆz
becomes
block
diagonal
almost
everywhere
rank
equals
sum
ranks
diagonal
elements
conclude
proper
gaussian
243
holds
equality
proves
theorem
13.
appendix
appendix
section
proof
theorem
every
m|x
k−1
k−1
244
stationarity
conditioning
reduces
entropy
conditioned
note
stationarity
independent
k−1
log
lim
k→∞
lim
m→∞
lim
k→∞
lim
m→∞
k−1
log
248
concludes
proof
theorem
14.
proof
corollary
suppose
exists
nonnegative
x−n
249
ﬁrst
show
k−1
250
second
step
show
249
implies
lim
k→∞
lim
m→∞
log
251
together
250
demonstrates
thus
proving
corollary
15.
prove
250
use
chain
rule
stationarity
fact
conditioning
reduces
entropy
obtain
xℓ=1hh
ℓ−1
xℓ=1hh
k−1
ℓ−1
k−1
252
k−1
obtained
250
next
show
249
implies
251
indeed
lim
k→∞
lim
m→∞
lim
k→∞
lim
m→∞
log
x−n
log
−n+1
x−n
log
k−1
245
lim
k→∞
lim
m→∞
thus
dividing
244
log
taking
ﬁrst
limit
limit
yields
lim
k→∞
lim
m→∞
x1|x
246
lim
k→∞
lim
m→∞
x−n
log
−n+1
log
253
nonnegative
integer
satisfying
249
ﬁrst
inequality
follows
chain
rule
second
inequal-
ity
follows
data
processing
inequality
upper-
bounding
second
mutual
information
−n+1
ﬁrst
limit
rhs
253
zero
second
limit
rhs
−n+1
zero
−n+1
bounded
proves
assumption
253
written
limk→∞
lemma
251
concludes
proof
corollary
x−n
proof
lemma
since
gaussian
conditional
mean
given
xk−1
written
xk|x0
xk−1
αℓxk−ℓ
254
xℓ=1
coefﬁcients
αk.7
conditional
variance
thus
given
see
e.g.
sec
10.6
αℓxk−ℓ
xℓ=1
cid:12
cid:12
cid:12
cid:12
cid:12
xℓ=1
αℓe−ı2πℓθ
cid:12
cid:12
cid:12
cid:12
cid:12
function
dfx
255
acknowledgment
fruitful
discussions
amos
lapidoth
gratefully
acknowledged
authors
wish
thank
associate
editor
matthieu
bloch
anonymous
referees
valuable
comments
references
r´enyi
dimension
entropy
probability
distributions
acta
mathematica
hungarica
vol
1-2
193–215
mar
1959
kawabata
dembo
rate-distortion
dimension
sets
measures
ieee
trans
inf
theory
vol
1564–1572
sep.
1994
koch
shannon
lower
bound
asymptotically
tight
ieee
trans
inf
theory
vol
6155–6161
nov.
2016
verd´u
r´enyi
information
dimension
fundamental
limits
almost
lossless
analog
compression
ieee
trans
inf
theory
vol
3721–3748
aug.
2010
shamai
shitz
verd´u
information
dimension
degrees
freedom
interference
channel
ieee
trans
inf
theory
vol
256–279
jan.
2015
stotz
b¨olcskei
degrees
freedom
vector
interference
channels
ieee
trans
inf
theory
vol
4172–4197
jul
2016
jalali
poor
universal
compressed
sensing
almost
lossless
recovery
ieee
trans
inf
theory
vol
2933–
2953
may
2017
rezagah
jalali
erkip
poor
compression-based
compressed
sensing
ieee
trans
inf
theory
vol
6735–
6752
oct.
2017
gutman
´spiewak
metric
mean
dimension
analog
compression
arxiv:1812.00458
math.ds
dec.
2018
cover
thomas
elements
information
theory
1st
wiley
interscience
1991.
xℓ=1
αℓe−ı2πℓθ
−1/2
1/2
256
bradley
basic
properties
strong
mixing
conditions
survey
open
questions
probab
surveys
vol
107–144
2005
horn
johnson
matrix
analysis
2nd
cambridge
analytic
closed
interval
−1/2
1/2
hence
either
constant
ﬁnitely
many
zeros
−1/2
1/2
moreover
all-zero
function
argued
contradiction
indeed
suppose
exist
255
irrespective
words
ﬁnd
linear
estimator
perfectly
predicts
xk−1
irrespective
sdf
clearly
contradiction
since
even
best
predictor
yields
i.i.d.
zero-mean
variance-σ2
gaussian
process
i.e.
f′x
thus
set
ﬁnite
therefore
lebesgue
measure
zero
since
=zz
|2dfx
257
since
furthermore
dfx
258
implies
f′x
hence
set
harmonics
f′x
contained
proof
completed
monotonicity
measures
fact
lebesgue
measure
zero
7more
precisely
coefﬁcients
correspond
lmmse
estimator
estimating
xk−1
lmmse
estimator
always
exists
even
though
necessarily
unique
cambridge
university
press
2013
shannon
theory
compressed
sensing
ph.d.
dissertation
princeton
university
2011
gallager
information
theory
reliable
communication
john
wiley
sons
1968
wiener
masani
prediction
theory
multivariate
stochas-
tic
processes
acta
mathematica
vol
111–150
1957
priestley
spectral
analysis
time
series
elsevier
academic
press
2005
gray
entropy
information
theory
2nd
new
york
springer
2011
papoulis
pillai
probability
random
variables
stochas-
tic
processes
4th
new
york
mcgraw
hill
2002
grenander
szeg¨o
toeplitz
forms
applications
university
california
press
1958
verd´u
optimal
phase
transitions
compressed
sensing
ieee
trans
inf
theory
vol
6241–6263
oct.
2012
balakrishnan
note
sampling
principle
continuous
signals
ire
trans
inf
theory
vol
143–146
jun
1957
ash
dol´eans-dade
probability
measure
theory
2nd
elsevier/academic
press
2000
csisz´ar
extremum
problem
information
theory
stud
sci
math
hung.
vol
57–71
1974
lapidoth
asymptotic
capacity
stationary
gaussian
fading
channels
ieee
trans
inf
theory
vol
437–446
feb.
2005
foundation
digital
communication
cambridge
cambridge
university
press
2009
bussgang
crosscorrelation
functions
amplitude-distorted
gaus-
sian
signals
res
lab
electronics
m.i.t.
cambridge
mass.
tech
rep.
216
mar
1952
gihman
skorohod
theory
stochastic
processes
springer
verlag
1980
lundquist
barrett
rank
inequalities
positive
semideﬁnite
matrices
linear
algebra
applications
vol
248
91–100
1996
bernhard
geiger
07–m
14–sm
born
graz
austria
1984.
received
dipl.-ing
degree
electrical
engineering
distinction
dr.
techn
degree
electrical
information
engineering
distinction
graz
university
technology
austria
2009
2014
respectively
2009
joined
signal
processing
speech
communication
laboratory
graz
university
technology
project
assistant
took
position
research
teaching
associate
lab
2010.
senior
scientist
erwin
schr¨odinger
fellow
institute
communications
engineering
technical
university
munich
germany
2014
2017
postdoctoral
researcher
signal
processing
speech
communication
laboratory
graz
university
technology
austria
2017
2018.
currently
senior
researcher
know-center
gmbh
graz
austria
research
interests
cover
information
theory
machine
learning
information-theoretic
model
reduction
markov
chains
hidden
markov
models
tobias
koch
02–m
09–sm
visiting
professor
ram
´on
cajal
research
fellow
signal
theory
communications
department
universidad
carlos
iii
madrid
uc3m
received
m.sc
degree
electrical
engineering
distinction
2004
ph.d.
degree
electrical
engineering
2009
eth
zurich
switzerland
june
2010
may
2012
marie
curie
intra-european
research
fellow
university
cambridge
also
research
intern
bell
labs
murray
hill
usa
2004
universitat
pompeu
fabra
upf
barcelona
spain
2007.
joined
signal
processing
group
uc3m
june
2012.
research
interests
digital
communication
theory
information
theory
dr.
koch
received
starting
grant
european
research
council
erc
ram
´on
cajal
research
fellowship
marie
curie
intra-european
fellowship
marie
curie
career
integration
grant
fellowship
prospective
researchers
swiss
national
science
foundation
2013–2016
served
vice
chair
spain
chapter
ieee
information
theory
society
