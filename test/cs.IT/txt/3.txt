info-clustering
efﬁcient
algorithm
network
information
flow
chung
chan
ali
al-bashabsheh
qiaoqiao
zhou
abstract—motivated
fact
entities
social
network
biological
system
often
interact
exchanging
in-
formation
propose
efﬁcient
info-clustering
algorithm
group
entities
communities
using
parametric
max-ﬂow
algorithm
meaningful
special
case
info-clustering
paradigm
dependency
structure
graphical
learned
readily
data
introduction
info-clustering
proposed
application
network
information
theory
problem
clustering
machine
learning
regards
object
piece
in-
formation
namely
random
variable
groups
random
variables
sufﬁciently
large
amount
mutual
information
together
clustering
often
important
ﬁrst
step
studying
large
biological
system
human
connectome
genome
also
identify
communities
social
network
resources
allocated
efﬁciently
based
communities
discovered
since
entities
social
biological
system
often
possess
information
interact
transmission
information
clustering
mutual
information
intuitively
gives
meaningful
results
using
multivariate
mutual
information
mmi
similarity
measure
info-clustering
deﬁnes
hierar-
chy
clusters
possibly
different
sizes
different
levels
mutual
information
clustering
solution
intimately
related
principal
sequence
partitions
psp
submodular
function
namely
entropy
function
set
random
variables
clustered
follows
clustering
solution
unique
solvable
using
polynomial
number
oracle
calls
evaluate
entropy
function
however
general
learning
entropy
function
ﬁnite
set
random
variables
data
takes
chan
zhou
institute
network
coding
chinese
university
hong
kong
shenzhen
key
laboratory
network
coding
key
technology
application
china
shen-
zhen
research
institute
chinese
university
hong
kong
email
cchan
inc.cuhk.edu.hk
al-bashabsheh
institute
network
coding
chinese
university
hong
kong
big
data
brain
computing
bdbc
center
beihang
university
beijing
china
e-mail
entropyali
gmail.com
work
described
paper
supported
grant
university
grants
committee
hong
kong
special
administrative
region
china
project
aoe/e-02/08
supported
partially
grant
shenzhen
science
technology
innovation
committee
jsgg20160301170514984
chinese
university
hong
kong
shenzhen
china
work
chan
supported
part
vice-chancellor
one-off
discretionary
fund
chinese
university
hong
kong
project
nos
vcf2014030
vcf2015007
grant
university
grants
committee
hong
kong
special
administrative
region
china
project
14200714
exponential
time
size
computation
psp
algorithm
without
approximation
also
takes
calls
submodular
function
minimization
sfm
algorithm
turn
makes
oracle
calls
evaluate
submodular
function
hence
practicality
general
info-clustering
algorithm
limited
sample
complexity
computational
complexity
fortunately
info-clustering
reduces
faster
algorithms
un-
der
special
statistical
models
also
easier
compared
general
model
learn
data
instance
markov
tree
model
info-clustering
reduces
edge-
ﬁltering
procedure
runs
time
furthermore
procedure
coincides
existing
functional
genomic
clustering
method
mutual
information
relevance
networks
mirn
rediscovering
simple
clustering
algo-
rithm
markov
tree
simpliﬁcation
info-clustering
paradigm
provides
theoretical
justiﬁcation
mirn
algorithm
helps
discover
algorithm
may
fail
markov
tree
assumption
hold
example
work
propose
efﬁcient
info-clustering
al-
gorithm
different
graphical
model
called
pair-
wise
independent
network
pin
using
idea
matroidal
network
link
model
mmi
concrete
operational
meaning
maximum
network
broad-
cast
throughput
info-clustering
solution
therefore
identiﬁes
clusters
large
intra-cluster
communication
rates
naturally
maps
communities
closely
related
entities
social
network
learning
pin
model
simpliﬁes
learning
weights
edges
graph
social
network
weight
edge
simply
amount/rate
communication
edge
incident
nodes
shown
proposition
info-clustering
solution
pin
model
obtained
psp
cut
function
weighted
graph
well-known
faster
sfm
algorithms
possible
cut
function
using
min-cut
max-ﬂow
algorithms
e.g.
see
12–14
algorithm
given
computes
psp
efﬁciently
reducing
problem
parametric
max-ﬂow
problem
capacities
edges
certain
monotonic
functions
parameter
reduction
carefully
done
parametric
max-ﬂow
algorithm
compute
psp
|3p|e|
time
set
edges
non-
zero
weight
adapt
algorithm
compute
info-
clustering
solution
modify
improve
performance
source
weighted
graph
represents
pin
deﬁned
2.1a
broadcast
message
bits
capacities
deﬁned
2.1c
info-clustering
solution
2.11
fig
identifying
clusters
pin
2.1a
brute-force
search
subsets
satisfying
threshold
constraint
2.9
preliminaries
info-clustering
formulation
let
ﬁnite
index
set
objects
want
cluster
without
loss
generality
assume
idea
info-clustering
treat
object
random
variable
denoted
san
serif
font
taking
values
ﬁnite
set
denoted
usual
math
font
cluster
objects
according
mutual
information
pzv
denotes
distribution
entire
vector
random
variables
deﬁnition
2.1
deﬁnition
2.4
hypergraphi-
cal
source
w.r.t
hypergraph
edge
functions
iff
independent
hyper
edge
variables
2.2
weight
function
hypergraphical
source
deﬁned
support
2.3a
2.3b
pin
model
example
corresponding
hypergraph
graph
supp
cid:8
cid:9
illustrate
idea
info-clustering
via
simple
example
shown
fig
corresponding
random
variables
deﬁned
deﬁnition
2.2
pairwise
independent
network
pin
iff
hypergraphical
w.r.t
graph
edge
function
i.e.
self
loops
2.1a
mutual
information
among
multiple
random
variables
measured
multivariate
mutual
information
mmi
deﬁned
independent
uniformly
random
variables
entropies
2.1b
pin
deﬁnition
2.2
correlation
represented
weighted
triangle
shown
fig
characterized
weight
function
2.1c
vertex
set
edge
set
supp
2.1d
note
ease
comparison
graph
used
example
illustrate
algorithm
formal
deﬁnition
hyper-
graphical
source
model
follows
2.4a
2.4b
cid:21
min
p∈π′
|p|
cid:20
xc∈p
pzv
kqc∈p
pzc
set
partitions
least
non-
empty
disjoint
subsets
may
also
write
explicitly
zc1
zck
2.5
note
shannon
mutual
informa-
tion
special
case
bipartition
sometimes
convenient
expand
using
shannon
mutual
information
5.18
follows
zc1
k−1
xi=1
zci
zsk
j=i+1
2.6
example
random
vector
deﬁned
2.1a
2.7a
reduces
shanon
mutual
information
min
cid:8
2,3
1,3
1,2
cid:9
min
cid:8
z3∧z2
z1∧z3
min
cid:8
2+5
cid:9
2.7b
cid:9
applied
2.6
calculate
partition
singletons
average
value
cut
separates
node
nodes
value
z1∧z3
cut
separates
node
node
note
sequence
two
cuts
effectively
partitions
vertex
set
singletons
expansion
clear
partition
singletons
optimal
case
since
mutual
information
nodes
large
indeed
optimal
partition
turns
clustering
random
variable
correlated
groups
general
set
optimal
partitions
2.4a
denoted
form
semi-lattice
w.r.t
partial
order
cid:22
partitions
∃c′
2.8
inequality
unique
denotes
strict
ﬁnest/minimum
partition
referred
funda-
mental
partition
theorem
5.2
threshold
set
clusters
deﬁned
deﬁnition
maximal
|b|
2.9
maximal
used
denote
inclusion-wise
maxi-
mal
elements
i.e.
maximal
∃b′
2.10
proposition
2.1
theorem
i.e.
non-singleton
subsets
fundamental
partition
maximal
subsets
clusters
mmi
larger
entire
set
example
applying
deﬁnition
2.9
clusters
mmi
calculated
2.7
clustering
solution
info-clustering
solution
consists
two
clusters
shown
fig
different
intervals
threshold
subset
feasible
solution
satisﬁes
threshold
constraint
2.9
entire
set
also
satisﬁes
threshold
constraint
maximal
recall
2.7b
unique
optimal
partition
1,2,3
therefore
ﬁnest
optimal
partition
cid:8
cid:9
2.12
expected
proposition
2.1
non-singleton
element
possible
subset
mmi
larger
turns
computation
mmi
fundamental
partition
entire
info-clustering
solution
done
strongly
polynomial
time
principal
sequence
partition
psp
entropy
function
2.13
psp
general
mathematical
structure
combinatorial
optimization
deﬁned
submodular
function
precisely
reveal-valued
set
function
said
submodular
iff
2.14
entropy
function
particular
submodular
function
psp
submodular
function
characterization
solutions
following
min
p∈π
2.15a
referred
dilworth
truncation
partition
one
non-empty
disjoint
subsets
xc∈p
2.15b
2.15c
n.b.
2.4a
without
trivial
partition
i.e.
every
submodu-
larity
implies
exists
unique
ﬁnest/minimum
w.r.t
partial
order
2.8
optimal
partition
2.15a
denoted
characterized
γℓ+1
...
2.16a
integer
sequence
critical
values
2.16b
convenience
sequence
successively
ﬁner
partitions
2.16c
=
2.11
sequence
partitions
together
corresponding
critical
values
referred
psp
psp
entropy
function
2.13
characterizes
info-clustering
solution
follows
proposition
2.2
corollary
ﬁnite
set
size
random
vector
=hmin
ˆhγ
cid:15
namely
non-singleton
elements
ﬁnest
optimal
par-
tition
dilworth
truncation
2.15a
iii
information
flow
interpretation
pin
mmi
interpreted
maximum
broadcast
throughput
network
hence
info-
clustering
reduces
clustering
network
information
ﬂow
applied
clustering
social
network
identify
communities
naturally
based
amount
information
ﬂow
precisely
treating
edge
undirected
com-
munication
link
capacity
total
bit
communicated
node
node
total
bits
communicated
node
seen
every
pair
distinct
nodes
broadcast
throughput
given
mmi
2.7a
fig
illustrates
bits
information
broadcast
entire
network
achieving
mmi
entire
set
random
variables
2.7b
interpretation
mmi
information
ﬂow
cluster
threshold
therefore
maximal
subnetwork
broadcast
throughput
larger
instance
cluster
1,2,3
subset
nodes
induced
subnetwork
throughput
exceeding
specializing
hyper-
graphical
model
shown
proposition
clustering
solutions
obtained
directly
non-singleton
subsets
psp
incut
function
precisely
weighted
graph
vertex
set
capacity
function
deﬁne
every
pair
vertices
otherwise
3.1
deﬁnes
capacity
function
weighted
digraph
vertex
set
edge
set
deﬁned
set
arcs
positive
capacity
example
fig
weighted
digraph
obtained
orienting
weighted
graph
fig
according
3.1
i.e.
directing
edge
incident
node
smaller
label
incident
node
larger
one
convenience
also
write
arbitrary
subsets
xv∈b1
xw∈b2
xw∈b1
3.2a
3.2b
3.2c
incut
function
weighted
digraph
deﬁned
3.3
2.17
incut
function
digraph
fig
shown
fig
calculated
3.4
compute
psp
ﬁrst
evaluate
2.15b
different
partitions
follows
3.5a
similarly
3.5b
fig
plots
dilworth
truncation
ˆgγ
2.15
minimum
partitions
seen
given
linear
integer
slope
−|p|
−|v
ˆgγ
piecewise
linear
consisting
example
line
segments
highlighted
blue
|−1
break
points
highlighted
red
general
number
line
segments
ﬁnest
optimal
partition
value


3.6
psp
corresponding
critical
values
annotated
fig
clustering
using
parametric
max-flow
proposition
psp
incut
function
digraph
coincides
psp
cut
function
divided
corresponding
undirected
graph
shown
com-
putable
running
parametric
max-ﬂow
algorithm
times
parametric
max-ﬂow
algorithm
introduced
runs
|2p|e|
times
using
well-known
ˆgγ
weighted
digraph
orienting
3.1
non-zero
values
incut
function
3.4
psp
3.6
dilworth
truncation
3.5
fig
computing
clusters
pin
model
2.1a
non-singleton
elements
psp
2.16
push-relable/preﬂow
algorithm
implemented
highest-level
selection
rule
hence
info-clustering
algorithm
solution
pin
model
obtained
|3p|e|
time
section
adapt
improve
algorithm
compute
desired
psp
info-clustering
solution
algorithm
illustrated
using
example
last
section
chosen
example
ease
comparison
ﬁrst
give
procedure
algorithm
computing
minimum
minimizer
2.15
submodular
function
assuming
parametric
submodular
function
minimizer
procedure
specialized
pin
model
chosen
incut
func-
tion
3.3
parametric
max-ﬂow
algorithm
applied
instead
consider
example
deﬁned
3.4
illustrated
fig
deﬁned
2.15c
line
initializes
xγ,1
xγ,1
4.2
4.1
becomes
min
xγ,1
min
algorithm
computing
psp
parametric
sfm
input
submodular
function
deﬁned
ﬁnite
ground
set
output
function
deﬁned
2.16a
xγ,1
set
inclusion-wise
minimum
minimizer
min
j∈b
4.1
=pi∈c
convenience
remove
every
intersects
add
max
min
gmax
end
end
end
piecewise
linear
function
plotted
fig
minimum
minimizer
therefore
given
=

4.3
=
cid:9
cid:8
cid:8
cid:9

4.4
initialized
line
line
4–5
update
next
initialized
line
min
1,2
:2∈b
loop
min
1,2,3
:3∈b
max
max
loop
update
desired
solution
3.6
characterized
psp
procedure
said
parametric
since
solution
computed
possible
values
rather
particular
value
realize
procedure
charac-
terization
2.16a
psp
2.16c
corresponding
critical
values
2.16b
computed
instead
minimization
4.1
submodular
function
mini-
mization
sfm
lattice
family
contrast
perform
4.1
growing
set
instead
entire
set
every
loop
idea
follows
algorithm
computing
dilworth
truncation
one
given
contrast
however
follow
consider
update
rule
lines
8–9
guarantees
piecewise
linear
one
break
point
possibly
ﬁnite
step
line
gives
always
contain
see
4.1
linear
without
breakpoint
pointed
update
rule
particularly
useful
parametric
procedure
since
complexity
often
grows
number
break
points
specializing
pin
models
incut
function
deﬁned
3.3
parametric
sfm
4.1
solved
parametric
min-cut
problem
shown
algorithm
algorithm
computing
parametric
sfm
4.1
parametric
min-cut
input
weighted
digraph
vertex
set
capacity
function
satisfying
3.1
output
minimum
minimizer
4.1
deﬁned
incut
function
3.3
deﬁne
weighted
digraph
vertex
set
fig
illustration
computation
psp
algorithm
subsequent
steps
following
line
give
max
xγ,1
gmax
γ,1
max
xγ,2
gmax
4.5a
4.5b
4.5c
4.5d
similarly
function
minimum
4.1
plotted
fig
follows
minimum
minimizer


4.6
given
4.4
illustrated
fig
lines
new
node
outside
capacity
function
initialized
max
−xγ
max
end
compute
minimum
minimizer
min
⊆u\
j∈t
4.7
minimum
s–j
cut
value
current
example
digraph
shown
fig
vertex
set
for-loop
sets
initializes
capacity
|−xγ,1|+
|xγ,1|+
|x|+
max
4.8
evaluating
capacities
initial
value
xγ,1
4.2
gives
non-decreasing
non-increasing
piecewise
linear
functions
respectively
shown
ﬁgure
|−xγ,1|+
|xγ,1|+
|−xγ,1|+
|xγ,1|+
weighted
digraph
construction
minimum
s–2
cut
fig
illustration
parametric
min-cut
problem
algorithm
for-loop
|−xγ,1|+
|−xγ,2|+
|−xγ,1|+
|−xγ,2|+
|xγ,2|+
|xγ,2|+
weighted
digraph
construction
s–3
max-ﬂow
min-cut
fig
illustration
parametric
min-cut
problem
algorithm
for-loop
similarly
digraph
shown
fig
vertex
set
instead
xγ,1
xγ,2
updated
functions
4.5
weighted
digraph
viewed
result
processing
weighted
digraph
follow
zero
default
contrast
additional
node
contraction
edge
removal
steps
reduce
number
vertices
therefore
complexity
solving
4.7
augment
digraph
two
new
nodes
outside
capacity
set
inﬁnity
add
arc
capacity
−xγ
else
add
arc
capacity
remove
outgoing
arcs
node
contract
node
node
remove
incoming
arcs
nodes
...
contract
nodes
node
4.7
procedure
illustrated
fig
fig
respectively
red
dotted
arcs
removed
nodes
circled
together
blue
lines
contracted
step
implies
formula
line
directly
step
gives
max
−xγ
xu∈v
reduces
formula
line
last
summation
zero
assumption
3.1
arcs
point
node
smaller
label
node
larger
label
reason
line
need
set
4.7
solved
parametric
max-
times
preﬂow
algorithm
implemented
highest
ﬂow
algorithm
j3p|e|
time
invoking
level
selection
rule
turn
runs
j2p|e|
times
procedure
described
algorithm
returns
characterization
solution
4.7
ℓ+1
...
4.9
integer
′+1
note
also
sufﬁciently
large
′+1
solve
4.7
ﬁxed
assume
following
subroutine
maxflow
4.11
takes
arguments
capacity
function
ﬁxed
parametric
vertex
set
deﬁned
source
node
sink
node
valid
preﬂow
associated
weighted
digraph
deﬁned
previous
arguments
returns
maximum
¯s–¯t
ﬂow
algorithm
solving
parametric
min-cut
problem
4.7
using
parametric
max-ﬂow
algorithm
input
weighted
digraph
vertex
set
give
max
capacity
function
created
algorithm
output
list
containing
characterizes
4.7
4.9
return
create
empty
lists
maxv∈
j−1
min
max
end
set
zero
ﬂow
maxflow
cγ−
add
empty
withdraw
element
compute
solution
4.10
deﬁne
weighted
digraph
¯dj
vertex
set
capacity
function
initialized
end
←pu∈s
c¯γ
←pw∈t
c¯γ
←pw∈t
min
ensure
anti-symmetry
end
maxflow
add
end
add
end
inclusion-wise
minimum
set
solves
min
¯t∈t
4.12
referred
minimum
¯s–¯t
cut
last
equality
initialized
line
algorithm
since
case
algorithm
returns
line
list
gives
desired
4.3
fig
shows
digraph
seen
solutions
minimization
4.7
speciﬁcally
interval
must
contain
critical
values
changes
critical
values
computed
iteratively
preﬂow
algorithm
maxflow
4.11
lines
applied
digraph
¯dj
capacities
derived
lines
15–17
evaluated
value
satisfying
4.10
either
resolves
entire
interval
case
solution
updated
line
reduces
problem
two
smaller
subproblems
later
processing
i.e.
original
interval
replaced
two
smaller
intervals
line
illustrate
procedure
consider
i.e.
shown
fig
input
algorithm
lines
2–3
give
max
max
min
max
max
min
max
max
used
values
4.5
since
case
line
skipped
line
invokes
preﬂow
algorithm
graph
shown
fig
min-cut
vertex
set
construction
max-ﬂow
4.13
otherwise
note
second
line
equations
ensures
anti-symmetry
property
ﬂow
function
i.e.
4.14
roughly
speaking
line
value
4.7
i.e.
constant
similarly
line
value
4.7
constant
one
critical
value
changes
illustrate
consider
i.e.
shown
fig
input
algorithm
lines
pairs
distinct
nodes
ﬂow
along
arc
indicated
fig
parentheses
next
corresponding
capacity
arc
tuple
added
line
retrieved
deleted
subsequently
inside
while-loop
line
line
l.h.s
4.10
3.5
2.5
3.5
3.5
3.5
2.5
¯d3
3.5
¯d3
3.5
3.5
¯d3
fig
illustration
parametric
max-ﬂow
algorithm
algorithm
given
see
fig
r.h.s
4.10
given
computed
solution
4.10
namely
3.5.
general
value
must
exist
unique
optimal
solutions
4.7
respectively
computation
time
since
sides
equations
piecewise
linear
break
points
new
weighted
digraph
¯dj
capacity
function
assigned
ﬁrst
for-loop
lines
15–17
obtained
setting
contracting
source
node
contracting
sink
node
removing
incoming
arcs
outgoing
arcs
second
for-loop
turns
valid
preﬂow
¯dj
recall
current
example
3.5
last
execution
algorithm
fig
shows
two
digraphs
top
one
digraph
3.5
bottom
one
new
weighted
digraph
¯d3
sets
ﬂow
indicated
top
digraph
3.5
satisfy
4.10
¯d3
annotated
max-ﬂow
min-cut
computed
line
23.
note
since
line
skipped
instead
line
adds
following
two
tuples
list
becomes
3.5
3.5
4.15
repeating
while-loop
ﬁrst
element
retrieved
shown
4.10
solved
value
similar
fig
fig
shows
digraph
top
¯d3
bottom
veriﬁed
satisﬁes
4.10
top
graph
min-cut
bottom
graph
since
case
new
element
added
line
25.
finally
repeating
while-loop
last
element
retrived
4.15
shown
fig
gives
min-cut
case
new
element
added
line
25.
since
empty
algorithm
terminates
gives
desired
4.6
yields
de-
sired
psp
3.6
therefore
info-clustering
solution
2.11
pin
model
2.1a
conclusion
adapted
parametric
max-ﬂow
algorithm
computing
psp
info-clustering
algorithm
clusters
graphical
network
based
information
ﬂow
edges
overall
running
time
|3p|e|
size
network
|e|
number
edges
communication
link
algorithm
simpliﬁes
general
info-clustering
algorithm
orders
magnitude
applicable
systems
social
networks
similarity
measured
mutual
information
implement
algorithm
large-scale
social
network
preﬂow
algorithm
may
made
distributive
adaptive
servers
may
deployed
different
parts
network
measure
store
information
exchange
rates
different
schrijver
combinatorial
optimization
polyhedra
efﬁciency
springer
2002
goldberg
efﬁcient
graph
algorithms
sequential
parallel
computers
ph.d.
dissertation
massachusetts
institute
technology
dept
electrical
engineering
computer
science
1987
goldberg
tarjan
new
approach
maximum-
ﬂow
problem
journal
acm
jacm
vol
921–940
1988
cherkassky
goldberg
implementing
pushrelabel
method
maximum
ﬂow
problem
algorithmica
vol
390–410
1997.
pair
nodes
push
relabel
operations
preﬂow
algorithm
done
locally
servers
ﬁrst
communicated
servers
necessary
preﬂow
network
may
stored
conjunction
clustering
solution
clusters
updated
incrementally
time
based
changes
information
exchange
rates
allocation
servers
resources
may
also
adapted
clustering
solution
instance
intra-cluster
communication
frequent
inter-cluster
communica-
tion
nodes
cluster
larger
mutual
information
may
assigned
server
changes
network
updated
frequently
without
much
communication
overhead
among
servers
references
chan
al-bashabsheh
zhou
kaced
liu
info-
clustering
mathematical
theory
data
clustering
ieee
transac-
tions
molecular
biological
multi-scale
communications
vol
64–91
june
2016
chan
al-bashabsheh
ebrahimi
kaced
liu
multivariate
mutual
information
inspired
secret-key
agreement
proceedings
ieee
vol
103
1883–1913
oct
2015.
lattice
partitions
submodular
function
linear
algebra
applications
vol
144
179
216
1990
narayanan
principal
yang
minimax
rates
entropy
estimation
large
alphabets
via
best
polynomial
approximation
ieee
trans
inf
theory
vol
3702–3720
2016
nagano
kawahara
iwata
minimum
average
cost
clus-
tering.
nips
lafferty
williams
shawe-taylor
zemel
culotta
eds
curran
associates
inc.
2010
1759–1767
chan
liu
clustering
random
variables
multivariate
mutual
information
chow-liu
tree
approximations
fifty-third
annual
allerton
conference
communication
control
comput-
ing
allerton
retreat
center
monticello
illinois
sep.
2015
butte
kohane
mutual
information
relevance
networks
functional
genomic
clustering
using
pairwise
entropy
measurements
pac
symp
biocomput
vol
2000
418–429
nitinawarat
barg
narayan
reznik
secret
key
generation
pairwise
independent
network
model
ieee
trans
inf
theory
vol
6482–6489
dec
2010
nitinawarat
narayan
perfect
omniscience
perfect
secrecy
steiner
tree
packing
ieee
trans
inf
theory
vol
6490–6500
dec.
2010
chan
matroidal
undirected
network
information
theory
pro-
ceedings
isit
2012
ieee
international
symposium
july
2012
1498–1502
hidden
ﬂow
information
proc
ieee
int
symp
inf
theory
st.
petersburg
russia
jul
2011
fujishige
iwata
minimizing
submodular
function
arising
concave
function
discrete
applied
mathematics
vol
211–215
1999
queyranne
minimizing
symmetric
submodular
functions
mathe-
matical
programming
vol
1-2
3–12
1998
jegelka
lin
bilmes
fast
approximate
submodular
minimization
advances
neural
information
processing
systems
2011
460–468
kolmogorov
faster
algorithm
computing
principal
se-
quence
partitions
graph
algorithmica
vol
394–
412
2010
gallo
grigoriadis
tarjan
fast
parametric
max-
imum
ﬂow
algorithm
applications
siam
journal
computing
vol
30–55
1989
chan
zheng
mutual
dependence
secret
key
agreement
proceedings
44th
annual
conference
information
sciences
systems
2010
fujishige
polymatroidal
dependence
structure
set
random
variables
information
control
vol
1978
