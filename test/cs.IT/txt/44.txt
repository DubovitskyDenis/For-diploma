partial
entropy
decomposition
decomposing
multivariate
entropy
mutual
information
via
pointwise
common
surprisal
robin
ince
institute
neuroscience
psychology
university
glasgow
robin.ince
glasgow.ac.uk
obtaining
meaningful
quantitative
descriptions
statistical
dependence
within
multivariate
systems
difﬁcult
open
problem
recently
partial
information
decomposition
pid
proposed
decompose
mutual
information
target
variable
components
redundant
unique
synergistic
within
different
subsets
predictor
variables
propose
apply
el-
egant
formalism
pid
multivariate
entropy
resulting
partial
entropy
decomposition
ped
implement
ped
entropy
redundancy
measure
based
pointwise
common
surprisal
natural
deﬁnition
closely
related
deﬁnition
show
approach
reveal
dyadic
triadic
generative
structure
multivariate
systems
indistinguishable
classical
shannon
measures
entropy
perspective
also
shows
misinformation
syner-
gistic
entropy
hence
includes
redundant
synergistic
effects
show
relationships
ped
two
predictors
derive
two
alternative
information
decompositions
illustrate
several
example
systems
reveals
entropy
terms
univariate
predictor
proper
subset
joint
suggest
previously
unrecognised
fact
explains
part
obtaining
consistent
pid
proven
difﬁcult
ped
also
allows
separate
quantiﬁcation
mechanistic
redundancy
related
function
system
versus
source
redundancy
arising
dependencies
inputs
important
distinction
existing
methods
address
new
perspective
provided
ped
helps
clarify
difﬁculties
encountered
pid
approach
resulting
decompositions
provide
useful
tools
practical
data
analysis
across
wide
range
application
areas
introduction
information
theory
originally
developed
tool
study
man-made
communication
systems
shannon
1948
cover
thomas
1991
however
also
provides
general
unifying
framework
many
types
statistical
analysis
recently
suggested
information
theoretic
measures
capable
fully
describing
dependency
structure
within
multivariate
systems
meaningful
way
james
crutchﬁeld
2016
therefore
measures
inadequate
discovering
intrinsic
causal
relations
james
crutchﬁeld
2016
important
problem
study
complex
systems
well
many
areas
experimental
sciences
neuroscience
systems
biology
etc.
james
crutchﬁeld
2016
introduce
two
simple
example
systems
equal
values
classical
information
theoretic
measures
i.e
equal
i-diagrams
yeung
1991
completely
different
generative
structure
show
existing
measures
distinguish
two
systems
even
clearly
reﬂect
different
generative
structures
challenge
pose
allocate
bits
entropy
two
systems
way
meaningfully
reﬂects
different
structures
dependency
suggest
requires
fundamentally
new
measure
propose
new
measure
believe
answers
challenge
james
crutchﬁeld
2016
williams
beer
2010
developed
elegant
mathematical
framework
called
partial
information
decomposition
pid
pid
decomposes
mutual
information
target
variable
multivariate
set
predictor
variables
set
terms
quantifying
information
arises
uniquely
redundantly
synergistically
within
different
subsets
predictors
timme
2013
propose
applying
framework
pid
directly
multivariate
entropy
rather
multivariate
mutual
information
chosen
target
variable
concepts
redundancy
shared
information
synergy
additional
information
arising
variables
considered
together
applied
directly
entropy
redundant
entropy
uncertainty
common
set
variables
synergistic
entropy
additional
uncertainty
arises
variables
combined
develop
consequences
partial
entropy
decomposition
ped
implementing
entropy
redundancy
measure
based
pointwise
common
surprisal
ince
2016
show
perspective
provides
new
insights
negative
local
information
values
misinfor-
mation
wibral
lizier
priesemann
2014
terms
quantify
synergistic
entropy
also
sheds
light
difﬁculties
around
obtaining
consistent
pid
even
axioms
required
meaningful
information
redundancy
measure
difﬁcult
pin
currently
debated
harder
2013
bertschinger
rauh
olbrich
jost
2014
grifﬁth
koch
2014
grifﬁth
chong
2014
rauh
bertschinger
2014
ince
2016
bertschinger
rauh
olbrich
jost
2013
olbrich
2015
grifﬁth
koch
2014
rauh
banerjee
2017
banerjee
2014
ﬁrst
brieﬂy
review
partial
information
decomposition
introduce
partial
entropy
decomposition
proposed
entropy
redundancy
measure
explore
relationships
ped
mutual
information
use
derive
two
different
decompositions
multivariate
mutual
information
show
ped
meaningfully
quantify
example
systems
james
crutchﬁeld
2016
compare
new
information
decompositions
existing
pid
approaches
range
examples
background
2.1
partial
information
decomposition
williams
beer
2010
propose
non-negative
decomposition
mutual
information
conveyed
set
predictor
variables
target
variable
reduce
total
multivariate
mutual
information
number
atoms
representing
unique
redundant
synergistic
information
subsets
consider
subsets
denoted
termed
sources
show
redundancy
structure
multi-variate
information
determined
collection
sets
sources
source
superset
formally
set
antichains
lattice
formed
power
set
set
inclusion
denoted
together
natural
ordering
deﬁnes
information
redundancy
lattice
crampton
loizou
2001
node
lattice
represents
partial
information
atom
value
given
partial
information
function
multiple
sources
indicates
redundancy
common
information
sources
figure
shows
structure
lattice
node
deﬁned
set
sources
sources
multivariate
indicated
braces
drop
explicit
notation
corresponds
source
containing
variable
node
contains
represents
information
common
shared
information
redundancy
value
node
lattice
measures
total
information
provided
node
partial
information
function
measures
unique
information
contributed
node
redundant
synergistic
unique
information
within
subsets
variables
information
redundancy
node
joint
mutual
information
partial
information
value
node
represents
synergy
information
arises
considered
together
value
node
determined
via
recursive
relationship
möbius
inverse
information
redundancy
values
lattice
cid:88
cid:22
set
sources
set
input
variables
deﬁning
node
question
note
two
variable
pid
bivariate
mutual
information
values
related
partial
information
terms
follows
thus
bivariate
mutual
information
split
four
terms
representing
contribution
shared
variables
unique
variable
synergistic
two
similarly
individual
mutual
information
values
decomposed
sum
shared
unique
contributions
figure
redundancy
lattice
two
variables
three
variables
modiﬁed
williams
beer
2010
partial
entropy
decomposition
apply
mathematical
framework
pid
williams
beer
2010
directly
multivariate
entropy
obtain
partial
entropy
decomposition
ped
consider
lattices
shown
figure
terms
pid
target
variable
represent
entropy
shared
redundant
unique
synergistic
variables
denote
overall
entropy
node
lattice
measures
total
entropy
provided
node
deﬁne
partial
entropy
function
deﬁned
analogously
via
möbius
inversion
measures
unique
entropy
contributed
node
redundant
synergistic
unique
entropy
within
subsets
variables
two
variable
case
redundancy
function
used
set
sources
denoted
following
notation
williams
beer
2010
ince
2016
nodes
lattice
entropy
redundancy
partial
entropy
values
given
table
direct
analogy
pid
123
node
label
redundancy
function
partial
entropy
-h∩
+h∩
represented
atom
entropy
available
together
synergy
unique
entropy
unique
entropy
entropy
shared
table
full
ped
two-variable
case
inserting
partial
entropy
values
deﬁnition
mutual
information
see
mutual
information
redundant
shared
entropy
minus
synergistic
entropy
therefore
way
interaction
information
conﬂates
redundant
synergistic
information
mutual
information
combines
redundant
synergistic
entropy
suggest
important
shift
perspective
helps
clarify
many
difﬁculties
observed
deﬁning
multivariate
redundancy
measures
mutual
information
obtaining
consistent
pids
fact
suggests
direct
interpretation
negative
local
pointwise
information
terms
called
misinformation
wibral
lizier
priesemann
2014
partial
entropy
terms
depend
particular
entropy
redundancy
measure
used
suggests
positive
local
terms
correspond
redundant
entropy
negative
local
misinformation
terms
correspond
synergistic
entropy
explicitly
case
common
surprisal
entropy
redundancy
measure
propose
therefore
misinformation
synergistic
entropy
makes
intuitive
sense
misinformation
means
observer
less
likely
see
two
particular
values
together
would
expected
independent
alternatively
increase
local
surprisal
observing
variables
together
versus
observing
independently
therefore
local
term
contributes
increase
entropy
variables
observed
together
deﬁnition
synergistic
entropy
even
deﬁning
entropy
redundancy
function
perspective
give
insights
problems
faced
deﬁning
axioms
information
redundancy
measure
satisfy
example
identity
property
proposed
one
axiom
harder
2013
considering
ped
see
synergistic
entropy
feature
right
hand
side
mutual
information
term
however
appear
term
left
hand
side
since
redundancy
include
synergistic
entropy
occurs
observed
together
suggests
identity
axiom
instead
deﬁned
3.1
measuring
shared
entropy
common
surprisal
following
four
axioms
suggested
information
redundancy
measure
williams
beer
2010
harder
2013
ince
2016
propose
considered
also
entropy
redundancy
measure
symmetry
self
redundancy
symmetric
respect
subset
equality
ak−1
ak−1
ak−1
monotonicity
ak−1
ak−1
note
separated
subset
equality
monotonicity
axiom
ince
2016
since
subset
equality
condition
required
redundancy
lattice
complete
description
dependency
structure
system
independent
whether
measure
monotonic
lattice
many
information
theoretic
quantities
formulated
expectation
multivariate
distribution
particular
functional
terms
expectation
taken
value
functional
speciﬁc
values
considered
variable
called
local
pointwise
terms
wibral
lizier
vögler
2014
lizier
2008
wibral
lizier
priesemann
2014
van
cruys
2011
church
hanks
1990
example
entropy
deﬁned
expectation
surprisal
cid:88
x∈x
surprisal
log
local
pointwise
functional
corresponding
entropy
measure
ince
2016
present
information
redundancy
measure
based
pointwise
common
change
surprisal
since
local
information
change
surprisal
apply
principles
deﬁne
measure
entropy
redundancy
based
pointwise
common
surprisal
deﬁne
common
surprisal
hcs
sum
positive
pointwise
co-information
values
bell
2003
matsuda
2000
jakulin
bratko
2003
timme
2013
ince
2016
motivated
iccs
terms
interaction
information
mcgill
1954
use
co-information
co-information
interaction
information
equivalent
magnitude
opposite
signs
odd
numbers
variables
context
well
known
set-theoretical
interpretation
information
theoretic
quantities
measures
quantify
area
sets
visualised
venn
diagrams
reza
1961
co-information
derived
set
intersection
multiple
entropy
values
set
theoretic
interpretation
also
applied
local
level
local
co-information
measures
set-theoretic
overlap
multiple
local
entropy
values
hence
common
shared
surprisal
local
co-information
negative
since
local
informations
intersection
calculated
positive
case
overlap
note
information
measure
iccs
care
taken
regarding
different
signs
change
surprisals
local
information
values
overlap
calculated
input
surprisal
value
always
positive
redundancy
function
want
measure
positive
overlap
simply
ignore
negative
values
hcs
cid:88
hcs
...
hcs
max
local
co-information
deﬁned
matsuda
2000
cid:88
k+1
cid:88
···
log2
surprisal
local
entropy
k=1
hai1
aik
two
variables
see
suggested
deﬁnition
provides
redundant
entropy
exactly
sum
positive
local
mutual
information
terms
synergistic
entropy
sum
negative
local
mutual
information
misinformation
terms
three
variable
redundancy
deﬁnition
co-information
requires
joint
distribution
three
sources
analogous
arguments
information
redundancy
measure
depend
pairwise
target-source
marginals
ince
2016
bertschinger
rauh
olbrich
jost
2014
suggest
three
way
shared
entropy
depend
pairwise
marginals
hence
use
maximum
entropy
distribution
preserves
two-way
marginals
symmetry
self-redundancy
axioms
satisﬁed
properties
co-information
matsuda
2000
subset-equality
also
satisﬁed
al−1
consider
values
al−1
al−1
al−1
al−1
al−1
al−1
ai1
al−1
al−1
al−1
al−1
cid:54
al−1
ai1
otherwise
···
non-zero
terms
ai1
al−1
ai1
therefore
terms
include
al−1
cancel
corresponding
order
term
including
al−1
al−1
subset
equality
holds
however
see
examples
monotonicity
satisﬁed
hcs
continuous
continuity
surprisal
linear
combination
surprisal
values
thresholded
therefore
continuous
differentiable
3.2
relating
variable
ped
variable
peds
consider
effect
marginalising
variable
three
variable
system
obtain
two
variable
system
marginalising
away
variable
two
variable
ped
terms
sum
three
variable
terms
including
sources
equality
relationships
detailed
table
illustrated
colored
terms
figure
relationships
noted
information
lattices
chicharro
panzeri
2017
distinguish
different
partial
entropy
values
denote
space
ped
considered
superscript
commonly
marginalised
ped
matter
partial
entropy
h123
available
synergistically
removed
unique
entropy
relations
come
structure
lattice
met
h12
consistent
entropy
redundancy
function
marginalised
term
h12
h12
h12
h12
variable
terms
h123
h123
h123
h123
h123
h123
h123
h123
h123
h123
h123
h123
h123
table
marginalising
variable
also
consider
two
variable
pid
obtained
combining
single
variable
equality
relationships
detailed
table
illustrated
colored
terms
figure
example
although
variable
appears
term
h123
figure
relations
variables
marginalised
variable
entropy
redundancy
lattices
figure
relations
entropy
redundancy
lattice
three
variables
considered
separately
bivariate
lattice
considered
single
combined
variable
123
123
123
quantiﬁes
entropy
available
synergistically
synergistic
combination
h123
also
context
variables
combined
means
partial
entropy
shared
since
arises
synergistically
combined
available
three
variables
considered
together
uniquely
since
variable
alone
already
contains
therefore
belongs
term
marginalised
term
variable
terms
123
h123
123
h123
h123
h123
h123
h123
h123
h123
h123
h123
h123
h123
h123
h123
h123
h123
h123
h123
table
combining
variables
3.3
mutual
information
ped
terms
partial
information
decomposition
considers
information
target
variable
two
predictor
variables
starting
simplest
case
two
predictor
variables
involves
three
variable
system
table
see
h13
h13
h123
h123
h123
h123
h123
h123
h123
similarly
terms
three
variable
space
drop
superscript
similarly
table
123
conditional
mutual
information
also
written
terms
partial
entropy
values
x3|x2
123
3.4
pid
ped
previous
section
obtained
expressions
bivariate
mutual
informations
involved
two
variable
partial
information
lattice
terms
three
variable
partial
entropy
terms
variable
target
directly
relate
two
univariate
mutual
information
partial
entropy
equations
eqs
separating
terms
common
two
suggests
following
partial
information
decomposition
subtracting
sum
terms
joint
mutual
information
yields
following
synergy
term
123
results
valid
pid
satisﬁes
eqs
however
produce
non-
negative
decomposition
hcs
measure
proposed
previously
noted
ince
2016
possible
variable
convey
unique
misinformation
see
example
imperfectrdn
section
4.4
partial
entropy
terms
subtracted
represent
misinformation
since
include
synergistic
entropy
predictor
target
variable
derive
pid
relations
two
three
variable
entropy
redundancy
lattices
also
possible
arrive
result
simply
considering
interpretation
partial
entropy
term
directly
using
insight
synergistic
entropy
misinformation
see
example
represents
unique
information
unique
misinformation
subtracted
unique
partial
information
term
clear
represents
redundant
information
redundant
entropy
shared
three
variables
represents
synergistic
information
synergistic
entropy
redundant
therefore
informative
misinformation
synergistic
entropy
common
subtracted
redundant
partial
information
term
however
terms
clear
interpretations
number
partial
entropy
terms
ambiguous
appear
twice
pid
opposite
signs
indeed
closer
consideration
shows
indeed
quantify
entropy
inherently
ambiguous
example
quantiﬁes
unique
misinformation
context
marginalized
also
quantiﬁes
synergistic
information
context
pid
since
quantiﬁes
redundant
entropy
appear
two
redundant
source
terms
available
also
considered
available
term
quantiﬁes
entropy
gained
addition
overlaps
entropy
however
considered
combined
variable
table
term
unique
entropy
since
fully
available
already
observing
similarly
unique
misinformation
context
well
synergistic
information
context
pid
synergistic
information
context
redundant
misinformation
context
quantiﬁes
redundant
synergistic
entropy
terms
3.5
alternative
pid
ignoring
ambiguous
partial
entropy
terms
presence
repeated
ambiguous
entropy
terms
partial
entropy
derived
pid
described
previous
section
together
suggests
alternative
decomposition
mutual
information
follows
since
term
admits
unambiguous
interpretation
deﬁne
123
since
approach
obtained
omitting
unambiguous
terms
call
decomposition
monosemous
pid
ped
note
provides
decomposition
joint
mutual
information
redundant
unique
synergistic
components
satisfy
univariate
mutual
information
general
obtained
sum
unique
redundant
information
univariate
mutual
informations
includes
partial
entropy
terms
appear
joint
mutual
information1
therefore
although
approach
preserve
one
main
properties
pid
suggest
nevertheless
interesting
potentially
useful
first
entropy
decomposition
perspective
shines
light
origin
difﬁculties
encountered
trying
obtain
consistent
pids
second
show
examples
alternative
approach
interesting
properties
example
provides
qualitatively
different
decompositions
existing
measures
simple
binary
operations
decompositions
seem
give
better
intuitive
match
functional
differences
examples
section
4.2
3.6
identity
axiom
ped
based
pids
canonical
example
illustrate
conceptual
problem
original
imin
measure
presented
williams
beer
2010
two-bit
copy
problem
harder
2013
timme
2013
grifﬁth
koch
2014
consider
two
independent
uniform
binary
variables
deﬁne
direct
copy
two
variables
since
independent
construction
overlapping
redundant
information
example
led
proposal
identity
axiom
harder
2013
previously
noted
entropy
decomposition
perspective
suggests
modiﬁcation
axiom
include
redundant
entropy
part
mutual
information
synergistic
1these
ambiguous
terms
entropy
note
case
original
two-bit
copy
problem
two
formulations
equivalent
since
local
misinformation
terms2
hence
synergistic
entropy
case
cid:54
see
since
nodes
lattice
must
zero
partial
entropy
values
therefore
full
monosemous
ped
based
pids
satisfy
modiﬁed
identity
axiom
equation
also
see
ped
derived
measures
satisfy
symmetry
choice
target
terms
bertschinger
rauh
olbrich
jost
2013
unique
information
terms
obviously
case
redundant
information
terms
yet
ﬁnd
counterexample
may
additional
constraints
higher
order
terms
lattice
could
ensure
target
symmetry
also
redundancy
term
example
sec
4.2.2
provides
counter
example
target
symmetry
monosemous
ped
synergy
term
full
ped
3.7
quantifying
mechanistic
source
redundancy
harder
2013
introduce
distinction
mechanistic
information
redundancy
arises
mechanism
considered
system
source
predictor
information
re-
dundancy
arises
relationship
inputs
independent
mechanism
source
information
redundancy
based
idea
dependent
contribute
information
redundancy
measured
target
example
correlated
information
redundancy
arises
solely
relationship
since
functional
mechanism
independent
contrast
case
binary
gate
inputs
independent
generally
accepted
information
redundancy
system
see
section
4.2.2
redundancy
therefore
purely
mechanistic
since
induced
functional
properties
system
study
importance
distinction
recognised
harder
2013
banerjee
grifﬁth
2015
wibral
priesemann
2016
existing
proposals
deﬁne
quantify
different
types
redundancy
show
mechanistic
source
information
redundancy
explicitly
separated
within
ped
based
information
decompositions
similar
h12
h12
∀x1
however
since
focussing
information
redundancy
following
since
argument
identity
axiom
concerned
h12
synergistic
entropy
effects
affect
information
redundancy
figure
h12
terms
right
hand
side
full
three
variable
entropy
decomposition
mechanistic
information
redundancy
means
entropy
shared
three
variables
context
full
system
inputs
output
ignored
partial
entropy
terms
corresponds
h12
implication
follows
30.
mechanistic
information
redundancy
corresponds
negative
partial
entropy
term
hope
reasoning
illustrates
monotonicity
axiom
ﬁrst
glance
appealing
appropriate
entropy
redundancy
measure3
monotonic
entropy
redundancy
measure
would
deﬁnition
unable
detect
mechanistic
redundancy
therefore
deﬁne
|min
hmech-3
hmech-3
hsource-3
h12∩
terms
non-negative
note
since
h12
30.
partial
entropy
terms
full
ped
mono-ped
approaches
include
synergistic
entropy
pairs
nodes
lie
entropy
lattice
quantify
source
redundancy
therefore
assign
terms
mechanistic
part
redundant
partial
information
approaches
source
information
redundancy
hsource-3
source
mechanistic
information
redundancy
hmech-3
hmech-3
mech
mech
full
monosemous
ped
approaches
respectively
figure
shows
values
two
example
systems
introduced
ﬁrst
system
output
copy
second
system
gate
cases
relationship
two
inputs
modulated
preserving
uniform
marginals
using
joint
distributions
3we
also
suggest
appropriate
either
information
redundancy
measure
see
section
4.2.2
figure
mechanistic
source
redundancy
monosemous
ped
partial
information
non-zero
partial
entropy
terms
shown
two
example
systems
function
dependence
inputs
output
output
monosemous-ped
neither
system
shows
synergy
copy
sum
two
terms
plotted
form
0.25
0.25−
correlation
parameter
range
0.25
two
inputs
independent
0.25
copy
copy
system
mechanistic
redundancy
joint
mutual
information
transitions
pure
unique
information
pure
source
redundancy
dependence
inputs
increased
similarly
source
redundancy
increases
dependence
mechanistic
redundancy
present
independent
inputs
unique
information
decrease
3.8
redundant
entropy
measure
dependence
pure
mutual
information
seen
mutual
information
difference
redundant
synergistic
entropy
however
contend
mutual
information
usually
interpreted
redundant
entropy
example
ﬁrst
coining
term
mutual
information
fano
1959
states
clearly
measure
extent
two
events
likely
occur
together
statistically
independent
however
technically
interpretation
applies
redundant
entropy
mutual
information
subtracts
synergistic
entropy
measure
extent
two
events
less
likely
occur
together
bearing
mind
surprisal
originally
termed
self-information
actual
term
mutual
information
almost
synonym
shared
entropy
suggest
many
difﬁculties
around
pid
including
focus
non-negativity
lack
clarity
basic
properties
information
redundancy
measure
satisfy
come
long-standing
misinterpretation
mutual
information
shared
entropy
without
full
appreciation
also
quantiﬁes
synergistic
effects
hope
ped
perspective
presented
help
clarify
long
held
incorrect
interpretation
mutual
information
exactly
interaction
information
conﬂates
redundant
information
synergistic
information
unsatisfactory
measure
shared
information
williams
beer
2010
propose
mutual
information
since
conﬂates
redundant
synergistic
entropy
pure
measure
shared
entropy
redundant
entropy
like
redundant
information
00.050.10.150.20.2500.20.40.60.8100.050.10.150.20.2500.20.40.60.81copy
x1and
bits
correlation
parametercorrelation
parametersource
red.mechanistic
red.unique
accessible
linear
combinations
classical
shannon
quantities
james
crutchﬁeld
2016
partial
entropy
decomposition
provides
way
quantify
therefore
propose
redundant
entropy
term
used
measure
bivariate
dependence
truly
measures
shared
synergistic
effects
simply
redundant
entropy
context
term
pure
mutual
information
order
preserve
familiar
notation
related
mutual
information
h12
pure
mutual
information
satisﬁes
many
fundamental
properties
mutual
information
conditional
pure
mutual
information
deﬁned
y|z
also
satisﬁes
chain
rule
cover
thomas
1991
figure
partial
entropy
decomposition
two
binary
variables..
illustrate
consider
bivariate
binary
system
uniform
marginals
relationship
two
variables
parametrically
modulated
following
36.
figure
shows
full
partial
entropy
decomposition
system
function
correlation
probability
parameter
variables
become
dependent
total
entropy
unique
partial
entropy
decrease
redundant
partial
entropy
mutual
information
two
increase
however
synergistic
entropy
independent
fully
dependent
inputs
rises
intermediate
levels
dependence
peaking
0.27
bits
0.16
0.41
gives
mutual
information
curved
form
redundant
entropy
close
linear
figure
shows
relationship
mutual
information
pure
mutual
information
family
systems
pure
mutual
information
greater
monotonically
increasing
function
mutual
information
conjecture
properties
hold
general
suggest
pure
mutual
information
provides
interesting
perspective
mutual
information
measure
dependence
two
systems
since
contrast
mutual
information
makes
explicitly
clear
effects
inclusion
synergistic
entropy
unlikely
00.050.10.150.20.2500.511.5200.20.40.60.8100.20.40.60.81correlation
parameter
bits
satisfy
many
properties
theorems
relating
mutual
information
coding
noisy
channels
perspective
synergy
likely
important
would
limit
performance
possible
coding
strategies
3.8.1
partial
information
decomposition
pure
mutual
information
without
synergistic
contributions
natural
intuitions
incorrectly
applied
decomposition
mutual
information
apply
pure
mutual
information
example
identity
axiom
correct
originally
formulated
since
partial
information
decomposition
obtained
trivially
four
constituent
terms
satisﬁes
pure
mutual
information
examples
presented
pid
pure
mutual
information
equal
monosemous
ped
based
pid
explicitly
report
values
although
might
true
general
however
purpose
introducing
pure
mutual
information
provide
perspective
results
obtained
monosemous
pid
provide
decomposition
satisfying
consider
pure
mutual
information
redundant
entropy
measure
dependence
decomposed
obviously
many
aspects
mutual
information
depend
inclusion
synergistic
effects
hope
considering
pure
mutual
information
useful
exercise
explicitly
demonstrate
synergistic
entropy
makes
obtaining
partial
information
decomposition
challenging
examples
code
implementing
methods
examples
available
https
//github.com/robince/partial-info-decomp
calculate
ibroja
compute
maximum
entropy
distributions
pairwise
marginal
constraints
use
dit
package
james
cheebee7i
2017
4.1
dyadic
triadic
structure
ﬁrst
consider
two
example
systems
classical
information
diagrams
dyadic
triadic
structure
james
crutchﬁeld
2016
challenge
posed
james
crutchﬁeld
2016
decompose
entropy
multivariate
systems
way
meaningfully
represents
different
generative
structures
pid
falls
short
requirement
asymmetry
account
two
bits
mutual
information
third
bit
entropy
also
present
system
believe
partial
entropy
decomposition
presented
directly
addresses
question
two
systems
4https
//github.com/dit/dit
http
//docs.dit.io/
illustrated
figure
james
crutchﬁeld
2016
consist
three
variables
consisting
two
uniform
bits
dyadic
system
fig
one
bit
shared
pair
variables
triadic
system
fig
one
bit
shared
betwen
three
variables
bit
forms
xor
relationship
three
figure
distributions
equivalent
classical
information
measures
dyadic
triadic
structure
variable
consists
two
bits
labelled
indicates
bits
coupled
distributed
identically
indicates
enclosed
variables
form
xor
relation
modiﬁed
james
crutchﬁeld
2016
results
partial
entropy
decomposition
systems
related
pure
xor
system
single
bit
uniform
variables
xor
conﬁguration
shown
figure
see
ped
reveals
different
structure
distributions
way
matches
generative
structure
dyadic
example
fig
bit
allocated
pairwise
univariate
redundancy
term
reﬂecting
three
pairwise
coupled
bits
form
system
triadic
example
fig
ﬁrst
glance
decomposition
little
harder
interpret
due
presence
negative
term
entropy
commonly
available
three
synergistic
pairs
variables
however
comparison
decomposition
pure
variable
binary
xor
fig
see
triadic
decomposition
correctly
reﬂects
single
bit
shared
variables
together
bits
xor
structure
therefore
suggest
entropy
decomposition
approach
addresses
challenge
posed
james
crutchﬁeld
2016
meaningfully
describe
multivariate
structure
two
systems
note
use
pairwise
maximum
entropy
distribution
necessary
obtain
single
triple-wise
redundant
bit
triadic
system
xor
decomposition
structure
consists
bit
available
synergistically
pair
variables
redundant
third
variable
however
since
three
nodes
add
bits
total
bits
entropy
xor
negative
term
three-way
overlap
pairwise
synergies
fig
note
although
hcs
general
monotonic
see
example
case
xor
values
monotonic
lattice
fig
non-monotonicity
cause
negative
term
case
ﬁrst
glance
hard
interpret
similar
situation
arose
information
decomposition
iccs
ince
2016
suggested
normalisation
procedure
obtain
non-negative
decomposition
however
hcs
values
lattice
fig
match
properties
xor
bit
available
~~~~~aaaaaabbbbbbyabxz~~+yxza+aa
figure
partial
entropy
decomposition
dyadic
system
fig
triadic
system
fig
xor
nodes
labelled
partial
entropy
values
bits
nodes
zero
values
labelled
hcs
values
xor
system
1111111-1
123
123
123
d111111000022222221112-111
123
variable
alone
system
fully
determined
bits
synergistic
pair
positive
partial
entropy
values
ped
fig
seem
correct
true
positive
nodes
bit
entropy
available
lower
nodes
lattice
indicates
redundant
entropy
shared
three
synergistic
pairs
actually
less
expected
lower
terms
lattice
corrects
double
counting
two
xor
bits
lower
level
level
lattice
section
3.7
noted
negative
values
correspond
mechanistic
redundancy
fully
developed
relationship
suggest
negative
value
also
interpreted
mechanistic
redundancy
effect
level
pairwise
synergistic
entropy
full
three
variable
lattice
shown
contains
values
full
decomposition
reveal
asymmetries
multivariate
system
suggest
many
cases
might
useful
consider
simpliﬁed
presentation
based
order
structure
ince
2016
considering
sum
terms
speciﬁc
order
reduces
three-variable
entropy
decomposition
terms
dyadic
triadic
xor
4.2
binary
logical
operations
4.2.1
xor
start
xor
ped
shown
figure
table
shows
pids
system
node
imin
ibroja
iccs
ped
mono-ped
table
pids
xor
bracketed
redundancy
values
decompose
redundant
partial
information
source
mechanistic
redundancy
respectively
note
case
full
monosemous
ped
denoted
mono-ped
based
decompositions
satisfy
well
however
mono-ped
matches
common
decomposition
obtained
methods
full
pid
ped
strikingly
different
ﬁrst
glance
full
ped
pid
seems
counter
intuitive
based
properties
xor
bit
unique
misinformation
comes
term
entropy
term
value
bit
represents
misinformation
appears
therefore
entropy
perspective
indeed
bit
unique
misinformation
variable
single
redundant
bit
also
seems
counter-intuitive
since
predictor-target
perspective
independent
arises
negative
three
way
redundant
pairwise
synergy
term
represents
mechanistic
redundancy
pairwise
synergistic
entropies
indeed
source
mechanistic
split
appears
mechanistic
redundancy
bits
synergy
arise
sum
non-zero
terms
lattice
appear
positive
sign
note
monosemous
ped
approach
gives
sense
net-effects
overall
bit
synergy
full
description
afforded
resolution
partial
entropy
approach
shows
even
simple
example
system
redundancy
synergy
level
individual
target-predictor
mutual
informations
4.2.2
next
consider
equivalent
information
theoretic
terms
ince
2016
figure
shows
ped
values
hcs
entropy
lattice
ped
reﬂects
asymmetry
system
note
negative
results
non-monotonic
hcs
values
lattice
hcs
hcs
0.1.
note
values
affected
use
pairwise
maximum
entropy
distribution
non-monotonicity
occurs
full
distribution
used
three
way
redundancy
node
three
way
univariate
redundancy
arises
single
non-zero
pointwise
term
entropy
perspective
redundant
local
entropy
context
three
variable
system
removed
variable
marginalised
away
calculation
therefore
representing
mechanistic
information
redundancy
figure
partial
entropy
decomposition
nodes
labelled
partial
entropy
values
bits
nodes
zero
values
labelled
hcs
values
node
labelled
green
table
shows
pids
system
note
pointwise
approaches
allocate
non-zero
unique
information
variables
contradiction
corollary
bertschinger
rauh
olbrich
jost
2014
however
suggest
equality
target-predictor
marginals
ensures
two
unique
information
values
equal
must
zero
neither
ped
based
approaches
agree
iccs
however
full
ped
decomposition
equivalent
obtained
iccs
full
distribution
used
redundancy
term
rather
pind
ince
2016
case
discrepancy
arises
asymmetry
system
fact
system
pind
equal
second
order
marginal
preserving
maximum
entropy
distribution
ped
approaches
correctly
identify
redundancy
purely
mechanistic
must
mechanistic
since
inputs
independent
another
interesting
property
monosemous
ped
decomposition
allocates
zero
synergy
comparing
values
decomposition
xor
suggests
approach
123
b0.50.50.8100.460.10.10.46-0.10.350.350.150.150.50.51.51.5121.521.5110.811005.150.1
123
qualitatively
discriminate
xor
based
synergy
term
alone
note
mono-ped
sum
unique
redundant
partial
information
terms
equal
univariate
mutual
information
therefore
full
interpretation
pid
mutual
information
applied
although
apply
pure
mutual
information
section
3.8
nevertheless
valid
case
non-negative
decomposition
joint
mutual
information
unambiguous
redundant
unique
synergistic
contributions
node
imin
ibroja
0.5
0.31
0.5
0.31
iccs
0.43
0.07
0.07
0.24
ped
0.29
0.21
0.21
0.10
0.1
mono-ped
0.35
0.35
0.10
0.1
table
pids
and/
bracketed
redundancy
values
decompose
redundant
partial
information
source
mechanistic
redundancy
respectively
consider
distribution
switch
target
variable
one
two
uniform
independently
distributed
input
bits
pids
system
shown
table
denote
two
inputs
output
three
pre-existing
methods
considered
agree
allocating
0.31
bits
unique
information
together
0.19
bits
synergy
shows
asymmetry
pid
terms
selected
target
variable
james
crutchﬁeld
2016
unique
information
target
bits
equal
unique
information
target
0.31
bits
similarly
redundancy
0.31
bits
equal
redundancy
bits
ped
approach
removes
asymmetries
since
entropy
lattice
preserved
reordering
variables
terms
change
location
variables
permuted
values
remain
however
unique
information
negative
−0.1
bits
hcs
term
figure
suggests
effective
misinformation
marginally
indepen-
dent
inputs
context
full
three
variable
system
fact
considering
pind
maximum
entropy
distribution
preserving
pairwise
marginals
ince
2016
dependence
indicated
non-zero
mutual
information
distribution
mutual
information
calculation
includes
pointwise
negative
misinformation
terms
reasons
discussed
asymmetric
pind
gives
different
perspective
ped
used
terms
equal
numerically
least
suggests
potential
misinformation
context
full
system
amount
redundancy
present
however
redundancy
correctly
identiﬁed
source
redundancy
mechanistic
redundancy
since
arises
dependence
4.2.3
sum
strictly
logic
gate
consider
sum
addition
two
binary
inputs
form
ternary
output
extension
binary
summation
operation
equivalent
system
termed
xorand
bertschinger
rauh
olbrich
jost
2014
table
shows
pids
node
imin
0.19
0.31
ibroja
0.19
0.31
iccs
0.19
0.31
ped
0.29
−0.10
mono-ped
0.15
−0.10
0.21
0.10
0.1
0.35
0.10
0.1
table
pids
and/
predicting
one
inputs
gate
output
input
bracketed
redundancy
values
decompose
redundant
partial
information
source
mechanistic
redundancy
respectively
system
contrast
xor
full
ped
agrees
existing
approaches
iccs
monosemous
version
gives
different
perspective
case
monosemous
ped
approach
satisfy
shows
unambiguous
redundancy
two
summands
0.5
bits
unique
information
0.5
bits
information
available
synergistically
non-zero
ped
terms
case
0.5
−0.5
node
imin
ibroja
0.5
0.5
iccs
0.79
0.21
0.21
0.29
ped
mono-ped
0.5
0.5
0.5
0.5
0.5
table
pids
sum
bracketed
redundancy
values
decompose
redundant
partial
information
source
mechanistic
redundancy
respectively
4.2.4
summary
together
examples
show
entropy
decomposition
perspective
particularly
pid
based
monosemous
ped
terms
opens
new
window
structure
redundancy
synergy
simple
systems
suggest
pid
based
monosemous
ped
terms
may
useful
seems
provide
intuitive
decomposition
systems
table
shows
xor
synergistic
information
synergistic
component
summation
redundancy
equal
amounts
unique
information
within
synergy
summands
shows
thresholding
sum
result
logical
removes
synergy
two
inputs
introduces
mechanistic
information
redundancy
node
xor
and/or
0.35
0.35
0.10
mechanistic
sum
0.5
0.5
0.5
table
mono-ped
based
pid
xor
sum
4.3
williams
beer
2010
examples
example
systems
originally
considered
williams
beer
2010
figure
reproduced
figure
ped
approaches
yield
pid
iccs
ince
2016
tables
9,10
note
example
equivalent
system
subtle
grifﬁth
chong
2014
figure
source
redundancy
present
example
mechanistic
redundancy
figure
probability
distributions
two
example
systems
black
tiles
represent
equiprobable
outcomes
white
tiles
zero-probability
outcomes
modiﬁed
williams
beer
2010
node
imin
0.33
0.33
0.33
0.95
ibroja
0.67
0.67
0.25
iccs
0.14
0.53
0.53
0.39
ped
0.14
0.53
0.53
0.39
0.39
mono-ped
0.14
0.53
0.53
0.39
0.39
table
pids
system
figure
figure
williams
beer
2010
bracketed
redundancy
values
decompose
redundant
partial
information
source
mechanistic
redundancy
respec-
tively
x3=0x3=1x3=2x3=0x3=1x3=2x1x1x2x20011x2x20011x2x200110011x1x10011x1x10011x1x1x2x20011x2x20011x2x200110011x1x10011x1x10011ab
node
imin
ibroja
iccs
ped
mono-ped
0.5
0.5
0.5
0.5
0.5
0.5
0.5
table
pids
system
figure
figure
williams
beer
2010
bracketed
redun-
dancy
values
decompose
redundant
partial
information
source
mechanistic
redundancy
respectively
4.4
examples
grifﬁth
koch
2014
grifﬁth
chong
2014
present
range
interesting
examples
careful
design
exhibit
different
combinations
redundancy
synergy
unique
information
0.4
0.1
0.5
table
deﬁnition
imperfectrdn
node
imin
ibroja
iccs
0.39
0.61
0.39
0.61
0.23
0.77
ped
0.16
0.23
−0.16
0.77
0.77
mono-ped
0.23
0.77
0.77
table
pids
imperfectrdn
grifﬁth
chong
2014
figure
bracketed
redundancy
values
decompose
redundant
partial
information
source
mechanistic
redundancy
respectively
ﬁrst
consider
imperfectrdn
grifﬁth
chong
2014
adapted
fig
probability
values
system
shown
table
ince
2016
example
interesting
one
iccs
reveals
unique
misinformation
second
predictor
iccs
pid
removed
thresholding
negative
values
see
full
ped
approach
produces
pid
equal
unthresholded
iccs
decomposition
including
unique
misinformation
term
monosemous
ped
approach
considers
unambiguous
terms
effect
thresholding
removes
counter-balanced
effect
term
misinformation
context
synergistic
information
context
note
case
neither
iccs
mono-ped
approaches
satisfy
rdnxor
consists
two
two-bit
value
inputs
two-bit
value
output
ﬁrst
component
redundantly
speciﬁes
ﬁrst
component
second
node
imin
ibroja
iccs
ped
mono-ped
table
pids
rdnxor
grifﬁth
chong
2014
figure
bracketed
redundancy
values
decompose
redundant
partial
information
source
mechanistic
redundancy
respectively
component
xor
second
components
system
therefore
contains
bit
redundant
information
bit
synergistic
information
every
value
redundant
synergistic
contribution
table
shows
decompositions
system
monosemous
ped
approach
agrees
existing
measures
xor
full
ped
indicates
bit
unique
misinformation
predictor
comes
terms
monosemous
pid
correctly
speciﬁes
bit
redundancy
source
redundancy
full
ped
also
separates
bits
redundancy
bit
source
construction
system
bit
mechanistic
arising
xor
redundancy
discussion
paper
start
simple
idea
apply
framework
partial
information
decomposition
williams
beer
2010
directly
multivariate
entropy
concepts
synergy
redundancy
applied
entropy
way
mutual
information
redundant
entropy
measures
uncertainty
shared
common
variables
synergistic
entropy
measures
extra
uncertainty
arises
variables
observed
together
present
individually
similarly
redundancy
lattice
multivariate
mutual
information
applied
entropy
deﬁne
entropy
redundancy
measure
based
pointwise
common
surprisal
directly
follows
natural
deﬁnition
shared
entropy
satisﬁes
axioms
required
validity
redundancy
lattice
closely
related
deﬁnition
mutual
information
positive
local
information
terms
correspond
redundant
entropy
negative
local
information
terms
synergistic
entropy
show
yields
consistent
approach
addresses
outstanding
questions
meaningfully
quantify
dependence
structure
multivariate
systems
james
crutchﬁeld
2016
well
suggesting
new
approaches
obtaining
decompositions
multivariate
mutual
information
suggest
partial
entropy
decomposition
approach
directly
addresses
call
made
james
crutchﬁeld
2016
new
measures
meaningfully
quantify
dependence
structure
multivariate
complex
systems
covered
basic
triadic
dyadic
examples
present
showing
ped
clearly
reveal
different
generative
structures
applications
ped
directly
different
model
systems
real
data
interesting
topic
future
work
problem
also
addressed
rosas
2016
employ
approach
symmetrises
information
redundancy
taking
minimum
choices
target
variable
ped
least
hcs
measure
admits
negative
partial
entropy
terms
negative
partial
entropy
values
occur
two
different
ways
first
since
hcs
monotonic
redundancy
lattice
williams
beer
2010
result
negative
partial
entropy
values
e.g
sec
4.2.2
second
even
cases
redundant
entropy
values
monotonic
lattice
particular
statistical
structure
system
also
induce
negative
partial
entropy
values
e.g
xor
sec
4.2.1
rauh
bertschinger
2014
agree
james
crutchﬁeld
2016
negative
atoms
subjectively
seen
ﬂaw
ability
meaningfully
distinguish
generative
structure
systems
ped
important
aesthetic
beneﬁts
non-negative
decomposition
suggest
negative
terms
actually
important
part
structure
multivariate
system
example
shown
negative
pairwise
entropy
redundancy
two
variables
interpreted
predictors
pid
framework
quantiﬁes
mechanistic
information
redundancy
system
similarly
3-way
entropy
redundancy
pairwise
synergies
negative
likely
indicate
presence
mechanistic
pairwise
redundancy
whether
terms
take
negative
values
whether
similar
interpretations
terms
mechanistic
redundancy
open
question
relationships
mutual
information
values
partial
entropy
decomposition
presented
valid
redundancy
measures
therefore
possible
alternative
measures
obtain
partial
entropy
values
bertschinger
rauh
olbrich
jost
2014
alternative
perspectives
redundancy
lattice
structure
chicharro
panzeri
2017
modiﬁcations
deﬁnition
hcs
perhaps
involving
use
different
distributions
pair-
wise
terms
e.g
pind
ince
2016
might
yield
non-negative
entropy
decomposition
however
unclear
whether
methods
would
able
quantify
mechanistic
redundancy
close
relationship
hcs
deﬁnition
mutual
information
appealing
manuscript
focus
implications
ped
perspective
obtaining
decom-
positions
mutual
information
mutual
information
based
entropy
concepts
synergy
redundancy
apply
equally
entropy
therefore
individual
partial
information
terms
valid
pid
must
quantifying
combinations
redundant
synergistic
entropy
hence
suggest
every
consistent
pid
admit
compatible
entropy
decomposition
considering
redundant
synergistic
entropy
directly
reveals
insight
local
misinfor-
mation
wibral
lizier
priesemann
2014
considered
difﬁcult
interpret
williams
beer
2010
synergistic
entropy
also
supports
deﬁnition
hcs
equivalence
true
locally
relationships
mutual
information
partial
entropy
terms
ﬁxed
structure
lattices
independent
redundancy
measure
used
sec
3.3
therefore
considering
hard
imagine
different
entropy
redundancy
function
could
yield
non-negative
decomposition
fails
match
pointwise
deﬁnition
mutual
information
equation
also
provides
insight
mutual
information
two
variables
includes
redundant
synergistic
effects
realisation
important
consequences
related
deﬁnitions
pid
example
identity
axiom
section
3.6
similarly
suggests
axioms
relating
partial
information
terms
mutual
information
bivariate
monotonity
axiom
bertschinger
rauh
olbrich
jost
2014
reconsidered6
also
gives
support
claim
partial
information
decomposition
must
admit
negative
terms
ince
2016
corresponding
example
unique
misinformation
6rather
suggest
redundant
partial
information
include
deﬁned
h12
redundant
entropy
effects
synergistic
entropy
misinformation
resulting
bivariate
monotonicity
entirely
unclear
means
one
variable
provide
negative
information
another
synergistic
entropy
arises
predictor
target
considered
together
e.g
imperfectrdn
sec
4.4
mechanistic
redundancy
system
target
variable
switched
e.g
sec
4.2.2
mutual
information
guaranteed
non-
negative
guarantee
unique
contributions
individual
variables
combinations
variables
corresponding
classical
shannon
quantity
considering
relationship
ped
multivariate
mutual
information
derive
two
alternative
information
decompositions
important
insight
arises
process
fact
entropy
terms
proper
subset
former
includes
partial
entropy
terms
appear
latter
fact
recognised
seems
key
factor
explaining
obtaining
consistent
pid
challenging
ﬁrst
ped
based
information
decomposition
satisﬁes
standard
properties
pid
including
eqs
2,3.
however
provides
quite
different
perspective
existing
approaches
several
examples
e.g
xor
sec
4.2.1
differences
arise
inherently
ambiguous
entropy
terms
correspond
example
unique
misinformation
context
univariate
predictor
mutual
information
well
synergistic
information
context
full
bivariate
predictor
mutual
information
terms
appear
univariate
mutual
information
joint
mutual
information
accessible
entropy
perspective
separated
considering
mutual
information
values
directly
however
since
ambiguous
terms
actually
occur
directly
bivariate
predictor
mutual
information
suggests
second
information
decomposition
term
monosemous
decomposition
longer
satisﬁes
nevertheless
valid
decomposition
joint
mutual
information
components
unambiguously
redundant
unique
synergistic
predictor
variables
fact
provides
genuinely
new
perspective
strikingly
different
results
existing
methods
simple
example
systems
suggest
alternative
approach
may
particularly
useful
gaining
insight
functional
structure
system
since
decomposition
simple
binary
operations
seems
nicely
reﬂect
generative
mechanism
sec
4.2.4
considering
measure
dependence
based
redundant
entropy
sec
3.8
illustrated
directly
difﬁculties
pid
come
inclusion
synergistic
effects
mutual
information
context
pure
mutual
information
measure
trivial
obtain
decomposition
satisﬁes
relationships
pid
examples
considered
approach
numerically
equivalent
monosemous
ped
serves
exercise
demonstrate
although
mono-ped
satisfy
perspective
decomposing
shared
entropy
similar
approach
satisfy
properties
pure
mutual
information
also
highlights
origin
many
failures
intuition
regarding
axioms
properties
pid
apply
previously
proposed
pure
mutual
information
incorrect
deﬁned
mutual
information
hitherto
appreciated
synergistic
entropy
component
ped
approach
also
allows
separate
quantiﬁcation
mechanistic
source
redundancy
harder
2013
ped
based
decompositions
even
types
redundancy
occur
together
system
e.g
sec
4.2.2
despite
importance
distinction
separate
properties
functional
mechanisms
dependencies
input
variables
knowledge
ﬁrst
proposal
quantifying
differences
practise
pid
ped
approaches
derived
single
entropy
lattice
consistent
respect
choice
target
way
existing
pid
measures
neither
non-negative
reasons
discussed
however
may
possible
obtain
theoretical
bounds
interesting
area
future
work
example
given
mutual
information
non-negative
two
variable
redundancy
lattice
redundant
entropy
must
greater
synergistic
entropy7
similar
multi-term
constraints
obtained
ped
expressions
individual
joint
mutual
informations
note
hcs
based
entropy
decompositions
mostly
consistent
iccs
based
pid
ince
2016
differences
induced
use
pind
iccs
induces
asymmetry
different
pids
multivariate
system
important
extend
ped
approach
larger
numbers
variables
example
four
variable
ped
yielding
three
variable
pids
relationship
lattices
hence
ped
expressions
different
mutual
information
terms
straightforward
formalise
expect
approach
apply
directly
one
question
concerns
use
marginally
constrained
maximum
entropy
distribution
four
source
redundant
entropy
terms
pairwise
constraints
continue
used
closer
arguments
pid
redundancy
depend
pairwise
target-predictor
marginals
higher
order
structure
bertschinger
rauh
olbrich
jost
2014
third
order
marginal
constraints
used
full
development
testing
higher
dimensional
example
systems
grifﬁth
koch
2014
grifﬁth
chong
2014
ince
2016
important
area
future
work
deﬁnition
hcs
easily
applicable
continuous
systems
ince
2016
future
work
implement
entropy
decomposition
gaussian
systems
offer
number
practical
advantages
barrett
2015
ince
2016
conclusion
suggest
problem
quantifying
structure
multivariate
depen-
dence
aided
simple
perspective
shift
considering
mutual
information
focus
instead
entropy
entropy
foundational
ﬁrst
order
quantity
information
theory
seems
natural
place
start
concepts
proved
difﬁcult
formalise
redundancy
synergy
elegant
framework
partial
information
decomposition
naturally
directly
applied
multivariate
entropy
believe
perspective
shift
helps
clarify
difﬁculties
around
information
theoretic
quantiﬁcation
multivariate
systems
terms
entropy
james
crutchﬁeld
2016
mutual
information
williams
beer
2010
hope
resulting
entropy
information
decompositions
help
clarify
theoretical
aspects
well
providing
useful
tools
practical
data
analysis
study
complex
systems
experimental
science
acknowledgements
thank
bill
phillips
daniel
chicharro
johannes
rauh
ryan
james
philippe
schyns
useful
discussions
ryan
james
producing
excellent
dit
package
7h∂
equality
references
banerjee
pradeep
2014
new
insights
information
decomposition
complex
systems
based
common
information
arxiv:1503.00709
math
c004
doi
3390/ecea-1-c004
arxiv
1503.00709.
banerjee
pradeep
virgil
grifﬁth
2015
synergy
redundancy
common
information
arxiv:1509.03706
math
arxiv
1509.03706.
barrett
adam
2015
exploration
synergistic
redundant
information
sharing
static
dynamical
gaussian
systems
physical
review
91.5
052802.
doi
10.1103/
physreve.91.052802
bell
anthony
2003
co-information
lattice
4th
international
sympo
sium
independent
component
analysis
blind
signal
separation
ica2003
nara
japan
921–
926.
bertschinger
nils
johannes
rauh
eckehard
olbrich
jürgen
jost
2013
shared
information—new
insights
problems
decomposing
information
complex
systems
proceedings
european
conference
complex
systems
2012.
thomas
gilbert
markus
kirkilionis
gregoire
nicolis
springer
proceedings
complexity
doi
10.1007/978-3-319-00395-5_35
springer
international
publishing
251–269
bertschinger
nils
johannes
rauh
eckehard
olbrich
jürgen
jost
nihat
2014
quantifying
unique
information
entropy
16.4
2161–2183
doi
10.3390/e16042161
chicharro
daniel
stefano
panzeri
2017
synergy
redundancy
dual
decompositions
mutual
information
gain
information
loss
entropy
19.2
71.
doi
10.3390/
e19020071
church
kenneth
ward
patrick
hanks
1990
word
association
norms
mutual
information
lexicography
comput
linguist
16.1
22–29
cover
t.m
j.a
thomas
1991
elements
information
theory
wiley
new
york
crampton
jason
george
loizou
2001
completion
poset
lattice
antichains
international
mathematical
journal
1.3
223–238
fano
1959
statistical
theory
information
nuovo
cimento
1955-1965
13.2
353–372
doi
10.1007/bf02724671
grifﬁth
virgil
edwin
chong
ryan
james
christopher
ellison
james
crutchﬁeld
2014
intersection
information
based
common
randomness
entropy
16.4
1985–
2000.
doi
10.3390/e16041985
grifﬁth
virgil
christof
koch
2014
quantifying
synergistic
mutual
information
guided
self-organization
inception
mikhail
prokopenko
emergence
complexity
computation
doi
10.1007/978-3-642-53734-9_6
springer
berlin
heidelberg
159–190
harder
malte
christoph
salge
daniel
polani
2013
bivariate
measure
redundant
information
physical
review
87.1
012130.
doi
10.1103/physreve.87.012130
ince
robin
2016
measuring
multivariate
redundant
information
pointwise
common
change
surprisal
arxiv:1602.05063
math
q-bio
stat
arxiv
1602.05063.
ince
robin
a.a.
bruno
giordano
christoph
kayser
guillaume
rousselet
joachim
gross
philippe
schyns
2016
statistical
framework
neuroimaging
data
analysis
based
mutual
information
estimated
via
gaussian
copula
human
brain
mapping
n/a–n/a
doi
10.1002/hbm.23471
jakulin
aleks
ivan
bratko
2003
quantifying
visualizing
attribute
interactions
arxiv
cs/0308002
arxiv
cs/0308002
james
ryan
cheebee7i
2017
dit/dit
v1.0.0.dev0
data
set
zenodo
doi
5281/zenodo.235071
james
ryan
james
crutchﬁeld
2016
multivariate
dependence
beyond
shannon
information
arxiv:1609.01233
cond-mat
stat
arxiv
1609.01233.
lizier
joseph
mikhail
prokopenko
albert
zomaya
2008
local
information
transfer
spatiotemporal
ﬁlter
complex
systems
physical
review
77.2
026110.
doi
10.1103/physreve.77.026110
matsuda
hiroyuki
2000
physical
nature
higher-order
mutual
information
intrinsic
corre-
lations
frustration
physical
review
62.3
3096–3102
doi
10.1103/physreve
62.3096.
mcgill
william
1954
multivariate
information
transmission
psychometrika
19.2
97–116
doi
10.1007/bf02289159
olbrich
eckehard
nils
bertschinger
johannes
rauh
2015
information
decomposition
synergy
entropy
17.5
3501–3517
doi
10.3390/e17053501
rauh
bertschinger
olbrich
jost
2014
reconsidering
unique
information
towards
multivariate
information
decomposition
2014
ieee
international
symposium
information
theory
isit
2014
ieee
international
symposium
information
theory
isit
2232–2236
doi
10.1109/isit.2014.6875230
rauh
johannes
pradeep
banerjee
eckehard
olbrich
jürgen
jost
nils
bertschinger
2017
extractable
shared
information
arxiv:1701.07805
math
arxiv
1701
07805.
reza
fazlollah
1961
introduction
information
theory
new
york
mcgraw-hill
rosas
fernando
vasilis
ntranos
christopher
ellison
soﬁe
pollin
marian
verhelst
2016
understanding
interdependency
complex
information
sharing
entropy
18.2
38.
doi
10.3390/e18020038
shannon
c.e
1948
mathematical
theory
communication
bell
syst
bell
systems
technical
journal
27.3
379–423
timme
nicholas
wesley
alford
benjamin
flecker
john
beggs
2013
synergy
redun-
dancy
multivariate
information
measures
experimentalist
perspective
journal
computational
neuroscience
36.2
119–140
doi
10.1007/s10827-013-0458-4
van
cruys
tim
2011
two
multivariate
generalizations
pointwise
mutual
information
proceedings
workshop
distributional
semantics
compositionality
disco
stroudsburg
usa
association
computational
linguistics
16–20
wibral
michael
joseph
lizier
viola
priesemann
2014
bits
biology
computational
intelligence
arxiv:1412.0291
physics
q-bio
arxiv
1412.0291.
wibral
michael
joseph
lizier
sebastian
vögler
viola
priesemann
ralf
galuske
2014
local
active
information
storage
tool
understand
distributed
neural
information
pro-
cessing
frontiers
neuroinformatics
doi
10.3389/fninf.2014.00001
wibral
michael
viola
priesemann
jim
kay
joseph
lizier
william
phillips
2016
partial
information
decomposition
uniﬁed
approach
speciﬁcation
neural
goal
functions
brain
cognition
doi
10.1016/j.bandc.2015.09.004
williams
paul
randall
beer
2010
nonnegative
decomposition
multivariate
information
arxiv:1004.2515
math-ph
physics
physics
q-bio
arxiv
1004.2515.
yeung
1991
new
outlook
shannon
information
measures
ieee
transactions
information
theory
37.3
466–474
doi
10.1109/18.79902
