knyazev
guided
signal
reconstruction
theory
guided
signal
reconstruction
theory
andrew
knyazev
senior
member
ieee
fellow
siam
akshay
gadde
student
member
ieee
hassan
mansour
member
ieee
dong
tian
member
ieee
abstract—an
axiomatic
approach
signal
reconstruction
formulated
involving
sample
consistent
set
guiding
set
describing
desired
reconstructions
new
frame-less
reconstruc-
tion
methods
proposed
based
novel
concept
recon-
struction
set
deﬁned
shortest
pathway
sample
consistent
set
guiding
set
existence
uniqueness
reconstruction
set
investigated
hilbert
space
guiding
set
closed
subspace
sample
consistent
set
closed
plane
formed
sampling
subspace
connec-
tions
earlier
known
consistent
generalized
regularized
reconstructions
clariﬁed
new
stability
reconstruction
error
bounds
derived
using
largest
nontrivial
angle
sampling
guiding
subspaces
conjugate
gradient
iterative
reconstruction
algorithms
proposed
illustrated
numerically
image
magniﬁcation
quotient
space
law
doubt
cut
introduction
signal
reconstruction
standard
problem
arises
nat-
urally
signal
processing
machine
learning
classical
example
reconstruction
band-limited
signals
time-domain
samples
recently
reconstruction
signals
graphs
signal
samples
subset
nodes
graph
gaining
popularity
e.g.
ﬁnds
applications
graph-based
semi-supervised
learning
see
e.g.
context
signals
considered
band-limited
respect
eigenvalues
graph
laplacian
hilbert
space
framework
allows
investigating
signal
re-
construction
general
concise
manner
end
consider
problem
determining
reconstruction
unknown
original
signal
measurement
hilbert
space
equipped
scalar
product
corresponding
norm
k·k
measurement
deﬁned
result
action
orthogonal
projector
onto
closed
subspace
called
sampling
subspace
original
signal
typically
known
sampled
original
signal
available
input
recon-
struction
method
since
sampling
involves
loss
information
need
priori
assumptions
original
signal
recovered
one
assumption
may
signal
belongs
closed
subspace
thought
target
reconstruction
subspace
alternatively
signal
may
lie
strictly
may
well
approximated
projection
subspace
prefer
call
knyazev
mansour
tian
mitsubishi
elec-
tric
research
laboratories
merl
201
broadway
8th
floor
cambridge
02139-1955
e-mail
knyazev
mansour
tian
merl.com
www
http
//www.merl.com/people/
knyazev
mansour
tian
gadde
university
southern
california
usc
los
angeles
intern
merl
e-mail
agadde
usc.edu
work
presented
parts
2015
ieee
global
conference
signal
information
processing
globalsip
orlando
2015
preliminary
version
manuscript
posted
arxiv
guiding
reconstruction
subspace
since
reconstructed
signal
may
necessarily
restricted
another
example
prior
structure
signal
belongs
compact
subset
determined
smoothness
case
reconstruction
minimizes
reconstruction
error
kˆf
naturally
desired
guiding
set
determined
using
model
form
description
desirable
reconstructed
signal
behavior
e.g.
learned
training
datasets
signals
natural
spectral
properties
spectral
transforms
e.g.
fourier
cosine
wavelet
transforms
used
transform
signals
spectral
domain
guiding
subspace
chosen
corresponding
certain
frequency
ranges
e.g.
assuming
desired
signal
band-limited
signals
without
self-evident
spectral
properties
sig-
nals
embedded
specially
constructed
structure
depending
type
signal
e.g.
graph
rieman-
nian
manifold
wherein
spectral
properties
determined
energy
norm
deﬁning
operator
e.g.
graph
lapla-
cian
laplace-beltrami
operator
correspondingly
energy
norm
constructed
using
given
guiding
signal
database
priori
learned
signals
de-
pending
signal
similarity
measure
signal
space
comprise
e.g.
correlation
coherence
divergence
metric
depending
type
signal
desired
re-
construction
properties
example
graph-based
signal
processing
edge
weights
determined
using
distances
vertex-localized
delta-function
signals
spanning
given
guiding
signal
guiding
subspace
chosen
approximate
invariant
subspace
energetic
operator
corresponding
certain
ranges
spectrum
e.g.
assuming
desired
signal
band-limited
components
primarily
low
part
spectrum
energetic
operator
notation
let
orthoprojectors
onto
closed
sub-
spaces
respectively
let
i−s
i−t
identity
operator
denote
orthoprojectors
onto
orthogonal
complements
let
denote
range
operator
null
space
e.g.
denotes
adjoint
measure
sample
element
projection
i.e
observed
sample
given
want
reconstruct
signal
reconstructed
split
two
orthogonal
components
observed
sample
contains
missing
information
reconstructed
knyazev
guided
signal
reconstruction
theory
prior
work
two
main
kinds
sample
consistent
set
signals
sample
reconstructions
known
subspace-based
constrained
reconstructions
using
oblique
projectors
leading
e.g.
energy
minimization-based
reconstructions
e.g.
generalized
abstract
splines
sec
practical
reconstruc-
tion
usually
performed
using
frames
context
separable
comes
e.g.
orthonormal
countable
frame
consequently
also
frame
frame
operator
tst
restricted
assuming
strict
positivity
minimal
gap
sec
iv-4
makes
inverse
frame
operator
bounded
general
approach
present
paper
frame-less
dealing
directly
orthogonal
projectors
onto
subspaces
closed
plane
call
consistent
plane.
generally
intersect
case
reconstruction
constrained
sets
required
solution
exist
need
additionally
solution
unique
need
otherwise
multiple
signals
samples
conditions
satisﬁed
unique
sample
consistent
solution
given
oblique
projector
along
non-uniqueness
caused
mathematically
resolved
replacing
quotient
space
replacement
assume
rest
section
practically
one
choose
unique
solution
imposing
additional
constraints
assumption
disadvantageous
restrictive
applications
even
intersect
ﬁnding
intersection
numerically
may
difﬁcult
intersection
may
sensitive
mutual
position
see
example
generalized
reconstruc-
cure
proposed
tion
scheme
oversampling
leads
smaller
consistent
plane
longer
intersects
strictly
guided
reconstructed
signal
deﬁned
point
smallest
distance
subspace
treated
literally
target
subspace
thus
enforcing
constraint
relaxing
using
least
squares
strictly
guided
generalized
reconstruction
methods
minimax
regret
may
sample
inconsis-
tent
since
place
reconstructed
signal
guiding
subspace
contrast
puts
reconstructed
signal
consistent
plane
relaxing
property
minimizing
instead
energy
reconstructed
signal
deﬁned
point
sample
consistent
plane
smallest
distance
approach
motivated
realization
practical
applications
bandwidth
expansion
narrowband
audio
signals
may
difﬁcult
explicitly
ﬁnd
frame
even
choose
trustworthy
target
reconstruction
subspace
thus
subspace
used
guide
true
target
trust
sampling
guiding
regularization-based
methods
suggested
de-
termine
reconstruction
solving
unconstrained
problem
minimizing
weighted
sum
loss
function
regulariza-
tion
term
using
regularization
parameter
regularization
parameter
needs
chosen
priori—a
common
difﬁculty
regularization-based
methods—and
may
greatly
affect
reconstruction
quality
authors
assume
existence
uniqueness
intersection
sample-consistent
reconstruction
plane
guiding
reconstruction
subspace
original
signal
main
contributions
let
assume
guiding
reconstruction
subspace
available
form
e.g.
implicitly
via
action
corresponding
possibly
approximate
orthogonal
projector
formulate
least
squares
approach
allows
implicit
frame-less
approximate
descriptions
e.g.
form
ﬁlter
function
approximately
suppressing
components
signal
additionally
least
squares
approach
allows
beneﬁt
oversampling
generalized
reconstruction
making
reconstruction
algorithms
stable
compared
classical
constrained
reconstructions
using
oblique
projectors
describe
uniﬁed
view
consistent
generalized
regularization
based
reconstruction
methods
novel
concept
reconstruction
set
introduced
explain
relates
regularization-based
methods
conditions
existence
uniqueness
reconstructed
signal
obtained
using
beyond
stability
reconstruction
error
bounds
derived
improve
following
bounds
suggest
numerically
efﬁcient
iterative
re-
construction
algorithm
based
conjugate
gradient
method
approximates
reconstruction
needs
actions
orthoprojectors
onto
subspaces
also
derive
convergence
rate
bounds
iterative
algorithms
reconstruction
error
bounds
depending
angles
subspaces
reconstruction
set
case
procedures
sampling
guiding
equally
trusted
guiding
set
contains
sample
consistent
signals
reminds
buridan
donkey
equally
hungry
thirsty
placed
precisely
midway
stack
hay
guiding
set
pail
water
sample
consistent
set
die
hunger
thirst
since
make
rational
decision
choose
one
save
hypothetical
donkey
deﬁne
set
re-
constructions
given
convex
combinations
strictly
guided
consistent
reconstructions
stated
guiding
set
subspace
may
contain
sample
consistent
solutions
samples
noisy
true
signal
lie
sample
consistent
plane
true
signal
may
entirely
contained
guiding
subspace
either
case
unclear
reconstruction
consistent
strictly
guided
generalized
better
choose
knyazev
guided
signal
reconstruction
theory
input
signal
consistent
reconstruction
consistent
set
proposed
reconstruction
generalized
reconstruction
guiding
set
consistent
reconstruction
generalized
reconstruction
fig
example
reconstruction
set
fig
reconstruction
set
metric
space
situation
illustrated
fig
simple
motivating
geometric
example
dimh
dims
dimt
set
signals
sample
evidently
line
lines
generally
intersect
reconstruction
constrained
lines
required
see
fig
observe
fig
one
hand
consistent
reconstruction
viewed
minimizer
distance
element
consistent
plane
guiding
subspace
hand
generalized
reconstruction
element
guiding
subspace
minimizing
distance
consistent
plane
clearly
equalities
hold
inf
∈sf
+s⊥
ˆt∈t
kˆf
ˆtk
inf
∈sf
+s⊥
ˆt∈t
kˆf
ˆtk
inf
inf
ˆt∈t
inf
∈sf
+s⊥
kˆf
ˆtk
minimizers
called
consistent
gener-
alized
reconstructions
respectively
giving
hint
deﬁne
reconstruction
set
shortest
pathway
set
consistent
plane
guiding
subspace
fig
reconstruction
set
closed
interval
end
points
consistent
reconstruction
generalized
reconstruction
unclear
one
procedures
sampling
guiding
trusted
element
reconstruction
set
becomes
valid
candidate
reconstruction
moreover
fig
discussion
suggest
propose
general
deﬁnition
reconstruction
set
shortest
pathway
given
guiding
set
sample
consistent
set
deﬁned
set
signals
sample-consistent
original
signal
see
fig
shortest
pathway
two
sets
formally
deﬁned
convex
set
elements
element
shortest
pathway
minimizes
sum
distance
element
ﬁrst
set
distance
element
second
set
consistent
reconstruction
intersection
re-
construction
set
consistent
set
generalized
strictly
guided
reconstruction
intersection
reconstruction
set
guiding
set
deﬁnition
one
needs
structure
metric
space
distance
thus
allowing
nonlinear
even
multi-valued
sampling
procedures
general
guiding
sets
example
interesting
recent
work
adcock
hansen
combines
generalized
reconstruction
inﬁnite
dimensional
compressed
sensing
natural
framework
banach
spaces
believe
notion
reconstruc-
tion
set
extended
framework
allowing
one
ﬁnd
reconstructed
signal
strictly
sparse
guided
reconstruction
subspace
identiﬁed
adcock
hansen
method
paper
however
limit
traditional
hilbert
space
framework
guiding
set
closed
subspace
sample
consistent
set
closed
plane
generalized
reconstruction
consistent
reconstruc-
tion
exist
unique
reconstruction
set
simply
convex
hull—a
closed
interval
case
exactly
illustrated
fig
space
another
possibility
addressed
deter-
ministic
setup
augmented
probabilistic
approach
signals
random
example
either
consistent
guiding
sets
may
determined
using
probability
distributions
case
reconstruction
set
also
determined
probability
distribution
using
statistical
distance
random
variables
samples
output
whole
reconstruction
set
multiple
reconstructed
signals
may
appropriate
applications
even
parametrization
using
end
points
reconstruction
set
possible
pick
single
reconstructed
signal
reconstruction
set
one
needs
extra
information
example
cost/quality
function
e.g.
buridan
donkey
choose
proper
healthy
mix
hay
water
following
given
dietary
function
one
may
output
signals
neighborhood
reconstruction
set
minimize
cost/quality
function
sec
show
select
optimal
solution
amount
noise
known
relate
reconstruction
set
regularization
methods
finally
cost/quality
function
trusted
consistent
guiding
set
one
may
choose
minimize
weighted
sum
cost/quality
function
distances
reconstruction
sampling
sets
knyazev
guided
signal
reconstruction
theory
iii
overview
reconstruction
hilbert
space
intersection
consists
signals
guiding
reconstruction
subspace
zero
samples
projections
important
role
reconstruction
stated
following
assumption
reconstruction
uniqueness
reconstruction
given
signal
unique
⊥∩t
otherwise
possible
reconstructions
form
closed
plane
deﬁned
possible
basic
assumptions
reconstruction
sample
consistent
reconstructed
signal
yields
sample
original
signal
i.e
sˆf
sample
sufﬁcient
reconstructed
signal
fully
de-
termined
signals
sample
original
signal
i.e
ˆf1
ˆf2
∀f1
sf1
sf2
guiding
subspace
reconstruction
signals
guid-
ing
reconstruction
subspace
reconstructed
within
subspace
i.e
reconstruction
stability
small
change
original
signal
results
proportionally
small
change
reconstructed
signal
signals
axioms
imply
repeated
reconstruction
change
signals
already
recon-
structed
signal
i.e
ˆf2−f2
⊥∩t
∀f2
ˆf1
arbitrary
indeed
sˆf1
sf1
let
denote
sˆf1
sf1
axiom
gives
ˆf2
ˆf3
using
sˆf1
sf2
ˆf3
ˆf1
using
sf1
thus
ˆf2
ˆf1
proves
claim
axioms
imply
full
conditional
reconstruction
signals
guiding
reconstruction
subspace
exactly
reconstructed
signals
i.e
indeed
equivalent
time
equivalent
thus
one
hand
want
deﬁne
reconstruction
opera-
tor
i.e
reconstructed
signal
given
requires
uniqueness
hand
nontrivial
intersection
naturally
appear
additional
applications
see
e.g.
information
one
decide
one
reconstruction
plane
better
worse
another
according
mathematically
resolve
issue
replacing
space
quotient-space
collapsing
zero
consistently
replacing
subspaces
similar
quotient-spaces
replacements
assume
rest
section
reconstruction
operator
correctly
deﬁned
list
possible
requirements
reconstruction
operator
matching
sample
consistent
reconstructed
signal
yields
sample
original
signal
i.e
sample
sufﬁcient
reconstructed
signal
fully
deter-
mined
sample
original
signal
i.e
identity
rf1
rf2
holds
∀f1
sf1
sf2
guiding
subspace
reconstruction
guiding
recon-
struction
subspace
r-invariant
i.e
inclusion
reconstruction
stability
reconstruction
operator
hold
continuous
note
implies
linear
reconstruc-
tion
operator
equivalent
identity
therefore
axioms
lead
i.e
reconstruction
operator
projector
idempotent
since
making
requirement
stricter
⊥∩t
addition
axioms
implies
full
conditional
reconstruction
signals
guiding
reconstruction
subspace
exactly
reconstructed
i.e
indeed
equivalent
equivalent
thus
recon-
structed
signal
always
constrained
guiding
case
actually
target
reconstruction
subspace
addition
results
single
valid
choice
recon-
struction
operator
given
oblique
projector
see
onto
subspace
along
orthogonal
complement
sampling
subspace
deﬁning
oblique
projector
requires
assuming
⊥+t
addition
together
necessary
sufﬁcient
existence
uniqueness
intersection
sample-consistent
reconstruction
plane
guiding
reconstruction
subspace
original
signal
see
linear
operator
satisﬁes
see
bounded
see
discussion
sec
viii
traditional
assumption
may
result
oblique
projector
large
norm
circumvent
assumption
authors
propose
general
constrained
reconstruction
using
oblique
projector
onto
subspace
along
orthogonal
complement
sampling
subspace
reconstruction
minimizes
distance
reconstructed
signal
within
guiding
subspace
sample-consistent
reconstruction
plane
distance
zero
re-
construction
sample
consistent
i.e
satisﬁes
otherwise
represents
generalized
reconstruction
assumptions
may
approximated
even
completely
abandoned
example
minimax
regret
leads
reconstruction
orthogonal
projector
onto
guiding
reconstruction
subspace
easily
meets
requirements
stricter
version
sometimes
target
even
guiding
reconstruction
sub-
space
available
known
assumptions
inapplicable
replaced
signal
energy
minimization
reconstructed
signal
solves
following
constrained
minimization
problem
khˆfk
subject
sˆf
inf
non-singular
operator
taking
approximates
core
minimization
problem
introduced
next
section
knyazev
guided
signal
reconstruction
theory
proposed
reconstruction
methods
sample
consistent
reconstruction
ﬁrst
propose
novel
formulation
algorithms
sample
consistent
reconstruction
used
relaxes
constraint
used
instead
mini-
mizing
energy
consistently
sample
provide
mathematical
background
taking
advantage
theory
developed
used
address
issues
existence
uniqueness
prove
giving
necessary
theoretical
foundation
supplementing
speciﬁcally
reconstructed
signal
determined
solution
following
constrained
minimization
problem
kˆf
tˆfk
subject
sˆf
inf
equivalent
problem
inf
ˆx∈s⊥
cid:10
cid:11
solutions
problems
correspondingly
unique
choose
solutions
corresponding
factor-spaces
e.g.
normal
i.e
smallest
norm
solutions
ˆfn
ˆxn
guarantee
uniqueness
required
deﬁne
reconstruction
operator
reconstruction
based
solving
satisﬁes
assumptions
design
assumptions
traditional
literature
solution
result
oblique
projection
method
resulting
algorithms
different
based
actions
orthogonal
projectors
without
necessarily
using
frames
moreover
need
neither
assumptions
makes
method
robust
applications
allows
choosing
greater
variety
subspaces
com-
pared
conventional
reconstruction
example
violating
assumption
allows
oversampling
e.g.
handling
noisy
data
sensors
advocated
problem
equivalently
written
following
operator
form
cid:0
s⊥t⊥
cid:1
cid:12
cid:12
−s⊥t⊥sf
|s⊥
denotes
operator
restriction
invariant
subspace
i.e
domain
s⊥t⊥
restricted
solution
problem
reconstructed
signal
equivalently
satisﬁes
s⊥t⊥ˆf
sˆf
operator
form
constrained
minimization
system
equations
particular
case
following
system
investigated
see
also
aˆf
bounded
self-adjoint
non-negative
operator
i.e
get
system
split
system
equivalent
cid:0
s⊥a
cid:1
cid:12
cid:12
asf
conditions
existence
uniqueness
solutions
equations
derived
adapted
reconstruction
problem
sec.s
systems
advantageous
numerical
solu-
tion
e.g.
solved
iteratively
sec
propose
conjugate
gradient
iterative
method
solving
matrix
orthoprojector
needed
iterative
solver
substituted
function
deﬁning
multiplication
orthoprojector
given
vector
multiplication
approximate
e.g.
implementing
action
signal
ﬁlter
describe
sec
instead
relying
traditional
frame-based
deﬁnition
guiding
subspace
moreover
generic
ﬁlter
may
substitute
analyzing
substitution
beyond
scope
present
paper
least
squares
minimization
formulations
equivalent
elegant
geometric
interpretation
cf.
equality
e.g.
lines
fig
second
minimization
problem
simply
determines
shortest
distance
be-
tween
sample-consistent
closed
plane
guiding
closed
subspace
indeed
ﬁrst
minimization
problem
inner
minimization
ﬁxed
vector
infˆt∈t
kˆf
ˆtk
always
solution
tˆf
using
orthogonality
argument
outer
minimization
exactly
problem
strictly
guided
generalized
reconstruction
last
minimization
problem
swap
order
minimization
compared
ﬁrst
minimization
problem
equality
call
solution
problem
strictly
guided
reconstructed
signal
sec
viii-b
discuss
additional
assumptions
turn
strictly
guided
reconstructed
signal
well-known
generalized
reconstructed
signal
proposed
analogy
operator
form
ﬁrst
minimiza-
tion
problem
minimization
problem
equivalent
cid:0
cid:1
indeed
ﬁxed
vector
minimization
problem
infˆf
∈sf
+s⊥
kˆf
equivalent
hilbert
space
orthogonality
condition
i.e
equivalent
s⊥t
solving
thus
turning
inf
min
minimization
due
linear
constraint
outer
minimization
minimizer
exists
satisﬁes
orthogonality
condition
cid:0
s⊥ˆt
cid:1
cid:0
cid:1
equivalent
completes
argument
interesting
compare
solution
constrained
frame-less
reconstruction
given
oblique
projector
⊥st
closed
subspace
along
closed
subspace
motivated
fig
prove
sec
viii
⊥st
additional
assumption
uniqueness
required
deﬁne
single-valued
operator
⊥st
applicability
assumptions
equivalent
case
also
discuss
strictly
guided
reconstruction
deﬁned
via
factor
space
analysis
knyazev
guided
signal
reconstruction
theory
equation
solved
iteratively
e.g.
conjugate
gradient
method
equivalently
transformed
tsf
tstˆt
tsf
providing
cid:12
cid:12
interesting
alternative
solving
example
equation
require
knowing
sampling
subspace
explicitly
contrast
moreover
matrix
orthoprojector
needed
replaced
sampling
function
deﬁning
multiplication
orthoprojector
given
vector
furthermore
sampling
function
approximate
necessarily
null-space
may
even
change
course
iterations
e.g.
varying
time
time-series
signals
depending
current
iterative
reconstructed
signal
flexibility
approximating
sampling
guiding
procedures
possible
formulation
appears
important
practical
applications
extensions
beyond
scope
present
paper
note
minimax
regret
reconstruction
tsf
interpreted
rudimentary
one-step
iterative
solver
zero
initial
guess
solving
tstˆt
tsf
implications
conditions
optimality
turning
attention
second
minimization
problem
shortest
distance
sample-consistent
closed
plane
guiding
closed
subspace
obtain
following
ﬁrst-order
necessary
sufﬁcient
conditions
optimality
cid:26
tˆf
s⊥ˆt
already
derived
sample
consistent
strictly
guided
recon-
structions
principle
computed
together
solving
system
equations
numerically
instead
doubling
number
unknowns
one
substitute
second
equation
s⊥ˆt
system
ﬁrst
one
tˆf
obtaining
equation
cid:0
ts⊥
cid:1
tsf
e.g.
sec
latter
equation
turns
already
considered
equation
since
ts⊥
indeed
exact
orthoprojector
onto
assume
throughout
paper
use
discover
important
identity
next
paragraph
multiplying
parts
ﬁrst
equation
see
multiplying
parts
second
equation
one
conﬁrms
already
used
deriving
equation
thus
moreover
follows
ps∩t
cid:16
cid:17
ps∩t
⊥ˆf
ps∩t
ps∩t
orthoprojector
onto
closed
subspace
come
simple
orthogonal
decomposition
ps∩t
knowing
allows
primarily
concentrate
rest
paper
consistent
reconstruction
reconstruction
set
regularization
assuming
minimization
problems
solutions
deﬁne
reconstruction
set
union
closed
intervals
end
points
tˆf
solution
unique
reconstruction
set
single
interval
illustrated
fig
within
reconstruction
set
sample
consistent
recon-
structed
signal
evidently
expected
smallest
assuming
sample
accurate
identities
pythagorean
theorem
immediately
imply
following
theorem
reconstruction
error
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
theorem
let
ˆfα
point
reconstruction
set
given
ˆfα
αˆf
reconstruction
error
given
cid:13
cid:13
cid:13
ˆfα
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
kps∩t
fk2
kps∩t
shortest
distance
deﬁned
sample-consistent
closed
plane
guiding
closed
subspace
trust
sample-consistent
closed
plane
actually
accurate
theorem
reconstruction
error
indeed
minimized
sample
consistent
reconstructed
signal
given
end
point
noise
sample
measurements
may
decide
trust
guiding
closed
subspace
sample
choose
output
reconstruction
convex
linear
combination
αˆf
tˆf
within
reconstruction
set
extreme
choice
gives
strictly
guided
reconstruction
tˆf
already
discussed
complete
reconstruction
set
determined
allows
selecting
single
reconstructed
signal
e.g.
minimizing
cost/quality
function
minimization
constrained
neighborhood
reconstruction
set
signal
energy
one
example
cost/quality
function
e.g.
leading
minimization
like
constrained
neighborhood
reconstruction
set
procedure
eliminates
typical
difﬁculty
choosing
good
regularization
parameter
regularization-based
inconsistent
methods
practical
applications
common
sampling
procedure
involves
inaccuracy
input
signal
inaccuracy
may
appear
due
one
combination
noise
limited
accuracy
sensor
providing
sampling
procedure
limited
precision
data
representing
input
signal
one
determine
level
inaccuracy
input
signal
relative
shortest
distance
kps∩t
sample
consistent
guiding
sets
cost/quality
function
constructed
takes
level
inaccuracy
account
one
also
relax
reconstruction
set
constraint
cost/quality
function
minimization
consider
alternative
formulations
e.g.
like
interior
point
methods
minimizing
weighted
sum
based
cost/quality
function
distance
reconstruction
set
based
primal-dual
relaxations
goes
beyond
scope
present
paper
knyazev
guided
signal
reconstruction
theory
uniqueness
reconstructed
signal
assume
hold
rest
section
following
theorem
gives
condition
reconstruc-
tion
uniqueness
theorem
based
lemma
4.2
let
solution
solution
solutions
unique
otherwise
solutions
form
plane
plane
⊥∩t
exists
unique
normal
solutions
minimal
norm
ˆxn
ˆfn
belong
intersection
corresponding
plane
closed
subspace
cid:0
cid:1
ˆfn
ˆxn
theorem
gives
enough
information
prove
theorem
reconstruction
method
satisﬁes
theorem
proof
according
solution
since
minimizing
quantity
turns
zero
smallest
possible
value
theorem
solutions
form
plane
since
solution
reconstruction
problem
reconstruction
determined
unique
vice
versa
consistently
assumption
happen
e.g.
number
samples
small
guiding
reconstruction
space
large
similar
issue
appears
dealing
non-unique
strictly
consistent
reconstructions
choosing
subspace
i.e
con-
straining
guiding
reconstruction
space
propose
different
approach
constraining
orthogonal
complement
sampling
subspace
reconstruction
determined
arbitrary
signal
intersection
section
iii
treat
plane
unique
element
quotient
space
factoring
intersection
quotient
space
mathematically
powerful
may
impractical
applications
practice
may
desired
choose
imposing
restrictions
reconstruction
single
solution
representing
equiva-
lence
class—the
whole
plane
solutions
minimum
norm
solution
one
choice
unique
representative
obtained
restricting
solution
cid:0
cid:1
suggested
theorem
however
minimum
norm
requirement
may
relevant
properties
signal
reconstructed
alternatively
obtain
well-deﬁned
unique
recon-
struction
choosing
solution
reconstruction
problem
reconstruction
determined
given
closed
subspace
normal
solution
special
case
cid:0
cid:1
note
uniqueness
condition
satisﬁed
equal
restriction
reconstruction
consistent
e.g.
choice
normal
solution
known
order
isomorphic
quotient
space
necessary
sufﬁcient
subspace
complimentary
i.e
cid:0
cid:1
cid:0
cid:1
assumptions
imply
solution
unique
proposition
additional
constraint
makes
reconstruction
unique
reconstruction
problem
becomes
inf
ˆx∈m∩s⊥
cid:10
cid:11
order
write
problem
unconstrained
form
similar
introduce
orthogonal
projectors
onto
onto
subspace
projector
onto
intersection
two
subspaces
closed
form
expression
terms
projectors
individual
subspaces
given
anderson-dufﬁn
formula
cid:0
cid:1
note
pseudo-inverse
cid:0
cid:1
bounded
minimal
gap
subspaces
positive
see
e.g.
theorems
2.15
2.18
projector
rewrite
equation
anal-
ogy
equivalent
form
−ft⊥sf
cid:0
ft⊥
cid:1
cid:12
cid:12
solution
applied
directly
needing
solved
via
conjugate
gradient
method
let
note
case
normal
solution
method
ﬁnd
normal
cid:0
cid:1
reconstruction
satisﬁes
imply
full
conditional
reconstruction
i.e
⊥∩t
already
know
select
unique
representative
recon-
struction
restricting
solution
s⊥ˆf
original
signal
representative
plane
⊥∩t
signals
indistinguishable
within
assumptions
order
match
unique
representative
satisfying
s⊥ˆf
representative
plane
original
signals
make
natural
assumption
unmeasured
component
s⊥f
make
fully
recoverable
next
theorem
theorem
let
s⊥f
unique
solution
reconstruction
proof
given
conditions
s⊥f
clearly
s⊥f
solution
since
unique
s⊥f
thus
even
unmeasured
part
true
signal
energy
outside
formulation
ensures
com-
ponents
fully
recovered
would
beneﬁcial
chosen
large
portion
signal
energy
expected
contained
next
discuss
conditions
existence
stability
reconstructed
signal
existence
stability
begin
stating
conditions
wellposedness
i.e
existence
stability
solution
problem
since
later
used
give
bound
reconstruction
error
denote
operator
cid:0
s⊥a
cid:1
cid:12
cid:12
obtaining
s⊥as
s⊥a
knyazev
guided
signal
reconstruction
theory
normal
solution
equation
depends
continuously
pseudo-inverse
operator
⊥/n
bounded
⊥/n
denotes
quotient
space
equivalent
operator
bounded
iff
closed
following
theorem
restates
conditions
terms
problem
theorem
based
theorem
4.3
normal
solution
ˆfn
ˆxn
ˆxn
s⊥ˆf
s⊥r
exists
depends
continuously
arbitrary
inf
x∈s⊥r
x6=0
moreover
condition
implies
axi
also
leads
upper
bound
kˆfnk2
ksfk2
+kˆxnk2
kˆxnk2
ρ2ks⊥
asf
taking
obtain
system
condition
equivalent
inf
x∈s⊥t
kt⊥xk
kxk
becomes
key
assumption
let
describe
via
concepts
minimal
gap
angles
subspaces
theorem
based
lemma
4.6
let
deﬁned
cos
θmax
cid:0
cid:1
inf
/∈t
dist
cid:0
cid:1
dist
minimal
gap
closed
subspaces
largest
non-trivial
angle
closed
subspaces
θmax
sup
π/2
proof
according
lemma
4.6
inf
x∈s⊥t
kt⊥xk
kxk
minimal
gap
sec
iv-4
equal
sine
friedrichs
angle
subspaces
see
e.g.
theorem
2.15
relationships
cid:0
cid:1
cid:0
cid:1
given
theorem
2.7
particular
cid:0
cid:1
sin
cid:0
inf
cid:0
cid:1
cid:1
inf
π/2
θmax
let
also
note
lemma
4.6
proof
theorem
included
supplementary
material
assumption
equivalent
assuming
sum
closed
latter
automatically
satisﬁed
⊥+t
traditionally
assumed
reconstruction
literature
see
e.g.
dim
case
e.g.
graph-based
signal
processing
every
subspace
automatically
closed
i.e.
assumptions
existence
theorems
automatically
hold
contrast
e.g.
theorem
4.1
requiring
⊥∩t
contradiction
however
since
theorem
4.1
postulates
existence
exact
reconstruction
i.e.
correctly
argues
signal
⊥∩t
exactly
reconstructed
since
unless
merely
claim
existence
solution
equation
deal
issues
stemming
separately
sec
theorems
immediately
imply
theorem
cos
θmax
exists
solution
reconstruction
problem
signal
normal
solution
ˆfn
unique
bounded
kˆfnk2
ksfk2
ks⊥t⊥sfk2/
cos4
θmax
let
note
theorem
applies
theorem
leaves
open
question
whether
condition
condition
still
necessary
case
rest
section
beyond
results
presented
address
question
using
powerful
theory
pair
two
orthogonal
projectors
see
e.g.
theorem
denote
subspace
orthogonal
four
subspaces
introduced
let
orthogonal
projector
onto
subspace
assumption
cos
θmax
necessary
sufﬁcient
existence
solution
reconstruction
problem
signal
normal
solution
ˆxn
giving
normal
sample
consistent
reconstruction
ˆfn
ˆxn
+sf
normal
strictly
guided
reconstruction
ˆtn
tˆfn
ˆfn
ps∩t
exists
depends
continuously
arbitrary
cos
θmax
cos
θmax
bound
holds
kˆxnk
kt⊥sp0fk/
cos
θmax
well
kˆxnk
ksp0fk
tan
θmax
kˆfnk2
kˆtnk2
kps∩t
fk2
ksfk2
kˆxnk2
proof
recall
subspace
orthogo-
nal
four
subspaces
s∩t
⊥∩t
s∩t
⊥∩t
one
hand
right-hand
side
equation
i.e
s⊥t⊥sf
set
possible
right-hand
sides
s⊥t⊥s
equation
proper
general
subspace
hand
ﬁve
spaces
including
invariant
orthogonal
projectors
knyazev
guided
signal
reconstruction
theory
hence
complements
let
denote
corresponding
restrictions
product
s⊥t⊥
also
h0-invariant
thus
closed
subspace
invariant
operator
cid:0
s⊥t⊥
cid:1
cid:12
cid:12
denoting
restriction
observe
operator
sum
operator
orthogonal
projector
onto
⊥∩t
operators
bounded
self-adjoint
spectrum
included
interval
except
extra
eigenvalue
⊥∩t
extra
eigenvalue
⊥∩t
smallest
point
spectrum
cos2
θmax
deﬁned
characterized
theorem
thus
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
k−1
cid:13
cid:13
1/ν2
sign
means
moore–penrose
pseudoinverse
therefore
substitute
equation
normal
solution
satisﬁes
ˆxn
exists
assuming
cos
θmax
obtain
bound
kˆxnk
ρks⊥t⊥sfk
1/ν2
sharp
due
equivalent
pythagorean
theorem
manipulate
expression
ˆxn
k−1
cid:0
s⊥t⊥sf
cid:1
cid:0
s0p0f
cid:1
s0p0f
0s⊥
newly
introduced
operator
deﬁned
bounded
extension
zero
h0∩s
i.e
k0u
k⋆u
h0∩s
k0u
orthogonal
subspace
kˆxnk
kk†
kk†
s0p0fk
kk†
kt⊥sp0fk/cos
θmax
s0p0fk
kkt⊥
0s⊥
0s⊥
0s⊥
s0p0fk
since
kk†
kk†
0s⊥
written
h0∩s⊥
0s⊥
kk†
0s⊥
k0|†
kk−1
cid:12
cid:12
h0∩s⊥
k0|†
h0∩s⊥
second
equality
arises
writing
orthogonal
decomposition
noting
alternatively
may
split
product
step
two
vanishes
follows
kˆxnk
kk†
kk†
kk†
ksp0fk
tan
θmax
s0p0fk
s0s0p0fk
s0kks0p0fk
0s⊥
0s⊥
0s⊥
last
equality
follows
writing
hence
0s⊥
†t⊥
†s⊥
†s⊥
†s⊥
positive
singular
values
finally
tan
θmax
follows
theorem
4.1
shows
operator
†s0
equal
tangent
angles
subspaces
cos
θmax
remains
show
solution
reconstruction
problem
may
fail
exist
signal
i.e
equation
k⋆x
−s⊥t⊥sf
may
solution
since
operator
bounded
closed
inverse
k−1
closed
unbounded
hence
basic
results
functional
analysis
state
operator
closed
unbounded
range
closed
thus
proper
subset
consequently
solution
fails
exist
p0f
complete
proof
noting
theorem
claims
normal
smallest
norm
strictly
guided
recon-
struction
ˆtn
follow
closed
0s⊥
ﬁnally
underline
none
bounds
derived
one
i.e
one
general
sharper
vii
reconstruction
error
bounds
original
signal
satisﬁes
⊥∩t
proposed
reconstruction
perfectly
recovers
suppose
obtain
reconstruction
solving
important
question
context
bound
error
solution
reconstruction
problem
evidently
unique
case
still
possible
bound
reconstruction
error
factor
space
cid:0
cid:1
let
orthogonal
projector
onto
cid:0
cid:1
ps⊥∩t
norm
error
factor
space
equals
norm
projection
error
subspace
representing
factor
space
cid:0
cid:1
words
need
bound
quantity
cid:13
cid:13
cid:13
removing
consideration
ps⊥∩t
part
original
signal
ignoring
non-unique
part
ps⊥∩t
reconstructed
signal
uniqueness
condition
holds
cid:0
cid:1
unique
normal
solution
ˆfn
problem
simply
drops
ps⊥∩t
part
original
signal
thus
term
kps⊥∩t
appears
upper
bound
kˆfn
cid:13
cid:13
cid:13
cid:16
cid:17
cid:13
cid:13
cid:13
ps⊥∩t
part
original
signal
visible
neither
sample
guiding
orthoprojector
thus
term
kps⊥∩t
expected
error
bound
cid:16
cid:17
cid:13
cid:13
cid:13
cid:16
cid:17
following
theorem
gives
reconstruction
error
bounds
theorem
let
cos
θmax
notation
theorem
let
consider
normal
solution
ˆxn
giving
normal
knyazev
guided
signal
reconstruction
theory
reconstruction
ˆfn
ˆxn
well
reconstruction
obtained
solving
let
orthoprojector
onto
deﬁned
theorem
following
bounds
hold
cid:13
cid:13
cid:13
cid:16
cid:17
cid:13
cid:13
cid:13
kps⊥∩t
fk2
cid:13
cid:13
ˆxn
s⊥p0f
cid:13
cid:13
cid:13
cid:13
cid:13
ˆfn
cid:13
cid:13
cid:13
kps⊥∩t
fk2+kps⊥∩t
fk2+
cid:13
cid:13
ˆxn
s⊥p0f
cid:13
cid:13
cid:13
cid:13
ˆxn
s⊥p0f
cid:13
cid:13
ks⊥t⊥p0fk/
cos2
θmax
cid:13
cid:13
ˆxn
s⊥p0f
cid:13
cid:13
kt⊥p0fk/
cos
θmax
bounds
sharp
proof
reconstructions
sample
consistent
i.e
sˆfn
sˆfn
using
ps⊥∩t
ps⊥∩t
ps∩t
ps∩t
obtain
orthogonal
decomposition
cid:16
cid:17
−ps⊥∩t
s⊥p0
cid:16
cid:17
similarly
orthogonal
decomposition
error
normal
reconstruction
ˆfn
−ps⊥∩t
ps⊥∩t
s⊥p0
cid:16
ˆfn
cid:17
last
term
identities
s⊥p0ˆf
s⊥p0ˆfn
s⊥p0
ˆxn
ˆxn
pythagorean
theorem
thus
proves
identities
statement
theorem
following
algebraic
transformations
proof
the-
get
orem
cid:0
s⊥t⊥
cid:1
cid:12
cid:12
s⊥∩h0
ˆxn
s⊥p0f
−k−1
−k−1
−k−1
−k−1
−k†
0s⊥
cid:0
s⊥t⊥sp0f
cid:1
s⊥p0f
cid:0
s⊥t⊥sp0f
k⋆s⊥p0f
cid:1
cid:0
s⊥t⊥sp0f
s⊥t⊥s⊥p0f
cid:1
cid:0
s⊥t⊥p0f
cid:1
p0f
finally
using
arguments
similar
proof
theorem
obtain
bounds
cid:0
s⊥t⊥p0f
cid:1
cid:13
cid:13
cid:13
cid:13
k−1
cid:13
cid:13
k−1
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
s⊥t⊥p0f
cid:13
cid:13
ks⊥t⊥p0fk/cos2
θmax
cid:13
cid:13
cid:13
cid:13
cid:13
p0f
cid:13
cid:13
kt⊥p0fk/cos
θmax
p0f
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
0s⊥
0s⊥
complete
proof
sharpness
bounds
shown
sec
c-c.
error
bounds
theorem
based
improve
extend
general
case
bound
kt⊥fk/cos
θmax
obtained
consistent
reconstruction
method
presented
dropping
unnecessary
assumptions
sampling
guiding
subspaces
made
error
bounds
theorem
based
new
neither
bounds
derived
one
fig
example
details
viii
alternative
equivalent
formulations
assume
uniqueness
section
quotient
space
reconstruction
oblique
projector
onto
subspace
along
sub-
space
denote
conventionally
used
com-
pute
reconstructed
signal
constrained
existence
relies
traditional
assumption
made
equivalent
θmax
π/2
cid:8
cid:9
spectral
norm
oblique
projector
determin-
ing
stability
reconstruction
equal
6.2
attributed
del
pasqua
1955
case
1/γ
cos
θmax
see
theorem
oversampling
make
intersection
nontrivial
i.e
nontrivial
orthogonal
decomposition
cid:0
cid:1
case
oblique
projector
deﬁned
whole
space
instead
deﬁned
within
subspace
cid:0
cid:1
latter
represents
quotient
space
speciﬁc
reconstruction
algorithm
implementing
idea
illustrated
fig
follows
let
ps∩t
orthogonal
projection
original
signal
subspace
difference
ps∩t
reduced
signal
representing
original
signal
quotient
space
oblique
projector
onto
subspace
along
subspace
deﬁned
within
acting
reduced
signal
ps∩t
gives
sample
consistent
ps∩t
call
ˆfg
generalized
reconstruction
since
generalized
reconstruction
see
next
section
ˆfg
ps∩t
sample
consistent
reconstructed
signal
ﬁnally
obtained
adding
subtracted
term
ps∩t
back
i.e
ˆfg
ps∩t
next
prove
quotient
space
reconstruction
method
results
previously
deﬁned
sample
consistent
reconstructed
signal
ps∩t
ˆfg
i.e.
obtain
reconstructions
theorem
10.
let
quotient
space
reconstruction
method
equivalent
gives
reconstructed
signal
solving
problem
ˆfg
knyazev
guided
signal
reconstruction
theory
proof
ﬁrst
discuss
conditions
recon-
structed
signal
uniqueness
approaches
proposition
assumption
necessary
sufﬁcient
quotient
space
constrained
reconstruction
uniqueness
since
subspaces
orthogonal
thus
former
affected
vanishing
latter
quotient
space
arguments
sec
applicable
well
quotient
space
constrained
reconstruction
viewed
extensions
arguments
quotient
space
second
compare
conditions
reconstructed
signal
existence
continuous
dependence
original
signal
subspace
necessarily
closed
domain
oblique
projector
reduced
signal
ps∩t
need
apply
arbitrary
closure
thus
necessary
sufﬁcient
existence
reconstructed
signal
using
quotient
space
constrained
reconstruction
arbitrary
original
signal
sum
two
closed
subspaces
closed
iff
minimal
gap
sec
iv-4
positive
case
iff
cid:0
cid:1
cos
θmax
theorem
identities
moreover
deﬁnition
minimal
gap
essentially
deﬁned
quotient
space
factoring
intersection
nontrivial
allow
implies
formula
1/γ
equation
6.2
spectral
norm
oblique
projector
deﬁned
within
subspace
remains
valid
even
conclude
assumption
θmax
π/2
necessary
sufﬁcient
existence
reconstructed
signal
using
quotient
space
reconstruction
arbitrary
original
signal
well
guarantees
stability
reconstruction
comparing
assumption
theorem
taking
account
also
conclude
θmax
π/2
necessary
sufﬁcient
existence
reconstructed
signal
approaches
quotient
space
reconstruction
minimization
remains
prove
approaches
also
give
reconstructed
signal
exists
let
reconstructed
signal
obtained
quotient
space
constrained
reconstruc-
tion
analyze
square
function
kt⊥ˆfk
minimized
using
following
identities
consequently
orthogonal
complement
also
t⊥-invariant
directly
veriﬁed
therefore
sum
t⊥ps∩t
cid:16
ps∩t
cid:17
conclude
also
orthogonal
pythagorean
theorem
applicable
second
identity
trivially
follows
t⊥ps∩t
ps∩t
since
observe
identity
ﬁrst
term
sums
constant
changing
minimization
since
ps∩t
simply
orthogonal
projection
original
signal
subspace
show
second
term
vanishes
minimizer
indeed
deﬁnition
quotient
space
reconstruction
ˆfg
ps∩t
ˆfg
ps∩t
sample
consistent
ps∩t
i.e
following
holds
sˆfg
ps∩t
ps∩t
conclude
t⊥ˆfg
orthogonal
sum
t⊥f
ps∩t
t⊥ˆfg
smallest
possible
norms
sˆf
sˆfg
ps∩t
i.e
reconstructed
signal
obtained
quotient
space
reconstruction
valid
minimizer
finally
comparing
identity
ˆfg
ps∩t
i.e.
ps∩t
immediately
implies
ˆfg
comparison
generalized
reconstruction
equivalent
quotient
space
approach
proposed
oblique
projector
onto
subspace
along
subspace
deﬁned
within
substi-
tuted
oblique
projector
⊥st
onto
subspace
along
resulting
generalized
reconstruction
ˆfg
⊥st
ps∩t
indeed
traditional
assumption
made
oblique
projector
onto
subspace
along
subspace
transforms
assumption
oblique
projector
⊥st
equivalent
θmax
π/2
analogy
to⊥
latter
automatically
satisﬁed
contrast
assumption
spectral
norm
oblique
projec-
tor
⊥st
equal
lemma
4.4
case
1/γ
cos
θmax
see
sec
thus
conclude
⊥st
cid:0
cid:1
theorem
reconstruction
error
ﬁrst
identity
holds
vector
sums
cid:13
cid:13
t⊥ps∩t
cid:13
cid:13
kps∩t
fk2
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:16
ps∩t
cid:17
cid:13
cid:13
cid:13
cid:16
ps∩t
cid:17
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
t⊥ˆf
cid:13
cid:13
cid:13
ps∩t
ps∩t
ps∩t
cid:16
ps∩t
cid:17
orthogonal
ps∩t
also
ps∩t
ps∩t
consistently
orthogonal
decomposition
hilbert
space
cid:0
cid:1
construction
quotient
space
reconstruction
moreover
subspace
trivially
invariant
respect
orthogonal
projector
kps∩t
fk2
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
ˆfg
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
bounded
theorem
extending
improving
bound
kˆfg
kt⊥fk/
cos
θmax
comparison
regularized
reconstruction
notation
formulated
using
following
unconstrained
quadratic
minimization
problem
regularization-based
methods
suggested
inf
cid:13
cid:13
cid:13
sˆfρ
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
hˆfρ
cid:13
cid:13
cid:13
knyazev
guided
signal
reconstruction
theory
operator
interpreted
ﬁlter
e.g.
may
ap-
proximate
case
problem
approximates
inf
cid:13
cid:13
cid:13
sˆfρ
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:16
ˆfρ
tˆfρ
cid:17
cid:13
cid:13
cid:13
problem
viewed
relaxation
authors
assume
exists
unique
inter-
section
sample-consistent
closed
plane
guiding
closed
subspace
claim
without
proof
minimizer
ˆfρ
approximates
intersection
prove
surprising
result
assumption
unique
intersection
minimizer
ˆfρ
equal
intersection
i.e
ˆfρ
actually
depend
trivial
consequence
even
stunning
result
set
solutions
varying
general
nothing
reconstruction
set
removed
end
points
belonging
sample-consistent
plane
guiding
subspace
theorem
11.
let
reconstruction
set
given
formula
ˆfα
αˆf
tˆf
solves
ˆfα
solves
problem
equivalent
following
linear
equation
cid:0
ρt⊥
cid:1
ˆfρ
hand
consistent
reconstruction
solves
i.e
s⊥t⊥ˆf
sˆf
taking
substituting
ˆfα
ˆfρ
obtain
elementary
calculations
proof
one
hand
minimization
problem
cid:18
cid:19
cid:16
αˆf
tˆf
cid:17
using
properties
projectors
sample
measurements
may
decide
trust
guiding
closed
subspace
sample
choose
output
reconstruction
convex
linear
combination
αˆf
tˆf
within
reconstruction
set
use
extreme
choice
results
strictly
guided
reconstruction
tˆf
speciﬁcally
reconstruction
noisy
otherwise
inaccurate
samples
substituted
represents
deviation
true
sample
select
knk
kˆf
tˆfk
numerator
knk
may
known
speciﬁcations
sampling
sensor
denominator
kˆf
tˆfk
easily
computable
directly
next
section
present
conjugate
gradient
based
methods
solve
proposed
reconstruction
problem
iterative
reconstruction
algorithms
iterative
algorithm
based
projection
convex
sets
pocs
reconstructing
band-limited
graph
signal
pre-
sented
starting
initial
guess
iteration
algorithm
projects
signal
resets
signal
samples
given
samples
pocs
method
interpreted
richardson
iterative
method
solving
xm−1
present
context
iteration
becomes
cid:0
s⊥t⊥
cid:1
cid:12
cid:12
−s⊥t⊥sf
s⊥tfm−1
fm−1
xm−1
theorem
extended
case
ﬁlter
approximates
orthoprojector
may
fail
general
ﬁlters
e.g.
practically
important
graph-
based
setup
polynomial
nonlinear
ﬁlters
exists
unique
intersection
sample-consistent
closed
plane
guiding
closed
subspace
intersection
tˆf
assumed
reconstruction
set
thus
trivially
reduced
single
element
tˆf
theorem
minimizer
ˆfρ
simply
ˆfρ
tˆf
matter
value
reconstruction
set
nontrivial
intention-
ally
move
reconstructed
signal
away
sample-
consistent
reconstruction
plane
toward
guiding
subspace
e.g.
assuming
sampling
procedure
noisy
sum
penalizes
moving
reconstructed
signal
away
sample-consistent
reconstruction
plane
guiding
subspace
speciﬁc
value
regularization
parameter
need
chosen
priori
e.g.
according
noise
level
problem
solved
directly
theorem
allows
choose
value
posteriori
determining
reconstruction
set
well
try
variety
choices
extra
costs
example
let
reconstruction
set
closed
interval
end
points
tˆf
trust
sample-
consistent
closed
plane
actually
accurate
choose
reconstruction
sample
consistent
solves
e.g.
minimization
problem
noise
pocs
method
conjugate
gradient
optimal
iterative
method
solving
linear
systems
linear
self-
adjoint
non-negative
operator
bounded
pseudo
inverse
basics
reviewed
appendix
would
like
use
solve
difﬁculty
lies
fact
s⊥t⊥
self-adjoint
general
however
shown
restriction
s⊥t⊥
invariant
subspace
self-adjoint
positive
semi-deﬁnite
proposition
let
two
orthoprojectors
operator
cid:0
s⊥t⊥
cid:1
cid:12
cid:12
self-adjoint
operator
lower
upper
bounds
hold
proof
s⊥u
since
orthoprojector
self-adjoint
thus
cid:10
s⊥t⊥u
cid:11
cid:10
t⊥u
s⊥v
cid:11
cid:10
t⊥u
cid:11
cid:10
t⊥v
cid:11
cid:10
s⊥u
t⊥v
cid:11
cid:10
s⊥t⊥v
cid:11
kui
cid:10
t⊥u
cid:11
cid:10
t⊥u
t⊥u
cid:11
use
solving
thanks
proposition
solution
unique
converges
unique
normal
solution
minimum
norm
needs
ini-
tialized
since
optimal
iterative
method
computes
efﬁcient
signal
reconstruction
solution
iterations
satisﬁes
taking
proves
operator
bounds
since
arg
min
x∈s⊥∩
¯km
cid:10
cid:11
knyazev
guided
signal
reconstruction
theory
¯km
plane
deﬁned
cid:0
s⊥t⊥
cid:1
cid:12
cid:12
−s⊥t⊥sf
note
super-resolution
using
preconditioned
suggested
image
reconstruction
multiple
low-resolution
frames
video
sequence
assuming
explicitly
known
imaging
models
convergence
analysis
convergence
speed
iterative
methods
solving
linear
system
depends
condition
number
since
operator
cid:0
s⊥t⊥
cid:1
cid:12
cid:12
self-adjoint
positive
semi-deﬁnite
possibly
non-trivial
null-space
special
considerations
apply
see
e.g.
references
proof
theorem
−s⊥t⊥sf
s⊥t⊥s
substitute
deﬁned
restriction
equation
even
though
practical
implementations
richardson
iterative
method
well
one
simply
multiplies
vectors
s⊥t
convergence
analysis
based
k⋆x
soon
iterative
errors
stay
within
subspace
see
latter
easily
achieved
choosing
simply
initiate
iterative
method
convergence
speed
determined
spectral
condition
number
operator
proof
theorem
smallest
point
σmin
spectrum
cos2
θmax
deﬁned
characterized
theorem
largest
point
σmin
bounded
one
hence
ki−
k⋆k
cos2
θmax
spectral
condition
number
bounded
σmax
σmin
cos2
θmax
iteration
initialized
relative
error
solution
iterations
satisﬁes
kxm
x∗k
kx∗k
cid:0
cos2
θmax
cid:1
actual
normal
solution
since
kxm
x∗k
xm−1
k⋆kkxm−1
x∗k
initialized
shown
relative
error
solution
obtained
iterations
satisﬁes
kxm
x∗kk
kx∗kk
cid:18
cid:19
cos
θmax
cid:19
cid:18
cos
θmax
see
e.g.
relative
error
pocs
decreases
geometrically
rate
depends
θmax
always
faster
pocs
acceleration
provided
becomes
pronounced
ill-conditioned
i.e
θmax
small
enough
reconstruction
bandlimited
graph
signals
notation
preliminaries
undirected
weighted
graph
collection
nodes
vertices
connected
set
edges
links
wij
j∈v
wij
denotes
edge
nodes
weight
wij
adjacency
matrix
graph
matrix
entries
wij
degree
node
sum
weights
edges
incident
i.e
wij
degree
matrix
diagonal
matrix
diag
combinatorial
laplacian
matrix
graph
deﬁned
use
normalized
form
laplacian
given
k−1/2lk−1/2
symmetric
positive
semi-
deﬁnite
matrix
set
real
eigenvalues
···
corresponding
orthogonal
set
eigenvectors
denoted
graph
signal
function
deﬁned
nodes
graph
value
signal
node
thus
graph
signal
also
represented
vector
indices
corresponding
nodes
graph
denote
subset
nodes
graph
collection
indices
denoting
complement
set
downsampled
signal
vector
reduced
length
|s|
obtained
taking
samples
subset
denote
space
signals
may
non-zero
values
identically
zero
known
eigenvalues
eigenvectors
provide
spectral
interpretation
i.e
notion
frequency
graph
signal
similar
fourier
transform
traditional
signal
processing
eigenvalues
thought
frequencies
high
eigenvalue
implies
higher
variation
corresponding
eigenvector
every
graph
signal
represented
eigenvector
basis
uii
compactly
graph
fourier
transform
gft
setting
bandlimited
signal
graph
deﬁned
signal
zero
gft
coefﬁcients
frequencies
greater
bandwidth
i.e
spectral
support
restricted
set
frequencies
words
ω-bandlimited
signal
energy
contained
within
subspace
spanned
eigenvectors
laplacian
eigenvalues
less
space
ω-bandlimited
signals
known
paley-wiener
space
denoted
reconstruction
problem
consider
problem
reconstructing
graph
signal
subsampled
version
assumption
band-limited
i.e
thus
sampling
subspace
reconstruction
subspace
permutation
groups
together
nodes
represent
projectors
cid:18
cid:19
cid:19
cid:18
preserves
samples
signal
sets
samples
zero
projector
low-pass
ﬁlter
written
graph
spectral
domain
xi=1
uiut
knyazev
guided
signal
reconstruction
theory
projector
orthogonal
complement
high
pass
ﬁlter
condition
bandlimited
graph
signal
uniquely
recovered
samples
given
equivalent
one
presented
theorem
xii
numerical
illustrations
section
apply
proposed
reconstruction
ap-
proaches
image
magniﬁcation
problem
problem
set-up
let
high
resolution
image
size
assume
samples
i.e.
low
resolution
version
downsizes
obtained
sampling
operator
image
factor
using
averaging
downsampling
adjoint
upsamples
low
resolution
image
simply
copying
pixel
value
block
get
back
image
thus
sampling
subspace
rw×w
space
images
take
constant
value
block
note
dims
w/r
projection
obtained
replacing
values
r×r
blocks
average
goal
estimate
input
signal
know
dct
captures
energy
natural
images
ﬁrst
low
frequency
coefﬁcients
thus
reasonable
guiding
subspace
space
images
bandlimited
lowest
frequencies
projector
subspace
simply
low
pass
ﬁlter
sets
higher
frequency
components
image
zero
also
decomposed
involves
taking
dct
setting
high
frequency
coefﬁcients
zero
whereas
converts
dct
coefﬁcients
spatial
domain
get
low
frequency
image
experiments
study
effect
dimt
k×k
quality
reconstruction
deﬁne
kscale
w/r
compares
dimensionality
sampling
guiding
subspace
kscale
corresponds
undersampling
problem
kscale
corresponds
oversampling
scenario
shorthand
used
denote
low
resolution
image
fdu
denote
projection
also
consider
scenario
samples
contaminated
noise
i.e.
i.i.d
gaussian
noise
result
input
image
becomes
approaches
study
compare
four
reconstruction
approaches
namely
consistent
reconstruction
ˆfc
generalized
reconstruction
ˆfg
regularized
reconstruction
ˆfα
minimax
regret
reconstruction
ˆfm
tfdu
consistent
reconstruction
ˆfc
calculated
ˆfc
fdu
solution
problem
s⊥t⊥x
−s⊥t⊥fdu
obtained
using
conjugate
gradient
method
generalized
reconstruction
ˆfg
computed
using
three
dif-
ferent
implementations
ﬁrst
implementation
solve
problem
fdu
using
conjugate
gradient
method
obtain
ﬁnal
reconstruction
given
sbt
lena
lena
0.5
kscale
α=0.7
scale
0.7
fig
effects
kscale
noise-free
reconstruction
ˆfg1
second
implementation
uses
projector
reconstruction
instead
sampling
operator
ˆfg2
conjugate
gradient
solution
problem
tstf
tfdu
third
implementation
ˆfc
supposed
available
generalized
reconstruction
computed
ˆfg3
tˆfc
mathematically
proved
implementations
would
produce
identical
reconstructions
conjugate
gradient
algorithm
converges
however
methods
algorithmically
distinct
may
converge
different
rates
shown
tests
later
regularized
reconstruction
ˆfr
posed
computed
solving
s+ρt⊥
fdu
via
conjugate
gradient
ˆfc
ˆfg
available
simply
take
convex
combination
ˆfα
αˆfc
ˆfg
theorem
ˆfr
ˆfα
although
two
solutions
mathematically
equivalent
upon
convergence
conjugate
gradient
similar
algorithmically
show
different
behavior
robustness
noise
small
ﬁxed
number
steps
experiments
observations
conduct
four
sets
experiments
study
different
aspects
reconstruction
methods
effect
under/oversampling
effect
noise
convergence
behavior
experiment
ﬁrst
experiment
take
noise
free
signal
fdu
input
observe
psnr
recon-
struction
different
methods
value
kscale
i.e.
amount
under/oversampling
varies
computing
ﬁrst
0.7.
fig
shows
plot
psnr
kscale
observe
undersampling
regime
i.e
kscale
ˆfc
equals
ˆfg
performs
better
ˆfm
case
oversampling
however
shows
ˆfc
offers
better
psnr
ˆfg
turn
performs
better
ˆfm
sampling
noise
free
method
keeps
samples
unchanged
expected
perform
better
example
reconstructed
images
ˆfg
ˆfc
dimh
256
256
dims
128
128
kscale
shown
fig
effect
reconstruction
quality
illustrated
fig
observe
increases
i.e.
samples
trusted
reconstruction
quality
improves
experiment
experiment
assume
noisy
i.i.d
gaussian
input
zero
mean
variance
0.001.
ﬁrst
focus
performance
ˆfα
varies
case
oversampling
factor
kscale
results
knyazev
guided
signal
reconstruction
theory
ˆfg
psnr=19.83db
ˆfc
psnr=26.29db
psnr=21.69db
ˆfg
psnr=19.73db
fig
noise-free
reconstruction
dims
128
128
dimt
kscale
lena
lena
0.5
kscale
scale
α=0.7
0.7
fig
effects
kscale
noisy
reconstruction
shown
fig
best
reconstruction
obtained
0.7.
observation
agrees
theoretically
suggested
optimal
value
αopt
kek2/kˆfg
ˆfck2
0.7.
next
analyze
performance
ˆfg
ˆfc
ˆfm
ˆfα=0.7
different
values
kscale
fig
minimax
regret
reconstruction
ˆfm
tfdu
contrast
noise-free
case
displayed
fig
produces
best
psnr
kscale
1.8
easily
explained
since
low-pass
ﬁlter
performing
image
denoising
thus
recommended
combine
reconstruction
procedure
pre-
possibly
post-denoising
e.g.
using
opposed
previous
noise
free
experiment
notice
fig
ˆfc
always
beat
ˆfg
noise
present
ˆfc
performs
better
ˆfg
heavy
oversampling
regime
example
kscale
2.5
observation
indicates
case
slight
oversampling
noise
ﬁltering
effect
projection
guiding
subspace
offsets
loss
due
sample
inconsistency
hand
heavy
oversampling
sample
consistency
requirement
important
also
observe
ˆfα
weighted
com-
bination
ˆfc
ˆfg
beat
ˆfc
ˆfg
kscale
1.5
example
image
offers
noise
suppression
deviating
much
consistency
requirement
fig
shows
example
noisy
input
image
reconstructed
images
experiment
experiment
study
relation-
ship
ˆfα
ˆfr
case
noisy
inputs
numerical
re-
sults
conﬁrm
parameter
known
beforehand
ﬁxed
two
approaches
despite
different
implementations
give
identical
reconstructions
however
parameter
needs
determined
application
ˆfα
clearly
favorable
ˆfr
terms
computation
complexity
computing
whole
set
solution
ˆfα
one
least
squares
problem
ˆfc
psnr=22.00db
ˆfα=0.7
psnr=22.88db
fig
reconstruction
results
noisy
inputs
kscale
lena
lena
scale
scale
axi
ter
axi
ter
fig
performance
different
implementations
ˆfg
needs
solved
compute
ˆfc
candidate
solution
points
calculated
αˆfc
tˆfc
since
ˆfg
tˆfc
hand
search
full
set
ˆfr
one
least
squares
problem
needs
solved
candidate
solution
may
computationally
feasible
experiment
previous
experiments
conju-
gate
gradient
algorithms
used
solve
least
squares
problem
allowed
converge
purpose
experiment
compare
reconstruction
methods
perform
iteration
conjugate
gradient
described
ˆfg
three
different
implementations
represented
ˆfg1
ˆfg2
ˆfg3
fig
noisy
input
compare
three
implementations
maximum
number
iterations
axiter
set
observe
ˆfg1
ˆfg2
cases
although
ˆfg3
different
number
iterations
seen
fig
difference
becomes
minor
number
iterations
equals
observation
also
holds
noise
free
inputs
since
ˆfg
three
implementations
ˆfα
also
different
corresponding
implementations
given
ˆfαi
αˆfc
ˆfgi
performance
reconstruction
methods
different
implementations
shown
fig
algorithms
conﬁgured
use
axiter
number
iterations
except
ˆfm
since
need
solve
least
square
problem
ˆfα2
omitted
knyazev
guided
signal
reconstruction
theory
lena
lena
scale
scale
axi
ter
axi
ter
fig
reconstructed
image
qualities
deﬁnition
let
two
closed
subspaces
projectors
respectively
let
denote
spectrum
cos−1
called
set
angles
subspace
subspace
angles
called
angles
subspaces
minimum
gap
also
expressed
terms
angles
subspaces
theorem
2.15
always
equal
ˆfα1
observe
ˆfα1
ˆfα2
performs
better
ˆfα3
case
heavier
oversampling
ˆfα
favorable
ˆfr
finally
ˆfr
shows
worse
performance
compared
approaches
xiii
conclusion
signal
reconstruction
problems
appear
many
application
areas
various
names
image
video
processing
signal
may
include
sets
images
video
sequences
depth
spectral
maps
patches
well
image-related
feature
vectors
common
image
video
processing
tasks
super-resolution
upscaling
magniﬁcation
in-painting
depth
recovery
increasing
image
dynamic
range
adding
video
frames
faster
refresh
rate
etc.
posed
signal
reconstruction
problems
even
seemingly
unrelated
tasks
framed
signal
reconstruction
problems
e.g.
classiﬁ-
cation
object
tracking
motion
prediction
audio
pro-
cessing
signal
may
include
audio
sequences
audio
spectral
maps
audio
feature
vectors
reconstruction
used
e.g.
upsampling
increasing
audio
frequency
dynamic
ranges
adding
synthetic
audio
channels
depth
reconstruction
audio
restoration
including
real-time
removal
impulse
noise
data
mining
applications
signal
reconstruction
ap-
pears
form
data
completion
interpolation
estimating
missing
data
predicting
future
data
e.g.
time
series
data
reconstruction
used
deal
faulty
sensors
data
extrapolation
help
predict
future
system
failures
efﬁcient
iterative
reconstruction
algorithms
allow
re-
constructing
signals
desired
properties
given
guiding
subspace
numerical
examples
noise-free
noisy
image
magniﬁcation
demonstrate
advantages
technology
although
tests
paper
limited
one
speciﬁc
example
signal
reconstruction
imaging
proposed
methodology
general
expected
effective
wide
range
applications
video
sound
processing
data
mining
real
time
security
artiﬁcial
intelligence
systems
appendix
basics
angles
subspaces
deﬁnition
minimum
gap
two
closed
subspaces
deﬁned
inf
/∈g
dist
dist
sin
inf
principal
angles
two
subspaces
deﬁned
simply
follows
deﬁnition
let
two
subspaces
dimensions
respectively
let
principal
angles
π/2
deﬁned
recursively
cos
max
u∈f
v∈g
kukkvk
subject
columns
two
matrices
span
cosines
principal
angles
also
called
canonical
correlations
let
projectors
respectively
eigenvalues
tg|f
related
angles
cos
condition
positiveness
inﬁmum
non-zero
angles
evidently
always
satisﬁed
ﬁnite
dimensional
spaces
although
may
approach
zero
dimension
increases
however
inﬁnite
dimensional
spaces
sequence
non-zero
angles
may
converge
zero
leading
zero
inﬁmum
relationship
angles
cid:0
cid:1
cid:0
cid:1
given
theorem
2.7
appendix
conjugate
gradient
method
introduction
conjugate
gradient
method
one
widely
used
methods
solving
linear
bounded
self-adjoint
non-negative
operator
easy
see
solving
equivalent
min
kxi
optimal
method
solving
problem
among
polynomial
iterative
methods
involve
mul-
tiplication
vector
main
step
iteration
put
formally
let
ﬁrst
deﬁne
plane
¯km
span
kx0
km−1
kx0
initial
guess
solution
¯km
equals
krylov
subspace
order
deﬁned
span
km−1b
knyazev
guided
signal
reconstruction
theory
π/2
π/2
π/2
origin
general
cid:21
t⊥sf
cid:20
cid:20
sin2
−f1
sin
cos
cid:21
subspace
span
therefore
restricting
s⊥t⊥
|s⊥
reduces
2-by-2
matrix
form
operator
scalar
form
cos2
operator
k|h0∩s⊥
extension
operator
form
cos2
cid:21
cid:20
consequently
sample
consistent
reconstruction
results
fig
example
unique
reconstruction
point
solution
m-th
iteration
satisﬁes
arg
min
¯km
arg
min
¯km
x∗kk
kzkk
kzi
denotes
induced
k-norm
denotes
actual
solution
shows
gives
best
possible
solution
iterations
thus
efﬁcient
iterative
method
appendix
simple
matrix
examples
clarify
illustrate
verify
somewhat
abstract
arguments
hilbert
spaces
section
present
several
matrix
examples
increasing
complexity
ﬁnally
representative
case
subspaces
space
important
subspaces
used
paper
non-trivial
time
important
quantities
explicitly
analytically
derived
start
cases
also
illustrated
geometrically
intuitively
appealing
case
first
consider
plane
span
stan-
dard
basis
vectors
let
sampling
subspace
span
guiding
subspace
span
ae2
real
scalar
without
loss
generality
assume
signal
consequently
sampled
signal
sample
consistent
space
span
fig
illustrates
example
showing
subspaces
well
signal
sampling
reconstruction
space
let
also
notice
intersect
unique
reconstruction
point
example
ˆxn
cid:20
af1
cid:21
cid:20
af1
cid:21
next
show
norms
operators
0s⊥
fact
cos
θmax
tan
θmax
respec-
tively
matrix
form
0s⊥
cid:20
cid:20
sin2
sin
cos
cos2
cid:21
sin
cos
matrix
form
cid:20
sin
cos
cos2
cid:21
matrix
form
cid:20
sin
cos
cid:21
corresponding
singular
values
cos
therefore
operator
sin
cos
cid:0
cid:1
respectively
form
sin2
sin
cos
form
form
sin
cos
cos2
cid:21
0s⊥
matrix
cid:20
tan
cid:21
whose
nonzero
singular
value
cos
hand
matrix
form
0s⊥
cid:20
tan
cid:21
singular
value
tan
knyazev
guided
signal
reconstruction
theory
tˆf
ˆfα
origin
sampling
space
span
target
space
span
ae2
be4
four
principle
angles
π/2
subspaces
sin
tan
cos
-0.5
0.5
1.5
2.5
cos
sin
tan
consequently
following
identities
hold
fig
example
showcasing
reconstruction
interval
sample
consistent
reconstruction
ˆfc
gen-
eralized
reconstruction
ˆfg
tˆfc
proposed
reconstruction
ˆfα
exists
anywhere
reconstruction
interval
subspaces
space
next
consider
space
span
sampling
plane
span
guiding
subspace
span
ae3
denote
angle
subspaces
cos
sin
tan
projection
operators
given
1+a2
1+a2
1+a2
1+a2
1+a2
signal
results
sampling
since
reconstruction
restricted
subspace
span
example
s⊥t⊥
|s⊥
moreover
subspace
span
nontrivial
therefore
reconstruction
subspace
span
k|h0∩s⊥
fig
illustrates
geometry
subspaces
notice
example
guiding
subspace
in-
tersect
sample
consistent
space
therefore
reconstruction
interval
exists
sample
consistent
reconstruction
ˆfc
generalized
reconstruc-
tion
ˆfg
tˆfc
proposed
reconstruction
ˆfα
exist
anywhere
reconstruction
interval
parametrized
plot
reconstructed
signal
ˆfα
0.7
corresponding
0.7.
t⊥sf


sin2
sin2
−f1
sin
cos
−f3
sin
cos


subspace
span
excludes
inter-
sections
span
span
span
span
therefore
restricting
s⊥t⊥
|s⊥
reduces
4-by-4
matrix
form
operator
following
2-by-2
matrix
form
cid:20
cos2
cos2
cid:21
operator
k|h0∩s⊥
extension
operator
form
cos2
cos2


consequently
sample
consistent
reconstruction
results
ˆxn
af1
bf3


af1
bf3


next
show
norms
operators
0s⊥
fact
cos
θmax
tan
θmax
respec-
tively
matrix
form
0s⊥
subspaces
space
finally
illustrate
example
eight
dimensional
space
span
consider
symbolic
signal
sin
cos
cos2

sin
cos
cos2

knyazev
guided
signal
reconstruction
theory
matrix
form
sin
cos

sin
cos

corresponding
singular
values
cos
cos
sin
cos
sin
cos
respectively
matrix
form
0s⊥
whose
nonzero
singular
values
cos
cos
hand
matrix
form


tan
0s⊥
tan
tan
tan


singular
values
tan
tan
thus
one
hand
kˆxnk2
a2|f1|2
b2|f3|2
|f1|2
|f3|2
ksp0fk2
tan2
θmax
since
θmax
hand
kˆxnk2
|f1|2
tan2
|f3|2
tan2
kt⊥sp0fk2/cos2
θmax
since
kt⊥sp0fk2
|f1|2
sin2
|f3|2
sin2
finally
kˆxnk2
|f1|2
tan2
|f3|2
tan2
ks⊥t⊥sp0fk2/cos4
θmax
since
ks⊥t⊥sp0fk2
|f1|2
sin2
cos2
|f3|2
sin2
cos2
three
inequalities
illustrate
three
bounds
proved
theorem
moreover
bounds
reconstruction
error
theorem
equal
ˆfk2
tan
θaf1
tan
θbf3
kps⊥∩t
fk2
kt⊥p0fk2/
cos2
θmax
ks⊥t⊥p0fk2/
cos4
θmax
kps⊥∩t
fk2
kt⊥p0fk2
sin
cos
sin2
cos2
sin
cos
sin
cos
sin2
cos2
sin
cos
cos2
tan
cos2
tan
ks⊥t⊥p0fk2
cos2
sin
cos
cos2
sin
cos
cos4
tan
cos4
tan
derivation
shows
every
bound
sharp
e.g.
turns
equality
references
gadde
knyazev
tian
mansour
guided
signal
reconstruction
application
image
magniﬁcation
2015
ieee
global
conference
signal
information
processing
globalsip
dec
2015
938–942
doi:10.1109/globalsip.2015.7418335
narang
gadde
sanou
ortega
localized
iterative
methods
interpolation
graph
structured
data
2013
ieee
global
conference
signal
information
processing
dec
2013
491–
494
doi:10.1109/globalsip.2013.6736922
wang
chen
generalized
graph
signal
sampling
reconstruction
2015
ieee
global
conference
signal
information
processing
globalsip
2015
orlando
usa
december
14-16
2015
2015
567–571
doi:10.1109/globalsip.2015.7418259
tremblay
borgnat
subgraph-based
ﬁlterbanks
graph
signals
ieee
transactions
signal
processing
vol
3827–3840
aug
2016
doi:10.1109/tsp.2016.2544747
tsitsvero
barbarossa
lorenzo
signals
graphs
uncertainty
principle
sampling
ieee
transactions
signal
processing
vol
4845–4860
sept
2016
doi:10.1109/tsp.2016.2573748
chen
varma
sandryhaila
kovaˇcevi´c
discrete
signal
processing
graphs
sampling
theory
ieee
transactions
signal
processing
vol
6510–6523
dec
2015
doi:10.1109/tsp.2015.2469645
chen
varma
singh
kovaˇcevi´c
signal
recovery
graphs
fundamental
limits
sampling
strategies
ieee
transactions
signal
information
processing
networks
vol
539–554
dec
2016
doi:10.1109/tsipn.2016.2614903
anis
gadde
ortega
efﬁcient
sampling
set
selection
bandlimited
graph
signals
using
graph
spectral
proxies
ieee
transactions
signal
processing
vol
3775–3789
july
2016
doi:10.1109/tsp.2016.2546233
gadde
anis
ortega
active
semi-supervised
learning
using
sampling
theory
graph
signals
proceedings
20th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
ser
kdd
new
york
usa
acm
2014
492–501
doi:10.1145/2623330.2623760
kheradmand
milanfar
general
reg-
ularized
similarity-based
image
restoration
ieee
transactions
image
processing
vol
5136
5151
2014
doi:10.1109/tip.2014.2362059
framework
unser
aldroubi
general
sampling
theory
nonideal
acquisition
devices
signal
processing
ieee
transactions
vol
2915–2925
1994
doi:10.1109/78.330352
eldar
sampling
arbitrary
sampling
reconstruction
spaces
oblique
dual
frame
vectors
journal
fourier
analysis
appli-
cations
vol
77–96
2003
doi:10.1007/s00041-003-0004-2
eldar
michaeli
beyond
bandlimited
sampling
sig-
nal
processing
magazine
ieee
vol
48–68
2009
doi:10.1109/msp.2009.932125
arias
corach
gonzalez
saddle
point
problems
bott-dufﬁn
inverses
abstract
splines
oblique
projections
linear
algebra
applications
vol
457
complete
61–75
2014
doi:10.1016/j.laa.2014.05.006
kato
perturbation
theory
linear
operators
berlin
springer
1995
doi:10.1007/978-3-642-66282-9
hirabayashi
unser
consistent
sampling
signal
recov-
ery
signal
processing
ieee
transactions
vol
4104–
4115
aug
2007
doi:10.1109/tsp.2007.895996
adcock
hansen
generalized
sampling
theorem
stable
reconstructions
arbitrary
bases
journal
fourier
analysis
applications
vol
685–716
2012
doi:10.1007/s00041-012-9221-x
stable
reconstructions
hilbert
spaces
resolution
gibbs
phenomenon
applied
computational
harmonic
analysis
vol
357
388
2012
doi:10.1016/j.acha.2011.07.004
adcock
hansen
poon
beyond
consistent
recon-
structions
optimality
sharp
bounds
generalized
sampling
application
uniform
resampling
problem
siam
journal
mathematical
analysis
vol
3132–3167
2013
doi:10.1137/120895846
eldar
dvorkind
minimum
squared-error
framework
generalized
sampling
signal
processing
ieee
transactions
vol
2155–2167
2006
doi:10.1109/tsp.2006.873488
knyazev
guided
signal
reconstruction
theory
bansal
raj
smaragdis
bandwidth
expansion
narrowband
speech
using
non-negative
matrix
factorization
ninth
european
conference
speech
communication
technology
2005
online
available
http
//www.merl.com/reports/docs/tr2005-135.pdf
corach
giribet
oblique
projections
sampling
problems
integral
equations
operator
theory
vol
307–322
2011
doi:10.1007/s00020-010-1858-8
knyazev
observations
degenerate
saddle
point
problems
computer
methods
applied
mechanics
engineering
vol
196
3742–3749
2007
doi:10.1016/j.cma.2006.10.019
adcock
hansen
inﬁnite-dimensional
computational
mathematics
vol
1263–1323
2016
doi:10.1007/s10208-015-9276-6
generalized
sensing
sampling
foundations
compressed
anderson
dufﬁn
series
parallel
addition
matrices
journal
mathematical
analysis
applications
vol
576–594
1969
doi:10.1016/0022-247x
90200-5
knyazev
jujunashvili
argentati
angles
inﬁnite
dimensional
subspaces
applications
rayleigh–ritz
alter-
nating
projectors
methods
journal
functional
analysis
vol
259
1323–1345
2010
doi:10.1016/j.jfa.2010.05.018
halmos
two
subspaces
trans
amer
math
soc.
vol
144
381–389
1969
doi:10.1090/s0002-9947-1969-0251519-5
zhu
knyazev
principal
angles
subspaces
tangents
journal
numerical
mathematics
vol
325–340
2013
doi:10.1515/jnum-2013-0013
szyld
many
proofs
identity
norm
oblique
projections
numer
algorithms
309–323
2006
doi:10.1007/s11075-006-9046-2
tian
mansour
knyazev
vetro
chebyshev
conjugate
gradient
ﬁlters
graph
image
denoising
multimedia
expo
workshops
icmew
2014
ieee
international
conference
july
2014
1–6
doi:10.1109/icmew.2014.6890711
knyazev
malyshev
accelerated
graph-based
nonlinear
denoising
ﬁlters
procedia
computer
science
vol
607–
616
2016
doi:10.1016/j.procs.2016.05.348
international
conference
computational
june
2016
online
available
http
//www.sciencedirect.com/science/article/pii/s1877050916307669
san
diego
california
usa
iccs
science
2016
2016
6-8
yang
duraiswami
davis
super-resolution
using
preconditioned
conjugate
gradient
method
second
international
conference
image
graphics
international
society
optics
photonics
2002
591–598
doi:10.1117/12.477201
bakhvalov
knyazev
preconditioned
iterative
methods
subspace
linear
algebraic
equations
large
jumps
coef-
ﬁcients
domain
decomposition
methods
scientiﬁc
engineer-
ing
computing
vol
180
1994
157–162
doi:10.1090/conm/180
daniel
conjugate
gradient
method
linear
nonlinear
operator
equations
siam
journal
numerical
analysis
vol
10–26
1967
doi:10.1137/0704002
chung
spectral
graph
theory
cbms
regional
conference
series
mathematics
1996
doi:10.1090/cbms/092
shuman
narang
frossard
ortega
van-
dergheynst
emerging
ﬁeld
signal
processing
graphs
ex-
tending
high-dimensional
data
analysis
networks
irregular
domains
signal
processing
magazine
ieee
vol
83–98
2013
doi:10.1109/msp.2012.2235192
pesenson
sampling
paley-wiener
spaces
combinatorial
graphs
transactions
american
mathematical
society
vol
360
5603–5627
2008
doi:10.1090/s0002-9947-08-04511-x
anis
gadde
ortega
towards
sampling
theorem
signals
arbitray
graphs
2014
ieee
international
conference
acoustics
speech
signal
processing
icassp
may
2014
3864–3868
doi:10.1109/icassp.2014.6854325
andrew
knyazev
graduated
faculty
computational
mathematics
cybernetics
moscow
state
university
1981
received
ph.d.
degree
numerical
mathematics
rus-
sian
academy
sciences
moscow
russia
1985.
mitsubishi
electric
research
laboratories
merl
professor
emeritus
university
colorado
denver
fellow
society
industrial
applied
mathematics
siam
senior
mem-
ber
ieee
years
academia
contributed
numerical
analysis
partial
differential
equations
computational
linear
algebra
emphasis
eigenvalue
problems
supported
nsf
doe
awards
graduated
ph.d.
students
distinguished
research
scientist
since
2012
research
interests
merl
algorithms
image
video
processing
data
sciences
optimal
control
material
sciences
numerical
simulation
complex
phenomena
100
publications
dozen
patent
applications
several
u.s.
international
patents
akshay
gadde
received
bachelor
technology
degree
electrical
engineering
in-
dian
institute
technology
iit
kharagpur
india
2011.
working
towards
ph.d.
electrical
engineering
university
southern
california
usc
los
angeles
since
2011
sup-
ported
provost
fellowship
recip-
ient
best
student
paper
award
icassp
2014.
research
interests
include
graph
signal
processing
machine
learning
applications
multimedia
data
processing
compression
hassan
mansour
received
bach-
elor
engineering
2003
american
uni-
versity
beirut
m.a.sc
2005
ph.d.
2009
degrees
department
electri-
cal
computer
university
british
columbia
ubc
vancouver
canada
currently
principal
research
scientist
multimedia
group
mitsubishi
electric
re-
search
laboratories
cambridge
prior
join-
ing
merl
pursued
postdoctoral
fellowship
departments
mathematics
computer
science
earth
ocean
sciences
ubc
graduate
studies
con-
ducted
research
scalable
video
coding
transmission
research
since
focused
theoretical
algorithmic
aspects
compressed
sensing
image
video
analytics
remote
sensing
array
signal
processing
dong
tian
received
ph.d.
degree
beijing
uni-
versity
technology
2001
m.eng
b.eng
degrees
automation
university
science
technology
china
ustc
1998
1995
respectively
senior
principal
member
research
staff
multimedia
group
mitsubishi
electric
research
laboratories
merl
cambridge
prior
joining
merl
worked
thom-
son
corporate
research
princeton
years
devoted
h.264/mpeg
avc
encoder
optimization
video
coding/processing
especially
standards
multiview
video
coding
mvc
later
video
coding
3dv
within
mpeg
jan.
2002
dec.
2005
postdoc
tampere
university
technology
finland
nokia
funded
project
made
contributions
video
coding
standards
applications
mobile
environments
current
research
interests
include
graph
signal
processing
point
cloud
processing
machine
learning
image/video
coding
processing
besides
academic
publications
us-granted
patents
senior
member
ieee
