expansion
kullback-­‐‑leibler
divergence
new
class
information
metrics
david
galas1*
gregory
dewey2
james
kunert-­‐graf1
nikita
sakhanenko1
1pacific
northwest
research
institute
720
broadway
seattle
washington
98122
2albany
college
pharmacy
health
sciences
106
new
scotland
avenue
albany
new
york
12208
communications
dgalas
pnri.org
axioms
2017
abstract
inferring
comparing
complex
multivariable
probability
density
functions
fundamental
problems
several
fields
including
probabilistic
learning
network
theory
data
analysis
classification
prediction
two
faces
class
problem
take
approach
simplifies
many
aspects
problems
presenting
structured
series
expansion
kullback-­‐leibler
divergence
-­‐
function
central
information
theory
-­‐
devise
distance
metric
based
divergence
using
möbius
inversion
duality
multivariable
entropies
multivariable
interaction
information
express
divergence
additive
series
number
interacting
variables
provides
restricted
simplified
set
distributions
use
approximation
model
data
truncations
series
yield
approximations
based
number
interacting
variables
first
terms
expansion-­‐truncation
illustrated
shown
lead
naturally
familiar
approximations
including
well-­‐known
kirkwood
superposition
approximation
truncation
also
induce
simple
relation
multi-­‐
information
interaction
information
measure
distance
distributions
based
kullback-­‐leibler
divergence
described
shown
true
metric
properly
restricted
expansion
shown
generate
hierarchy
metrics
connects
work
information
geometry
formalisms
give
example
application
metrics
graph
comparision
problem
shows
formalism
applied
wide
range
network
problems
provides
general
approach
systematic
approximations
numbers
interactions
connections
related
quantitative
metric
keywords
multivariable
dependence
interaction
information
kullback-­‐leibler
divergence
information
metrics
entropy
graph
distances
________________________________________________________________________
introduction
problem
representing
inferring
dependencies
among
variables
central
many
fields
fundamental
data
analysis
large
data
sets
well
describing
approximating
behavior
physical
chemical
biological
systems
many
modes
particles
component
interactions
complex
systems
usually
modeled
graphs
hypergraphs
inference
data
represents
central
problem
statistical
inference
machine
learning
approaches
directed
general
class
inference
problem
many
years
literature
physical
chemistry
among
related
fields
abounds
approaches
general
representation
problem
6,7,11,12
key
related
problem
axioms
2017
measuring
difference
approximations
useful
metric
probability
distributions
relationships
neural
networks
statistical
mechanics
general
class
problem
also
explored
certainly
indication
complexity
number
variables
interact
functionally
interdependent
important
characteristic
complexity
system
engage
number
problems
work
central
function
information
theory
kullback-­‐leibler
divergence
shown
close
heart
problems
goal
paper
describe
approach
simplifies
aspects
problems
different
way
focusing
interesting
useful
symmetries
entropy
relative
entropy
kullback-­‐leibler
divergence
k-­‐l
referred
divergence
rest
paper
particularly
important
practical
applications
divergence
true
multivariable
probability
density
function
pdf
approximation
specific
metric
measures
distances
approximations
paper
structured
around
recognition
exploitation
several
properties
divergence
central
function
information
theory
first
show
divergence
admits
simple
series
expansion
increasing
numbers
variables
successive
term
affect
expansion
multivariable
cross-­‐entropy
relative
entropy
term
divergence
using
möbius
duality
multivariable
entropies
multivariable
interaction
information
2-­‐5
allows
series
expansion
number
interacting
variables
used
approximation
parameter
interactions
considered
accurate
approximation
illustrate
derivation
known
factorizations
pdf
truncating
expansion
small
numbers
variables
well-­‐known
simple
approximations
emerge
including
kirkwood
superposition
approximation
three
variables
widely
used
approximation
theory
liquids
6-­‐8
approximations
like
seminal
approximation
method
chow
liu
closely
connected
expansion
expand
specific
approach
explore
extend
connection
future
work
axioms
2017
divergence
expansion
propose
entirely
general
extended
degree
leads
number
useful
relationships
information
theory
measures
following
section
define
new
simple
metric
probability
density
functions
show
meets
requirements
true
metric
unlike
approach
jensen-­‐shannon
divergence
-­‐
measure
based
symmetrizing
k-­‐l
divergence
-­‐
use
fisher
metric
embed
functions
riemannian
manifold
metric
provides
large
class
information
metrics
calculate
distances
directly
thereby
easily
measure
relations
approximations
among
applications
examine
cases
specific
pdf
function
classes
e.g.
gaussian
poisson
find
explicit
forms
functions
finally
examine
briefly
metric
distances
implied
different
truncations
divergence
expansion
describe
application
character
description
networks
expanding
divergence
concepts
maximum
entropy
minimum
divergence
used
devise
approaches
inference
best
estimate
true
probability
density
function
data
set
relation
true
approximate
probability
density
function
pdf
consider
set
variables
many
values
constituting
data
set
best
characterized
kullback-­‐leibler
divergence
true
pdf
approximation
𝑃′𝜈
divergence
given
log
𝑃𝑠𝑃*𝑠
traverses
possible
states
approximated
entropy
called
cross-­‐entropy
log𝑃*𝑠
𝐷𝑃∥𝑃*
𝐻′𝜈
defined
axioms
2017
form
clear
approximate
joint
entropy
must
greater
since
divergence
simply
difference
true
entropy
cross
entropy
𝐷𝑃∥𝑃*
=𝐻′𝜈
−𝐻𝜈
know
divergence
always
non-­‐negative
consequence
well-­‐known
jensen
inequality
approximation
approximation
gets
better
better
divergence
converges
zero
approximation
joint
entropy
measure
accuracy
approximation
minimizing
set
constraints
assumptions
must
optimum
using
information
theory
measures
related
joint
entropies
equation
however
also
used
good
effect
specifically
use
möbius
inversion
relation
entropy
interaction
information
3-­‐5
relationship
written
sum
subsets
exchanged
symmetric
form
τ⊆ν
|τ|+1i
|τ|+1h
relation
equation
still
holds
τ⊆ν
symmetry
derives
inherent
structure
subset
lattice
hypercube
inserting
joint
entropy
expression
equation
gives
sum
subsets
variables
𝐷𝑃∥𝑃*
456𝐼*𝜏
−𝐻𝜈
group
terms
number
variables
subset
introduce
notation
indicate
size
subsets
sum
rearranged
expansion
axioms
2017
𝐷𝑃∥𝑃*
56𝐼*𝜏
−𝐻𝜈
symbol
indicates
subset
variables
cardinality
𝜏m|=m
becomes
full
set
variables
expansion
degrees
number
variables
full
expansion
includes
terminates
truncations
series
truncate
expansion
various
degrees
numbers
variables
setting
interaction
information
terms
truncation
point
equal
zero
generate
series
increasingly
accurate
ever
complex
approximations
truncation
generates
specific
probability
density
function
relation
form
specific
factorization
setting
zero
interaction
information
expression
form
sum
entropies.1
key
result
expansion
divergence
truncation
factorization
probability
density
function
results
setting
higher
interaction
informations
zero
thus
expansion
represents
method
approximation
simplification
specifically
limits
degree
variable
dependencies
approximations
result
truncating
expansion
cross
entropy
first
degrees
familiar
ones
since
number
dependent
variables
driver
complexity
begin
pairwise
approximations
stepwise
increase
number
first
truncations
show
character
expansion
process
truncation
m=1
considering
simplest
possible
truncation
setting
first
term
equal
zero
≡𝐴6−𝐻𝜈
note
setting
imply
higher
terms
τm+1
etc.
also
zero
truncation
approximation
necessarily
sets
higher
terms
zero
𝐷𝑃∥𝑃′
=𝐷6𝑃∥𝑃′
𝐻*𝑋
−𝐻𝜈
axioms
2017
truncation
requires
m=2
terms
pairs
zero
=𝑃𝑋
𝑃𝑋b
𝑋b∈𝜈
equations
implies
independence
pairs
variables
simplest
factorization
determines
pairwise
probability
functions
note
actually
determine
form
three-­‐way
higher
pdf
truncation
requires
however
three-­‐
variable
interaction
information
zero
fact
combined
equations
=𝑃𝑋
𝑃𝑋b𝑃𝑋e
results
full
three-­‐way
factorization
pdf
variables
added
use
interaction
information
recursion
relation
equation
derive
higher
pdf
implied
truncation
finally
m=1
truncation
yields
fully
factored
pdf
note
result
derives
minimizing
expression
divergence
since
expression
minimum
implication
result
data
analysis
simply
solution
classic
problem
determining
optimal
pdf
assumption
independence
variables
fixed
expectation
values
defined
parameters
usually
represented
lagrange
multipliers
physics
implication
would
simply
independent
particles
observables
etc.
leads
simple
boltzmann
distribution
equilibrium
pdf
becomes
complex
course
truncate
expansion
higher
level
truncation
m=2
truncation
requires
𝐼′𝑋
equation
implies
axioms
2017
factorization
xi|
∀τ2
let
denote
cross
entropy
term
truncation
𝐼*𝜏j
𝐷j𝑃∥𝑃′
𝐻*𝑋
4k⊆
−𝐻𝜈
≡𝐴j−𝐻𝜈
cross
entropy
term
determined
pdf
see
minimization
divergence
truncation
expansion
equivalent
approximation
made
chow-­‐liu
physical
terms
ignoring
pairwise
interaction
terms
hamiltonian
precisely
probabilistic
version
kirkwood
superposition
approximation
6-­‐8
approximation
used
physics
dense
multiparticle
systems
like
liquids
resulting
pair
correlation
function
used
deriving
many
thermodynamic
properties
liquids
singer
related
general
theoretical
constructs
like
percus-­‐yevick
approximation
bogoliubov-­‐born-­‐green-­‐
kirkwood-­‐yvon
bbgky
hierarchy
truncation
m=3
parallel
express
truncation
approximation
next
level
using
three
terms
𝐷m𝑃∥𝑃′
𝐻*𝑋
4k⊆
𝐼*𝜏j
4o⊆
𝐼*𝜏m
−𝐻𝜈
≡𝐴m−𝐻𝜈
axioms
2017
terms
cross
entropies
term
becomes
xix
xixk
jxk
xix
jxk
see
truncation
assuming
four-­‐variable
cross-­‐interaction
information
zero
minimizing
divergence
imply
approximation
pdf
xix
jxk
xixk
jxk
note
also
expressed
simply
terms
deltas
used
analysis
dependency
partial
measure
complexity
three
variables
quantity
conditional
mutual
information
seen
recursion
relation
equation
12.
approximation
approximation
𝐻=𝐻*
truncation
expansion
leading
complex
𝐴m−
𝐻*𝑋
𝑋b|𝑋e
indicated
right-­‐hand
side
based
cross
entropy
representations
variable
interactions
taken
higher
levels
course
leads
turn
higher-­‐level
complex
factorizations
pdf
factorizations
simply
seen
setting
cross
interaction
information
variables
equal
zero
inferring
implied
pdf
factors
relation
deltas
truncation
relation
implies
another
simple
equivalence
direct
intuitive
meaning
connects
simple
way
differential
interaction
information
general
recursion
relation
interaction
information
derive
set
simple
equivalences
information
set
variables
general
multi-­‐variable
recursion
relation
interaction
axioms
2017
νn−1
νn−1|
νn−1|
νn−1
choices
set
𝜈n-­‐1
set
missing
thus
truncation
setting
left
side
zero
implies
exactly
relations
one
choice
implication
truncation
criterion
divergence
m=n
interaction
information
conditioned
variable
set
interaction
information
remaining
n-­‐1
variables
note
conditional
equation
within
sign
asymmetric
delta
function
variables
truncation
divergence
seen
equivalent
simplification
truncation
asymmetric
delta
truncation
m=2
would
mean
conditional
mutual
informations
equal
mutual
information
equivalent
specifying
independence
conditional
variable
multi-­‐information
easy
show
truncation
embodied
equation
also
implies
simple
relation
multi-­‐information
called
complete
correlation
watanabe
interaction
information
multi-­‐information
defined
ω𝜐u
quantity
often
used
measure
overall
multivariable
dependence
since
goes
zero
variables
independent
always
positive
several
drawbacks
distinguish
degrees
dependence
number
variables
metric
show
elementary
proof
general
case
truncation
variables
illustrate
simple
expression
multi-­‐information
terms
interaction
information
3-­‐
4-­‐variable
cases
case
truncation
n=3
𝐼𝜈m
relation
simply
sum
three
mutual
informations
14a
axioms
2017
=𝐼𝑋
+𝐼𝑋
easy
see
direct
calculation
using
marginal
entropies
n=4
=𝐼𝑋
+𝐼𝑋
+𝐼𝑋
+𝐼𝑌
−𝐼𝑋
−𝐼𝑋
−𝐼𝑌
−𝐼𝑌
−𝐼𝑍
𝐼𝜏m
𝐼𝜏j
14b
relation
14a
strongly
intuitive
sense
3-­‐variable
interaction
information
zero
multi-­‐information
simply
sum
mutual
information
three
pairs
similar
less
intuitive
relationship
embodied
4-­‐variable
case
equation
14b
general
case
suggested
divergence
expansion
also
expressed
using
multi-­‐information
limited
number
variables
well
series
truncation-­‐approximate
probability
density
functions
following
way
consider
series
functions
related
true
untruncated
probability
density
function
pdf
variables
results
setting
interaction
information
equal
zero
subsets
z56𝐻𝜂
=0⇒𝐻𝜏
divergence
converges
zero
series
number
variables
increases
z⊂4
lim
→u𝐷𝑃∥𝑃
divergence
therefore
induces
topology
series
functions
proof
follows
directly
definitions
note
multi-­‐information
metric
metric
specifically
gives
distance
measure
different
pdfs
problem
received
much
attention
axioms
2017
metric
provides
clear
measure
function
space
complete
formalism
around
k-­‐l
divergence
approximations
devising
simple
pdf
metric
information
geometry
simple
metric
although
sometimes
thought
distance
measure
probability
distributions
kullback–leibler
divergence
true
metric
among
disqualifying
properties
asymmetry
however
much
work
devoted
development
geometric
measures
information
particularly
differential
geometry
symmetric
divergences
defined
derivative
form
hessian
divergence
yield
metric
tensor
known
fisher
information
metric
riemannian
metric
tensor
used
extensively
real
metric
essential
complete
quantitative
theory
even
useful
relatively
simple
direct
finite
distances
functions
differential
manifold
fisher
metric
must
determined
integration
along
geodesics
simpler
metrics
allow
direct
calculation
distance
probability
density
functions
describe
simple
information
metric
consider
problem
comparing
two
approximate
distributions
using
another
pdf
reference
function
use
k-­‐l
divergence
define
metric
simply
absolute
value
difference
two
k-­‐l
divergences
using
reference
function
definition
embodied
following
equation
𝔇c𝑅∥𝑆
𝐷𝑃∥𝑅
−𝐷𝑃∥𝑆
𝑙𝑜𝑔
𝑅𝑠𝑆𝑠
next
establish
𝑅∥𝑆
indeed
properties
metric
function
space
metric
following
four
properties
show
fulfilled
definition
axioms
2017
equation
assures
summation
non-­‐negative
condition
hold
choices
therefore
metric
property
may
apply
specific
spaces
must
examined
case
illustrate
non-­‐negativity
𝔇c𝑅∥𝑆
assured
absolute
value
identity
indiscernibles
𝔇c𝑅∥𝑆
=𝑆𝜈
𝔇c𝑅∥𝑆
metric
must
also
true
𝑙𝑜𝑔
𝑅𝑠𝑃𝑠
−𝑙𝑜𝑔
𝑆𝑠𝑃𝑠
𝔇c𝑅∥𝑆
unless
=𝑆𝜈
otherwise
metric
pseudometric
later
specific
cases
symmetry
𝔇c𝑅∥𝑆
=𝔇c𝑆∥𝑅
𝔇c𝑅∥𝑆
subadditivity
obeying
triangle
inequality
𝔇c𝑅∥𝑆
≤𝔇𝑅∥𝑄
+𝔇𝑄∥𝑆
𝑃𝑠𝑙𝑜𝑔
𝑅𝑠𝑄𝑠
𝑃𝑠𝑙𝑜𝑔
𝑅𝑠𝑆𝑠
𝔇c𝑅∥𝑆
𝑃𝑠𝑙𝑜𝑔
𝑄𝑠𝑆𝑠
𝑃𝑠𝑙𝑜𝑔
𝑅𝑠𝑄𝑠
𝑃𝑠𝑙𝑜𝑔
𝑄𝑠𝑆𝑠
𝑃𝑠𝑙𝑜𝑔
𝑅𝑠𝑄𝑠
𝑄𝑠𝑆𝑠
=𝔇c𝑅∥𝑄
+𝔇c𝑄∥𝑆
therefore
true
metric
function
space
pdf
use
directly
𝑙𝑜𝑔
𝑅𝑠𝑃𝑠
−𝑙𝑜𝑔
𝑆𝑠𝑃𝑠
inequality
holds
sums
real
numbers
triangle
inequality
applies
=𝔇c𝑆∥𝑅
measure
information
distance
cases
function
spaces
however
subspaces
true
metrics
pseudometrics
distinct
axioms
2017
functions
zero
distance
since
metric
determined
reference
function
examine
properties
metrics
intriguing
similarity
metric
distance
functions
defined
third
reference
function
represents
large
class
metrics
determined
choice
function
lies
bayesian
statistics
could
say
defining
reference
pdf
prior
pdf
𝑅∥𝑆
measures
distance
two
posterior
functions
dirichlet
metric
example
reference
pdf
prior
dirichlet
distribution
fact
defines
metric
function
space
inspires
ask
specific
probability
density
variable
set
leads
simple
expression
metric
measuring
distance
successive
posteriors
one
monitor
convergence
bayesian
updating
steady
state
distribution
distance
measure
also
used
assess
quantitatively
close
different
posterior
models
could
define
uniform
gaussian
metric
reference
uniform
gaussian
special
metrics
functional
forms
yield
metric
spaces
particular
properties
could
define
uniform
𝑅∥𝑆
=1ℕ
𝑙𝑜𝑔
𝑅𝑠𝑆𝑠
number
values
total
set
variables
take
consider
vector
always
metric
since
𝔇c𝑅∥𝑆
unless
=𝑆𝜈
interesting
class
metrics
generated
choosing
gaussian
reference
functions
also
gaussian
illustrate
particularly
simple
expression
distances
case
single
variable
let
reference
function
defined
normal
axioms
2017
functions
measured
distribution
variance
mean
designated
6jp
q/ks𝑒𝑥𝑝
vwx
jsk
6jp
q/ksq𝑒𝑥𝑝
vwxq
jsqk
6jp
q/ksk𝑒𝑥𝑝
vwxk
jskk
distance
𝔇y𝑅∥𝑆
𝑙𝑜𝑔
𝑅𝑥𝑆𝑥
sqk
xwxk
6sqk−
6skk
xwxq
𝔇y𝑅∥𝑆
𝑙𝑜𝑔
sksq
skj
skk
easily
evaluated
using
simple
properties
gaussians
19a
19b
19c
explicit
expression
distance
simple
special
cases
first
standard
deviations
distance
dependent
mean
values
independent
standard
deviation
reference
function
likewise
means
distance
depends
standard
deviation
independent
mean
reference
function
another
special
case
worth
pointing
reference
function
chosen
dirac
delta
function2
could
considered
limiting
case
gaussian
vanishing
standard
deviation
expression
equation
simplifies
metric
space
defined
single
parameter
mean
reference
function
distance
expression
key
property
dirac
delta
function
x-­‐‑x0
integral
function
yields
specific
value
function
𝛿𝑥−𝑥
𝑓𝑥𝑑𝑥=𝑓𝑥
axioms
2017
−𝑙𝑜𝑔
sksq
sqk
xkwx
skk
𝑅∥𝑆
xqwx
reference
function
𝜇≦𝜇6
example
case
distance
includes
different
ratios
defines
distance
note
standard
deviations
equal
expression
extremely
simple
assure
metric
rather
pseudometric
chose
function
space
proportional
squares
distance
reference
mean
function
space
case
many
functions
zero
distance
apart
restricted
class
set
reference
mean
zero
clear
relevant
measure
distance
squares
ratios
mean
standard
deviation
one-­‐dimensional
case
simple
geometric
interpretation
illustrated
figure
two
different
ways
𝜇1𝜎1+𝜇2𝜎2
xqsq
xksk
expression
area
simply
area=
distance
figure
metric
distance
gaussian
equation
dirac
delta
function
reference
metric
mean
zero
represented
hyperbolic
function
distance
vertical
axis
simplicity
metric
distance
deviation
zero
plane
absolute
value
saddle
point
another
geometric
metaphor
distance
area
blue
annular
region
divided
𝜇1𝜎1−𝜇2𝜎2
notice
exception
single
log
term
right-­‐hand
side
equation
expression
quadratic
form
ratios
mean
standard
deviation
ratios
standard
deviations
reference
standard
deviation
axioms
2017
general
dirac
delta
reference
function
metric
carry
much
information
functions
function
class
restricted
becomes
interesting
useful
logs
whose
difference
metric
equation
often
called
surprisals
information
theory
case
metric
essentially
much
surprising
specific
point
mention
multiple
delta
function
metrics
used
distance
coordinates
surprisal
point
distances
leads
naturally
multi-­‐dimensional
space
representation
log
ratios
three-­‐
dimensional
representation
example
reflects
three
chosen
points
functions
compared
another
interesting
metric
space
results
selecting
three
functions
reference
measured
functions
poisson
distributions
discrete
valued
functions
=𝜆𝑘𝑘
𝑒−𝜆
yield
particularly
simple
metric
distance
reference
function
parameter
two
distance
simply
𝔇𝑃𝜆1∥𝜆2
𝜆1−𝜆log𝜆1
𝜆2−𝜆log𝜆2
course
distance
vanishes
goes
reference
much
smaller
two
𝜆≪𝜆6
distance
linear
difference
much
larger
𝜆≫𝜆6
distance
proportional
difference
logs
cases
distance
true
metric
distinct
functions
zero
distance
reference
set
one
hand
easy
see
pairs
functions
either
sides
one
zero
distance
choice
reference
function
pseudometric
large
number
possible
special
metrics
based
wide
range
possible
continuous
distributions
could
used
reference
functions
many
lead
interesting
functional
expressions
explore
see
comprehensive
list
axioms
2017
functions
field
guide
continuous
probability
distributions
available
gavin
crooks
website3
measuring
independence
variable
subsets
next
consider
comparing
probability
given
single
variable
𝑅𝑋6
conditional
probability
variable
given
remaining
set
variables
𝑅𝑋6𝜈−
chosen
variable
independent
others
𝑅𝑋6
distance
zero
𝔇𝑅𝑋1
∥𝑅𝑋1𝜈−
𝑅𝑋6𝜈−
result
independent
reference
function
𝑅𝑠1
also
𝔇𝑅𝑋1
∥𝑅𝑋1𝜈−
𝑙𝑜𝑔
𝑅𝑠1𝑠−
𝑅𝑠1
𝑅𝑠−
𝑙𝑜𝑔
𝑅𝑠1𝑠−
𝑅𝑠−
denotes
state
single
variable
denotes
state
variables
∥𝑅𝜈
generalizing
therefore
𝔇𝑅𝑋1
∥𝑅𝑋1𝜈−
=𝔇𝑅𝑋1𝑅𝜈−
single
variable
subset
𝔇𝑅𝜈′
∥𝑅𝜈′𝜈−𝜈′
=𝔇𝑅𝜈′𝑅𝜈−𝜈′
∥𝑅𝜈
course
dependent
reference
function
except
limit
distance
goes
zero
compendium
found
website
http
//threeplusone.com/fieldguide.pdf
axioms
2017
comparing
approximations
different
truncated
series
application
metric
spaces
lies
area
statistical
physics
example
considers
reduced
probability
distribution
functions
approximate
true
distribution
functions
high
degree
interactions
highly
multivariable
non-­‐equilibrium
problems
defined
trajectories
could
directly
approached
apparatus
approximations
often
involved
physically
motivated
simplifying
truncation
relationships
like
discussed
formalism
developed
used
calculate
distance
probability
functions
truncated
different
levels
approximation
allows
assessment
convergence
higher
level
truncations
terms
distance
converging
zero
approximate
functions
result
truncations
variable
number
expansion
different
numbers
variables
directly
compared
quantitative
metric
truncations
described
define
forms
density
functions
factors
actual
pdf
determined
true
reference
pdf
comparing
distributions
truncated
first
second
order
probability
functions
𝑃6*
𝑃j*
determined
factorizations
equations
distance
two
truncation
approximations
relative
reference
function
referring
equations
expression
simplifies
𝔇𝑃2′∥𝑃1′
𝐷𝑃2′∥𝑃
−𝐷𝑃1′∥𝑃
𝐴2−𝐴1
+𝐻′
−𝐻′
𝐴2−𝐴1
𝔇𝑃2′∥𝑃1′
𝜏2⊂𝜈
𝔇𝑃2′∥𝑃1′
24a
24b
axioms
2017
recall
sum
cross
mutual
information
pairs
variables
defined
reference
function
distance
equation
24b
represents
distance
functions
pairwise
dependence
independence
general
distance
two
different
truncation
approximations
seen
easily
expansion
equation
distance
simply
absolute
value
sum
terms
present
one
truncated
series
application
networks
use
metric
spaces
devise
simple
direct
way
estimating
distance
two
networks
problem
attracted
attention
many
years
begin
considering
networks
terms
subsets
dependent
variables
variables
nodes
pairwise
dependencies
graph
furthermore
consider
measures
dependence
weights
edges
mutual
information
variable
pairs
provides
weights
higher-­‐degree
dependencies
corresponding
network
hypergraph
let
consider
metric
spaces
apply
graphs
general
formalism
described
used
hypergraphs
direct
analogy
extending
analysis
hypergraphs
adds
additional
consideration
address
parallel
clear
graph
describing
dependencies
present
dataset
example
would
result
truncation
divergence
m=2
level
following
previous
discussion
divergence
used
quantitate
approximation
represented
graph
moreover
metrics
defined
present
simple
way
calculate
real
metric
distances
graphs
let
consider
example
let
choose
simplicity
uniform
probability
density
variable
set
reference
function
resulting
simple
expression
metric
shown
equation
18.
number
values
set
variables
take
consider
vector
two
acyclic
graph
defined
density
functions
equation
...
xi|
...
xi|
axioms
2017
using
uniform
density
metric
relationship
mutual
informations
𝑅∥𝑆
=1ℕ
distributions
cid:137
cid:138
𝑙𝑜𝑔𝑅
𝑋𝑖|𝑋𝑗
𝑋𝑖|𝑋𝑗
=1ℕ
sums
weights
edges
note
using
graphs
weights
mutual
information
nodes
describe
dataset
exactly
like
chow-­‐liu
approach
example
distance
simply
sum
differences
sets
nodes
two
graphs
simple
result
interesting
subtlety
differences
may
positive
negative
absolute
value
overall
sum
absence
edge
zero
mutual
information
one
graph
may
compensated
different
absence
leave
distance
formalism
guarantees
distance
true
metric
distance
reference
functions
also
produce
metrics
lead
complex
results
easy
visualize
extension
hypergraphs
natural
extension
leads
results
increasingly
difficult
see
case
demonstrates
use
formalism
network
comparison
based
information
functions
applications
network
comparison
results
explored
later
paper
conclusions
kullback-­‐leibler
divergence
means
comparing
probability
density
functions
played
central
role
information
theory
used
several
practical
purposes
data
analysis
machine
learning
model
inference
provided
ways
explore
key
ideas
fields
information
theory
thermodynamics
show
continue
yield
new
results
divergence
expanded
number
interacting
variables
yielding
systematic
hierarchy
truncations
approximations
probability
density
function
effectively
hierarchy
factorizations
factorizations
since
focus
kinds
degrees
independence
central
thought
hierarchies
axioms
2017
spaces
ever
complex
functions
evermore
complex
dependencies
relationship
set
entropies
set
interaction
informations
möbius
inversion
relation
fundamental
symmetry
manifest
full
symmetry
spectrum
deeper
yet
reflects
number
relationships
information-­‐
related
measures
based
symmetry
9,10
since
relations
also
used
express
cross
entropy
differently
generate
different
expansions
divergence
different
structures
intriguing
area
may
yield
additional
useful
applications
yet
explored
noted
introduction
several
areas
potential
application
ideas
relation
one
level
truncation
hierarchy
chow-­‐liu
approximation
noted
immediately
suggests
extension
chow-­‐liu
algorithm
higher
levels
chow-­‐
liu-­‐like
hypergraphs
remains
explored
fully
addressed
future
publication
applications
networks
network
inference
suggested
notion
metric
classes
based
specific
reference
functions
interesting
divergence
provides
basis
new
finite
difference
metric
gives
measures
distances
pdf
real
estimated
continuous
discrete
give
example
application
metrics
graph
comparison
problem
example
shows
entire
formalism
brought
bear
wide
range
network
problems
possible
simplify
graph
distance
measures
given
set
specific
constraints
optimizing
choice
reference
function
tailoring
metric
specific
classes
graphs
example
enable
simplification
model
inference
cases
ideas
addressed
future
work
added
constraint
specific
function
forms
pdf
-­‐
exponential
family
functions
like
gaussians
example
-­‐
natural
extension
leads
number
specific
approximations
metric
form
considerations
raise
question
strategy
used
select
reference
function
least
two
considerations
important
true
metric
rather
pseudometric
provided
reference
function
function
space
selected
provide
property
could
simple
picking
right
parameter
range
pdf
illustrated
poisson
axioms
2017
gaussian
pdfs
another
consideration
accuracy
calculated
distances
depend
relevant
functions
expected
reference
function
might
chosen
near
region
function
space
avoid
subtract
two
large
divergences
find
distance
issues
important
practical
somewhat
problem
specific
effective
use
concepts
systematically
considered
future
work
relationship
metric
eqn
fisher
information
metric
obtained
convergence
zero
distance
wide
variety
metrics
derived
symmetrizing
divergence
various
ways
jensen-­‐shannon
divergence
one
several
others
use
variations
theme
averaging
cross
entropy
terms
various
ways
proposed
metric
first
knowledge
use
third
probability
density
function
define
character
metric
space
though
consideration
information-­‐geometric
interpretation
difference
k-­‐l
divergences
connection
k-­‐l
divergence
differences
using
third
distribution
expected
log-­‐likelihood
ratios
use
building
riemannian
metrics
also
discussed
previously
17,18
approaches
quite
distinct
may
connected
future
work
general
approach
advantage
indicated
simple
examples
shown
metric
space
tailored
character
function
space
suggest
equation
defines
could
interpreted
general
sense
finite
difference
form
fisher
metric
metric
also
used
directly
compare
bayesian
estimators
pdf
iteratively
updated
measure
convergence
application
general
approach
described
wide
range
multivariable
problems
including
data
analysis
model
inference
multivariable
physical
problems
problems
involving
complex
biological
systems
useful
providing
new
analysis
methods
new
insights
axioms
2017
acknowledgements
work
supported
part
nih
common
fund
extracellular
rna
communication
consortium
ercc
1u01hl126496-­‐01
bill
melinda
gates
foundation
pacific
northwest
research
institute
authors
thank
anonymous
referee
number
specific
corrections
suggestions
conflicts
interest
authors
declare
conflict
interest
founding
sponsors
role
design
study
collection
analyses
interpretation
data
writing
manuscript
decision
publish
results
axioms
2017
references
kullback
leibler
r.a.
information
sufficiency
annals
mathematical
statistics
1951
79–86
doi:10.1214
mcgill
multivariate
information
transmission
psychometrika
1954
97–
116
doi:10.1007/bf02289159
bell
co-­‐information
lattice.
ica
2003
eds
amari
al.
nara
japan
2003
jakulin
bratko
quantifying
visualizing
attribute
interactions
approach
based
entropy
http
//arxiv.org/abs/cs.ai/0308002
2004
sakhanenko
n.a
galas
d.j
biological
data
analysis
information
theory
chem
phys
2004
121
3657-­‐3666
doi
http
//dx.doi.org/10.1063/1.1776552
singer
maximum
entropy
formulation
kirkwood
superposition
approximation
problem
multivariable
dependence
measures
shadows
algorithm
comput
biol
2015
1-­‐20
doi
10.1089/cmb.2015.0051
kirkwood
statistical
mechanics
fluid
mixtures
chem
phys
1935
300
doi
cochran
r.v
lund
l.h..
kirkwood
superposition
approximation
chem
phys
1964
41,3499-­‐‑3504
doi
http
//dx.doi.org/10.1063/1.1725757
http
//dx.doi.org/10.1063/1.1749657
galas
d.j
sakhanenko
n.a
multivariate
information
measures
unification
using
möbius
operators
subset
lattices
arxiv:1601.06780
2016
10.
galas
d.j.
sakhanenko
n.a.
skupin
ignac
describing
complexity
systems
multivariable
set
complexity
information
basis
systems
biology
comput
biol
2014
118-­‐140
doi
10.1089/cmb.2013.0039
11.
watanabe
information
theoretic
analysis
multivariate
correlation
ibm
res
dev
1960
66-­‐82
12.
nemenman
shafee
bialek
entropy
inference
revisted
advances
neural
information
processing
systems
eds.dietterich
becker
ghahramani
cambridge
2002
13.
lin
tegmark
deep
cheap
learning
work
well
arxiv:1608.08225
2016
14.
amari
nagaoka
methods
information
geometry
translations
mathematical
monographs
v191
american
mathematical
society
isbn
978-­‐0821805312
2000
15.
chow
liu
c.n
1968
approximating
discrete
probability
distributions
dependence
trees
ieee
transactions
information
theory
it-­‐14
462–467
doi:10.1109/tit.1968.1054142
16.
nielsen
family
statistical
symmetric
divergences
based
jensen's
inequality
arxiv:1009.4004
2010
17.
eguchi
copas
interpreting
kullback-­‐leibler
divergence
neyman-­‐pearson
lemma
multivariate
anal.
97:2034-­‐2040
2006
cheng
hua
wang
qin
geometry
signal
detection
applications
radar
signal
processing
entropy
1-­‐17
2016
