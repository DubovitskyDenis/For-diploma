learnability
unknown
quantum
measurements
hao-chung
cheng1
min-hsiu
hsieh2
ping-cheng
yeh3
graduate
institute
communication
engineering
national
taiwan
university
taiwan
r.o.c
1,3
faculty
engineering
information
technology
university
technology
sydney
australia1,2
centre
quantum
computation
intelligent
systems
abstract
quantum
machine
learning
received
signiﬁcant
attention
recent
years
promising
progress
made
development
quantum
algorithms
speed
traditional
machine
learning
tasks
work
however
focus
investigating
information-theoretic
upper
bounds
sample
complexity—how
many
training
samples
suﬃcient
predict
future
behaviour
unknown
target
function
kind
problem
arguably
one
fundamental
problems
statistical
learning
theory
bounds
practical
settings
completely
characterised
simple
measure
complexity
main
result
paper
learning
unknown
quantum
measurement
upper
bound
given
fat-shattering
dimension
linearly
proportional
dimension
underlying
hilbert
space
learning
unknown
quantum
state
becomes
dual
problem
byproduct
recover
aaronson
famous
result
proc
soc
463
3089–3144
2007
solely
using
classical
machine
learning
technique
addition
famous
complexity
measures
like
covering
numbers
rademacher
complexities
derived
explicitly
able
connect
measures
sample
complexity
various
areas
quantum
information
science
e.g
quantum
state/measurement
tomography
quantum
state
discrimination
quantum
random
access
codes
may
independent
interest
lastly
assistance
general
bloch-sphere
representation
show
learning
quantum
measurements/states
mathematically
formulated
neural
network
consequently
classical
algorithms
applied
eﬃciently
accomplish
two
quantum
learning
tasks
introduction
statistical
learning
theory
machine
learning
branch
artiﬁcial
intelligence
aims
devise
algorithms
machines
systematically
learn
historic
data
typically
separated
unsupervised
learning
supervised
learning
unsupervised
learning
machine
useful
ﬁnding
hidden
structure
e.g
clustering
density
estimation
within
unlabeled
data
supervised
learning
machine
equipped
power
predict
class
infer
characteristic
structured
data
ﬁgures
merit
learning
machine
include
computational
complexity
measures
run-time
eﬃciency
learning
algorithm
sample
complexity
determines
number
queries
membership
made
learning
algorithm
hypothesis
function
probably
approximately
correct
pac
iii
model
complexity
otherwise
called
generalization
error
deﬁned
discrepancy
out-of-sample
error
in-sample
error
note
model
complexity
closely
related
sample
complexity
sense
learning
machine
large
model
complexity
requires
samples
accurately
approximate
target
function
results
high
sample
complexity
current
research
trends
include
reduction
computational
complexity
due
large
volume
data
set
big
data
well
high
dimensional
features
data
point
balance
model
complexity
in-sample
error
training
data
set
trained
well
without
occurrence
overﬁtting
quantum
information
processing
qip
active
ﬁeld
studies
computational
capability
quantum
systems
recent
years
qip
achieved
signiﬁcant
breakthroughs
factorizing
large
prime
integers
exponential
speed-up
searching
unstructured
database
quadratic
speed-up
two
famous
examples
two
features
qip
result
dramatic
improvement
classical
information
processing
superposition
principle
contrary
classical
bit
takes
discrete
value
either
quantum
bit
qubit
linear
combination
two
quantum
states
cid:105
cid:105
principle
consequence
fundamental
property
quantum
mechanics—the
linearity
schr¨odinger
wave
equation
therefore
superposition
principle
allows
outcomes
parallel
quantum
computation
stored
single
quantum
state
gives
quantum
machines
computing
ability
classical
devices
entanglement
quantum
entanglement
remarkable
phenomenon
quantum
theory
resource
plays
crucial
role
e-mail
address
f99942118
ntu.edu.tw1
minhsiuh
gmail.com2
pcyeh
ntu.edu.tw3
numerous
results
including
quantum
shannon
theory
9–12
quantum
error-correcting
codes
13–16
features
make
qip
multidisciplinary
research
area
broad
range
promising
applications
owing
successful
achievements
qip
researchers
begun
explore
whether
qip
advance
subjects
classical
computer
science
consequently
interdisciplinary
area
quantum
machine
learning
attracted
substantial
interest
lately
central
problems
two-fold
ﬁrst
kind
problem
investigates
qip
improve
classical
tasks
converting
classical
algorithms
partially
totally
quantum
algorithm
precisely
one
studies
quantum
machines
serve
accelerate
process
improve
computational
eﬃciency
reduce
sample
complexity
transforming
classical
training
data
special
sets
quantum
states
call
line
research
quantum
computational
learning
19–37
hand
certain
fundamental
quantum
problems
inference
unknown
quantum
states
operations
hidden
structure
underlying
quantum
system
ﬁts
well
setting
statistical
learning
theory
however
requires
certain
generalisation
current
theory
machine
learning
accommodate
operator-valued
inputs
and/or
outputs
term
line
research
quantum
statistical
learning
38–54
current
achievements
quantum
computational
learning
come
quantum
enhancement
computation
procedures
optimization
inner
product
big
data
ability
compute
classical
functions
parallel
example
servedio
gortler
19–22
considered
two
standard
learning
models
boolean
functions
angluin
exact
learning
membership
queries
valiant
pac
learning
examples
deﬁning
quantum
extensions
classical
oracles
manipulate
classical
binary
data
shown
quantum
oracles
classical
machines
polynomially
equivalent
terms
sample
complexity
anguita
2003
used
method
durr
hoyer
perform
optimization
process
support
vector
machine
svm
a¨ımeur
brassard
gambs
2006
applied
modiﬁed
grover
algorithm
clustering
problems
lloyd
2013
28–30
introduced
quantum
random
access
memory
store
classical
data
proposed
eﬃcient
density
matrix
exponentiation
method
improve
computational
procedure
supervised
unsupervised
svm
algorithms
additionally
lloyd
also
provided
quantum
algorithms
execute
topological
analysis
big
data
pudenz
lidar
2012
considered
veriﬁcation
software
applied
adiabatic
quantum
computation
methods
solve
quadratic
binary
optimization
problem
wiebe
kappor
svore
2014
microsoft
research
modiﬁed
lloyd
approach
proposed
quantum
nearest-neighbor
algorithm
surprisingly
showed
number
queries
depends
sparsity
maximum
value
training
data
rather
feature
dimension
wang
2014
combined
phase
estimation
dense
hamiltonian
simulation
technique
improve
performance
curve
ﬁtting
cross
considered
problem
learning
parity
functions
presence
noise
showed
quantum
oracle
computationally
eﬃcient
classical
counterpart
schuld
presented
quantum
pattern
classiﬁcation
discussed
advantages
recently
wiebe
successfully
applied
quantum
computers
perform
important
machine
learning
task—deep
learning
refer
interested
readers
ref
table
1.1
wittek
provides
detailed
comparisons
existing
quantum
machine
learning
algorithms
quantum
statistical
learning
a¨ımeur
brassard
gambs
introduced
task
quantum
clustering
goal
group
similar
quantum
states
according
ﬁdelity
measure
putting
dissimilar
states
diﬀerent
clusters
ref
gambs
2008
studied
task
quantum
classiﬁcation
training
data
set
contains
pure
states
classical
binary
classes
forming
statistical
mixture
states
class
helstrom
measurement
forms
binary
classiﬁer
minimizes
training
error
gut¸ˇa
kotlowski
2010
researched
problem
classifying
two
unknown
mixed
states
used
technique
local
asymptotic
normality
derive
optimal
classiﬁer
sen´ıs
also
proposed
strategy
perform
quantum
state
classiﬁcation
nevertheless
approaches
developed
gambs
gut¸ˇa
kotlowski
sen´ıs
essentially
quantum
hypothesis
testing
rather
quantum
since
consider
model
complexity
sample
complexity
problems
bisio
2010
considered
learning
unitary
transformation
storing-retrieving
problem
proposed
algorithm
learning
based
quantum
network
bisio
2011
generalised
previous
work
quantum
instruments
gross
flammia
integrated
compressed
sensing
methods
quantum
states
tomography
proposed
algorithm
practically
learn
row-rank
quantum
states
latest
work
braunstein
2014
studied
quantum
version
decision
tree
qdt
model
input
variable
output
label
represented
pure
states
cid:105
cid:105
von
neumann
entropy
used
node
splitting
criterion
construct
quantum
decision
tree
classiﬁer
several
works
engaged
developing
possible
models
quantum
neural
1our
catalogue
quantum
diﬀerent
learning
class
lcontext
subscript
goal
refers
learning
goal
superscript
context
refers
training
data
set
and/or
learner
abilities
introduced
a¨ımeur
brassard
gambs
according
authors
corresponds
pure
classical
tasks
learner
access
quantum
computer
accelerate
classical
problems
belongs
learning
class
goal
figure
current
development
quantum
machine
learning
quantum
computational
learn-
ing
investigates
quantum
machines
serve
accelerate
process
improve
com-
putational
eﬃciency
reduce
sample
complexity
transforming
classical
training
data
special
sets
quantum
states
line
research
input
space
output
space
classical
hand
quantum
statistical
learning
studies
inference
unknown
quantum
states
operations
hidden
structure
quantum
system
term
quantum
version
classical
statistical/stochastic
model
quantum
stochastic
model
quantum
machine
learning
quantum
computational
learning
quantum
statistical
learning
computational
complexity
angluin
a¨ımeur
pudenz
lidar
cross
emphet
lloyd
27–30
wang
wiebe
schuld
sample
complexity
servedio
19–22
sample
complexity
aaronson
work
quantum
stochastic
model
bisio
gross
classiﬁcatioin
qdt
qnn
qhmm
51–54
network
qnn
another
interesting
topic
hidden
quantum
markov
model
hqmm
51–54
state
system
described
density
operator
transitions
determined
completely
positive
trace-nonincreasing
map
shown
hqmm
implemented
open
quantum
systems
instantaneous
feedback
summarise
current
development
supervised
quantum
figure
note
majority
previous
works
quantum
machine
learning
focused
computational
aspects
learning
algorithm
issue
sample
complexity
exhibited
original
quantum
learning
setting
e.g
state/process
tomography
rarely
touched
aaronson
pioneered
study
learnability
problems
quantum
regime
derived
upper
bounds
sample
complexity
learning
quantum
states
work
start
machine
learning
point
view
formalize
problems
learning
quantum
measurements
quantum
states
learning
real-valued
functions
banach
space
learning
unknown
quantum
measurement
apply
sequence
quantum
states
measurement
apparatus
obtain
statistics
measurement
outcome
goal
infer
likely
quantum
measurement
hypothesis
set
behaves
like
target
measurement
collected
data
paper
mainly
focus
learning
unknown
two-outcome
measurement
resembles
yes-no
instrument
multi-outcome
measurements
results
easily
generalised2
learning
quantum
states
hand
training
data
set
collection
two-outcome
measurements
associated
statistics
core
problem
analyse
whether
target
quantum
measurement
learnable
characterise
performance
learning
tasks
1.1.
contributions
work
work
answer
following
two
questions
quantum
2in
scenario
learning
multi-outcome
measurements
povm
element
considered
two-outcome
povm
hence
learnability
povm
element
derived
following
proposed
paradigm
note
problem
tackled
multi-label
learning
algorithms
also
called
multi-target
prediction
multivariate
regression
many
quantum
states
suﬃcient
learn
quantum
measurement
assume
unknown
two-outcome
quantum
measurement
device
prepare
set
quantum
states
randomly
drawn
unknown
distribution
suppose
outcome
statistics
set
quantum
states
known
infer
unknown
quantum
measurement
quantum
states
hand
many
samples
quantum
states
needed
learning
machine
decide
optimal
quantum
measurement
hypothesis
set
chosen
candidate
approximate
target
measurement
desired
accuracy
questions
typical
sample
complexity
problems
statistical
learning
theory
answer
lies
proper
quantiﬁcation
eﬀective
size
hypothesis
set
paper
propose
framework
see
section
connect
problems
learning
two-outcome
measurements
tasks
learning
real-valued
linear
functional
quantum
states
exploiting
banach
space
theory
noncommutative
khintchine
inequalities
random
matrix
theory
prove
theorem
4.1
complexity
measure—fat-shattering
dimension—is
upper
bounded
d/2
framework
complexity
measures
covering
numbers
rademacher
complexity
derived
result
number
required
sample
states
learn
unknown
quantum
measurement
proportional
dimension
hilbert
space
many
quantum
measurements
suﬃcient
learn
quantum
state
following
paradigm
learning
quantum
measurements
similarly
formalize
problem
learning
unknown
state
dual
problem
unlike
aaronson
employ
tools
solely
statistical
learning
theory
show
theorem
5.1
fat-shattering
dimension
log
d/2
learning
qudit
state
addition
also
derive
covering
number
rademacher
complexity
results
show
three
complexity
measures
characterised
logarithmically
proportional
hilbert
dimension
lastly
formulating
quantum
learning
problems
bloch-sphere
representation
show
equiva-
lent
neural
network
hence
classical
algorithms
practically
applied
perform
quantum
tasks
several
ﬁelds
may
relate
beneﬁt
work
quantum
state/measurement
tomography
quantum
state
tomography
diﬃcult
task
physics
number
unknown
parameters
multi-partite
quantum
system
grows
exponentially
aaronson
pointed
quantum
serve
alternative
approach
quantum
state
tomography
surprisingly
learning
unknown
target
state
within
given
accuracy
requires
number
measurements
grows
logarithmically
dimension
work
push
aaronson
result
one
step
consider
application
machine
learning
framework
study
quantum
measurement
tomography
best
knowledge
results
direction
hope
result
learning
quantum
measurements
stimulate
investigation
problem
quantum
state
discrimination
goal
quantum
state
discrimination
determine
identity
state
ensemble
whenever
states
mutually
orthogonal
perfectly
discriminated
therefore
possible
way
ambiguous
state
discrimination
goal
minimizing
error
probability
given
show
fat-shattering
dimension
guarantees
set
quantum
states
discriminated
two
subsets
worst
error
probability
greater
1/2
following
reasoning
quantum
states
hypothesis
set
used
distinguish
two-outcome
measurements
quantum
random
access
codes
-qra
coding
stands
encoding
n-bit
sequence
m-qubit
receiver
recover
one
bits
successful
probability
least
information-
theoretic
inequalities
provide
upper
bound
fat-shattering
dimension
learning
quantum
states
alternatively
use
complexity
measure—pseudo
dimension—to
show
exists
-qra
coding
scheme
22m
result
coincides
work
hayashi
see
section
5.4
discussions
paper
organised
follows
section
introduce
background
statistical
learning
theory
especially
supervised
learning
describe
important
complexity
measures
section
formalise
uniﬁed
framework
relate
problems
learning
quantum
measurements
learning
quantum
states
learning
real-valued
functions
based
proposed
approach
derive
learning
quantum
measurements
prove
main
results
section
addition
discuss
interpretations
ambiguous
set
discrimination
also
derive
covering
numbers
rademacher
complexity
section
consider
problem
learning
quantum
states
discuss
relationship
qra
codes
section
formulate
learning
problem
bloch-sphere
representation
discuss
possible
algorithms
e.g
neural
networks
implement
quantum
learning
tasks
conclude
paper
section
notation
paper
denote
hilbert
space
trace
operator
calculated
orthonormal
basis
let
denote
set
self-adjoint
operators
hilbert-
schmidt
inner
product
deﬁned
cid:104
cid:105
subscript
omitted
context
clear
denote
schatten
p-norm
operator
cid:88
cid:18
cid:88
i≥1
ekm
cid:107
cid:107
|λi
cid:19
1/p
eigenvalue
denote
cid:107
cid:107
supi
|λi
operator
norm
clearly
cid:107
cid:107
cid:107
cid:107
correspond
trace
norm
hilbert-schmidt
norm
cid:107
cid:107
respectively
slightly
abusing
notation
also
denote
conventional
cid:96
norm
cid:107
cid:107
deﬁne
unit
ball
associated
cid:107
cid:107
set
bounded
operators
denoted
schatten
norms
set
operators
ﬁnite
schatten
∞-norm
likewise
set
operators
ﬁnite
schatten
1-norm
called
set
trace
class
operators
quantum
state
also
called
density
operators
hilbert
space
positive
semi-deﬁnite
operator
unit
trace
identify
state
space
set
quantum
states
i.e
cid:23
positive
operator-valued
measure
povm
ﬁnite
set
positive
semi-deﬁnite
operators
i∈i
cid:88
i∈i
denotes
identity
operator
povm
element
called
quantum
eﬀect
serves
instrument
perform
yes-no
measurement
denote
set
eﬀects
eﬀect
space
cid:22
cid:22
constants
denoted
independent
parameters
values
may
change
line
line
notation
cid:46
means
constant
cid:39
means
cid:46
cid:38
summarise
notation
table
appendix
background
statistical
learning
theory
starting
point
section
mathematical
formalism
supervised
machine
learning
describe
eﬀectiveness
learning
machine
examine
number
samples
required
produce
almost
optimal
function
error
rate
desired
accuracy
shown
later
bound
sample
complexity
closely
related
measures
complexity
characterise
size
function
class
2.1.
supervised
machine
learning
generally
speaking
supervised
learning
task
infers
function
learning
model
observing
data
response
data
work
focus
deﬁnitions
agnostic
pac
learnability
sample
complexity
supervised
machine
learning
comprehensive
introduction
refer
readers
literature
refs
60–65
consider
probability
space
called
input
space
measurable
space
called
output
space
closed
subset
real
line
probability
distribution
assumed
ﬁxed
known
training
data
set
i.e
sampled
independently
identically
according
measure
supervised
learning
aims
construct
function
approximates
functional
relationship
input
variable
output
variable
observed
training
data
set
evaluate
performance
approximation
deﬁne
loss
function
measurable
map
cid:96
expected
risk
also
called
out-of-sample
error
cid:96
loss
function
usually
taken
absolute
error
square
error
i.e
cid:96
cid:96
since
interested
minimising
expected
risk
hence
target
function
bayes
function
convenience
consider
square
error
work
loss
functions
satisfy
lipschitz
condition
easily
generalised3
deﬁned
achieve
minimum
expected
risk
called
bayes
risk
i.e
2.1
inﬁmum
taken
possible
measurable
functions
deterministic
function
almost
surely
goal
learner
identify
target
function
collection
functions
called
hypothesis
set
set
real-valued
functions
deﬁned
input
space
learning
algorithm
hypothesis
set
mapping
assigns
every
training
data
candidate
function
i.e
lbayes
inf
n=1zn
eﬀectiveness
learning
algorithm
measured
number
data
required
produce
almost
optimal
function
sense
2.1
therefore
introduce
one
fundamental
concepts
supervised
machine
learning—agnostic
probably
approximately
correct
pac
learning
model
deﬁnition
2.1
agnostic
pac
learnability
def
3.3
hypothesis
set
agnostic
pac
learnable
exist
function
learning
algorithm
following
property
every
every
distribution
running
learning
algorithm
samples
generated
algorithm
returns
hypothesis
probability
least
choice
training
examples
inf
f∈f
however
expected
risk
cid:96
calculated
since
unknown
evaluate
agreement
candidate
function
training
data
set
called
empirical
risk
also
called
in-sample
error
cid:98
cid:88
i=1
cid:96
example
one
well-known
learning
algorithms
empirical
risk
minimization
erm
principle
assigns
function
training
data
set
almost
optimal
data
i.e
2.2
cid:98
following
reasoning
agnostic
pac
model
goal
hence
estimate
generalisation
error
one
way
evaluate
performance
learning
algorithm
relate
risk
empirical
risk
f∈f
cid:98
arg
min
algorithm
outputs
leads
deﬁnition
uniform
glivenko-cantelli
class
ugc
class
deﬁnition
2.2.
say
hypothesis
set
uniform
glivenko-cantelli
class
every
cid:98
cid:98
sup
f∈f
cid:98
cid:41
cid:12
cid:12
cid:12
cid:98
cid:12
cid:12
cid:12
cid:40
n→∞
sup
lim
sup
f∈f
loss
function
cid:96
lipschitz
function
satisﬁes
lipschitz
condition
cid:96
cid:96
l|f
possible
quantity
called
lipschitz
constant
denote
cid:96
set
cid:96
complexity
measures
e.g
covering
number
rademacher
complexity
class
cid:96
diﬀerent
hypothesis
set
lipschitz
constant
i.e
cid:96
/l
cid:96
lrn
therefore
homogeneity
may
assume
loss
function
absolute
error
square
error
deriving
sample
complexity
problems
4note
use
term
hypothesis
set
function
class
interchangeably
throughout
paper
uniformity
respect
members
possible
probability
measures
domain
addition
conditions
learnability
also
consider
bound
rate
uniform
convergence
every
let
ﬁrst
integer
every
probability
measure
cid:40
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:98
cid:41
2.3
sup
f∈f
quantity
satisﬁed
2.3
called
glivenko-cantelli
sample
complexity
hypothesis
set
accuracy
conﬁdence
sample
complexity
encapsulates
number
samples
required
learn
set
functions
ugc
class
suﬃcient
agnostic
pac
learnability5
theorem
2.1
uniform
convergence
corollary
4.4
training
data
set
called
-representative
respect
domain
hypothesis
set
loss
function
cid:96
distribution
vapnik
studied
relation
ugc
class
learnability
showed
hypothesis
set
cid:12
cid:12
cid:12
cid:98
cid:12
cid:12
cid:12
every
every
probability
distribution
ugc
class
guarantees
/2-
representative
set
probability
least
agnostic
pac
learnable
furthermore
erm
algorithm
agnostic
pac
learner
result
consider
generalisation
error
sample
complexity
hypothesis
set
performance
criterion
investigate
whether
underlying
learning
problem
agnostic
pac
learnability
summary
fundamental
problems
two-fold
ﬁrst
conditions
machine
agnostic
pac
learnable
secondly
sample
complexity
determines
rate
uniform
convergence
information-theoretic
eﬃciency
hypothesis
set
next
subsection
several
complexity
measures
introduced
characterise
richness
eﬀective
size
hypothesis
set
appendix
show
sample
complexity
expressed
terms
complexity
measures
2.2.
measures
sample
complexity
discussed
interested
parameters
eﬀectively
measure
size
given
hypothesis
set
well-known
measures
information
complexity6
function
class
combinatorial
parameters
covering
numbers
rademacher
complexity
ﬁrst
combinatorial
parameter—vapnik-chervonenkis
dimension—was
introduced
vapnik
cher-
vonenkis
learning
boolean
functions
deﬁnition
2.3
dimension
let
set
-valued
functions
domain
say
shatters
set
every
subset
exists
function
let
dimension
domain
denoted
vcdim
vcdim
sup
|s|
shattered
pollard
generalised
concept
dimension
introduced
pseudo
dimension
quantify
sample
complexity
real-valued
function
class
parameterised
version
pollard
pseudo-dimension
scale-sensitive
dimension
also
called
fat-shattering
dimension
introduced
kearns
schapire
deﬁnition
2.4
pseudo
dimension
let
set
real-valued
functions
domain
say
set
pseudo-shattered
exists
set
i=1
every
function
deﬁne
pseudo
dimension
called
shattering
function
set
pdim
sup
|s|
pseudo-shattered
desirable
property
pseudo
dimension
useful
main
theorems
theorem
2.2
pollard
5agnostic
pac
learnable
also
called
learnable
erm
say
erm
algorithm
consistent
recent
works
consider
stability
issues
learning
algorithm
one
criterion
learnability
however
paper
deal
issues
stability
refer
interested
readers
refs
references
therein
6the
complexity
measures
introduced
section
generalisation
bounds
derived
section
information-theoretic
sense
learning
algorithms
based
agnostic
pac
model
regardless
computational
resources
vector
space
real-valued
functions
pdim
dim
subset
vector
space
cid:48
real-valued
functions
pdim
dim
cid:48
deﬁnition
2.5
fat-shattering
dimension
let
set
real-valued
functions
domain
every
set
said
-shattered
exists
set
i=1
every
function
deﬁne
fat-shattering
dimension
domain
fatf
sup
|s|
-shattered
called
shattering
function
set
set
i=1
called
witness
-shattering
underlying
space
clear
denote
fatf
witness
set
equal
constant
call
level
fat-shattering
dimension
fatf
ref
relationship
fat-shattering
dimension
pseudo-dimension
given
theorem
2.3
anthony
bartlett
let
set
real-valued
functions
fatf
pdim
ﬁnite
set
pseudo-shattered
-shattered
iii
function
fatf
non-increasing
pdim
lim↓0
fatf
sides
may
inﬁnite
note
possible
pseudo-dimension
inﬁnite
even
fat-shattering
dimension
ﬁnite
positive
addition
combinatorial
parameters
bounding
sample
complexity
quantities
called
covering
number
measure
size
function
class
ﬁnite
approximating
set
concept
covering
number
dates
back
kolmogorov
used
many
areas
mathematics
deﬁnition
2.6
covering
number
let
metric
space
let
every
set
called
-cover
every
covering
number
minimum
cardinality
-covering
set
respect
metric
samples
every
sample
let
n−1
cid:80
sample
function
denote
cid:107
cid:107
cid:0
n−1
cid:80
characterise
size
function
class
machine
learning
investigate
metrics
endowed
i=1
δxi
empirical
measure
supported
cid:107
cid:107
max1≤i≤n
i=1
cid:1
1/p
covering
number
scale
respect
norm
deﬁnition
2.7
entropy
number
every
class
let
sup
sup
sup
call
log
entropy
number
respect
log
uniform
entropy
number
signiﬁcance
uniform
measures
complexity
i.e
uniform
entropy
number
combinatorial
param-
eters
lies
characterise
ugc
class
however
bounds
loose
bartlett
mendelson
considered
techniques
concentration
measures
empirical
processes
proposed
random
aver-
age
quantity—rademacher
complexity
capture
size
ugc
class
directly
leads
sharp
complexity
bounds
deﬁnition
2.8
rademacher
complexity7
let
probability
measure
set
uniformly
bounded
functions
every
positive
integer
deﬁne
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
i=1
cid:12
cid:12
cid:12
cid:12
cid:12
sup
f∈f
γif
i=1
independent
random
variables
distributed
according
equal
probability
also
independent
rademacher
complexity
associated
class
i=1
independently
takes
values
i=1
quantity
called
remark
complexity
measures
related
among
77–79
fatf
cid:46
log
cid:46
cid:46
fatf
log
cid:18
cid:19
sum
results
presented
far
complexity
measures
combinatorial
parame-
ters
e.g
dimension
fat-shattering
dimension
covering
numbers
rademacher
complexity
hypothesis
set
control
rate
uniform
convergence
computing
quantities
given
hypothesis
set
according
eqs
b.1
b.2
b.3
b.4
estimate
bounds
sample
complexity
learning
problems
framework
learning
quantum
measurements
quantum
states
section
unify
two
quantum
learning
problems
hand
learning
linear
functionals
section
3.3
justify
proposed
quantum
learning
model
practical
situations
3.1.
quantum
learning
problems
linear
functional
matrices
recall
physical
theory
aims
predict
events
observed
experiments
describing
three
types
apparatus
preparation
transformation
measurement
preparation
process
system
embodied
state
eﬀect
measurement
produces
either
yes
outcomes
order
observe
physical
experiment
however
according
statistical
nature
quantum
theory
probabilities
occurrence
predicted
counting
multiple
measurements
precisely
assume
system
prepared
state
outcome
every
two-outcome
measurement
takes
form
probability
distribution
cid:104
cid:105
note
linear
functional
state
space
i.e
theory
learning
-valued
functions
called
probabilistic
concepts
following
proposition
establishes
one-to-one
correspondence
proposition
3.1
correspondence
two-outcome
measurement
linear
functional
prop
2.30
given
hilbert
space
let
eﬀect
i.e
linear
map
interval
exists
bounded
operator
cid:104
cid:105
furthermore
operator
unique
following
sense
let
cid:104
e1ϕ
cid:105
cid:104
e2ϕ
cid:105
every
cid:105
proposition
states
every
two-outcome
measurement
identiﬁed
linear
functional
state
space
consequently
problem
learning
unknown
two-outcome
quantum
measurement
equivalent
learning
real-valued
linear
functional
quantum
states
subsequently
call
eﬀect
represent
either
linear
functionals
two-outcome
measurement
conversely
measurement
apparatus
chosen
measurement
outcome
every
state
distributed
cid:104
cid:105
therefore
take
state
space
set
linear
functionals
eﬀect
space
following
proposition
proposition
3.2
correspondence
quantum
state
linear
functional
eﬀect
space
given
hilbert
space
let
probability
measure
exists
quantum
state
cid:104
cid:105
furthermore
diﬀerent
determines
diﬀerent
probability
measures
i.e
exists
operator
eρ1
cid:54
eρ2
similarly
according
one-to-one
correspondence
learning
unknown
quantum
state
coincides
learning
real-valued
linear
functional
eﬀect
space
authors
deﬁne
rademacher
complexity
normalisation
term
rather
ref
convenient
bound
sample
complexity
e.g
b.4
follow
notation
used
3.2.
learning
linear
functionals
banach
space
previous
section
establish
relationship
quantum
measurements/states
linear
functional
matrices
duality
theorem
see
theorem
3.1
two
quantum
learning
problems
uniﬁed
problem
learning
membership
banach
space
furthermore
real-valued
function
associates
target
quantity
banach
space
isomorphic
linear
functional
input
space
i.e
element
dual
space
input
space
example
assume
input
space
unit
ball
schatten
p-class
i.e
hypothesis
set
represented
linear
functionals
polar8
i.e
1/p
1/q
cid:8
cid:55
cid:104
cid:105
cid:9
cid:0
cid:1
duality
formalism
problems
estimating
complexity
measures
subset
banach
space
transformed
following
question
whether
set
linear
functionals
agnostic
pac
learnable
theorem
3.1
duality
bounded
operator
trace
class
theorems
19.1
19.2
fix
hilbert
space
map
cid:55
isometric
isomorphism
space
bounded
operators
dual
space
set
trace
classes
operators
conversely
map
cid:55
isometric
isomorphism
mendelson
schechtman
ﬁrst
investigated
fat-shattering
dimension
sets
linear
functionals
banach
space
proposed
following
useful
result
lemma
3.1
mendelson
schechtman
set
-shattered
bx∗
i=1
linearly
independent
every
cid:88
i=1
cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
i=1
cid:13
cid:13
cid:13
cid:13
cid:13
|ai|
aixi
restricting
values
set
unit
ball
banach
space
bx∗
dual
unit
ball
series
banach
space
points
rademacher
series
deﬁned
cid:80
i=1
core
idea
lemma
3.1
calculate
rademacher
i=1
symmetric
-valued
random
variables
additionally
following
duality
formula
schatten
p-norm
estimate
range
linear
functional
helpful
derive
complexity
measures
theorem
3.2
duality
formula
cid:107
cid:107
theorem
7.1
deﬁne
1/q
1/p
i=1
γixi
cid:107
cid:107
sup
b∈md
cid:107
cid:107
techniques
mendelson
schechtman
lemma
3.1
duality
formula
theorem
3.2
used
upper
bound
fat-shattering
dimension
rademacher
complexity
via
rademacher
series
remains
compute
rademacher
series
banach
space
complexity
measures
leave
details
sections
3.3.
justiﬁcation
quantum
learning
model
proceeding
derive
complexity
mea-
sures
ﬁrst
address
two
practical
issues
may
arise
quantum
learning
setting
yes
outcome
observed
rather
outcome
statistics9
measurement
apparatus
perfect
e.g
measurement
errors
training
data
set
however
show
sample
complexities
two
scenarios
remain
lipschitz
constant
output
space
consists
binary
measurement
outcomes
rather
measurement
statistics
case
training
sample
equals
probability
πxi
probability
πxi
show
covering
number
remains
training
sample
πxi
considered
8in
convex
analysis
convex
body
convex
compact
set
nonempty
interior
gauge
convex
body
also
known
minkowski
functional
deﬁned
cid:107
cid:107
inf
symmetric
respect
origin
unit
ball
associated
norm
cid:107
cid:107
inner
product
cid:104
cid:105
deﬁne
polar
cid:26
cid:27
symmetric
case
unit
ball
dual
space
cid:107
cid:107
ball
schatten
∞-class
considering
hilbert-schmidt
inner
product
9the
situation
also
occur
one
measurement
performed
sup
k∈k
cid:104
cid:105
unit
ball
schatten
1-class
sd∞
unit
sd∞
polar
quantum
machine
learning
setting
complexity
measures
easily
follow
argument
assume
underlying
loss
function
cid:96
satisﬁes
lipschitz
condition
i.e
exists
3.1
cid:96
cid:96
l|f
denoting
expected
risk
expressed
follows
cid:96
exey
cid:96
cid:96
cid:96
cid:96
cid:48
third
equality
use
fact
resp
outcome
occurs
probability
resp
cid:96
cid:96
last
line
introduce
induced
loss
function
cid:96
cid:48
distance
cid:96
cid:48
calculated
cid:96
cid:48
cid:96
cid:48
cid:96
cid:48
|px
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
cid:96
l|f
l|f
l|f
second
inequality
follows
triangle
inequality
next
line
due
lipschitz
condition
upper
bounded
l|f
exactly
relation
shows
distance
cid:96
cid:48
upper
bound
cid:96
cid:96
see
3.1
recall
deﬁnition
2.6
clearly
covering
numbers
respect
induced
loss
function
original
loss
function
bounded
quantity
therefore
generalisation
error
2.3
sample
complexity
change
scenario
cid:96
cid:48
noise
involved
measurement
procedure
case
assume
training
sample
random
variable
models
measurement
error
following
reasoning
calculate
expected
risk
follows
cid:96
exen
cid:96
cid:96
cid:48
cid:96
thus
last
line
let
cid:96
cid:48
cid:96
cid:48
cid:96
cid:48
|en
cid:96
cid:96
l|en
l|f
therefore
original
complexity
measures
depends
distance
loss
function
induced
sample
complexity
hold
learning
quantum
measurements
section
follow
quantum
learning
framework
presented
section
explicitly
show
derive
upper
bound
fat-shattering
dimension
rademacher
complexity
covering/entropy
number
discuss
complexity
measures
relate
quantum
state
discrimination
recall
problem
learning
unknown
quantum
measurement
goal
learn
ﬁxed
i=1
unknown
eﬀect
training
data
set
πρi
distribute
independently
according
unknown
measure
note
learning
equivalent
learning
two-outcome
povm
due
correspondence
quantum
eﬀect
linear
functional
cid:55
cid:104
cid:105
input
space
proposition
3.1
consider
hypothesis
set
consists
quantum
eﬀects10
i=1
hypothesis
set
chosen
subset
eﬀects
space
target
eﬀect
may
belong
goal
choose
eﬀect
hypothesis
set
approximates
target
well
discuss
issue
section
also
note
sometimes
denote
subset
sometimes
denote
linear
functionals
formed
subset
conv
sd∞
conv
sd∞
following
present
main
result
question
many
quantum
states
needed
learn
quantum
measurement
exactly
sample
complexity
problem
introduced
section
2.1.
tackle
problem
estimate
complexity
measures
characterise
size
hypothesis
set
4.1.
fat-shattering
dimension
learning
quantum
measurements
ﬁrst
step
use
common
trick
convex
analysis
namely
symmetrisation
state
space
eﬀect
space
embed
subset
banach
space
words
symmetric
convex
hull
state
space
forms
unit
ball
schatten
1-class
conv
denotes
convex
hull
operation
similarly
input
space
elements
sd∞
hypothesis
set
consists
linear
functionals
paremeterised
main
reason
introducing
sd∞
unit
balls
polar
hilbert-schmidt
inner
product
thus
apply
mendelson
schechtman
result
lemma
3.1
estimate
fat-shattering
dimension
following
main
result
result
theorem
4.1
fat-shattering
dimension
learning
quantum
measurements
1/2
integer
pdim
fate
min
d/2
proof
ﬁrst
present
outline
proof
according
deﬁnition
fat-shattering
dimension
follows
function
fatf
non-increasing
hence
ﬁrst
objective
check
whether
fat-shattering
dimension
unbounded
equivalently
suﬃces
ﬁnd
pseudo
dimension
bounds
fat-shattering
dimension
theorem
2.3
secondly
assume
set
points
-shattered
ﬁnd
inequality
relate
prove
claim
pseudo
dimension
since
vector
space
dimension
sd∞
subset
embed
sd∞
real
vector
space
dimension
since
function
class
subset
d2-dimensional
vector
space
theorem
2.2
obtain
pdim
-shattered
sd∞
i=1
independent
uniform
random
variables
denote
rademacher
series
cid:80
fat-shattering
dimension
consider
set
i=1
γixi
also
called
rademacher
random
variables
selecting
lemma
3.1
4.1
adopt
probabilistic
method
upper
bound
right-hand
side
4.1
ﬁnd
quantity
upper
bounds
cid:107
cid:80
result
remains
ﬁnd
upper
bound
expected
norm
rademacher
series
cid:107
cid:80
i=1
γixi
cid:107
realization
i=1
cid:107
cid:80
i=1
γixi
cid:107
i=1
i=1
γixi
cid:107
order
upper
bound
rademacher
series
need
powerful
noncommutative
khintchine
inequalities
proposition
4.1
noncommutative
khintchine
inequalities
let
i=1
independent
rademacher
random
variables
cid:16
cid:107
cid:80
cid:107
cid:80
i=1
xix
cid:16
cid:107
cid:80
1/2
cid:107
1/2
cid:107
i=1
aia
i=1
cid:107
cid:80
1/2
cid:107
i=1
i=1
deterministic
matrices
cid:17
1/p
cid:17
1/p
1/2
cid:107
inf
xi=ai+bi

cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
i=1
γixi
cid:13
cid:13
cid:13
cid:13
cid:13
means
equality
holds
absolute
constant
depending
denotes
complex
conjugate
operation
note
haagerup
musat
proved
result
also
holds
i=1
independent
standard
complex
gaussian
random
variables
cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
cid:13
cid:13
cid:13
cid:13
cid:13
γixi
invoking
proposition
4.1
since
square
operation
preserves
problem
reduced
ﬁnding
i.e
convexity
cid:80
i=1
i=1
i=1
i=1
cid:46
γixi
max
x∈sd
cid:13
cid:13
cid:13
cid:13
cid:13
cid:33
1/2
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:32
cid:88
cid:33
1/2
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:112
|λi|
subject
cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:32
cid:88
cid:88
cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
cid:112
i=1
cid:107
cid:80
i=1
γixi
cid:107
cid:46
max
x∈sd
cid:88
j=1
j=1
i=1
cid:107
cid:107
|λj|
max
∈sd
max
x∈sd
cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
i=1
essentially
convex
optimisation
problem
since
square
root
concave
attain
maximum
|λj|
1/d
4.2
γixi
consequently
realization
d/2
proves
claim
∀xi
combined
4.1
cid:3
following
proposition
demonstrate
upper
bound
tight
proposition
4.2.
considering
hilbert
space
exist
inﬁnitely
many
sets
quantum
states
1/2-shattered
eﬀect
space
proof
consider
arbitrary
mutually
orthogonal
rank-1
projection
operators
pure
states
input
states
every
denote
cid:104
cid:80
easily
check
cid:80
cid:42
cid:88
i∈b
i=1
i∈b
cid:105
note
one
cid:43
i∈b
cid:104
cid:105
similarly
result
i=1
1/2-shattered
cid:3
4.2.
rademacher
complexity
following
paradigm
section
4.1
calculate
rademacher
com-
plexity
eﬀect
space
via
duality
formula
theorem
3.2
noncommutative
khintchine
in-
equality
proposition
4.1.
theorem
4.2
rademacher
complexity
learning
quantum
measurements
assume
input
space
state
space
hypothesis
set
rademacher
complexity
cid:16
cid:17
proof
recall
deﬁnition
rademacher
complexity
deﬁnition
2.8
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:43
cid:12
cid:12
cid:12
cid:12
cid:12
γife
cid:104
cid:105
cid:88
i=1
γixi
i=1
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:42
cid:13
cid:13
cid:13
cid:13
cid:13
i=1
γixi
nrn
sd∞
sup
e∈sd∞
sup
e∈sd∞
sup
e∈sd∞
cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
i=1
cid:46
third
line
due
duality
formula
theorem
3.2
last
relation
follows
4.2
cid:3
completes
proof
4.3.
entropy
number
covering
number
related
entropy
number
follows
directly
rademacher
complexity
sudakov
minoration
theorm
theorem
4.3
sudakov
minoration
theorem
let
index
set
let
t∈t
sub-gaussian
process11
l2-metric
i.e
cid:107
cid:107
log
1/2
sup
t∈t
cid:107
cid:107
constant
corrollary
4.1
entropy
number
learning
quantum
measurements
assume
input
space
state
space
hypothesis
set
covering
number
function
class
log
d/2
proof
upper
bound
empirical
entropy
number
rademacher
complexity
follows
directly
sudakov
minoration
theorem
denote
vector-valued
stochastic
process
independently
drawn
according
distribution
distance
measure
calculated
γ1f
γnf
cid:33
1/2
cid:107
cid:107
cid:32
cid:88
i=1
cid:107
cid:107
invoke
theorem
4.3
4.2
obtain
log
log
cid:0
supf∈f
cid:107
cid:107
cid:1
2rn
note
right-hand
side
last
line
depend
distribution
hence
entropy
number
log
supµn
log
d/2
follows
cid:3
11a
stochastic
process
called
sub-guassian
exists
exp
θxt
exp
σ2θ2/2
note
gaussian
process
rademacher
process
belong
sub-gaussian
process
pseudo
dimension
eﬀect
space
pdim
means
need
parameters
exactly
determine
povm
element
note
coincides
number
measurements
quantum
measurement
tomography
since
lies
d2-dimensional
real
vector
space
however
relax
criterion
tolerating
accuracy
eﬀect
space
covered
exp
d/2
balls
radius
words
need
log
d/2
samples
identify
ball
target
povm
element
lies
meaning
entropy
number
applying
quantum
learning
model
quantum
measurement
tomography
specify
pac
candidate
povm
element
accuracy
conﬁdence
d/2
samples
quadratically
speed-up
original
scheme
4.4.
relationship
quantum
state
discrimination
quantum
state
discrimination
studies
optimally
distinguish
set
quantum
states
according
ﬁgure
merit
nevertheless
limitations
quantum
state
discrimination
states
always
perfectly
discriminated
moreover
may
necessary
ﬁnd
exact
state
scenario
therefore
zhang
ying
considered
quantum
set
discrimination
goal
identify
set
given
state
belongs
relate
concepts
fat-shattering
dimension
quantum
set
discrimination
deﬁnition
4.1
-separable
set
set
linearly
separable
respect
set
subset
exists
-strip
separates
complement
words
exist
cid:104
cid:105
/2
cid:104
cid:105
/2
diﬃcult
see
2-separable
set
correspond
task
quantum
set
discrimination
ensemble
error
probability
given
state
classiﬁed
set
greater
one
interesting
question
ask
maximum
cardinality
2-separable
set
following
proposition
shows
fat-shattering
dimension
equals
quantity
proposition
4.3.
denote
function
class
cid:104
cid:105
assume
exists
set
2-separable
respect
maximum
cardinality
set
fatf
proof
recall
deﬁnition
2.5
set
2-separable
respect
fat
proposition
equivalent
show
fat
fat
fat
fat
deﬁnition
suﬃces
show
fat
fat
given
choose
set
largest
integer
-shattered
i=1
witnessing
shattering
without
loss
generality
assume
cid:54
1/2
choose
arbitrary
subset
contains
deﬁnition
fat-shattering
dimension
exists
function
set
cid:104
cid:105
also
cid:104
cid:105
denote
since
convex
set
cid:48
satisﬁes
cid:104
cid:105
cid:104
cid:105
similarly
let
cid:48
cid:48
cid:104
cid:48
cid:105
1/2
cid:104
cid:48
cid:105
1/2
argument
holds
cid:54
1/2
follows
level
fat-shattering
dimension
witnessed
1/2
cid:3
also
achieves
cardinality
-shattered
set
completes
proof
section
consider
problem
learning
unknown
quantum
state
cid:48
training
data
set
cid:48
i=1
independently
sampled
according
unknown
distribution
cid:48
proposition
3.2
hypothesis
set
consists
linear
functional
cid:55
cid:104
cid:105
i=1
learning
quantum
states
cid:48
similarly
embed
input
space
unit
ball
schatten
∞-class
i.e
sd∞
hypothesis
set
collection
linear
functionals
input
space
i.e
following
aim
calculate
complexity
measures
characterise
sample
complexity
learning
quantum
states
interesting
see
proofs
derived
section
i.e
complexity
measures
learning
quantum
states
parallel
previous
section
i.e
complexity
measures
learning
quantum
measurements
due
duality
relation
theorem
3.1.
finally
discuss
relationship
fat-shattering
dimension
quantum
random
access
codes
5.1.
fat-shattering
dimension
learning
quantum
states
framework
presented
section
characterising
input
space
sd∞
hypothesis
set
cid:48
consisting
linear
functionals
elements
cid:48
therefore
main
result
deriving
fat-shattering
dimension
state
space
theorem
5.1
fat-shattering
dimension
learning
quantum
states
1/2
integer
pdim
fatq
min
log
d/2
embedded
real
vector
space
dimension
pdim
pseudo
dimension
state
space
lies
set
cid:107
cid:107
sphere
since
∂sd
fat-shattering
dimension
every
proof
following
fashion
proof
theorem
4.1
ﬁrst
estimate
pseudo
dimension
fat-shattering
dimension
i.e
∂sd
i=1
γixi
cid:107
however
scenario
learning
quantum
states
input
space
lies
schatten
∞-class
esti-
mate
spectral
norm
rademacher
series
beneﬁting
recent
development
matrix
concentration
inequalities
tropp
proved
following
results
proposition
5.1
upper
bound
rademacher
series
consider
ﬁnite
sequence
deterministic
her-
mitian
matrices
dimension
let
independent
rademacher
variables
form
matrix
rademacher
series
i=1
sd∞
calculate
rademacher
series
cid:107
cid:80
cid:88
γixi
5.1
2σ2
log
compute
variance
parameter
note
result
also
holds
case
standard
complex
gaussian
variables
invoking
tropp
development
matrix
concentration
inequalities
see
proposition
5.1
cid:107
cid:0
cid:1
cid:107
cid:107
cid:107
cid:112
cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
cid:13
cid:13
cid:13
cid:13
cid:13
cid:112
i=1
γixi
cid:13
cid:13
cid:13
straightforward
computation
shows
cid:33
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
cid:112
log
i=1
cid:107
cid:80
lemma
3.1
selecting
cid:107
cid:80
i=1
γixi
cid:107
i=1
γixi
cid:107
combining
inequalities
log
d/2
cid:3
realization
cid:13
cid:13
cid:13
cid:80
log
∀xi
sd∞
cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
cid:32
cid:88
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
2σ2
log
γiγjxixj
get
γixi
γixi
γixi
i=1
i=1
i=1
i=1
completing
proof
5.2.
rademacher
complexity
repeating
procedure
introduced
section
4.2
compute
rademacher
complexity
state
space
theorem
5.2
rademacher
complexity
learning
quantum
states
assume
input
space
eﬀect
space
hypothesis
set
deﬁned
state
space
rademacher
complexity
hypothesis
set
proof
recall
deﬁnition
rademacher
complexity
nrn
sup
ρ∈sd
cid:17
i=1
cid:16
cid:112
log
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:88
cid:12
cid:12
cid:12
cid:12
cid:12
cid:42
cid:88
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:88
cid:46
cid:112
log
sup
ρ∈sd
sup
ρ∈sd
γiei
i=1
i=1
i=1
γifρ
cid:104
cid:105
γiei
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:43
cid:12
cid:12
cid:12
cid:12
cid:12
forth
line
due
duality
formula
theorem
3.2.
last
relation
follows
5.1
completes
cid:3
proof
5.3.
entropy
number
corrollary
5.1
entropy
number
learning
quantum
states
assume
input
space
function
class
deﬁned
state
space
covering
number
function
class
log
log
d/2
compared
entropy
number
eﬀect
space
result
state
space
proportional
logarithmic
dimension
intuition
behind
unit
ball
schatten
∞-class
much
larger
unit
ball
schatten
1-class
thus
requires
-radius
ball
cover
whole
eﬀect
space
state
space
volumetric
perspective
fact
evident
denote
lebesgue
measure
banach
space
schatten
class
calculated
|1/d2
|1/
d2−1
cid:39
cid:19
1/d2
cid:18
|sd∞|
|sd
cid:39
shows
volume
eﬀect
space
essentially
exponential
dimension
state
space
recall
complexity
measures
quantity
estimate
eﬀective
size
hypothesis
set
accordingly
reasonable
complexity
measures
eﬀect
space
exponentially
compared
state
space
words
results
theorem
4.1
demonstrate
richness
eﬀect
space
5.4.
relationship
quantum
random
access
codes
learnability
quantum
states
ﬁrst
addressed
aaronson
ingeniously
applied
results
quantum
random
access
coding
provide
information-theoretic
upper
bound
fat-shattering
dimension
learning
m-qubit
quantum
states
ﬁrst
give
deﬁnitions
qra
codes
discuss
aaronson
result
deﬁnition
5.1
quantum
random
access
coding
-qra
coding
function
maps
n-bit
strings
m-qubit
states
satisfying
following
every
exists
povm
i-th
bit
exists
-qra
coding
fact
sets
i=1
1/2
-shattered
constant
value
1/2
witnesses
shattering
5.2
1/2
2n
therefore
inequality
gives
upper
bound
level
fat-shattering
dimension
i.e
fatq
1/2
m/2
conversely
fat-shattering
dimension
scale
1/2
guarantee
existence
qra
coding
since
may
1/2
provide
upper
bound
success
probability
exists
however
case
functions
bounded
range
gurvits
utilised
pigeonhole
principle
relate
level
fat-shattering
dimension
fat-shattering
dimension
theorem
5.3
gurvits
hypothesis
set
consisting
-valued
functions
5.3
−1fatf
fatf
/2
fatf
/2
deﬁnition
fatf
fatf
however
theorem
dependencies
dimension
order
level
fat-shattering
dimension
fat-shattering
dimension
consequently
5.2
fatf
m/2
leads
fatf
m/2
according
inequalities
5.3
thus
recover
aaronson
result
theorem
5.4
aaronson
fat-shattering
dimension
learning
class
m-qubits
fatf
m/2
remark
unknown
whether
fatf
fatf
proposition
5.2.
22m
-qra
coding
1/2
positive
integer
hayashi
showed
22m
-qra
coding
1/2
result
directly
derived
theorem
5.1
shows
pdim
dimension
m-qubit
upper
bound
pseudo
dimension
shows
22m
two-outcome
povms
shattered
function
class
state
space
coincides
hayashi
result
algorithms
quantum
machine
learning
previous
sections
demonstrate
information-theoretical
analysis
quantum
learning
problems
section
provide
constructive
way
implement
quantum
tasks
representing
learning
framework
bloch
space
gather
materials
derivations
concerning
bloch-sphere
representation
appendix
recall
c.6
function
class
rank-k
eﬀects
mixture
represented
following
aﬃne
functional
cid:18
cid:0
cid:1
cid:19
conv
cid:55
bloch
vector
quantum
state
see
c.3
parameterises
function
hypothesis
set
moreover
turn
written
called
activation
function
bloch
vector
rd2−1
input
vector
rd2
input
weights
map
cid:55
thought
function
computed
linear
perceptron
using
terminology
theory
neural
network
called
single-layer
neural
network
see
appendix
details
considering
function
class
whole
eﬀect
space
exploit
convexity
eﬀect
space
obtain
following
result
cid:80
cid:88
k=0
cid:0
cid:1
k=0
called
two-layer
neural
network
also
called
single-hidden
layer
net
based
formulation
tasks
learning
quantum
measurements
implemented
existing
neural
network
algorithms
classical
algorithms
note
neural
network
formulation
learning
quantum
states
follows
way
virtue
duality
additionally
fat-shattering
dimension
easily
bounded
classical
results
neural
networks
following
corollary
corrollary
6.1.
suppose
hypothesis
set
consists
rank-k
projection
operators
mixture
fatfk
proof
since
linear
function
class
rd−1
invoking
classical
results
anthony
bartlett
fatf
a2b2
cid:55
cid:104
cid:105
cid:107
cid:107
cid:107
cid:107
rd2−1
cid:114
therefore
remains
calculate
coeﬃcients
c.4
since
cid:107
cid:107
cid:115
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
cid:13
result
follows
cid:3
see
corollary
fat-shattering
dimension
increases
rank
approaches
half
hilbert
space
dimension
means
classes
form
hierarchical
structure
operationally
hypothesis
set
chosen
ﬁrst
enlarged
conv
forth
whole
eﬀect
space
considered
called
structural
risk
minimisation
srm
usually
adopted
classical
avoid
overﬁtting
give
two
examples
illustrate
concepts
corollary
6.1.
example
6.1
learning
rank-1
projection
valued
measures
pvms
qubit
system
attains
upper
bound
fat-shattering
dimension
rank-1
projection
operators
mixture
qubit
system
bounded
fatf1
42
consider
two
quantum
states
ρr1
cid:105
cid:104
ρr2
cid:105
cid:104
corresponding
bloch
vectors
shatter
two
quantum
states
construct
four
quantum
eﬀects
bloch
vectors
n00
n11
n10
n01
since
angles
states
eﬀects
either
π/4
3π/4
en00ρr1
en00
ρr2
en11ρr1
en11
ρr2
en10ρr1
en10ρr2
en01ρr1
en01ρr2
case
three
quantum
states
follows
similarly
consider
-shatter
clearly
four
quantum
eﬀects
nijk
calculations
eight
quantum
eﬀects
achieve
fat-shattering
dimension
fatf1
-shatter
achieve
fat-shattering
dimension
fatf1
worth
emphasising
dual
problem
learning
quantum
states
equivalent
learning
quantum
measurements
hypothesis
set
consists
rank-1
projections
mixture
reason
two
mathematical
objects
exactly
i.e
conv
scenario
dual
problem
results
optimal
sense
quantum
random
access
codes
i.e
2,1,0.85
-qra
codes
furthermore
note
measurements
2,1,0.85
-qra
codes
input
states
ρr1
ρr2
example
mutually
unbiased
bases
mub
attain
upper
bound
qubit
system
example
6.2
rank
equals
half
hilbert
space
dimension
consider
quaternary
hilbert
space
i.e
first
show
exist
two
quantum
states
1/2-shattered
convex
hull
rank-1
projection
operators
consider
two
arbitrary
diﬀerent
quantum
states
i=1
function
class
1/2-shatter
set
must
eﬀect
eρ1
eρ2
clearly
achieved
rank-1
projection
two
quantum
states
equal
contradicts
assumption
second
show
exist
two
quantum
states
1/2-shattered
rank-2
projection
operators
assume
cid:105
cid:104
construct
four
quantum
eﬀects
follows
1
e11

e01
0

e10
1

e00
0

computational
basis
two
quantum
states
1/2-shattered
four
quantum
eﬀects
example
demonstrates
set
rank-2
projections
richer
set
rank-1
projections
terms
complexity
measures
cid:114
remark
readers
may
contemplate
pros
cons
bloch-sphere
representation
analysing
fat-
shattering
dimension
indeed
bloch-sphere
representation
provides
geometric
picture
concrete
ideas
linear
relation
quantum
measurements
states
furthermore
example
6.1
see
extreme
points
projection
operators
mub
play
role
fat-shattering
dimension
however
diﬃcult
fully
characterise
region
bloch
space
best
knowledge
convenient
metric
used
bloch-sphere
representation
euclidean
norm
corresponds
hilbert-schmidt
norm
schatten
2-norm
state
space
i.e
since
cid:107
cid:107
recalling
conv
cid:0
cid:1
cid:107
ρr1
ρr2
cid:107
sd∞
conv
cid:0
cid:1
hilbert-schmidt
norm
cid:107
cid:107
eﬃcient
characterising
state
space
regions
bloch
sphere
representative
valid
states
hand
unit
ball
schatten
2-class
suﬃcient
contain
sd∞
scale
cid:107
cid:107
may
overestimate
eﬀective
size
hilbert-schmidt
norm
factor
eﬀect
space
result
directly
analysing
linear
functionals
sd∞
eﬃcient
way
calculating
fat-shattering
dimension
emphasise
bloch-sphere
representation
quantum
measurements/states
transformed
euclidean
space
existing
algorithms
e.g
perceptron
learning
algorithm
neural
network
svm
etc
applied
conduct
learning
tasks
also
worth
considering
metrics
e.g
bures
metric
cid:96
norms
bloch-sphere
representation
parameterisation
methods
e.g
weyl
operator
basis
polarisation
operator
basis
majorana
representation
etc
quantum
framework
leave
future
work
learning
-outcome
povm
measurement
procedure
discussed
far
training
data
set
consists
πρi
j=0
simply
follow
i=1
j=0
cid:80
called
multi-target
prediction
multi-label
classiﬁcation
target
independently
learned
individual
function
class
πρi
π1ρi
πnρi
density
matrix
cid:98
based
randomly
sampled
pauli
operators
certain
constraint
coeﬃcients
worth
mentioning
gross
flammia
proposed
quantum
state
tomography
method
via
compressed
sensing
similar
setting
learning
quantum
states
main
goal
work
concentrate
states
well
approximated
density
matrices
rank
cid:28
reconstruct
crd
log6
show
residual
part
best
rank-r
approximation
cid:107
cid:98
cid:107
c0rλ
cid:107
cid:107
conclusions
table
complexity
measures
quantum
learning
problems
learning
quantum
measurements
learning
quantum
states
pseudo
dimension
uniform
entropy
number
log
fat-shattering
dimension
fatf
rademacher
complexity
sample
complexity
d/2
d/2
max
log
1/δ
/2
log
d/2
log
d/2
log
max
log
log
1/δ
/2
paper
formalise
problems
learning
quantum
measurements
quantum
states
anal-
yse
learnability
solved
sample
complexity
problems
learning
quantum
measurements
quan-
scenario
learning
two-outcome
quantum
measurements
fat-shattering
dimension
tum
states
min
cid:8
cid:0
d/2
cid:1
cid:9
also
showed
fat-shattering
dimension
dual
problem—learning
quantum
states—is
min
cid:8
cid:0
log
d/2
cid:1
cid:9
proof
entirely
based
tools
classical
learning
theory
pro-
vides
alternative
proof
aaronson
result
also
derived
important
complexity
measures
two
tasks
results
summarized
table
results
demonstrated
learning
unknown
measurement
daunting
task
learning
unknown
quantum
state
intuition
since
eﬀect
space
much
larger
state
space
reasonable
fat-shattering
dimension
eﬀect
space
larger
finally
exploiting
general
bloch-sphere
representation
show
learning
problems
equivalent
neural
network
classical
algorithms
applied
learn
unknown
quantum
measurement
state
work
could
provide
new
viewpoint
study
quantum
state
measurement
tomography
also
discuss
connections
quantum
learning
problems
ﬁelds
qip
existence
qra
codes
quantum
state
discrimination
hope
development
results
would
stimulate
theoretical
studies
quantum
statistical
learning
applications
quantum
information
processing
related
areas
discovered
appendix
notation
table
appendix
sample
complexity
terms
complexity
measure
section
2.2
introduce
several
complexity
measures
section
list
well-known
deviation
formula
express
generalisation
error
sample
complexity
terms
complexity
measures
established
set
boolean
functions
ugc
class
i.e
pac
learnable
ﬁnite
dimension
additionally
ﬁnite
dimension
provides
upper
bound
sample
complexity
boolean
function
class
theorem
b.1
vapnik
let
absolute
constant
class
boolean
functions
ﬁnite
dimension
every
provided
log
2/
log
2/δ
therefore
sample
complexity
bounded
b.1
sup
sup
f∈f
cid:40
cid:41
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:98
cid:26
max
log
log
cid:27
following
reasoning
theorem
b.1
analogous
results
drawn
hypothesis
set
ugc
class
ﬁnite
fat-shattering
dimension
every
following
theorem
theorem
b.2
bartlett
absolute
constant
every
consisting
bounded
functions
every
cid:40
cid:12
cid:12
cid:12
cid:98
fatf
/8
log
2/
log
8/δ
cid:26
sup
f∈f
sup
max
fatf
log
cid:41
cid:12
cid:12
cid:12
cid:27
log
provided
b.2
therefore
sample
complexity
bounded
entropy
number
distribution-independent
closely
related
learnability
function
class
dudley
showed
class
consisting
bounded
functions
ugc
class
every
addition
following
theorem
theorem
b.3
polland
let
set
bounded
functions
every
8/2
probability
measure
cid:40
sup
f∈f
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:98
8n1
/8
exp
n2
128
log
lim
n→∞
cid:41
table
summary
notation
mathematical
meaning
separable
hilbert
space
dimension
linear
space
set
real
numbers
positive
integers
linear
space
d-dimensional
complex
vectors
set
self-adjoint
operators
trace
function
conjugate
transpose
b†a
hilbert-schmidt
inner
product
also
stands
conventional
inner
product
set
bounded
operators
set
trace
class
operators
i.e
ﬁnite
trace
zero
operator
identity
operator
cid:23
standard
partial
ordering
schatten
p-norm
reduces
cid:96
norms
cid:107
cid:107
unit
ball
schatten
p-class
unit
vector
quantum
state
i.e
povm
element
i.e
state
space
set
states
eﬀect
space
set
povm
elements
input
space
called
instances
domain
set
output
space
called
labels
domain
set
hypothesis
set
functions
distribution
training
data
set
elements
independently
according
loss
function
probability
expectation
random
variable
cid:96
ensemble
error
vapnik-chervonenkis
dimension
function
class
pseudo
dimension
function
class
fat-shattering
dimension
function
class
level
fat-shattering
dimension
function
class
covering
number
metric
entropy
number
rademacher
complexity
function
class
uniformly
-valued
random
variables
called
rademacher
variables
big
notation
means
positive
constant
cid:46
cid:38
i=1
cid:96
empirical
error
training
data
set
1/n
cid:80
notation
cid:104
cid:105
cid:23
cid:107
cid:107
cid:105
cid:96
vcdim
pdim
fatf
fatf
log
cid:46
cid:39
cid:98
every
cid:40
sup
sup
f∈f
cid:12
cid:12
cid:12
cid:98
cid:12
cid:12
cid:12
cid:41
provided
log
log
2/δ
therefore
sample
complexity
bounded
b.3
theorem
b.4
bartlett
mendelson
probability
least
max
log
cid:26
cid:27
cid:40
sup
f∈f
cid:41
cid:12
cid:12
cid:12
cid:12
cid:12
cid:12
cid:98
cid:26
log
max
cid:27
provided
max
log
1/δ
therefore
sample
complexity
bounded
b.4
vector
c.1
cid:113
d−1
appendix
learning
framework
bloch-sphere
representation
illustrating
state
space
ﬁnite
dimensional
hilbert
space
convenient
adopt
geometric
parameterisation
method
called
bloch-sphere
representation
100–102
provide
another
point
view
quantum
learning
framework
key
idea
represent
quantum
objects
euclidean
space
wherein
classical
techniques
traditional
applied
although
bloch-sphere
representation
method
may
direct
machinery
used
sections
gain
insights
quantum
problems
based
orthogonal
basis
λd2−1
state
represented
bloch
dot
product
corresponds
conventional
euclidean
inner
product
ρrλi
deﬁne
bloch
vector
space
set
bloch
vectors
representative
valid
states
calculate
linear
functional
acting
state
denotes
convex
hull
rank-k
projection
operators
pnρr
riλi
cdr
d2−1
cid:88
i=1
i
cid:115
cid:115
rd2−1
cid:19
cid:18
cid:18
cdr
cdn
cid:19
consequently
aﬃne
functionals
elements
convex
hull
rank-1
projection
operators
i.e
cid:55
order
characterise
quantum
eﬀects
associate
higher
dimensional
projection
operators
useful
consider
algebraic
properties
projection
operators
set
projection
operators
vector
space
corresponds
orthocomplemented
lattice
therefore
sum
two
projections
say
projection
orthogonal
i.e
based
fact
let
pn1
pnd
arbitrary
mutually
orthogonal
rank-one
projections
associate
unit
bloch
vector
pni
veriﬁed
c.1
bloch
vectors
form
-dimensional
regular
simplex
since
angle
two
bloch
vectors
cdni
cardinality
cid:80
cos−1
d−1
slight
abuse
notation
denote
rank-k
projection
summation
arbitrary
diﬀerent
projections
set
pn1
pnd
formally
denote
index
set
pni
adopt
convention
empty
sum
zero
hence
rank-k
projection
acts
state
i∈ik
c.2
c.3
cid:88
i∈ik
cid:88
i∈ik
cid:115
centroid
-face
simplex
∆d−1
subtended
vectors
i∈ik
.the
cid:96
2-norm
calculated
euclidean
distance
center
simplex
∆d−1
centroid
-face
c.4
cid:107
cid:107
intuitively
interpret
value
operator
acting
state
scaled
since
every
quantum
eﬀect
composed
extremal
eﬀects
i.e
projection
operators
eﬀect
space
103
represent
enρr
cid:0
cid:88
c.5
cid:80
k=0
i=0
cid:107
cid:107
maxk∈
0,1
...
cid:1
cid:113
d−k
d−1
utilising
bijection
relationship
quantum
state
corresponding
bloch
vectors
associate
input
space
bloch
vector
space
i.e
denote
function
class
linear
functionals
acting
according
c.2
c.6
rank-0
projection
operator
class
consists
one
element
i.e
see
equation
aﬃne
coeﬃcient
ﬁxed
consists
linear
functionals
class
quantum
eﬀects
c.5
similar
result
conv
cid:55
cid:18
cid:0
cid:19
cid:1
cid:55
rd2−1
cid:113
d−k
d−1
clearly
upper
bounded
cid:107
cid:107
bounded
function
class
consisting
aﬃne
functionals
however
easily
convert
formulation
linear
form
letting
intuition
behind
characterising
learnability
quantum
measurements
need
bound
complexity
measures
class
linear
functionals
appendix
neural
networks
brieﬂy
introduce
theory
neural
networks
readers
may
refer
ref
details
basic
computing
unit
neural
network
simple
perceptron
see
fig
computes
function
input
vector
adjustable
parameters
weights
particular
weight
known
threshold
function
called
activation
function
scenario
binary
classiﬁcation
activation
function
may
chosen
sign
function
case
real-value
outputs
may
satisfy
lipschitz
conditions
note
decision
boundary
binary
perceptrons
aﬃne
subspace
deﬁned
equation
using
simple
perceptron
binary
classiﬁcation
problem
perceptron
learning
algorithm
pca
ﬁnds
adequate
parameters
well
training
data
set
algorithm
starts
arbitrary
initial
parameter
updates
parameter
misclassiﬁed
data
example
function
computes
algorithm
adds
element-wise
ﬁxed
step
constant
pca
iterates
termination
criterion
reached
second
example
two-layer
networks
also
called
single-hidden
layer
nets
see
fig
network
compute
function
form
wkσ
v0i
cid:88
output
weights
v0i
input
weights
positive
integer
number
hidden
units
one
use
gradient
descent
procedure
adjust
parameters
minimize
squared
errors
training
data
i=1
figure
consider
qubit
system
measurement
characterised
simple
perceptron
3-dimensional
input
data
activation
function
node
bias
node
corresponding
bias
weight
input
vector
bloch
vector
output
variable
computed
simple
perceptron
hence
problem
learning
unknown
measurement
infer
simple
perceptron
i.e
values
i=1
input
vector
summation
activation
output
variable
figure
single-hidden
layer
net
computes
3-dimensional
input
data
activation
function
three
hidden
units
correspond
value
v0k
corresponds
bias
weight
k-th
hidden
unit
single-hidden
net
represents
quantum
measurement
input
vector
summation
hidden
layer
summation
output
variable
v02
v03
v04
v33
v34
references
vladimir
vapnik
nature
statistical
learning
theory
springer
1995
vladimir
vapnik
statistical
learning
theory
wiley-interscience
1998
tom
mitchell
machine
learning
mcgraw-hill
1997
valiant
theory
learnable
communications
acm
1134–1142
1984
doi
10.1145/
1968.1972
yaser
abu-mostafa
malik
magdon-ismail
husan-tien
lin
learning
data
short
course
aml-
book.com
2012
michael
nielsen
issac
chuang
quantum
computation
quantum
information
cambridge
university
press
2000
peter
shor
polynomial-time
algorithms
prime
factorization
discrete
logarithms
quantum
com-
puter
siam
review
303–332
1999
doi
10.1137/s0036144598347011
lov
grover
quantum
mechanics
helps
searching
needle
haystack
physical
review
letters
325–328
1997
doi
10.1103/physrevlett.79.325
arxiv
quant-ph/9706033
nilanjana
datta
min-hsiu
hsieh
one-shot
entanglement-assisted
quantum
classical
communication
ieee
transactions
information
theory
1929–1939
2013
doi
10.1109/tit.2012.2228737
arxiv:1105.3321
quant-ph
min-hsiu
hsieh
mark
wilde
trading
classical
communication
quantum
communication
entanglement
quantum
shannon
theory
ieee
transactions
information
theory
4705–4730
2010
doi
10.1109/
tit.2010.2054532
arxiv:0901.3038
quant-ph
min-hsiu
hsieh
mark
wilde
entanglement-assisted
communication
classical
quantum
information
ieee
transactions
information
theory
4682–4704
2010
doi
10.1109/tit.2010.2053903
arxiv:0811
4227
quant-ph
min-hsiu
hsieh
igor
devetak
andreas
winter
entanglement-assisted
capacity
quantum
multiple-access
channels
ieee
transactions
information
theory
3078–3090
2008
doi
10.1109/tit.2008.924726
arxiv
quant-ph/0511228
todd
brun
igor
devetak
min-hsiu
hsieh
correcting
quantum
errors
entanglement
science
314
5798
436–439
2006
doi
10.1126/science.1131563
arxiv
quant-ph/0610092
todd
brun
igor
devetak
min-hsiu
hsieh
catalytic
quantum
error
correction
ieee
transactions
information
theory
3073–3089
2014
doi
10.1109/tit.2014.2313559
arxiv
quant-ph/0608027
mark
wilde
min-hsiu
hsieh
zunaira
babar
entanglement-assisted
quantum
turbo
codes
ieee
transac-
tions
information
theory
1203–1222
2014
doi
10.1109/tit.2013.2292052
arxiv:1010.1256
quant-ph
min-hsiu
hsieh
wen-tai
yen
li-yi
hsu
high
performance
entanglement-assisted
quantum
ldpc
codes
need
little
entanglement
ieee
transactions
information
theory
1761–1769
2011
doi
10.1109/tit.2011
2104590
arxiv:0906.5532
quant-ph
peter
wittek
quantum
machine
learning
quantum
computing
means
data
mining
academic
press
2014
maria
schuld
ilya
sinayskiy
francesco
petruccione
introduction
quantum
machine
learning
contem-
porary
physics
1–14
2014
doi
10.1080/00107514.2014.964942
arxiv:1409.3097
quant-ph
rocco
servedio
steven
gortler
quantum
versus
classical
learnability
proceedings
16th
annual
ieee
conference
computational
complexity
ieee
computer
society
2000
doi
10.1109/ccc.2001.933881
arxiv
quant-ph/0007036
rocco
servedio
separating
quantum
classical
learning
automata
languages
programming
springer
science+business
media
2001
1065–1080
doi
10.1007/3-540-48224-5_86
rocco
servedio
steven
gortler
equivalences
separations
quantum
classical
learnability
siam
journal
computing
1067–1092
2004
doi
10.1137/s0097539704412910
alp
atici
rocco
servedio
improved
bounds
quantum
learning
algorithms
quantum
information
process
355–386
2005
doi
10.1007/s11128-005-0001-2
arxiv
quant-ph/0411140
davide
anguita
al.
quantum
optimization
training
support
vector
machines
neural
networks
5-6
763–770
2003
doi
10.1016/s0893-6080
00087-x
esma
a¨ımeur
gilles
brassard
s´ebastien
gambs
quantum
clustering
algorithms
proceedings
24th
international
conference
machine
learning
icml
acm
press
2007
doi
10.1145/1273496.1273497
kristen
pudenz
daniel
lidar
quantum
adiabatic
machine
learning
quantum
information
process
2027–2070
2012
doi
10.1007/s11128-012-0506-4
arxiv:1109.0325
quant-ph
esma
a¨ımeur
gilles
brassard
s´ebastien
gambs
quantum
speed-up
unsupervised
learning
machine
learn-
ing
261–287
2012
doi
10.1007/s10994-012-5316-5
nathan
wiebe
daniel
braun
seth
lloyd
quantum
algorithm
data
fitting
physical
review
letters
109
050505
050505
2012
doi
10.1103/physrevlett.109.050505
arxiv:1204.5242
quant-ph
seth
lloyd
mohseni
mohseni
patrick
rebentrost
quantum
algorithms
supervised
unsupervised
ma-
chine
learning
2013
arxiv:1307.0411
quant-ph
patrick
rebentrost
masoud
mohseni
seth
lloyd
quantum
support
vector
machine
big
data
classiﬁcation
physical
review
letters
113
130503
130503
2014
doi
10.1103/physrevlett.113.130503
arxiv:1307.0471
quant-ph
seth
lloyd
masoud
mohseni
patrick
rebentrost
quantum
principal
component
analysis
nature
physics
631–633
2014
doi
10.1038/nphys3029
arxiv:1307.0401
quant-ph
nathan
wiebe
ashish
kapoor
krysta
svore
quantum
nearest-neighbor
algorithms
machine
learning
2014
arxiv:1401.2142
quant-ph
guoming
wang
quantum
algorithms
curve
ﬁtting
2014
arxiv:1402.0660
quant-ph
maria
schuld
ilya
sinayskiy
francesco
petruccione
quest
quantum
neural
network
quantum
information
process
2567–2586
2014
doi
10.1007/s11128-014-0809-8
arxiv:1408.7005
quant-ph
seth
lloyd
silvano
garnerone
paolo
zanardi
quantum
algorithms
topological
geometric
analysis
big
data
2014
arxiv:1408.3106
quant-ph
andrew
cross
graeme
smith
john
smolin
quantum
learning
robust
noise
2014
arxiv:1407.5088
quant-ph
maria
schuld
ilya
sinayskiy
francesco
petruccione
quantum
computing
pattern
classiﬁcation
pri-
cai
2014
trends
artiﬁcial
intelligence
springer
science+business
media
2014
208–220
doi
10.1007/978-3-
319-13560-1_17
arxiv:1412.3646
quant-ph
nathan
wiebe
ashish
kapoor
krysta
svore
quantum
deep
learning
2014
arxiv:1412.3489
quant-ph
esma
a¨ımeur
gilles
brassard
s´ebastien
gambs
machine
learning
quantum
world
2006
2006
431–442
doi
10.1007/11766247_37
scott
aaronson
learnability
quantum
states
proceedings
royal
society
mathematical
physical
engineering
sciences
463
2088
3089–3114
2007
doi
10.1098/rspa.2007.0113
arxiv
quant-ph/0608142
s´ebastien
gambs
quantum
classiﬁcation
2008
arxiv:0809.0444
quant-ph
m˘ad˘alin
gut¸˘a
wojciech
kot
cid:32
lowski
quantum
learning
asymptotically
optimal
classiﬁcation
qubit
states
new
journal
physics
123032
2010
doi
10.1088/1367-2630/12/12/123032
arxiv:1004.2468
quant-ph
alessandro
bisio
al.
optimal
quantum
learning
unitary
transformation
physical
review
032324
2010
doi
10.1103/physreva.81.032324
arxiv:0903.0543
quant-ph
alessandro
bisio
al.
quantum
learning
algorithms
quantum
measurements
physics
letters
375
3425–3434
2011
doi
10.1016/j.physleta.2011.08.002
arxiv:1103.0480
quant-ph
david
gross
al.
quantum
state
tomography
via
compressed
sensing
physical
review
letter
105
2010
doi
10.1103/physrevlett.105.150401
arxiv:0909.3304
quant-ph
steven
flammia
al.
quantum
tomography
via
compressed
sensing
error
bounds
sample
complexity
eﬃcient
estimators
new
journal
physics
095022
2012
doi
10.1088/1367-2630/14/9/095022
arxiv:1205
2300
quant-ph
gael
sent´ıs
al.
quantum
learning
without
quantum
memory
scientiﬁc
reports
2012
doi
10.1038/srep00708
arxiv:1208.0663
quant-ph
gael
sent´ıs
mˇadˇalin
gut¸ˇa
gerardo
adesso
quantum
learning
coherent
states
physical
review
031002
2014
arxiv:1410.8700
quant-ph
songfeng
samuel
braunstein
quantum
decision
tree
classiﬁer
quantum
information
process
757–770
2013
doi
10.1007/s11128-013-0687-5
elizabeth
behrman
james
steck
quantum
neural
network
computes
relative
phase
2013
ieee
symposium
swarm
intelligence
sis
ieee
2013
doi
10.1109/sis.2013.6615168
arxiv:1301.2808
quant-ph
mikhail
altaisky
natalia
kaputkina
krylov
quantum
neural
networks
current
status
prospects
development
physics
particles
nuclei
1013–1032
2014
doi
10.1134/s1063779614060033
alex
monras
almut
beige
karoline
wiesner
hidden
quantum
markov
models
non-adaptive
read-out
many-body
states
applied
mathematical
computational
sciences
2010
arxiv:1002.2337
quant-ph
jenniﬁer
barry
daniel
barry
scott
aaronson
quantum
partially
observable
markov
decision
processes
physical
review
2014
doi
10.1103/physreva.90.032311
arxiv:1406.2858
quant-ph
lewis
clark
al.
hidden
quantum
markov
models
open
quantum
systems
instantaneous
feedback
143–151
2015
doi
10.1007/978-3-319-10759-2_16
arxiv:1406.5847
quant-ph
alex
monr´as
andreas
winter
quantum
learning
classical
stochastic
processes
completely-positive
realization
problem
2014
arxiv:1412.3634
quant-ph/
dana
angluin
queries
concept
learning
machine
learning
319–342
1988
doi
10.1007/bf00116828
christoph
d¨urr
peter
høyer
quantum
algorithm
ﬁnding
minimum
1996
arxiv
quant-ph/9607014
vittorio
giovannetti
seth
lloyd
lorenzo
maccone
quantum
random
access
memory
physical
review
letter
100
2008
doi
10.1103/physrevlett.100.160501
arxiv:0708.1879
quant-ph
fran¸coise
lust-piquard
gilles
pisier
non
commutative
khintchine
paley
inequalities
arkiv
f¨or
matematik
1-2
241–260
1991
doi
10.1007/bf02384340
masahito
hayashi
al.
4,1
-quantum
random
access
coding
exist—one
qubit
enough
recover
one
four
bits
new
journal
physics
129–129
2006
doi
10.1088/1367-
2630/8/8/129
arxiv
quant-
ph/0604061
luc
devroye
l´aszl´o
orﬁ
g´abor
lugosi
probabilistic
theory
pattern
recognition
springer
1997
martin
anthony
peter
barlett
neural
network
learning
theoretical
foundations
cambridge
university
press
1999
olivier
bousquet
stephane
boucheron
g´abor
lugosi
introduction
statistical
learning
theory
advanced
lectures
machine
learning
vol
3176
lecture
notes
computer
science
springer
2003
169–207
doi
10.1007/
978-3-540-28650-9_8
shahar
mendelson
geometric
parameters
learning
theory
geometric
aspects
functional
analysis
springer
science+business
media
2004
193–235
doi
10.1007/978-3-540-44489-3_17
trevor
hasite
robert
tibshirani
jerome
friedman
elements
statistical
learning
data
mining
inference
prediction
springer
2011
shai
shalev-shwartz
shai
ben-david
understanding
machine
learning
theory
algorithms
cambridge
university
press
2014
balas
natarajan
occam
razor
functions
proceedings
sixth
annual
conference
computational
learning
theory
colt
acm
1993
370–376
doi
10.1145/168304.168380
peter
bartlett
shahar
mendelson
rademacher
gaussian
complexities
risk
bounds
structural
re-
sults
journal
machine
learning
research
463–482
2003
doi
10.1007/3-540-44581-1_15
david
haussler
decision
theoretic
generalizations
pac
model
neural
net
learning
applications
information
computation
100
78–150
1992
doi
10.1016/0890-5401
90010-d
vladimir
vapnik
principles
risk
minimization
learning
theory
advances
neural
information
processing
systems
nips
1992
831–838
shai
shalev-shwartz
al.
learnability
stability
uniform
convergence
journal
machine
learning
research
2635–2670
2010
silvia
villa
lorenzo
rosasco
tomaso
poggio
learnability
complexity
stability
59–69
2013
doi
10.1007/978-3-642-41136-6_7
arxiv:1303.5976
stat.ml
vladimir
vapnik
alexey
chervonenkis
necessary
suﬃcient
conditions
uniform
convergence
means
expectations
theory
probability
applications
532–553
198
doi
10.1137/1126059
pollard
convergence
stochastic
processes
springer-verlag
new
york/berlin
1984
michael
kearns
robert
schapire
eﬃcient
distribution-free
learning
probabilistic
concepts
389
1990
doi
10.1016/b978-1-55860-146-8.50035-7
andrey
kolmogorov
vladimir
tihomirov
-entropy
-capacity
sets
functional
spaces
american
mathematical
society
translations
277–364
1961
shahar
mendelson
notes
statistical
learning
theory
advanced
lectures
machine
learning
springer
science+business
media
2003
1–40
doi
10.1007/3-540-36434-x_1
richard
dudley
sizes
compact
subsets
hilbert
space
continuity
gaussian
processes
journal
functional
analysis
290–330
1967
doi
10.1016/0022-1236
90017-1
vladimir
sudakov
gaussian
random
processes
measures
solid
angles
hilbert
space
russian
doklady
akademii
nauk
sssr
197
412–415
1971
shahar
mendelson
roman
vershynin
entropy
combinatorial
dimension
inventiones
mathematicae
152
37–55
2003
doi
10.1007/s00222-002-0266-3
teiko
heinosaari
m´ario
ziman
mathematical
language
quantum
theory
uncertainty
entangle-
ment
cambridge
university
press
2012
paul
busch
quantum
states
generalized
observables
simple
proof
gleason
theorem
physical
review
letter
2003
doi
10.1103/physrevlett.91.120403
arxiv
quant-ph/9909073
john
conway
course
operator
theory
graduate
studies
mathematics
book
american
mathematical
society
1999
shahar
mendelson
gideon
schechtman
shattering
dimension
sets
linear
functionals
annals
probability
1746–1770
2004
doi
10.1214/009117904000000388
arxiv
math/0410096
math.pr
eric
carlen
trace
inequalities
quantum
entropy
introductory
course
entropy
quantum
contemporary
mathematics
vol
529
entropy
quantum
american
mathematical
society
providence
2010
73–140
doi
10.1090/conm/529/10428
uﬀe
haagerup
magdalena
musat
best
constants
noncommutative
khintchine-type
inequalities
journal
functional
analysis
250
588–624
2007
doi
10.1016/j.jfa.2007.05.014
arxiv
math/0611160
math.oa
aad
van
der
vaart
jon
wellner
weak
convergence
weak
convergence
empirical
processes
springer
series
statistics
1996
16–28
doi
10.1007/978-1-4757-2545-2_3
richard
dudley
uniform
central
limit
theorems
cambridge
studies
advanced
mathematics
cambridge
university
1996
eric
chitambar
min-hsiu
hsieh
revisiting
optimal
detection
quantum
information
physical
review
020302
2013
doi
10.1103/physreva.88.020302
eric
chitambar
runyao
duan
min-hsiu
hsieh
local
operations
classical
communication
suﬃce
two-qubit
state
discrimination
ieee
transactions
information
theory
1549–1561
2014
doi
10.1109/tit.2013.2295356
arxiv:1308.1737
quant-ph
shengyu
zhang
mingsheng
ying
set
discrimination
quantum
states
physical
review
2002
doi
10.1103/physreva.65.062322
joel
tropp
user-friendly
tail
bounds
sums
random
matrices
foundations
computational
mathematics
389–434
2011
doi
10.1007/s10208-011-9099-z
arxiv:1004.4389
math.pr
andris
ambainis
al.
dense
quantum
coding
quantum
finite
automata
journal
acm
496–511
2002
doi
10.1145/581771.581773
lenoid
gurvits
note
scale-sensitive
dimension
linear
bounded
functionals
banach
spaces
algorithmic
learning
theory
vol
1316
lecture
notes
computer
science
springer
2002
352–363
doi
10.1007/3-
540-63577-7_54
andris
ambainis
al.
quantum
random
access
codes
shared
randomness
2008
arxiv:0810.2937
quant-ph
vladimir
vapnik
estimation
dependences
based
empirical
data
springer-verlag
new
york/berlin
1982
anselm
blumer
al.
learnability
vapnik-chervonenkis
dimension
journal
acm
929–965
1989
doi
10.1145/76359.76371
peter
bartlett
philip
long
robert
williamson
fat-shattering
learnability
real-valued
functions
journal
computer
system
sciences
434–452
1996
doi
10.1006/jcss.1996.0033
noga
alon
al.
scale-sensitive
dimensions
uniform
convergence
learnability
journal
acm
615–631
1997
doi
10.1145/263867.263927
richard
dudley
evarist
gin´e
joel
zinn
uniform
universal
glivenko-cantelli
classes
journal
theo-
100
retical
probability
485–510
1991
doi
10.1007/bf01210321
ingemar
bengtsson
karol
´zyczkowski
geometry
quantum
states
introduction
quantum
entanglement
cambridge
university
press
2008
101
gen
kimura
bloch
vector
-level
systems
physcis
letters
314
339–349
2003
doi
10.1016/s0375-
9601
00941-1
arxiv
quant-ph/0301152
102
gen
kimura
andrzej
kossakowski
bloch-vector
space
-level
systems
spherical-coordinate
point
view
open
systems
information
dynamics
207–229
2005
doi
10.1007/s11080-005-0919-y
arxiv
quant-ph/0408014
103
edward
davies
quantum
theory
open
systems
academic
press
london
1976
