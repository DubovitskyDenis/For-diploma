scale
decentr
mdp
heurist
search
jill
dibangoy
christoph
amato
inria
loria
campu
scientiﬁqu
franc
comput
scienc
laboratori
massachusett
institut
technolog
cambridg
usa
camato
arnaud
doniec
lill
nord
franc
mine
douai
douai
franc
abstract
decentr
partial
observ
markov
cision
process
rich
el
cooper
tainti
often
intract
solv
optim
transit
tion
independ
gener
subclass
shown
complex
optim
algorithm
subclass
still
inefﬁci
practic
paper
ﬁrst
vide
updat
proof
optim
polici
depend
histori
agent
local
observ
present
new
algorithm
base
heurist
search
abl
expand
search
node
use
constraint
mizat
show
experiment
result
ing
approach
mdp
solver
result
show
reduct
comput
time
creas
scalabl
multipl
order
tude
number
benchmark
introduct
substanti
progress
algorithm
tiagent
sequenti
decis
make
repres
traliz
partial
observ
markov
decis
process
algorithm
abl
exploit
domain
structur
present
ticularli
success
unfortun
gener
problem
even
method
solv
moder
size
problem
timal
decentr
markov
decis
process
independ
transit
observ
repres
gener
subclass
complex
rather
nexp
algorithm
ing
subclass
recent
propos
approach
often
solv
much
larger
problem
method
solv
truli
larg
problem
agent
paper
present
novel
algorithm
optim
solv
independ
transit
vation
combin
heurist
search
constraint
mizat
show
one
cast
depend
transit
observ
continu
terminist
mdp
state
probabl
distribut
state
origin
call
state
occup
distribut
allow
adapt
ou
mdp
techniqu
solv
decentr
mdp
follow
insight
design
algorithm
state
occup
explor
perform
similarli
learn
polici
select
cordanc
decentr
pomdp
techniqu
result
approach
abl
leverag
problem
structur
heurist
limit
space
polici
explor
bound
valu
efﬁcient
gener
polici
use
constraint
optim
algorithm
term
markov
polici
search
mp
shown
much
efﬁcient
algorithm
approach
use
independ
transit
observ
remaind
paper
organ
follow
first
provid
motiv
exampl
util
properti
independ
transit
tion
next
describ
framework
discu
relat
work
present
theoret
sult
show
optim
polici
independ
transit
observ
depend
agent
histori
proven
offer
gener
proof
permit
addit
sight
next
describ
decentr
markov
polici
search
algorithm
combin
constraint
optim
heurist
search
efﬁcient
produc
optim
lution
independ
transit
servat
final
present
empir
evalu
algorithm
respect
solver
appli
decentr
mdp
show
abil
action
motiv
real
problem
control
tion
multipl
space
explor
rover
one
use
nasa
explor
surfac
mar
distribut
sensor
net
surveil
sensor
net
main
team
stationari
moveabl
uav
satellit
sensor
must
coordin
track
target
sensor
independ
transit
tion
particularli
suit
model
distribut
domain
applic
aim
ﬁnding
optim
schedul
amount
gener
power
collect
gener
unit
given
demand
oper
constraint
time
horizon
figur
uncertainti
scenario
grid
inspir
seuken
zilberstein
background
relat
work
solv
problem
multipl
order
magnitud
larger
includ
agent
motiv
exampl
illustr
characterist
decentr
partial
observ
markov
decis
process
interest
consid
simpl
uncertainti
domain
figur
scenario
two
agent
want
meet
soon
ble
grid
world
agent
possibl
action
includ
move
north
south
west
east
stay
place
action
given
agent
affect
agent
take
action
agent
sens
inform
case
respond
locat
agent
tial
inform
insufﬁci
determin
global
state
world
mainli
agent
mit
explicitli
commun
local
locat
howev
instantan
commun
allow
agent
partial
tion
togeth
would
reveal
true
state
world
agent
joint
locat
presenc
joint
full
observ
properti
differenti
gener
partial
observ
model
includ
agent
partial
inform
togeth
map
multipl
differ
state
world
sequenc
decis
model
depend
entir
past
histori
action
observ
agent
ever
experienc
tainti
problem
sinc
transit
observ
affect
agent
agent
decis
pend
last
piec
partial
inform
agent
locat
characterist
appear
mani
applic
includ
mar
explor
rover
domain
section
review
decentr
mdp
model
assumpt
transit
observ
denc
associ
notat
relat
work
deﬁnit
decentr
mdp
traliz
mdp
consist
ﬁnite
set
state
denot
set
local
servat
agent
ﬁnite
set
joint
action
set
local
action
agent
transit
function
denot
probabl
transit
state
state
take
joint
action
reward
function
note
reward
receiv
execut
joint
action
state
note
decentr
mdp
distinguish
state
jointli
fulli
observ
properti
sure
global
state
would
known
agent
share
observ
given
step
extern
uncertainti
problem
follow
trivial
deﬁnit
state
observ
agent
parameter
initi
state
tion
agent
oper
bound
number
step
typic
refer
problem
horizon
model
refer
decentr
mdp
solv
decentr
mdp
given
plan
horizon
start
state
distribut
seen
ﬁnding
individu
polici
maxim
expect
cumul
reward
step
problem
addit
assumpt
interest
decentr
mdp
exhibit
two
properti
ﬁrst
transit
independ
tion
local
observ
agent
depend
previou
local
observ
local
action
taken
agent
deﬁnit
transit
independ
assumpt
decentr
mdp
said
transit
independ
transit
function
exist
local
also
implicitli
assum
observ
independ
state
observ
function
agent
depend
dynam
agent
assum
mdp
state
factor
local
observ
becom
transit
independ
preliminari
deﬁnit
notat
goal
solv
ﬁnd
ize
determinist
joint
polici
dividu
polici
sequenc
decis
rule
addit
call
decentr
decis
rule
time
decis
rule
paper
distinguish
tween
markov
decis
rule
decis
rule
time
map
local
histori
local
action
sequenc
cision
rule
deﬁn
polici
contrast
markov
decis
rule
time
map
local
observ
sequenc
markov
decis
rule
deﬁn
markov
polici
moreov
worth
ing
decentr
markov
polici
exponenti
smaller
decentr
one
state
occup
anoth
import
notion
per
state
occup
system
trol
decentr
markov
polici
denot
start
given
moreov
rent
state
occup
depend
past
ize
markov
polici
previou
state
local
action
occup
decentr
markov
decis
rule
follow
denot
sake
simplic
also
denot
state
occup
space
horizon
standard
simplex
distinct
belief
state
state
occup
may
thought
belief
state
enc
formal
belief
state
given
lief
state
inform
agent
state
ical
condit
singl
joint
tori
total
probabl
properti
overal
state
occup
summar
mation
world
state
contain
belief
state
horizon
word
doubli
exponenti
joint
histori
summar
gle
state
occup
make
use
local
mation
relat
work
section
focu
approach
solv
mdp
independ
transit
observ
well
relev
solut
method
thorough
troduct
solut
method
reader
refer
becker
ﬁrst
describ
transit
observ
independ
subclass
solv
optim
approach
call
coverag
set
rithm
consist
three
main
step
first
set
augment
mdp
creat
incorpor
joint
reward
local
reward
function
agent
best
spons
agent
polici
found
ing
augment
mdp
final
joint
polici
highest
valu
agent
best
respons
turn
algorithm
optim
keep
track
complet
set
polici
candid
agent
requir
larg
amount
time
memori
petrik
zilberstein
reformul
coverag
set
gorithm
bilinear
program
therebi
allow
tion
approach
util
bilinear
program
use
anytim
algorithm
provid
onlin
bound
solut
qualiti
iter
represent
also
better
abl
take
advantag
spars
joint
reward
distribut
repres
independ
reward
linear
term
compress
joint
reward
matrix
result
greatli
increas
efﬁcienc
mani
case
agent
reward
often
depend
agent
ear
program
still
inefﬁci
due
lack
reward
sparsiti
gener
approxim
approach
tempt
scale
larger
problem
horizon
gener
full
set
polici
may
optim
approach
known
algorithm
introduc
seuken
zilberstein
success
reﬁn
algorithm
sampl
forward
bound
number
belief
state
back
gener
next
step
polici
one
traliz
polici
belief
state
avoid
explicit
enumer
possibl
polici
mar
zilberstein
perform
backup
solv
correspond
constraint
optim
problem
cop
repres
decentr
backup
although
techniqu
suboptim
ize
backup
appli
exact
set
strate
algorithm
speciﬁc
decentr
backup
build
decentr
polici
maxim
respect
belief
state
polici
avail
agent
associ
cop
given
set
abl
one
local
observ
agent
set
domain
domain
variabl
correspond
agent
set
polici
avail
agent
set
soft
constraint
one
joint
observ
soft
constraint
map
assign
real
valu
intuit
valu
repres
expect
ward
accru
agent
togeth
perceiv
given
joint
observ
follow
given
ize
polici
sinc
decentr
polici
consist
polici
easi
see
maxim
sum
soft
constraint
yield
maxim
decentr
polici
closer
model
framework
aim
model
multiag
teamwork
agent
strong
local
interact
often
binari
tion
reward
model
domain
compos
among
set
agent
tial
bodi
work
extend
gener
niqu
discuss
exploit
local
tion
nair
introduc
mal
algorithm
model
name
gener
optim
algorithm
goa
domain
contain
nari
interact
reason
expect
goa
perform
gener
algorithm
method
use
similar
strategi
select
polici
candid
ever
domain
contain
primarili
binari
tion
gener
agent
reward
depend
mani
agent
goa
like
form
gener
algorithm
worth
note
transit
servat
independ
make
tion
transit
observ
independ
make
differ
assumpt
reward
model
partial
observ
speciﬁc
sume
reward
decompos
sum
local
reward
model
set
agent
reward
model
transit
observ
independ
gener
allow
global
reward
agent
consid
agent
one
set
assum
state
jointli
fulli
observ
state
fulli
determin
combin
local
tion
agent
make
limit
assumpt
model
therefor
make
differ
assumpt
address
complex
choic
model
depend
assumpt
best
match
domain
ing
solv
theoret
properti
section
demonstr
main
theoret
result
paper
optim
polici
decentr
mdp
solver
aim
calcul
optim
decentr
polici
maxim
expect
mul
reward
arg
maxπ
follow
theorem
prof
decentr
markov
polici
yield
optim
perform
decentr
mdp
independ
transit
observ
goldman
establish
optim
markov
polici
agent
assumpt
agent
choos
markov
polici
state
maliti
markov
polici
agent
matter
teammat
polici
also
construct
proof
manner
directli
relat
polici
valu
rather
inform
set
may
clear
reader
theorem
optim
decentr
markov
polici
independ
transit
tion
optim
polici
agent
depend
local
state
agent
histori
local
polici
arg
proof
without
loss
gener
construct
proof
induct
two
agent
agent
tive
ﬁrst
show
last
step
problem
agent
polici
depend
local
histori
step
last
agent
choos
local
action
maxim
valu
base
possibl
local
tori
agent
result
state
system
base
transit
observ
independ
use
decentr
polici
shown
due
space
limit
arg
includ
full
proof
claim
intuit
hold
agent
receiv
inform
agent
local
histori
due
transit
denc
therefor
repres
agent
polici
last
step
longer
depend
tori
therefor
polici
last
step
either
agent
depend
histori
allow
deﬁn
valu
function
last
step
induct
step
show
polici
step
depend
histori
polici
step
also
depend
local
histori
show
agent
perspect
repres
agent
polici
step
valu
function
assum
depend
histori
transit
show
independ
repres
agent
polici
step
arg
arg
longer
depend
local
histori
therefor
polici
either
agent
depend
local
histori
step
problem
establish
sufﬁcient
statist
select
decentr
markov
decis
rule
theorem
sufﬁcient
statist
state
occup
sufﬁcient
statist
decentr
markov
decis
rule
proof
build
upon
proof
optim
traliz
markov
polici
theorem
note
optim
decentr
markov
polici
start
given
arg
maxπ
substitut
plu
sum
pair
yield
arg
maxπ
state
occup
denot
tribut
decentr
markov
polici
produc
horizon
henc
arg
maxπ
state
occup
summar
possibl
joint
histori
decentr
markov
ici
produc
horizon
estim
joint
sion
rule
thu
state
occup
sufﬁcient
tic
decentr
markov
decis
rule
sinc
mate
depend
upon
state
occup
longer
possibl
joint
state
belief
state
belief
state
ﬁcient
select
directli
action
mdp
pomdp
decentr
pomdp
respect
mainli
caus
statist
summar
inform
world
state
singl
agent
perspect
state
occup
instead
summar
inform
world
state
perspect
team
agent
constrain
execut
polici
independ
set
joint
action
select
independ
instead
select
jointli
decentr
markov
decis
rule
optim
criterion
section
present
optim
criterion
base
polici
valu
function
ﬁrst
deﬁn
expect
immedi
reward
tion
given
quantiti
denot
immedi
reward
take
decis
rule
system
state
occup
time
step
let
repres
expect
total
reward
cision
make
horizon
polici
use
system
state
occup
ﬁrst
time
step
space
decentr
markov
polici
expect
total
reward
given
say
decentr
markov
polici
mal
total
reward
criterion
whenev
decentr
markov
polici
follow
bellman
principl
optim
one
separ
problem
ﬁnding
optim
polici
simpler
subproblem
subproblem
sist
ﬁnding
polici
optim
deﬁn
valu
function
υστ
control
ize
markov
polici
follow
υστ
υστ
quantiti
υστ
denot
expect
sum
reward
attain
start
state
occup
take
one
joint
action
accord
take
next
joint
tion
accord
slightli
abus
tion
write
valu
function
control
unknown
decentr
markov
polici
use
denot
space
bound
valu
function
horizon
decentr
markov
decis
rule
deﬁn
linear
transform
lστ
lστ
valu
function
built
valu
function
follow
mand
exhaust
backup
oper
algorithm
exhaust
variant
use
maxστ
lστ
exhaust
variant
set
equat
denot
optim
tion
worth
note
decentr
markov
polici
solut
iti
equat
greedi
respect
valu
function
exhaust
variant
consist
three
major
step
initi
step
line
backup
oper
step
line
updat
step
line
repeat
tion
step
converg
point
decentr
markov
polici
found
markov
polici
search
section
comput
optim
decentr
markov
polici
given
initi
state
occup
plan
horizon
note
state
occup
use
calcul
heurist
algorithm
ﬁnal
choic
step
depend
state
cie
result
nonstationari
polici
agent
map
local
observ
action
step
cast
decentr
mdp
continu
determinist
mdp
state
state
occup
tribut
action
decentr
markov
polici
deﬁn
transit
ping
denot
reward
function
techniqu
appli
continu
determinist
mdp
also
appli
decentr
mdp
independ
transit
observ
sake
efﬁcienc
focu
optim
techniqu
exploit
initi
inform
learn
algorithm
use
solv
determinist
mdp
approach
updat
state
agent
actual
visit
plan
stage
therefor
suitabl
continu
state
space
algorithm
name
markov
polici
search
mp
trate
adapt
algorithm
solv
decentr
mdp
independ
transit
servat
mp
algorithm
reli
lower
upper
bound
exact
valu
function
ning
horizon
use
follow
deﬁnit
function
denot
reward
accru
take
decis
rule
state
occup
follow
polici
deﬁn
valu
function
remain
plan
horizon
denot
set
store
decentr
markov
decis
rule
state
occup
thu
repres
valu
state
occup
formal
lστ
next
describ
two
variant
mp
algorithm
exhaust
variant
replac
state
state
occup
tribut
action
decentr
markov
decis
rule
algorithm
second
variant
us
constraint
optim
program
instead
memori
algorithm
mp
algorithm
begin
initi
bound
begin
σgreedi
arg
maxστ
updat
upper
bound
valu
function
σgreedi
updat
lower
bound
valu
function
initi
initi
lower
bound
valu
function
decentr
markov
ici
randomli
gener
polici
πrand
σrand
υσrand
σrand
initi
upper
bound
valu
tion
underli
mdp
πmdp
σmdp
υσmdp
σmdp
exhaust
backup
oper
choos
ize
markov
decis
rule
σgreedi
yield
est
valu
explicit
enumer
possibl
decentr
markov
decis
rule
ﬁrst
store
decentr
markov
decis
rule
visit
state
occup
togeth
correspond
ue
henc
greedi
decentr
markov
decis
rule
σgreedi
arg
maxστ
state
cupanc
updat
lower
upper
bound
updat
lower
bound
valu
function
base
decentr
markov
cie
πgreedi
σgreedi
select
trial
πgreedi
yield
valu
higher
rent
lower
bound
υπgreedi
set
υσgreedi
σgreedi
otherwis
leav
lower
bound
unchang
updat
upper
bound
valu
function
base
decentr
markov
sion
rule
σgreedi
valu
function
follow
lσgreedi
theoret
guarante
exhaust
variant
mp
yield
advantag
drawback
one
hand
inherit
theoret
guarante
gorithm
particular
termin
decentr
markov
polici
within
mal
decentr
markov
polici
inde
upper
bound
valu
function
never
underestim
exact
valu
state
occup
updat
upper
bound
valu
state
occup
base
upon
greedi
decis
rule
state
occup
hand
exhaust
variant
algorithm
requir
tive
enumer
possibl
decentr
markov
cision
rule
backup
step
algorithm
line
mdp
techniqu
exhaust
enumer
hibit
sinc
action
space
often
manag
central
mdp
plan
howev
space
central
markov
decis
rule
increas
exponenti
increas
observ
agent
haustiv
variant
scale
problem
moder
number
observ
local
state
two
agent
constraint
optim
formul
overcom
memori
limit
exhaust
ant
use
constraint
optim
instead
tive
backup
oper
precis
constraint
timiz
program
return
greedi
decentr
markov
decis
rule
σgreedi
state
occup
visit
without
perform
exhaust
enumer
constraint
optim
formul
variabl
associ
decis
rule
agent
local
observ
action
space
main
variabl
state
associ
singl
soft
constraint
assign
valu
υσmdp
σmdp
joint
action
valu
denot
reward
cru
horizon
take
joint
action
state
follow
underli
mdp
joint
polici
remain
plan
horizon
decentr
markov
decis
rule
also
associ
singl
soft
constraint
assign
valu
lστ
υσmdp
σmdp
object
straint
optim
model
ﬁnd
assign
σgreedi
σgreedi
arg
maxστ
action
variabl
aggreg
valu
maxim
state
formal
wish
ﬁnd
better
understand
constraint
optim
program
use
instead
note
deﬁnit
map
henc
get
σgreedi
arg
maxστ
thu
constraint
optim
program
return
decentr
markov
sion
rule
highest
valu
techniqu
solv
constraint
optim
formul
abound
literatur
constraint
program
allow
mani
differ
approach
util
theoret
guarante
constraint
optim
ant
yield
guarante
exhaust
variant
without
major
drawback
exhaust
enumer
decentr
markov
decis
rule
instead
us
constraint
optim
formul
return
greedi
decentr
markov
decis
rule
often
much
efﬁcient
exhaust
enumer
henc
retain
properti
stop
algorithm
time
solut
within
optim
decentr
markov
polici
comparison
cop
base
algorithm
rich
bodi
work
replac
exhaust
backup
oper
constraint
optim
formul
decentr
control
set
constraint
tion
program
comput
decentr
polici
given
belief
state
mp
also
take
vantag
constraint
optim
formul
remain
fundament
differ
differ
lie
cop
formul
heurist
search
exist
cop
base
algorithm
decentr
control
author
tri
ﬁnd
best
assign
histori
stead
case
cop
formul
aim
map
local
observ
local
action
provid
erabl
memori
time
save
moreov
exist
rithm
proceed
back
polici
backward
rection
last
step
ﬁrst
use
set
belief
state
contrast
mp
algorithm
proce
ward
expand
state
occup
distribut
lect
greedili
decis
rule
final
mp
algorithm
return
optim
solut
wherea
cop
base
gorithm
return
local
optim
lution
approxim
solut
return
algorithm
plan
central
lief
state
constitut
sufﬁcient
statist
empir
evalu
evalu
algorithm
use
sever
benchmark
decentr
mdp
literatur
benchmark
compar
algorithm
algorithm
solv
note
compar
method
sinc
benchmark
allow
agent
interact
teammat
time
reason
expect
optim
pomdp
method
goa
outperform
algorithm
present
report
benchmark
optim
valu
togeth
run
time
second
differ
plan
horizon
mp
variant
run
mac
osx
machin
intel
ram
avail
solv
constraint
optim
problem
use
aolib
bilinear
program
approach
list
aolib
librari
avail
follow
websit
mp
ice
ipg
blp
exh
recycl
robot
meet
grid
cop
ice
ipg
blp
cop
meet
grid
navig
mit
ipg
ice
blp
cop
navig
isr
navig
pentagon
tabl
experiment
result
cop
exh
variant
mp
well
label
ice
ipg
blp
blp
run
intel
mac
ram
time
limit
hour
use
best
avail
version
bilinear
program
approach
iter
best
respons
version
standard
ramet
gener
solut
method
perform
well
special
approach
expect
result
differ
singl
order
magnitud
compar
coverag
set
algorithm
bilinear
program
method
shown
efﬁcient
avail
test
problem
provid
valu
exhaust
variant
exh
small
problem
constraint
optim
formul
cop
problem
test
algorithm
six
mark
recycl
robot
navig
largest
est
benchmark
could
ﬁnd
literatur
pare
algorithm
ipg
blp
heurist
search
consist
perform
gener
exact
solver
ipg
algorithm
competit
altern
approach
perform
well
problem
duce
reachabl
result
vide
matthij
spaan
conduct
differ
machin
similarli
result
ipg
lect
differ
machin
result
time
result
ipg
directli
compar
method
like
differ
small
constant
factor
would
obtain
test
machin
result
seen
tabl
benchmark
cop
variant
mp
outperform
algorithm
result
show
cop
variant
produc
http
http
problem
deﬁnit
avail
follow
websit
mal
polici
much
le
time
test
benchmark
exampl
meet
grid
problem
cop
variant
comput
optim
polici
approxim
time
faster
blp
ipg
algorithm
respect
also
note
cop
variant
use
medium
larg
domain
exampl
larg
domain
exh
variant
ran
memori
cop
variant
pute
optim
solut
horizon
yet
exh
variant
comput
optim
solut
small
problem
faster
cop
variant
instanc
recycl
robot
horizon
exh
variant
comput
optim
solut
time
faster
cop
variant
mp
algorithm
due
overhead
constraint
optim
formul
lack
ture
util
mani
differ
reason
result
mp
algorithm
outperform
ipg
mainli
perform
polici
search
space
traliz
polici
instead
mp
rithm
perform
polici
search
space
ize
markov
polici
exponenti
smaller
decentr
polici
mp
outperform
blp
algorithm
mainli
dimens
solut
represent
speciﬁc
number
bilinear
term
blp
approach
grow
polynomi
horizon
problem
caus
perform
well
larg
problem
larg
horizon
tightli
coupl
reward
valu
continu
evalu
mp
algorithm
domli
gener
instanc
multipl
agent
dom
instanc
built
upon
recycl
robot
problem
describ
sutton
barto
given
model
associ
singl
agent
choos
number
interact
event
interact
event
pair
joint
state
action
reward
emax
randomli
chosen
structur
tie
agent
togeth
sinc
reward
model
decompos
among
group
agent
effort
provid
insight
degre
interact
among
agent
distinguish
tween
four
class
depend
number
interact
event
class
randomli
choos
emax
emax
denot
number
joint
state
action
pair
depict
figur
constraint
formul
allow
deal
larger
number
agent
calcul
timal
valu
function
instanc
class
report
averag
comput
time
cop
ant
abl
scale
agent
horizon
second
could
also
produc
result
agent
second
use
power
machin
also
seen
increas
number
interact
event
problem
substanti
increas
amount
time
requir
solv
lem
show
even
dens
reward
matrix
approach
continu
perform
well
despit
high
run
time
mp
ﬁrst
gener
algorithm
scale
team
two
agent
without
take
tage
local
interact
exampl
blp
algorithm
current
stand
solv
problem
moreov
techniqu
exploit
small
number
local
interact
among
agent
scale
multipl
agent
problem
agent
interact
oret
result
also
describ
novel
algorithm
combin
heurist
search
constraint
optim
efﬁcient
produc
optim
solut
class
problem
new
algorithm
term
learn
markov
polici
mp
shown
scale
larg
problem
plan
horizon
reduc
comput
time
tipl
order
magnitud
previou
approach
also
abl
demonstr
scalabl
respect
number
agent
domain
agent
result
show
approach
could
appli
mani
larg
realist
domain
futur
plan
explor
extend
mp
gorithm
class
problem
larger
team
agent
instanc
may
abl
produc
mal
solut
gener
class
vide
approxim
result
extend
idea
occup
distribut
problem
furthermor
scalabl
approach
larger
ber
agent
encourag
pursu
method
increas
even
particular
think
proach
could
help
increas
number
agent
act
conjunct
structur
model
local
interact
spars
joint
reward
matrix
bilinear
program
approach
acknowledg
would
like
thank
fran
oliehoek
mou
review
help
comment
initi
version
paper
well
matthij
spaan
marek
petrik
provid
algorithm
result
code
tive
research
support
part
afosr
muri
project
refer
amato
dibangoy
zilberstein
crement
polici
gener
intern
pomdp
confer
autom
plan
schedul
page
thessaloniki
greec
proceed
figur
mp
perform
increas
number
agent
plan
horizon
random
stanc
recycl
robot
scenario
conclus
futur
work
ara
dutech
investig
matic
program
ﬁnite
horizon
decentr
pomdp
journal
artiﬁci
intellig
research
astrom
optim
control
markov
journal
ce
incomplet
state
inform
mathemat
analysi
applic
paper
explor
new
theori
algorithm
ing
independ
transit
observ
provid
new
proof
optim
polici
depend
agent
histori
subclass
gener
previou
barto
bradtk
singh
yee
gullap
pinett
learn
act
use
dynam
program
artiﬁci
genc
cpu
time
sec
class
scalabl
agent
agent
agent
agent
agent
agent
nair
tamb
yokoo
pynadath
marsella
tame
decentr
pomdp
toward
efﬁcient
polici
comput
multiag
proceed
intern
joint
set
confer
artiﬁci
intellig
page
nair
varakantham
tamb
yokoo
network
distribut
pomdp
synthesi
tribut
constraint
optim
pomdp
proceed
aaai
confer
artiﬁci
tellig
page
oliehoek
spaan
vlassi
timal
approxim
function
traliz
pomdp
journal
artiﬁci
intellig
research
petrik
zilberstein
anytim
coordin
use
separ
bilinear
program
proceed
aaai
confer
artiﬁci
intellig
page
petrik
zilberstein
bilinear
program
approach
multiag
plan
journal
cial
intellig
research
putterman
markov
decis
process
crete
stochast
dynam
program
john
wiley
son
new
york
seuken
zilberstein
formal
model
gorithm
decentr
decis
make
certainti
journal
autonom
agent
agent
system
spaan
oliehoek
amato
ing
optim
heurist
search
via
increment
expans
proceed
nation
joint
confer
artiﬁci
intellig
page
sutton
barto
reinforc
learn
introduct
mit
press
cambridg
zilberstein
washington
bernstein
mouaddib
control
etari
rover
advanc
control
robot
page
london
becker
zilberstein
lesser
goldman
solv
transit
independ
ize
markov
decis
process
journal
artiﬁci
intellig
research
bellman
dynam
program
princeton
univers
press
bernstein
amato
hansen
berstein
polici
iter
decentr
control
markov
decis
process
journal
artiﬁci
tellig
research
bernstein
givan
immerman
berstein
complex
decentr
control
markov
decis
process
mathemat
tion
research
dechter
constraint
optim
constraint
process
page
morgan
kaufmann
san
francisco
dibangoy
mouaddib
increment
prune
heurist
proceed
ing
intern
joint
confer
autonom
agent
multiag
system
budapest
hungari
gallag
discret
stochast
process
kluwer
academ
publish
boston
goldman
zilberstein
decentr
trol
cooper
system
categor
plexiti
analysi
journal
artiﬁci
intellig
search
korf
heurist
search
artiﬁci
tellig
kumar
zilberstein
namic
program
decentr
pomdp
structur
interact
proceed
tional
confer
autonom
agent
agent
system
page
kumar
zilberstein
backup
decentr
pomdp
complex
new
rithm
proceed
intern
enc
autonom
agent
multiag
system
page
toronto
canada
kumar
zilberstein
toussaint
scalabl
multiag
plan
use
probabilist
infer
proceed
intern
joint
confer
artiﬁci
intellig
page
lesser
ortiz
tamb
editor
tribut
sensor
network
multiag
perspect
volum
kluwer
academ
publish
may
