probabilist
reason
via
deep
learn
neural
associ
model
quan
hui
andrew
xiaodan
zhu
nation
engin
laboratori
speech
languag
inform
process
depart
electr
engin
comput
scienc
york
univers
canada
univers
scienc
technolog
china
hefei
anhui
china
nation
research
council
canada
ottawa
canada
iflytek
research
hefei
china
email
quanliu
zhling
xiaodan
siwei
yuhu
abstract
paper
propos
new
deep
learn
approach
call
neural
associ
model
nam
probabilist
sone
artiﬁci
intellig
propos
use
neural
work
model
associ
two
event
main
neural
network
take
one
event
input
comput
condit
probabl
event
model
like
two
event
associ
actual
mean
condit
probabl
vari
applic
depend
model
train
work
two
case
studi
investig
two
nam
structur
name
deep
neural
network
dnn
neural
net
rmnn
sever
probabilist
reason
task
includ
recogn
textual
entail
tripl
cation
knowledg
base
commonsens
reason
experiment
result
sever
popular
dataset
deriv
wordnet
freebas
conceptnet
demonstr
dnn
rmnn
perform
equal
well
signiﬁcantli
outperform
convent
method
avail
reason
task
moreov
pare
dnn
rmnn
superior
knowledg
fer
model
quickli
extend
unseen
relat
observ
train
sampl
prove
effect
propos
model
work
appli
nam
solv
challeng
winograd
schema
problem
experi
conduct
set
problem
prove
propos
model
potenti
commonsens
reason
introduct
reason
import
topic
artiﬁci
intellig
attract
consider
attent
search
effort
past
decad
mccarthi
minski
mueller
besid
tradit
logic
reason
probabilist
reason
studi
typic
genr
order
handl
knowledg
tainti
reason
base
probabl
theori
pearl
neapolitan
probabilist
reason
use
predict
condit
probabl
one
event
given
anoth
event
method
bilist
reason
includ
bayesian
network
jensen
markov
logic
network
richardson
domingo
graphic
model
koller
friedman
take
bayesian
network
exampl
condit
copyright
probabl
two
associ
event
calcul
posterior
probabl
accord
bay
theorem
possibl
event
model
graph
ture
howev
method
quickli
becom
intract
practic
task
number
possibl
event
usual
larg
recent
year
distribut
represent
map
crete
languag
unit
continu
vector
space
gain
signiﬁc
popular
along
develop
neural
network
bengio
collobert
mikolov
main
beneﬁt
embed
tinuou
space
smooth
properti
help
ture
semant
related
discret
event
tialli
generaliz
unseen
event
similar
idea
knowledg
graph
embed
propos
sent
knowledg
base
continu
space
bord
socher
wang
nickel
use
smooth
tation
possibl
reason
relat
among
iou
entiti
howev
reason
remain
extrem
challeng
problem
partial
requir
effect
encod
world
knowledg
use
power
model
exist
kb
quit
spars
even
recent
creat
kb
yago
nell
freebas
captur
fraction
world
knowledg
order
take
advantag
spars
knowledg
base
approach
knowledg
graph
ding
usual
adopt
simpl
linear
model
rescal
nickel
tresp
kriegel
trans
bord
neural
tensor
network
socher
bowman
although
deep
learn
techniqu
achiev
great
gress
mani
domain
speech
imag
lecun
bengio
hinton
progress
commonsens
reason
seem
slow
paper
propos
use
deep
neural
network
call
neural
associ
model
nam
commonsens
reason
differ
ist
linear
model
propos
nam
model
us
layer
nonlinear
activ
deep
neural
net
model
associ
condit
probabl
two
sibl
event
propos
nam
framework
symbol
event
repres
continu
space
need
explicitli
specifi
depend
structur
among
event
requir
bayesian
network
deep
neural
network
use
model
associ
tween
two
event
take
one
event
input
comput
condit
probabl
anoth
event
comput
dition
probabl
associ
may
gener
model
variou
reason
problem
entail
enc
relat
learn
causat
model
work
studi
two
model
structur
nam
ﬁrst
model
standard
deep
neural
network
dnn
second
model
us
special
structur
call
relat
ulat
neural
net
rmnn
experi
sever
bilist
reason
task
includ
recogn
textual
ment
tripl
classiﬁc
kb
monsens
reason
demonstr
dnn
rmnn
outperform
convent
method
rmnn
model
shown
effect
edg
transfer
learn
model
quickli
extend
new
relat
observ
train
sampl
furthermor
also
appli
propos
nam
model
challeng
commonsens
reason
problem
recent
propos
winograd
schema
levesqu
davi
morgenstern
problem
view
altern
ture
test
ture
support
model
train
nam
propos
forward
method
collect
associ
pair
larg
unstructur
text
pair
extract
procedur
start
construct
vocabulari
thousand
common
verb
adject
base
extract
pair
per
extend
nam
model
solv
winograd
schema
problem
achiev
accuraci
set
effect
exampl
undoubtedli
realiz
commonsens
sone
still
much
work
done
mani
problem
solv
detail
discuss
would
given
end
paper
motiv
associ
event
paper
aim
model
associ
relationship
tween
event
use
neural
network
method
make
clear
main
work
ﬁrst
describ
characterist
event
possibl
associ
relationship
tween
event
base
analysi
event
associ
present
motiv
propos
neural
associ
model
commonsens
reason
main
tic
event
follow
massiv
natur
situat
number
event
massiv
mean
associ
space
model
larg
spars
event
occur
diali
life
spars
challeng
task
ideal
captur
similar
differ
event
time
associ
event
appear
erywher
consid
singl
event
play
basketbal
ampl
shown
figur
singl
event
would
ciat
mani
event
person
play
basketbal
would
win
game
meanwhil
would
injur
case
person
could
make
money
play
basketbal
well
moreov
know
person
play
ketbal
coach
regular
game
typic
associ
event
howev
need
recogn
task
model
event
associ
ident
perform
classiﬁc
classiﬁc
typic
map
event
featur
space
one
ﬁnite
categori
class
event
associ
need
comput
associ
probabl
two
arbitrari
event
may
sampl
sibli
inﬁnit
set
map
relationship
event
ation
would
play
ball
could
support
make
money
someon
make
stock
trade
could
make
money
well
speciﬁc
associ
relationship
event
includ
effect
spatial
tempor
paper
treat
gener
relat
consid
spars
use
kb
figur
exampl
associ
event
paper
believ
model
associ
relationship
event
fundament
work
monsens
reason
could
model
event
tion
well
may
abil
solv
mani
monsens
reason
problem
consid
main
acterist
discret
event
event
associ
two
son
given
describ
motiv
advantag
distribut
represent
method
resent
discret
event
continu
vector
space
vide
good
way
captur
similar
crete
event
advantag
neural
network
method
neural
work
could
perform
univers
approxim
ear
model
easili
hornik
stinchcomb
white
time
paper
take
account
distribut
represent
neural
network
method
artiﬁci
intellig
research
mine
larg
size
use
data
knowledg
model
learn
alway
challeng
follow
section
paper
present
preliminari
work
data
collect
respond
experi
made
solv
sens
reason
problem
play
basketballwininjuredmak
moneyb
coacheddrink
waterstock
trade
neural
associ
model
nam
paper
propos
use
nonlinear
model
name
neural
associ
model
probabilist
reason
main
goal
use
neural
net
model
associ
probabl
two
event
domain
condit
possibl
event
domain
project
continu
space
without
specifi
explicit
depend
structur
among
follow
ﬁrst
introduc
neural
associ
el
nam
gener
model
framework
tic
reason
next
describ
two
particular
nam
ture
model
typic
data
nam
gener
figur
nam
framework
gener
figur
show
gener
framework
nam
ciat
two
event
gener
nam
work
event
ﬁrst
project
continu
space
deep
neural
network
nonlinear
use
model
like
two
event
associ
neural
network
take
embed
one
event
anteced
input
comput
tional
probabl
event
quent
event
binari
true
fals
nam
model
may
use
sigmoid
node
comput
take
multipl
mutual
exclus
valu
use
softmax
node
may
need
use
multipl
embed
one
per
valu
nam
explicitli
specifi
differ
event
actual
relat
may
mutual
exclus
contain
sect
nam
use
separ
comput
tional
probabl
pair
event
task
actual
physic
mean
tional
probabl
vari
applic
depend
model
train
tabl
list
possibl
applic
applic
languag
model
causal
reason
lexic
entail
textual
entail
knowledg
tripl
classiﬁc
caus
effect
tabl
applic
nam
languag
model
anteced
event
sentat
histor
context
consequ
event
next
word
take
one
valu
causal
sone
repres
caus
effect
respect
exampl
eat
cheesi
cake
happi
indic
like
may
caus
binari
true
fals
event
model
may
add
node
model
differ
fect
grow
fat
moreov
may
add
softmax
node
model
event
happi
scale
similarli
knowledg
tripl
classiﬁc
data
given
one
tripl
consist
head
entiti
ject
relat
predic
binari
event
indic
whether
tail
entiti
object
true
fals
final
applic
recogn
lexic
textual
entail
may
deﬁn
premis
pothesi
gener
nam
use
model
inﬁnit
number
event
point
tinuou
space
repres
possibl
event
work
simplic
consid
nam
ﬁnite
number
binari
event
formul
easili
extend
gener
case
compar
tradit
method
like
bayesian
work
nam
employ
neural
net
univers
tor
directli
model
individu
pairwis
event
associ
probabl
without
reli
explicit
depend
ture
therefor
nam
learn
pure
train
sampl
without
strong
human
prior
edg
potenti
scalabl
task
learn
nam
assum
set
observ
exampl
event
pair
note
train
set
normal
includ
itiv
neg
sampl
denot
posit
ple
true
neg
sampl
als
independ
assumpt
statist
relat
learn
srl
getoor
nickel
log
likelihood
function
nam
model
express
follow
denot
logist
score
function
deriv
nam
numer
comput
condit
probabl
detail
given
later
paper
stochast
gradient
descent
sgd
method
may
use
maxim
hood
function
lead
maximum
likelihood
estim
mle
nam
follow
two
case
studi
consid
two
nam
structur
ﬁnite
number
output
node
model
pair
event
ﬁnite
number
binari
ﬁrst
model
typic
dnn
associ
anteced
event
put
consequ
event
output
present
model
structur
call
neural
net
suitabl
data
vector
spaceev
spaceev
neural
networksassoci
dnnspr
dnn
nam
ﬁrst
nam
structur
tradit
dnn
shown
figur
use
data
tration
given
tripl
spond
label
true
fals
cast
comput
follow
figur
dnn
structur
nam
firstli
repres
head
entiti
phrase
tail
titi
phrase
two
embed
vector
similarli
relat
also
repres
vector
call
relat
code
hereaft
secondli
combin
embed
head
entiti
relat
feed
layer
dnn
input
dnn
consist
rectiﬁ
linear
relu
hidden
layer
nair
hinton
input
feedforward
process
max
repres
weight
matrix
bia
layer
respect
final
propos
calcul
sigmoid
score
tripl
associ
probabl
use
last
hidden
layer
output
tail
entiti
vector
sigmoid
function
network
paramet
nam
structur
sent
may
jointli
learn
maxim
likelihood
function
neural
network
rmnn
particularli
data
follow
idea
xue
propos
use
modul
neural
net
rmnn
shown
figur
rmnn
us
oper
dnn
project
entiti
relat
continu
space
shown
figur
connect
speciﬁc
relat
code
hidden
layer
network
figur
neural
network
rmnn
shown
later
structur
superior
knowledg
fer
learn
task
therefor
layer
rmnn
stead
use
linear
activ
signal
comput
previou
layer
relat
code
follow
repres
normal
weight
matrix
weight
matrix
layer
topmost
layer
calcul
ﬁnal
score
tripl
use
relat
code
way
rmnn
paramet
includ
jointli
learn
base
maximum
likelihood
estim
rmnn
model
particularli
suitabl
edg
transfer
learn
model
quickli
extend
new
relat
observ
sampl
relat
case
may
estim
new
relat
code
base
avail
new
sampl
keep
whole
network
unchang
due
small
size
new
relat
code
reliabl
estim
small
number
new
sampl
furthermor
model
manc
origin
relat
affect
sinc
model
origin
relat
code
chang
transfer
learn
experi
section
evalu
propos
nam
model
variou
reason
task
ﬁrst
describ
experiment
setup
report
result
sever
reason
task
includ
textual
entail
recognit
tripl
siﬁcat
kb
commonsens
reason
knowledg
transfer
learn
experiment
setup
ﬁrst
introduc
common
experiment
ting
use
experi
entiti
sentenc
resent
repres
compos
code
head
entiti
vectortail
entiti
vectorfscor
functionw
relationhead
entiti
vectorfw
tail
entiti
vectorsv
head
head
relationshead
entiti
vectorfw
tail
entiti
vectorsv
head
head
transferingvector
spaceev
spaceev
neural
networksassoci
dnnsp
relat
vectorhead
entiti
vectortail
entiti
vectorfassoci
herew
head
entiti
vectortail
entiti
vectorfassoci
herew
relat
code
head
entiti
vectortail
entiti
vectorfscor
functionw
relationhead
entiti
vectorfw
tail
entiti
vectorsv
head
head
relationshead
entiti
vectorfw
tail
entiti
vectorsv
head
head
transferingvector
spaceev
spaceev
neural
networksassoci
dnnsp
relat
vectorhead
entiti
vectortail
entiti
vectorfassoci
herew
head
entiti
vectortail
entiti
vectorfassoci
herew
relat
vector
word
vector
socher
word
vector
initi
mikolov
word
embed
model
train
larg
english
wikipedia
corpu
dimens
word
embed
set
experi
dimens
relat
code
set
relat
code
domli
initi
network
structur
use
relu
nonlinear
activ
function
network
eter
initi
accord
glorot
bengio
meanwhil
sinc
number
train
exampl
probabilist
reason
task
rel
small
adopt
dropout
approach
hinton
ing
process
avoid
problem
learn
process
nam
need
use
neg
ple
automat
gener
randomli
turb
posit
tripl
task
use
provid
develop
set
tune
best
train
hyperparamet
exampl
test
number
hidden
layer
among
initi
learn
rate
among
dropout
rate
among
final
lect
best
set
base
perform
opment
set
ﬁnal
model
structur
us
hidden
layer
learn
rate
dropout
rate
set
respect
experi
model
train
learn
rate
halv
perform
develop
set
decreas
dnn
rmnn
train
use
stochast
gradient
descend
sgd
gorithm
notic
nam
model
converg
quickli
epoch
recogn
textual
entail
understand
entail
contradict
fundament
languag
understand
conduct
experi
popular
recogn
textual
entail
rte
task
aim
recogn
entail
relationship
pair
english
sentenc
experi
use
snli
dataset
bowman
conduct
rte
periment
entail
contradict
instanc
label
entail
convert
tion
experi
snli
dataset
contain
hundr
thousand
train
exampl
use
ing
nam
model
sinc
data
set
includ
relat
data
investig
dnn
structur
task
ﬁnal
nam
result
along
baselin
formanc
provid
bowman
list
ble
model
edit
distanc
bowman
classiﬁ
bowman
lexic
resourc
bowman
dnn
accuraci
tabl
experiment
result
rte
task
result
see
propos
dnn
base
nam
model
achiev
consider
improv
iou
tradit
method
indic
better
model
entail
relationship
natur
languag
sent
sentenc
continu
space
conduct
abilist
reason
deep
neural
network
tripl
classiﬁc
kb
section
evalu
propos
nam
model
two
popular
knowledg
tripl
classiﬁc
dataset
name
socher
deriv
net
freebas
predict
whether
new
tripl
tion
hold
base
train
fact
databas
dataset
contain
uniqu
entiti
involv
differ
relat
total
dataset
cover
relat
entiti
tabl
summar
tic
two
dataset
dataset
ent
train
dev
test
tabl
statist
kb
tripl
classiﬁc
dataset
number
relat
ent
size
entiti
set
goal
knowledg
tripl
classiﬁc
predict
whether
given
tripl
correct
ﬁrst
use
train
data
learn
nam
model
ward
use
develop
set
tune
global
threshold
make
binari
decis
tripl
classiﬁ
true
otherwis
fals
ﬁnal
accuraci
calcul
base
mani
triplet
test
set
classiﬁ
correctli
experiment
result
dataset
given
tabl
compar
two
nam
el
method
report
two
dataset
result
clearli
show
nam
method
dnn
rmnn
achiev
compar
perform
tripl
classiﬁc
task
yield
consist
improv
exist
method
particular
rmnn
model
yield
absolut
improv
ular
neural
tensor
network
ntn
socher
respect
dnn
rmnn
el
much
smaller
ntn
number
paramet
scale
well
number
relat
type
increas
exampl
dnn
rmnn
model
million
paramet
ntn
million
although
rescal
trans
model
million
paramet
size
go
quickli
task
thousand
relat
type
addit
train
time
dnn
rmnn
much
shorter
ntn
trans
sinc
model
verg
much
faster
exampl
obtain
least
time
speedup
ntn
commonsens
reason
similar
tripl
classiﬁc
task
socher
work
use
conceptnet
liu
singh
construct
new
commonsens
data
set
name
model
sme
bord
trans
bord
transh
wang
transr
lin
ntn
socher
dnn
rmnn
avg
tabl
tripl
classiﬁc
accuraci
hereaft
build
ﬁrst
select
fact
conceptnet
relat
typic
commonsens
lation
usedfor
capableof
see
figur
relat
randomli
divid
extract
fact
three
set
train
dev
test
final
order
creat
test
set
classiﬁc
randomli
switch
entiti
whole
vocabulari
correct
tripl
get
total
test
tripl
half
posit
sampl
half
tive
exampl
statist
given
tabl
dataset
ent
train
dev
test
tabl
statist
dataset
dataset
design
answer
sens
question
like
camel
capabl
journey
across
desert
propos
nam
model
answer
question
calcul
associ
probabl
camel
capabl
journey
across
desert
paper
compar
two
nam
method
ular
ntn
method
socher
data
set
overal
result
given
tabl
see
nam
method
outperform
ntn
task
dnn
rmnn
model
obtain
similar
perform
model
ntn
dnn
rmnn
posit
neg
total
tabl
accuraci
comparison
furthermor
show
classiﬁc
accuraci
relat
rmnn
ntn
figur
show
accuraci
rmnn
vari
among
differ
lation
desir
createdbi
tice
commonsens
relat
desir
bleof
harder
other
like
createdbi
desir
rmnn
overtak
ntn
almost
relat
knowledg
transfer
learn
knowledg
transfer
variou
domain
istic
featur
crucial
cornerston
human
learn
section
evalu
propos
nam
model
figur
accuraci
differ
relat
figur
accuraci
test
set
new
tion
causesdesir
shown
function
use
train
sampl
causesdesir
updat
relat
code
accuraci
origin
relat
remain
knowledg
transfer
learn
scenario
adapt
train
model
unseen
relat
ing
sampl
new
relat
randomli
select
relat
causesdesir
experi
relat
contain
train
sampl
test
sampl
experi
use
relat
train
baselin
nam
model
dnn
rmnn
transfer
learn
freez
nam
paramet
includ
weight
entiti
sentat
learn
new
relat
code
sire
given
sampl
last
learn
relat
code
along
origin
nam
model
use
classifi
new
sampl
causesdesir
test
set
obvious
transfer
learn
affect
model
perform
origin
relat
model
chang
figur
show
result
knowledg
transfer
learn
relat
causesdesir
increas
train
ple
gradual
result
show
rmnn
perform
much
better
dnn
experi
cantli
improv
rmnn
new
relat
total
train
sampl
causesdesir
strate
structur
connect
relat
code
hidden
layer
lead
effect
learn
new
tion
code
rel
small
number
train
ple
next
also
test
aggress
learn
strategi
transfer
learn
set
simultan
date
network
paramet
learn
dnnrmnn
new
relat
code
result
shown
figur
strategi
obvious
improv
perform
new
relat
especi
add
train
ple
howev
expect
perform
nal
relat
deterior
dnn
improv
manc
new
relat
use
train
sampl
howev
perform
remain
origin
relat
drop
dramat
rmnn
show
advantag
dnn
transfer
learn
set
accuraci
new
lation
increas
accuraci
origin
relat
drop
slightli
figur
transfer
learn
result
updat
network
paramet
left
ﬁgure
show
result
new
relat
right
ﬁgure
show
result
origin
relat
extend
nam
winograd
schema
data
collect
previou
experi
section
task
alreadi
contain
manual
construct
train
data
ever
mani
case
want
realiz
ﬂexibl
sens
reason
real
world
condit
obtain
train
data
also
challeng
ical
sinc
propos
neural
associ
model
ical
deep
learn
techniqu
lack
train
data
would
make
difﬁcult
train
robust
model
therefor
paper
make
effort
tri
mine
ful
data
model
train
ﬁrst
step
work
collect
relationship
set
common
word
phrase
believ
type
knowledg
would
key
compon
model
sociat
relationship
discret
event
section
describ
idea
automat
pair
collect
well
data
collect
result
ﬁrst
introduc
common
vocabulari
creat
queri
gener
detail
algorithm
pair
collect
present
final
follow
tion
present
data
collect
result
common
vocabulari
queri
gener
avoid
data
sparsiti
problem
start
work
construct
vocabulari
common
word
current
investig
construct
vocabulari
tain
verb
adject
shown
tabl
vocabulari
includ
verb
word
verb
phrase
adject
word
procedur
construct
vocabulari
straightforward
ﬁrst
extract
word
phrase
divid
tag
wordnet
miller
conduct
tag
larg
corpu
get
occurr
frequenc
word
phrase
scan
tag
corpu
final
sort
word
phrase
frequenc
select
top
result
set
categori
verb
word
verb
phrase
adject
word
size
tabl
common
vocabulari
construct
mine
effect
event
pair
queri
gener
base
common
vocabulari
gener
search
queri
pair
two
word
phrase
current
focu
extract
ciation
relationship
verb
adject
even
small
vocabulari
search
space
larg
lead
ten
million
pair
work
deﬁn
sever
pattern
word
phrase
base
two
ular
semant
dimens
passiv
osgood
use
verb
rob
arrest
exampl
contain
pattern
activ
tive
activ
neg
passiv
posit
passiv
ativ
therefor
queri
form
rob
arrest
would
contain
possibl
dimens
shown
figur
task
mine
relationship
two
word
phrase
becom
task
get
ber
occurr
possibl
link
figur
typic
dimens
typic
queri
automat
pair
collect
base
creat
queri
section
present
procedur
extract
pair
larg
structur
text
overal
system
framework
shown
figur
queri
search
goal
queri
search
ﬁnd
possibl
sentenc
may
contain
input
queri
sinc
number
queri
larg
structur
queri
hashmap
conduct
string
match
ing
text
scan
detail
search
program
start
dnnrmnntext
corpusvocabsentencesresult
rob
activ
positiveact
negativepass
positivepass
neg
arrest
activ
positiveact
negativepass
positivepass
negativeassoci
link
figur
automat
pair
collect
system
framework
conduct
lemmat
tag
denci
pars
sourc
corpu
scan
corpu
begin
end
deal
sentenc
tri
ﬁnd
match
word
phrase
use
hashmap
strategi
help
reduc
search
complex
linear
size
corpu
prove
efﬁcient
experi
match
base
depend
ing
result
ﬁnd
one
phrase
queri
would
check
whether
phrase
associ
least
one
subject
object
correspond
sentenc
time
record
whether
phrase
posit
neg
activ
passiv
moreov
help
cide
relationship
would
check
whether
phrase
link
connect
word
ical
connect
word
use
work
ﬁnalli
extract
pair
design
simpl
match
rule
similar
work
peng
khashabi
roth
two
phrase
one
queri
share
subject
relationship
tween
straightforward
subject
one
phrase
object
phrase
need
pli
passiv
pattern
phrase
relat
object
match
idea
similar
work
pose
peng
khashabi
roth
use
queri
rest
rob
exampl
ﬁnd
sentenc
tom
arrest
tom
rob
man
obtain
denci
pars
result
shown
figur
verb
arrest
rob
share
subject
pattern
arrest
passiv
add
occurr
correspond
sociat
link
link
activ
posit
pattern
rob
passiv
posit
pattern
arrest
figur
depend
pars
result
sentenc
tom
arrest
tom
rob
man
data
collect
result
tabl
show
corpu
use
collect
effect
pair
correspond
data
collect
result
extract
approxim
pair
differ
corpu
corpu
gigaword
graff
novel
zhu
cbtest
hill
bnc
burnard
result
pair
tabl
data
collect
result
differ
corpu
winograd
schema
challeng
base
experi
describ
previou
tion
could
conclud
neural
associ
model
potenti
effect
commonsens
ing
evalu
effect
propos
neural
associ
model
paper
conduct
periment
solv
complex
winograd
schema
leng
problem
levesqu
davi
morgenstern
morgenstern
davi
ortiz
winograd
schema
commonsens
reason
task
propos
recent
year
treat
altern
ture
test
ture
new
task
would
interest
see
whether
neural
network
method
abl
solv
problem
section
describ
progress
made
attempt
meet
winograd
schema
challeng
make
clear
main
task
winograd
schema
ﬁrstli
introduc
high
level
afterward
introduc
system
framework
well
correspond
modul
propos
tomat
solv
winograd
schema
problem
final
experi
discuss
human
annot
effect
dataset
discuss
present
winograd
schema
winograd
schema
evalu
system
sens
reason
abil
base
tradit
cult
natur
languag
process
task
corefer
tion
levesqu
davi
morgenstern
saba
winograd
schema
problem
care
design
task
easili
solv
without
commonsens
knowledg
fact
even
solut
tradit
erenc
resolut
problem
reli
semant
world
knowledg
rahman
strube
scribe
detail
copi
word
levesqu
davi
morgenstern
small
read
comprehens
test
involv
singl
binari
question
two
exampl
trophi
would
brown
suitcas
joan
made
sure
thank
susan
help
big
big
answer
trophi
answer
suitcas
given
given
help
answer
joan
answer
susan
correct
answer
obviou
human
be
question
correspond
ing
four
featur
text
corpusvocabsentencesresult
rob
activ
positiveact
negativepass
positivepass
neg
arrest
activ
positiveact
negativepass
positivepass
negativeassoci
link
two
parti
mention
sentenc
noun
phrase
two
male
two
femal
two
inanim
ject
two
group
peopl
object
pronoun
possess
adject
use
tenc
refer
one
parti
also
right
sort
second
parti
case
male
femal
inanim
object
group
question
involv
determin
refer
noun
possess
adject
answer
alway
ﬁrst
parti
mention
sentenc
repeat
sentenc
clariti
answer
second
parti
word
call
special
word
appear
sentenc
possibl
question
place
anoth
word
call
altern
word
thing
still
make
perfect
sens
answer
chang
solv
problem
easi
sinc
requir
monsens
knowledg
quit
difﬁcult
collect
follow
section
go
describ
work
solv
winograd
schema
problem
via
neural
network
method
system
framework
paper
propos
commonsens
knowledg
requir
mani
winograd
schema
problem
could
muliz
associ
relationship
discret
event
use
sentenc
joan
made
sure
thank
susan
help
given
exampl
commonsens
knowledg
man
receiv
help
thank
man
give
help
believ
ing
associ
event
receiv
help
thank
give
help
thank
make
decis
pare
associ
probabl
help
help
model
well
train
inequ
help
get
help
follow
idea
propos
lize
data
construct
previou
section
tend
nam
model
solv
problem
design
two
framework
train
nam
model
design
appli
four
linear
format
matrix
matrix
activ
posit
activ
neg
passiv
posit
passiv
tive
transform
caus
event
effect
event
use
nam
model
effect
associ
relationship
caus
fect
event
figur
model
framework
hand
tion
treat
typic
dimens
shown
ure
distinct
relat
relat
vector
correspond
nam
model
current
use
rmnn
structur
nam
figur
model
framework
dataset
avail
label
train
nam
model
base
two
tion
straightforward
network
paramet
ing
relat
vector
linear
transform
ce
learn
standard
stochast
gradient
descend
algorithm
experi
section
introduc
current
experi
solv
winograd
schema
problem
ﬁrst
lect
dataset
construct
standard
dataset
subsequ
experiment
setup
describ
detail
present
experiment
result
sion
would
made
end
section
dataset
paper
base
http
label
effect
problem
among
avail
question
experi
tabl
show
typic
exampl
problem
label
three
verb
adject
phrase
correspond
two
pariti
pronoun
label
phrase
also
record
correspond
pattern
word
respect
use
word
lift
exampl
gener
lift
activ
posit
pattern
lift
activ
neg
pattern
lift
passiv
posit
pattern
lift
passiv
neg
pattern
exampl
sentenc
man
lift
son
weak
identifi
weak
lift
lift
man
son
resspect
commonsens
somebodi
weak
would
like
effect
lift
rather
lift
main
work
nam
solv
problem
calcul
associ
probabl
phrase
experiment
setup
setup
nam
effect
task
similar
set
previou
task
repres
phrase
neural
associ
model
use
bow
approach
compos
phrase
word
vector
sinc
lari
use
experi
contain
mon
verb
adject
oov
word
phrase
base
bow
method
phrase
would
useless
word
contain
oov
paper
remov
test
sampl
useless
phrase
result
test
sampl
network
set
set
embed
size
causeeffectneur
associ
modelcauseeffectneur
associ
modelrelationtransformtransformcauseeffectneur
associ
modelcauseeffectneur
associ
modelrelationtransformtransform
schema
text
man
lift
son
weak
man
lift
son
heavi
ﬁsh
ate
worm
tasti
ﬁsh
ate
worm
hungri
mari
tuck
daughter
ann
bed
could
sleep
mari
tuck
daughter
ann
bed
could
work
tom
threw
schoolbag
ray
reach
top
stair
tom
threw
schoolbag
ray
reach
bottom
stair
jackson
greatli
inﬂuenc
arnold
though
live
two
centuri
earlier
jackson
greatli
inﬂuenc
arnold
though
live
two
centuri
later
weak
heavi
tasti
hungri
tuck
bed
tuck
bed
reach
top
reach
bottom
live
earlier
live
later
lift
lift
eat
eat
tuck
bed
tuck
bed
throw
throw
inﬂuenc
inﬂuenc
lift
lift
eaten
eaten
sleep
work
thrown
thrown
inﬂuenc
inﬂuenc
tabl
exampl
dataset
label
winograd
schema
challeng
dimens
relat
vector
set
hidden
layer
nam
model
hidden
layer
size
set
learn
rate
set
experi
time
better
control
model
train
set
learn
rate
learn
embed
ce
relat
vector
neg
sampl
import
model
train
task
system
gener
neg
sampl
randomli
select
differ
pattern
respect
pattern
effect
event
tive
sampl
exampl
posit
train
sampl
hungri
activ
posit
caus
eat
activ
posit
may
gener
neg
sampl
like
hungri
activ
itiv
caus
eat
passiv
posit
hungri
activ
posit
caus
eat
activ
neg
nam
system
neg
sampl
method
much
straightforward
randomli
select
differ
fect
event
whole
vocabulari
exampl
shown
possibl
neg
sampl
would
hungri
tive
posit
caus
happi
activ
posit
hungri
activ
posit
caus
talk
activ
posit
result
experiment
result
shown
tabl
result
ﬁnd
propos
nam
el
achiev
accuraci
dataset
construct
winograd
schema
speciﬁc
system
perform
slightli
better
system
model
accuraci
tabl
result
nam
winograd
schema
effect
dataset
test
result
ﬁnd
nam
perform
well
test
exampl
instanc
call
phone
nario
propos
nam
gener
correspond
ciation
probabl
follow
paul
tri
call
georg
phone
cess
success
paul
georg
call
answer
paul
paul
tri
call
georg
phone
avail
avail
paul
georg
call
answer
georg
test
exampl
ﬁnd
model
answer
question
correctli
calcul
tion
probabl
probabl
probabl
call
inequ
relationship
associ
probabl
reason
commonsens
exampl
jim
yell
kevin
upset
call
simpl
smaller
larger
upset
jim
upset
kevin
upset
answer
jim
jim
comfort
kevin
upset
upset
jim
upset
kevin
upset
answer
kevin
exampl
also
convey
commonsens
edg
daili
life
know
somebodi
upset
would
like
yell
peopl
also
like
would
comfort
peopl
conclus
paper
propos
neural
associ
model
nam
probabilist
reason
use
neural
network
model
associ
probabl
two
event
domain
work
investig
two
model
structur
name
dnn
rmnn
nam
mental
result
sever
reason
task
shown
dnn
rmnn
outperform
exist
od
paper
also
report
preliminari
result
use
nam
knowledg
transfer
learn
found
propos
rmnn
model
quickli
adapt
new
relat
without
sacriﬁc
perform
inal
relat
prove
effect
nam
model
appli
solv
complex
commonsens
reason
problem
winograd
schema
levesqu
davi
morgenstern
support
model
train
task
propos
straightforward
method
lect
associ
phrase
pair
text
corpu
experi
conduct
set
winograd
schema
problem
dicat
neural
associ
model
solv
lem
success
howev
still
long
way
ﬁnalli
achiev
automat
commonsens
reason
acknowledg
want
thank
gari
marcu
new
york
siti
use
comment
commonsens
reason
also
want
thank
ernest
davi
leora
stern
charl
ortiz
wonder
organ
make
ﬁrst
winograd
schema
challeng
happen
paper
support
part
scienc
nolog
develop
anhui
provinc
china
grant
fundament
research
fund
tral
univers
grant
gic
prioriti
research
program
chines
academi
scienc
grant
refer
bengio
bengio
ducharm
vincent
janvin
neural
probabilist
languag
model
journal
machin
learn
research
bord
bord
glorot
weston
bengio
joint
learn
word
mean
resent
semant
pars
proceed
aistat
bord
bord
usuni
weston
yakhnenko
translat
bed
model
data
proceed
nip
bowman
bowman
ang
pott
larg
annot
corpu
man
arxiv
preprint
learn
natur
languag
infer
bowman
bowman
recurs
ral
tensor
network
learn
logic
reason
arxiv
preprint
burnard
burnard
user
refer
guid
british
nation
corpu
version
collobert
collobert
weston
bottou
karlen
kavukcuoglu
kuksa
natur
languag
process
almost
scratch
journal
machin
learn
research
getoor
getoor
relat
learn
mit
press
glorot
bengio
glorot
bengio
understand
difﬁculti
train
deep
feedforward
neural
network
proceed
aistat
introduct
statist
sutskev
deep
learn
introduct
srivastava
salakhutdinov
improv
neural
network
arxiv
preprint
graff
graff
kong
chen
maeda
english
gigaword
linguist
data
consortium
philadelphia
hill
hill
bord
chopra
ston
goldilock
principl
read
child
book
explicit
memori
represent
arxiv
preprint
hinton
hinton
krizhevski
ing
featur
detector
hornik
stinchcomb
white
hornik
comb
white
univers
approxim
unknown
map
deriv
use
multilay
feedforward
network
neural
network
jensen
jensen
bayesian
network
volum
ucl
press
london
koller
friedman
koller
friedman
probabilist
graphic
model
principl
niqu
mit
press
lecun
bengio
hinton
lecun
bengio
natur
hinton
levesqu
davi
morgenstern
levesqu
davi
morgenstern
winograd
schema
challeng
aaai
spring
symposium
logic
tion
commonsens
reason
lin
lin
liu
sun
liu
zhu
learn
entiti
relat
embed
knowledg
graph
complet
proceed
aaai
liu
singh
liu
singh
net
practic
commonsens
reason
toolkit
ogi
journal
mccarthi
mccarthi
applic
cumscript
formal
knowledg
tiﬁcial
intellig
mikolov
mikolov
chen
corrado
dean
efﬁcient
estim
word
tion
vector
space
arxiv
preprint
miller
miller
wordnet
cal
databas
english
commun
acm
minski
minski
societi
mind
simon
schuster
morgenstern
davi
ortiz
morgenstern
davi
ortiz
plan
execut
evalu
winograd
schema
challeng
magazin
mueller
mueller
commonsens
sone
event
calculu
base
approach
morgan
mann
nair
hinton
nair
hinton
rectiﬁ
linear
unit
improv
restrict
boltzmann
chine
proceed
icml
neapolitan
neapolitan
probabilist
sone
expert
system
theori
algorithm
pace
independ
publish
platform
nickel
nickel
murphi
tresp
gabrilovich
review
relat
machin
ing
knowledg
graph
arxiv
preprint
nickel
tresp
kriegel
nickel
tresp
kriegel
factor
yago
scalabl
machin
learn
link
data
proceed
www
acm
osgood
osgood
natur
surement
mean
psycholog
bulletin
pearl
pearl
probabilist
reason
ligent
system
network
plausibl
reason
peng
khashabi
roth
peng
khashabi
roth
solv
hard
corefer
problem
urbana
rahman
rahman
corefer
resolut
world
knowledg
ing
annual
meet
associ
putat
linguist
human
languag
volum
associ
comput
tic
richardson
domingo
richardson
domingo
markov
logic
network
machin
learn
saba
saba
winograd
schema
leng
socher
socher
chen
man
reason
neural
tensor
network
proceed
nip
knowledg
base
complet
strube
strube
non
util
mantic
corefer
resolut
corbon
remix
naacl
workshop
corefer
resolut
beyond
ontonot
ture
ture
comput
machineri
intellig
mind
wang
wang
zhang
feng
chen
knowledg
graph
embed
translat
hyperplan
proceed
aaai
cites
xue
xue
jiang
dai
liu
fast
adapt
deep
neural
network
base
discrimin
code
speech
recognit
dio
speech
languag
process
tran
zhu
zhu
kiro
zemel
nov
urtasun
torralba
fidler
align
book
movi
toward
visual
planat
watch
movi
read
book
ceed
ieee
intern
confer
comput
vision
