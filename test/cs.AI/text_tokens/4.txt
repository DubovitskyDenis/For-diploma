optim
sens
via
bandit
relax
mix
observ
domain
mikko
lauri
risto
ritala
sequenti
decis
make
uncertainti
studi
mix
observ
domain
goal
maxim
amount
inform
obtain
partial
observ
stochast
process
constraint
impos
fulli
observ
intern
state
upper
bound
optim
valu
function
deriv
relax
constraint
identifi
condit
relax
problem
arm
bandit
whose
optim
polici
easili
comput
upper
bound
appli
prune
search
space
origin
problem
effect
solut
qualiti
assess
via
simul
experi
empir
result
show
effect
prune
search
space
target
monitor
domain
introduct
deploy
autonom
agent
robot
equip
appropri
set
sensor
allow
autom
ecut
variou
inform
gather
task
task
includ
monitor
identiﬁc
process
autom
explor
data
collect
campaign
environ
human
presenc
sire
infeas
robot
mobil
sensor
platform
whose
action
optim
maxim
inform
measur
data
target
state
known
probabl
siti
function
pdf
state
call
belief
state
maintain
inform
convey
measur
data
incorpor
belief
state
bayesian
ﬁltere
sume
markovian
dynam
condit
independ
measur
data
given
system
state
problem
partial
observ
markov
decis
process
pomdp
optim
inform
gather
studi
context
sensor
manag
review
ing
pomdp
sensor
manag
present
problem
formul
decis
process
uncertainti
goal
ﬁnd
control
polici
map
belief
state
action
follow
maxim
expect
sum
discount
reward
horizon
time
reward
associ
action
may
depend
either
true
state
system
belief
state
former
encod
object
reach
favor
state
avoid
costli
one
use
navig
obstacl
avoid
latter
option
allow
inform
theoret
reward
mutual
inform
appli
variou
sequenti
inform
gather
problem
robot
see
problem
termin
lauri
ritala
depart
autom
enc
engin
tamper
univers
technolog
box
tamper
finland
email
special
stop
action
execut
natur
model
task
may
stop
certain
level
conﬁdenc
state
reach
find
optim
polici
pomdp
comput
hard
sever
approxim
method
gest
algorithm
track
alpha
vector
set
point
belief
space
alpha
vector
may
use
approxim
optim
polici
belief
state
onlin
plan
method
ﬁnd
optim
action
current
belief
state
instead
represent
optim
polici
problem
cast
search
tree
belief
state
reachabl
current
belief
state
variou
histori
combin
onlin
method
mont
carlo
simul
evalu
util
action
lead
approxim
algorithm
abl
handl
problem
state
mix
observ
domain
part
state
space
fulli
observ
belief
space
union
dimension
subspac
one
valu
fulli
servabl
state
variabl
robot
system
often
exhibit
mix
observ
may
exploit
deriv
efﬁcient
pomdp
algorithm
bandit
mab
model
sequenti
also
appli
sensor
manag
play
one
arm
mab
collect
reward
depend
state
arm
arm
randomli
transit
new
state
arm
remain
stationari
solut
mab
index
polici
easier
comput
solut
gener
pomdp
aforement
research
appli
reward
tion
depend
true
state
action
expect
reward
linear
belief
state
featur
leverag
mani
solut
algorithm
inform
theoret
quantiti
entropi
mutual
inform
would
use
reward
function
optim
sens
problem
nonlinear
belief
state
classic
pomdp
algorithm
appli
solv
problem
paper
studi
pomdp
mix
observ
mutual
inform
reward
function
approach
especi
suit
optim
sens
problem
robot
domain
remov
constraint
avail
action
obtain
relax
problem
optim
valu
relax
problem
obtain
upper
bound
optim
valu
pomdp
identifi
condit
relax
problem
mab
easili
comput
optim
solut
upper
bound
appli
onlin
plan
algorithm
prune
search
space
paper
organ
follow
section
observ
pomdp
deﬁn
section
iii
method
solv
problem
discuss
section
two
relax
deriv
provid
upper
bound
optim
valu
function
section
determin
condit
relax
mab
empir
result
provid
section
section
vii
conclud
paper
mix
observ
pomdp
notat
denot
random
variabl
set
percas
letter
realiz
random
variabl
member
set
lowercas
letter
time
instant
distinguish
write
realiz
time
respect
agent
robot
anoth
sensor
platform
intern
state
captur
dynam
straint
oper
sensor
devic
intern
state
evolv
accord
determinist
dynam
model
deﬁn
control
action
ﬁnite
set
action
allow
intern
state
let
denot
set
random
infer
variabl
agent
wish
obtain
mation
dynam
variabl
govern
stochast
model
deﬁn
markov
chain
complet
state
system
problem
featur
mix
observ
ternal
state
fulli
observ
infer
variabl
partial
observ
agent
observ
follow
observ
model
deﬁn
agent
maintain
belief
state
consist
determinist
fulli
observ
intern
state
pdf
initi
belief
state
given
given
belief
state
action
observ
belief
state
next
time
instant
given
belief
updat
equat
pdf
infer
variabl
obtain
bayesian
ﬁlter
predict
pdf
normal
factor
denot
prior
probabl
observ
given
sequenc
action
observ
uncertainti
result
intern
state
thu
equival
deﬁn
set
allow
action
via
belief
state
agent
object
encod
reward
function
object
maxim
expect
sum
discount
reward
horizon
decis
discount
factor
consid
reward
function
let
mutual
inform
posterior
state
observ
deﬁn
entropi
predict
pdf
second
term
expect
entropi
posterior
pdf
prior
pdf
problem
instanc
pomdp
bellman
principl
optim
solut
may
found
via
backward
time
recurs
procedur
known
valu
iter
optim
valu
function
map
belief
state
maximum
expect
sum
discount
reward
optim
polici
follow
next
decis
optim
valu
function
comput
max
start
optim
polici
remain
decis
found
extract
argument
maxim
recurs
continu
iii
solv
pomdp
reward
pomdp
reward
function
expect
linear
belief
state
horizon
optim
valu
function
ﬁnite
tion
convex
hull
set
hyperplan
belief
space
mani
exact
approxim
ofﬂin
algorithm
pomdp
reli
piecewis
eariti
convex
valu
function
reward
function
mutual
inform
entropi
use
optim
sens
problem
nonlinear
belief
state
thu
ofﬂin
algorithm
applic
solv
recurs
reward
function
onlin
plan
method
ﬁnd
optim
action
current
belief
state
instead
close
form
represent
optim
polici
explicit
represent
polici
requir
nonlinear
reward
tion
constitut
addit
difﬁculti
onlin
plan
tree
graph
belief
state
reachabl
current
belief
state
construct
current
belief
state
root
tree
belief
state
comput
via
ad
child
node
node
desir
search
depth
reach
valu
leaf
tree
propag
back
root
accord
suboptim
action
may
sometim
prune
search
tree
prune
optim
valu
execut
action
belief
state
upper
bound
lower
bound
given
action
suboptim
successor
node
may
prune
tree
bound
may
similarli
propag
via
number
belief
state
search
tree
reduc
altern
onlin
tree
search
includ
special
approxim
method
howev
limit
small
lem
approxim
appli
reced
horizon
control
principl
reduc
valu
iter
gaussian
belief
case
theoret
treatment
nonlinear
convex
reward
function
pomdp
refer
reader
bound
valu
function
optim
polici
attain
optim
valu
belief
state
polici
achiev
valu
lower
bound
optim
valu
simpl
choic
set
greedi
polici
option
includ
random
polici
blind
polici
alway
execut
singl
ﬁxed
action
upper
bound
found
deriv
two
relax
version
origin
pomdp
problem
remov
constraint
applic
action
set
intern
state
reachabl
subset
singl
time
step
set
intern
state
reachabl
step
ﬁrst
relax
obtain
remov
constraint
impos
intern
state
follow
time
univers
sensor
relax
given
pomdp
problem
univers
sensor
ation
contain
action
stochast
part
replac
consid
action
applic
intern
state
reachabl
within
decis
current
time
step
obtain
sensor
relax
sensor
relax
given
pomdp
problem
sensor
relax
ˆak
ˆak
set
action
possibl
intern
state
reachabl
within
time
step
current
intern
state
ˆak
optim
valu
either
relax
problem
greater
equal
optim
valu
origin
problem
let
denot
optim
valu
function
respect
let
denot
valu
function
greedi
polici
given
one
machin
play
agent
per
action
state
machin
evolv
agent
may
affect
machin
play
remain
current
state
machin
independ
machin
play
contribut
reward
gittin
show
optim
polici
mab
greedi
index
alloc
polici
arm
alloc
index
known
gittin
index
calcul
optim
select
yield
highest
index
valu
index
polici
optim
action
irrevoc
action
avail
stage
may
chosen
later
stage
reward
exclud
effect
discount
factor
index
polici
usual
much
easier
comput
backward
induct
solut
pomdp
index
polici
gener
optim
observ
pomdp
section
action
irrevoc
due
constraint
impos
intern
state
howev
relax
ﬁxed
action
space
follow
three
properti
requir
relax
mab
result
deriv
hold
restrict
case
well
properti
relat
properti
given
condit
valu
subset
infer
variabl
stationari
dirac
delta
function
properti
observ
condit
prior
corollari
infer
variabl
independ
subset
properti
hold
independ
preserv
posterior
furthermor
hold
optim
valu
bound
bandit
index
polici
pomdp
relax
relax
deﬁn
pomdp
solv
even
relax
problem
may
thu
tional
intract
task
motiv
identifi
pomdp
whose
relax
easili
comput
optim
polici
bandit
mab
problem
play
one
arm
mab
collect
reward
depend
state
arm
four
requir
distinguish
mab
among
gener
stochast
control
problem
exactli
equat
seen
hold
appli
given
prior
model
satisfi
equat
seen
hold
two
step
first
due
independ
ture
prior
posterior
similarli
second
see
appli
step
lead
state
main
result
determin
condit
pomdp
relax
mab
proposit
mab
equival
pomdp
relax
properti
fulﬁll
prior
relax
arm
bandit
problem
proof
sketch
consid
four
requir
mab
introduc
properti
establish
arm
bandit
partli
satisfi
requir
rest
ment
satisﬁ
properti
establish
state
bandit
arm
requir
satisﬁ
independ
properti
ﬁrst
part
corollari
latter
part
corollari
show
requir
satisﬁ
proposit
hold
optim
polici
respect
optim
valu
thu
greedi
index
polici
valu
much
easier
comput
gener
pomdp
let
consid
follow
exampl
problem
monitor
reactiv
target
agent
locat
everi
time
step
agent
may
either
stay
move
one
neighbor
locat
applic
action
let
assum
valu
target
present
locat
target
react
agent
presenc
agent
record
measur
accord
implement
onlin
search
belief
state
reachabl
current
belief
state
appli
lower
upper
bound
prune
suboptim
action
appli
greedi
lower
bound
section
upper
bound
section
compar
approach
exhaust
search
reachabl
belief
state
equival
use
lower
upper
bound
pomcp
algorithm
give
recommend
next
action
execut
base
seri
mont
carlo
simul
case
properti
satisﬁ
deﬁn
proposit
hold
denot
markov
chain
paramet
transit
probabl
sampl
uniformli
random
set
initi
belief
state
satisfi
independ
assumpt
infer
variabl
sampl
uniformli
random
greedi
mab
polici
give
valid
upper
bound
see
appli
rtbss
bound
henc
alway
ﬁnd
optim
solut
veriﬁ
simul
number
visit
node
search
tree
belief
state
shown
fig
tighter
appli
upper
bound
sinc
bound
result
lower
equal
number
visit
node
comparison
averag
number
visit
node
exhaust
search
shown
tabl
note
appli
either
bound
greatli
reduc
number
visit
node
case
order
magnitud
although
reduct
number
visit
node
substanti
evalu
bound
comput
cost
must
balanc
save
visit
fewer
node
point
discuss
detail
next
subsect
fals
neg
posit
probabl
respect
reward
function
consid
relax
problem
properti
immedi
seen
satisﬁ
properti
satisﬁ
target
remain
stationari
agent
present
may
chosen
freeli
properti
satisﬁ
observ
depend
valu
empir
evalu
ran
simul
experi
monitor
problem
deﬁn
infer
variabl
arrang
rectangular
connect
grid
agent
allow
move
grid
sens
target
examin
two
case
ﬁrst
case
properti
satisﬁ
second
case
relax
properti
allow
infer
variabl
chang
state
case
optim
horizon
vari
decis
paramet
implement
belief
space
search
rtbss
algorithm
present
rtbss
node
expand
node
expand
fig
number
search
tree
node
expand
rtbss
upper
bound
diagon
line
show
two
valu
equal
pomcp
recommend
coincid
optim
tion
reliabl
number
simul
creas
optim
horizon
short
see
tabl
compar
valu
optim
action
mend
pomcp
two
differ
differ
tabl
tabl
averag
number
node
expand
exhaust
search
percentag
rtbss
solut
agre
optim
node
solut
properti
satisfi
dynam
bound
percentag
pomcp
recommend
agre
tabl
optim
simul
two
valu
perform
loss
comput
mean
valu
maximum
valu
result
shown
tabl
iii
perform
loss
tend
greater
fewer
simul
greater
optim
horizon
number
simul
increas
mean
perform
loss
low
indic
averag
pomcp
perform
well
compar
optim
solut
howev
even
mean
perform
loss
low
worst
case
perform
loss
follow
pomcp
recommend
may
signiﬁcantli
greater
problem
suboptim
action
may
lead
unaccept
perform
loss
method
rtbss
valid
bound
may
prefer
pomcp
case
properti
satisﬁ
next
examin
case
properti
satisﬁ
set
dynam
model
markov
chain
consid
three
subcas
distinguish
rate
state
transit
slow
medium
fast
slow
dynam
paramet
sampl
uniformli
random
slow
medium
namic
med
fast
dynam
ast
experi
repeat
randomli
sampl
initi
belief
state
dynam
model
belief
satisﬁ
independ
assumpt
infer
variabl
med
slow
ast
problem
quit
similar
one
tion
pomcp
perform
also
observ
good
averag
mab
equival
proposit
satisﬁ
relax
problem
thu
upper
bound
approxim
optim
rtbss
guarante
examin
effect
solut
provid
rtbss
result
summar
tabl
tabl
show
percentag
solut
equal
optim
solut
case
slow
medium
fast
dynam
either
univers
sensor
upper
bound
sensor
upper
bound
optim
solut
found
major
case
percentag
decreas
optim
horizon
slow
medium
fast
greater
rate
dynam
faster
sinc
often
like
bound
obtain
univers
sensor
relax
overestim
optim
valu
consequ
better
agreement
optim
solut
observ
result
suggest
may
still
reason
approxim
upper
bound
valu
greedi
polici
relax
problem
even
optim
guarante
efﬁcienc
prune
search
tree
affect
signiﬁcantli
compar
case
previou
subsect
appli
either
bound
dramat
reduc
number
visit
node
search
tree
examin
mean
time
requir
ﬁnd
solut
belief
state
either
tive
search
prune
repres
comparison
present
fig
case
medium
dynam
exhaust
search
perform
fastest
comput
burden
comput
bound
outweigh
save
visit
fewer
node
search
advantag
prune
search
tree
becom
appar
best
appli
prune
order
magnitud
faster
exhaust
search
upper
bound
fastest
use
upper
bound
faster
exhaust
search
compar
comput
time
pomcp
rtbss
meaning
experi
run
differ
comput
platform
differ
implement
search
tree
case
medium
dynam
exhaust
optim
horizon
fig
mean
runtim
per
decis
millisecond
function
optim
horizon
exhaust
search
appli
upper
bound
perform
loss
pomcp
compar
optim
tabl
iii
simul
mean
max
mean
max
mean
max
mean
max
mean
max
vii
conclus
optim
sens
problem
domain
intern
state
fulli
observ
set
infer
variabl
partial
observ
formul
pomdp
object
sequenti
maxim
mutual
inform
infer
variabl
vation
upper
bound
optim
valu
function
found
relax
constraint
origin
problem
three
condit
fulﬁll
relax
problem
mab
first
action
relat
uniqu
subset
infer
variabl
secondli
infer
variabl
subset
correspond
current
action
evolv
infer
variabl
remain
stationari
final
observ
depend
infer
variabl
subset
relat
current
action
optim
solut
mab
problem
greedi
index
alloc
polici
much
easier
ﬁnd
solv
gener
pomdp
pomdp
solv
search
effect
bound
prune
search
space
empir
veriﬁ
target
monitor
problem
find
optim
action
requir
search
fraction
reachabl
belief
state
compar
exhaust
search
comput
time
best
order
magnitud
smaller
appli
prune
comput
save
becom
appar
save
due
reduc
search
space
size
exceed
addit
cost
comput
bound
futur
work
includ
studi
applic
olog
wider
rang
mix
observ
domain
tivat
posit
result
optim
greedi
polici
restless
bandit
problem
believ
may
exist
class
stochast
control
problem
current
known
greedi
polici
optim
identifi
class
would
expand
applic
result
refer
kaelbl
littman
cassandra
plan
act
partial
observ
stochast
domain
artiﬁci
intellig
vol
hero
cochran
kastella
dation
applic
sensor
manag
new
york
springer
chong
kreucher
hero
partial
observ
markov
decis
process
approxim
adapt
sens
discret
event
dynam
system
vol
may
charrow
kumar
michael
approxim
represent
control
polici
maxim
mutual
inform
autonom
robot
vol
atanasov
daniilidi
pappa
inform
acquisit
sens
robot
algorithm
error
bound
ieee
int
conf
robot
autom
icra
hong
kong
china
june
lauri
ritala
stochast
control
maxim
mutual
inform
activ
sens
icra
workshop
robot
home
industri
look
first
hong
kong
china
june
hansen
pomdp
minat
proceed
nation
confer
artiﬁci
intellig
vancouv
canada
juli
papadimitri
tsitsikli
complex
markov
decis
process
mathemat
oper
research
vol
spaan
vlassi
perseu
random
valu
iter
pomdp
journal
artiﬁci
intellig
search
vol
pineau
gordon
thrun
anytim
tion
larg
pomdp
journal
artiﬁci
intellig
research
vol
ross
pineau
paquet
onlin
plan
algorithm
pomdp
journal
artiﬁci
intellig
research
vol
silver
veness
plan
larg
pomdp
advanc
neural
inform
process
system
vancouv
canada
ong
png
hsu
lee
plan
uncertainti
robot
task
mix
observ
intern
journal
robot
research
vol
hero
cochran
sensor
manag
past
present
futur
ieee
sensor
journal
vol
bellman
dynam
program
princeton
new
jersey
ton
univers
press
smallwood
sondik
optim
control
partial
observ
markov
process
ﬁnite
horizon
oper
search
vol
lovejoy
survey
algorithm
method
partial
observ
markov
decis
process
annal
oper
research
vol
hauskrecht
approxim
partial
abl
markov
decis
process
journal
artiﬁci
intellig
research
vol
krishnamurthi
djonin
structur
threshold
polici
dynam
sensor
schedul
partial
observ
markov
decis
process
approach
ieee
transact
signal
process
vol
araya
buffet
thoma
charpillet
pomdp
extens
reward
advanc
neural
inform
process
system
vancouv
canada
gittin
bandit
process
dynam
alloc
index
journal
royal
statist
societi
seri
methodolog
paquet
ross
hybrid
pomdp
algorithm
proceed
aama
workshop
sequenti
decis
make
uncertain
domain
hakod
japan
may
ahmad
liu
javidi
zhao
krishnamachari
optim
myopic
sens
multichannel
opportunist
access
ieee
transact
inform
theori
vol
