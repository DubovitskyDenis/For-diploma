towards
machine
intelligence
kamil
rocki
ibm
research
san
jose
95120
usa
kmrocki
us.ibm.com
abstract
exists
theory
single
general-purpose
learning
al-
gorithm
could
explain
principles
operation
theory
assumes
brain
initial
rough
architecture
small
li-
brary
simple
innate
circuits
prewired
birth
proposes
signiﬁcant
mental
algorithms
learned
given
current
un-
derstanding
observations
paper
reviews
lists
ingredients
algorithm
architectural
functional
perspectives
introduction
recently
much
progress
made
area
supervised
learning
however
one
greatest
challenges
remaining
artiﬁcial
intelligence
research
advancing
ﬁeld
unsupervised
learning
algorithms
especially
autonomous
learning
complex
spatiotemporal
patterns
poses
great
challenge
paper
reviews
lists
ingredients
possible
general-purpose
learning
algorithm
given
current
state
knowledge
neocortex
found
mammals
deemed
place
intelligence
originates
studied
extensively
past
decades
date
still
consensus
principles
operation
theories
suggest
single
learning
algorithm
might
suﬃcient
explain
intelligence
theories
considered
ever
since
mountcastle
discovery
simple
uniform
architecture
cortex
six
horizontal
layers
organized
vertical
structures
called
cortical
columns
columns
thought
basic
repeating
functional
units
neocortex
discovery
might
suggest
brain
regions
perform
simi-
lar
operations
region-speciﬁc
algorithms
another
famous
ex-
periment
supporting
hypothesis
showed
rewiring
auditory
part
brain
ferrets
able
learn
interpret
visual
inputs
knowledge
necessary
ingredients
algorithm
shaped
neuroscientiﬁc
discoveries
empirical
evaluation
eﬀectiveness
algorithms
metacognition
observations
points
may
considered
general
assumptions
reverse-engineering
general-purpose
learning
algorithm
kamil
rocki
ingredients
2.1
unsupervised
real
world
almost
data
unlabeled
although
nobody
kwons
precise
rules
used
human
brain
learning
one
assume
learn
mostly
unsupervised
way
speciﬁcally
newborn
learns
world
diﬀerent
objects
interact
might
even
way
provide
super-
vised
signal
him/her
appropriate
sensory
representations
i.e
visual
auditory
need
developed
ﬁrst
another
piece
evidence
supervised
learning
may
obtained
simple
calculation
assuming
approximately
1014
synapses
109
seconds
human
lifetime
enough
capacity
store
memories
rate
105
bits/second
there-
fore
seems
reasonable
brain
learns
model
world
directly
environment
motivates
hypothesis
predominance
un-
supervised
learning
since
way
acquiring
much
information
absorbing
data
perceptual
inputs
even
teacher
present
learning
must
done
learning
associations
events
without
super-
vision
unsupervised
learning
researched
extensively
found
closely
connected
process
entropy-maximization
regularization
compression
means
evolution
brains
adapted
act
data
compactors
particular
goal
unsupervised
learning
might
ﬁnd
codes
disentangle
input
sources
describe
original
information
less
redundant
interpretable
way
throwing
much
data
possible
without
losing
information
example
operation
observed
visual
cortex
might
even
happen
early
retina
learns
patterns
appearing
natural
environ-
ment
assigns
high
probability
patterns
contrast
cortex
assigns
low
probability
random
combinations
real
world
data
said
lie
near
non-linear
manifold
within
higher-dimensional
space
manifold
shape
deﬁned
data
probability
distribution
clustering
equivalent
learning
manifolds
able
separate
well
enough
given
task
2.2
compositional
humans
learn
concepts
sequential
order
ﬁrst
making
sense
simple
pat-
terns
representing
complex
ones
terms
previously
learned
abstractions
ability
read
might
serve
example
first
learn
see
recognize
pen
strokes
letters
words
able
under-
stand
complex
sentences
contrast
non-compositional
approach
would
attempt
read
straight
ink
patterns
piece
paper
brain
might
adapted
way
reﬂect
fact
world
inherently
hierarchical
observation
also
inspired
deep
learning
movement
used
hierarchical
approach
model
real
world
data
achieving
unprecedented
per-
formance
many
tasks
way
deep
learning
algorithms
automatically
towards
machine
intelligence
compose
multiple
layers
representations
data
gives
rise
models
yield
increasingly
abstract
associations
concepts
hence
names
used
deep
learning
algorithms
representation
learning
feature
learning
among
others
main
distinction
deep
approach
previous
generation
machine
learning
structure
data
discovered
automatically
general-purpose
learning
pro-
cedure
without
need
hand-engineer
feature
detectors
11,44
scheme
agrees
well
idea
unsupervised
learning
mentioned
way
abstract
hierarchical
representations
might
natural
by-products
data
compression
given
theoretical
empirical
evidence
favor
deep
representation
learning
one
could
formulate
requirement
type
brain-like
architecture
deep
containing
many
hierarchical
levels
2.3
sparse
distributed
existence
cortical
columns
neocortex
linked
func-
tional
importance
arrangement
column
typically
responds
sensory
stimulus
representing
certain
body
part
region
sound
vision
cells
belonging
cell
excited
simultaneously
therefore
acting
feature
detector
time
column
active
receiving
strong
input
signal
spikes
prohibit
nearby
columns
becoming
ac-
tive
lateral
inhibition
mechanism
leads
sparse
activity
patterns
fact
strongly
active
columns
inhibited
forces
learned
patterns
invariant
possible
giving
rise
independent
feature
detec-
tors
cortex
one
might
expect
sparse
distributed
representations
brain
sdrs
coincidental
since
possess
im-
portant
properties
information-theoretic
perspective
distributed
important
order
disentangle
underlying
causes
variation
i.e
melody
instrument
pitch
loudness
sparsity
aﬀects
elements
learning
good
features
proven
given
certain
sparsity
signal
may
correctly
reconstructed
even
fewer
samples
sampling
theorem
re-
quires
ever
since
discovery
selective
features
detectors
edge
detectors
center-surround
receptive
ﬁelds
hubel
wiesel
1959
learning
biologically
plausible
sparse
distributed
representations
input
pat-
terns
hot
research
topic
shown
sdrs
signiﬁcantly
noise-resistant
dense
representations
another
important
property
distributed
representations
appreciated
number
distinguishable
regions
scales
exponentially
number
parameters
used
describe
true
non-distributed
representa-
tions
sparse
distributed
representations
combinatorially
much
expressive
given
observation
simple
see
discriminative
point
view
higher
levels
abstractions
sdrs
preferred
way
rep-
resenting
inputs
since
learning
procedure
produces
form
preserves
much
information
possible
making
code
short/simple
possible
also
corresponds
ﬁnding
minimum-entropy
codes
5,37
in-line
kamil
rocki
fig
eﬃcient
learning
sdrs
sparse
distributed
representations
sdrs
simplify
learning
temporal
dependencies
provide
mechanism
generalization
out-of-
domain
prediction
occam
razor
minimum
description
length
mdl
rules
postulate
simple
solutions
chosen
complex
ones
al-
lows
manipulating
sparse
representations
throughout
large
network
simpliﬁes
learning
higher
level
concepts
see
dimensionality
reduction
redundancy
reduction
2.4
objectiveless
chinese
room
argument
states
learning
improve
performance
measure
given
task
necessarily
lead
improving
understanding
task
context
supervised
learning
is-
sue
since
clearly
care
performance
measure
however
unsupervised
learning
considered
desired
outcome
would
learn
trans-
ferrable
concepts
could
even
hypothesized
following
gradient
objective
function
one
may
prohibit
learning
procedure
discov-
ering
unknown
state-space
progress
learning
equivalent
close
objective
one
hypothesis
objective
problem
clearly
learning
algorithm
goal
might
deﬁned
broadly
theory
curiosity
creativity
beauty
described
schmidhuber
rightred
circleis
moving
towards
machine
intelligence
2.5
scalable
large
network
human
brain
might
computationally
eﬃ-
cient
separate
local
learning
gray
matter
adjusting
higher
level
connec-
tions
layers/regions
white
matter
functional
distinction
would
reﬂect
structural
hierarchy
predominant
deep
learning
meth-
ods
described
real
world
biological
technological
social
trans-
portation
types
real-world
networks
neither
completely
random
deﬁnitely
regular
instead
topology
lies
somewhere
fig
example
small
world
network
edge
encodes
presence
long-
distance
connection
corresponding
regions
macaque
brain
figure
bor-
rowed
so-called
small
world
networks
may
nature
solution
hierarchical
structure
allowing
separate
parallel
local
global
updates
synapses
scalability
unsupervised
learning
lower
levels
goal-oriented
ﬁne-tuning
higher
regions
study
neocortex
reveals
presence
small
world
networks
columnar
organization
reﬂects
local
connectiv-
ity
cerebral
cortex.the
brain
inherently
parallel
machine
without
separate
instruction-issuing
memory
storage
areas
instead
parts
neocortex
participate
big
diﬀerence
compared
von-neumann
architecture
describing
majority
computing
systems
organized
main
bottleneck
current
systems
concerns
data
movement
implies
additional
bandwidth
power
latency
requirements
cpus
typi-
cally
optimized
serial
tasks
mitigating
negative
eﬀects
archi-
tecture
deep
cache
hierarchy
losing
parallelism
involved
gpus
brain-like
layout
equal
processing
units
private
memory
actually
operate
parallel
without
collid-
ing
however
problem
moving
data
still
exists
either
cpu
gpu
inside
gpu
problem
persists
fact
quite
〈〉田spike-timing
dependent
...
。。125.219.39.95
もw3
verse
gmail
calendar
etime
tau
arxiv
box
reddit
https
//homes.cs.washin
...
feedforward
horizontal，…
small
world
network
-g…
www.nbb.cornell.edu/ne…
cortical
rewiring
info…
ducノ筆、hj
－vι7、・分一c
av品vutω
...
ii0-fig
lnnermo喝tcore
undirected
vers
network
innermost
core
central
subnetwork
far
tightly
integrated
overall
network
information
likely
spreads
swi代旬withinthe
innermost
core
overall
network
overall
network
communicates
午，，.，胸同＿，＿.，噌『・－·、，u司品噌
f朝間『『円圃剛－·－・
一ι－·a一一・・」＿.，圃酬咽f恒．園町晶盲目j」ーム」�.。白白
kamil
rocki
easy
show
virtually
impossible
achieve
peak
performance
processors
data
fed
fast
enough
moreover
data
transfers
major
energy
consumption
factors
parallel
gpu-like
devices
therefore
radical
approach
may
needed
order
im-
prove
performance
signiﬁcantly
von-neumann
architecture
needs
changed
one
memory
compute
hardware
allows
functionality
already
appeared
concept
in-place
processing
assumes
however
diﬀerent
approach
also
needed
thinking
algorithms
process
communication-aware
algorithm
design
already
started
advent
multi-core
cpus
gpus
fpgas
next
step
design
communication-less
algorithms
ongoing
eﬀort
super-
computing
community
noticed
signiﬁcant
progress
made
without
reducing
information
transfer-overhead
functional
ingredients
given
low-level
properties
learning
algorithm
overall
goal
learning
learning
path
look
like
kind
behavior
would
considered
stepping
stone
towards
machine
intelli-
gence
way
describe
precise
way
even
basic
question
means
machine
algorithm
intelligent
needs
clariﬁcation
according
goal-directed
behavior
considered
essence
intelligence
however
implies
necessary
suf-
ﬁcient
condition
intelligent
behavior
rationality
paper
questions
statement
humans
often
far
rational
creativity
fall
deﬁnition
risk-taking
might
rational
yet
essential
innovation
therefore
far
appealing
theories
universal
intel-
ligence
broader
priors
theory
curiosity
creativity
beauty
described
schmidhuber
previous
section
introduces
problems
may
arise
objective
based
learning
chinese
room
argument
algorithm
attempts
map
inputs
outputs
without
motivation
learn
anything
beyond
task
given
intelligent
algorithm
strong
among
names
able
reveal
hidden
knowledge
might
even
discoverable
humans
section
de-
scribes
functional
ingredients
learning
procedure
would
violate
generality
assumption
3.1
compression
learning
may
likened
formal
information-theory
based
concept
in-
formation
compression
assuming
goal
build
compact
useful
representations
environment
ﬁnding
minimum
en-
tropy
codes
interpretation
relates
representation
learning
anal-
ogy
building
compression
scheme
neocortex
one
way
looking
towards
machine
intelligence
task
considering
general
artiﬁcial
intelligence
general
purpose
compres-
sor
one
able
discover
probability
distribution
source
however
free
lunch
theorem
states
completely
general-
purpose
learning
algorithm
exist
words
given
compressor
exists
data
distribution
perform
poorly
implies
must
exist
restrictions
class
problems
learning
sys-
tem
address
well
previous
section
already
mentioned
fortunately
general
plausible
smoothness
prior
depth
prior
also
see
complete
list
sensible
assumptions
3.2
prediction
whereas
smoothness
prior
may
considered
type
spatial
coherence
assumption
world
mostly
predictable
corresponds
temporal
generally
spatiotemporal
coherence
probably
important
ingredient
general-purpose
learning
procedure
assumption
states
things
close
time
close
space
vice
versa
purely
spatial
analogy
huge
image
space
yet
tiny
fraction
possible
real
images
true
spatiotemporal
patterns
assumption
sequence
spatial
patterns
coherent
restricts
spectrum
future
spatial
states
likely
occam
razor
rule
mdl
principle
state
simple
solutions
favored
complex
ones
therefore
learning
better
representations
goal
even
without
objective
assumed
task
given
priori
best
observe
learn
predict
one
ﬁrst
working
examples
proof
concept
principle
history
compression
employed
recurrent
architecture
proposed
schidmuber
3.3
understanding
ability
predict
equivalent
understanding
since
given
moment
cause
prediction
could
inferred
given
state
context
therefore
learning
predict
may
general
requirement
intelligent
behav-
ior
fact
postulated
brain
constantly
predict
future
states
compare
predictions
sensory
inputs
read-
accordingly
might
seem
equivalent
backpropagating
error
entire
network
however
biological
perspective
prediction/expectation
readjustment
neurons
likely
operating
locally
3.4
sensorimotor
scientists
demonstrated
brain
predicts
consequences
eye
movements
based
see
next
ﬁndings
implications
un-
derstanding
human
attention
applications
robotics
despite
fact
practice
experienced
perceived
twice
human
brains
able
form
kamil
rocki
stable
representation
abstract
concepts
make
accurate
predictions
de-
spite
changes
context
mental
representations
help
explain
rapid
eye
movements
known
saccades
eyes
move
rapidly
approximately
three
times
second
order
capture
new
visual
information
jump
new
image
falls
onto
retina
however
experience
quickly-
changing
sequence
images
instead
see
stable
image
fig
brain
uses
mechanism
order
redirect
attention
since
approximately
retina
provides
sharp
image
fovea
operation
extensively
researched
neuroscientiﬁc
perspective
provides
one
visible
brain
activities
sensorimotor
connections
needed
order
know
changes
image
result
internal
eye
movement
one
hypothesis
basic
repeating
functional
unit
neocor-
tex
sensorimotor
model
every
part
brain
performs
sensory
motor
processing
extent
complex
cells
visual
cortex
invariant
small
changes
inputs
patterns
might
mapped
purely
spatially
may
represent
spatiotemporal
patterns
i.e
invariant
rep-
resentation
given
action
experiments
support
claim
showing
similar
mechanism
operating
diﬀerent
type
sensory
inputs
3.5
spatiotemporal
invariance
thinking
motor
command
abstract
way
possible
show
order
disambiguate
multiple
predictions
one
needs
inject
addi-
tional
context
paper
assumes
predictions
associated
uncertainty
bayesian
approach
instead
assuming
single
point
prediction
distribution
highly
multimodal
additional
con-
text
equivalent
integrating
evidence
makes
predictions
speciﬁc
need
abstract
spatiotemporal
concepts
illustrated
simple
example
given
two
images
fig
obvious
classiﬁcation
based
purely
spatial
aspect
pattern
inadequate
much
natural
way
grouping
two
objects
function
requires
ability
imagine
whether
particular
object
used
certain
way
case
open
door
considerations
apply
objects
chairs
much
natural
learn
concepts
spatiotemporal
ideas
rather
predominantly
depend
spatial
appearance
considering
ability
imagine/dream/hallucinate
widespread
implementation
sensorimotor
functionality
brain
surprising
concept
manipulating
compact
spatiotemporal
thought
might
necessary
reasoning
perspec-
tive
transfer
learning
majority
analogies
make
temporal
nature
importance
learning
transformations
real-world
recognized
research
community
15,22,48,65,69,70,75
still
needs
attention
towards
machine
intelligence
fig
face
example
spatiotemporal
concept
micro-saccades
sequences
low-level
spatial
patterns
fovea
pooled
temporally
mid-level
concept
eye
nose
macro-saccades
task-oriented
movement
moving
nose
eyes
mouth
fig
example
spatiotemporal
concept
low
level
spatial
featuresmicro
saccadesmacro
saccadesmid
level
spatiotemporal
featureshigh
level
spatiotemporal
concept
kamil
rocki
3.6
context
update/pattern
completion
last
functional
component
postulated
paper
continuous
the-
ory
loop
bottom-up
predictions
top-down
context
hypothesis
interconnectedness
enables
perceptual
completion
higher
lay-
ers
make
hypotheses
inferences
coming
lower
layers
predictions
iteratively
reﬁned
based
hypotheses
may
likened
working
memory
theory
non-episodic
memories
held
in-
volving
hippocampus
analogy
expectation
maximization
learning
procedure
commonly
used
boltzmann
machines
samples
obtained
iteratively
alternating
unit
activations
two
connected
layers
35,57
see
fig
real-world
analogy
process
solving
cross-
word
sudoku
puzzle
ﬁlling
missing
words
sentence
problems
may
require
iterative
solution
reﬁnement
procedure
fig
illustration
iterative
context
update
every
prediction
changes
context
slightly
vice-versa
acknowledgments
partial
support
work
provided
defense
advanced
research
projects
agency
darpa
would
like
thank
members
machine
intel-
ligence
group
ibm
research
numenta
suggestions
many
interesting
discussions
hypothesishypothesis
hypothesis
inputpredictionprediction
prediction
towards
machine
intelligence
references
ahmad
hawkins
properties
sparse
distributed
representations
application
hierarchical
temporal
memory
2015
baboulin
donfack
dongarra
grigori
rmy
tomov
class
communication-avoiding
algorithms
solving
general
dense
linear
systems
cpu/gpu
parallel
machines
procedia
computer
science
2012
http
//www.sciencedirect.com/science/article/pii/s187705091200124x
proceed-
ings
international
conference
computational
science
iccs
2012
barlow
redundancy
reduction
revisited
network
computation
neural
systems
241–253
2001
http
//dx.doi.org/10.1080/net.12.3.241.253
pmid
11563528
barlow
h.b
unsupervised
learning
neural
computation
295–311
1989
barlow
h.b.
kaushal
t.p.
mitchison
g.j
finding
minimum
entropy
codes
neural
computation
412–423
1989
barlow
h.b.
kaushal
t.p.
mitchison
g.j
finding
minimum
entropy
codes
neural
computation
412–423
1989
http
//dblp.uni-trier.de/db/
journals/neco/neco1.html
barlowkm89
bell
a.j.
sejnowski
t.j.
independent
components
natural
scenes
edge
ﬁlters
vision
research
3327–3338
1997
bengio
learning
deep
architectures
foundations
trends
machine
learning
1–127
2009
also
published
book
publishers
2009
bengio
deep
learning
representations
looking
forward
statistical
lan-
guage
speech
processing
1–37
springer
2013
10.
bengio
courville
vincent
representation
learning
review
new
perspectives
ieee
trans
pattern
anal
mach
intell
1798–1828
aug
2013
http
//dx.doi.org/10.1109/tpami.2013.50
11.
bengio
courville
a.c.
vincent
unsupervised
feature
learning
deep
learning
review
new
perspectives
corr
abs/1206.5538
2012
http
arxiv.org/abs/1206.5538
12.
bengio
lecun
scaling
learning
algorithms
towards
bottou
chapelle
decoste
weston
eds
large
scale
kernel
machines
mit
press
2007
http
//www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_
chapter2007.pdf
13.
bengio
monperrus
discovering
shared
structure
manifold
learning
2004
14.
bottou
machine
learning
machine
reasoning
essay
ma-
chine
learning
133–149
january
2014
http
//leon.bottou.org/papers/
bottou-mlj-2013
15.
boulanger-lewandowski
bengio
vincent
modeling
temporal
depen-
dencies
high-dimensional
sequences
application
polyphonic
music
generation
transcription
proceedings
twenty-nine
international
conference
machine
learning
icml
acm
2012
http
//icml.cc/discuss/2012/590
html
16.
cand
e.j.
romberg
j.k.
tao
stable
signal
recovery
incomplete
inaccurate
measurements
comm
pure
appl
math
1207–1223
aug
2006
http
//dx.doi.org/10.1002/cpa.20124
17.
deng
deep
learning
methods
applications
foundations
trends
signal
processing
3–4
197–387
2014
kamil
rocki
18.
diamond
m.e.
von
heimendahl
knutsen
p.m.
kleinfeld
ahissar
whisker
sensorimotor
system
nat
rev
neurosci
601–612
aug
2008
http
//dx.doi.org/10.1038/nrn2411
19.
dlugosch
brown
glendenning
leventhal
noyes
eﬃcient
scalable
semiconductor
architecture
parallel
automata
processing
parallel
distributed
systems
ieee
transactions
3088–3098
2014
20.
domingos
master
algorithm
quest
ultimate
learning
machine
remake
world
penguin
books
limited
2015
https
//books
google.com/books
id=pjrkcqaaqbaj
21.
donoho
d.l
compressed
sensing
ieee
trans
inf
theor
1289–1306
apr
2006
http
//dx.doi.org/10.1109/tit.2006.871582
22.
elman
j.l
finding
structure
time
cognitive
science
179–211
1990
23.
erhan
bengio
courville
manzagol
p.a.
vincent
bengio
unsupervised
pre-training
help
deep
learning
journal
machine
learning
research
625–660
2010
24.
f¨oldi´ak
young
m.p
sparse
coding
primate
cortex
arbib
m.a
handbook
brain
theory
neural
networks
895–898
mit
press
1995
25.
goodfellow
courville
bengio
deep
learning
2015
http
//goodfeli
github.io/dlbook/
book
preparation
mit
press
26.
graves
liwicki
fernandez
bertolami
bunke
schmidhuber
novel
connectionist
system
improved
unconstrained
handwriting
recognition
ieee
transactions
pattern
analysis
machine
intelligence
2009
27.
graves
jaitly
towards
end-to-end
speech
recognition
recurrent
neural
networks
proc
31st
international
conference
machine
learning
icml
1764–1772
2014
28.
gregor
lecun
learning
representations
maximizing
compression
corr
abs/1108.1169
2011
http
//arxiv.org/abs/1108.1169
29.
hawkins
ahmad
neurons
thousands
synapses
theory
sequence
memory
neocortex
arxiv
preprint
arxiv:1511.00083
2015
30.
hawkins
blakeslee
intelligence
times
books
2004
31.
hinton
g.e.
sejnowski
t.e
learning
relearning
boltzmann
machines
parallel
distributed
processing
vol
282–317
mit
press
1986
32.
hinton
sejnowski
unsupervised
learning
foundations
neural
com-
putation
bradford
book
mcgraw
hill
book
company
1999
https
//books.google.com/books
id=yj04y0lje4cc
33.
hinton
salakhutdinov
reducing
dimensionality
data
neural
networks
science
313
5786
504–507
2006
34.
hinton
g.e
learning
representations
unlearning
beliefs
http
//www.ircs
upenn.edu/pinkel/lectures/hinton/hinton_pinkeltranscription_2003.pdf
2003
online
accessed
23-november-2015
35.
hinton
g.e
practical
guide
training
restricted
boltzmann
machines
montavon
orr
g.b.
mller
k.r
eds
neural
networks
tricks
trade
2nd
lecture
notes
computer
science
vol
7700
599–619
springer
2012
http
//dblp.uni-trier.de/db/series/lncs/lncs7700.html
hinton12
36.
hyv¨arinen
hurri
hoyer
p.o
natural
image
statistics
probabilistic
approach
early
computational
vision.
vol
39.
springer
science
business
media
2009
37.
hyv¨arinen
karhunen
oja
independent
component
analysis
john
wiley
sons
2001
towards
machine
intelligence
38.
kanerva
sparse
distributed
memory
mit
press
cambridge
usa
1988
39.
kowler
eye
movements
past
years
vision
research
1457
1483
2011
http
//www.sciencedirect.com/science/article/pii/
s0042698910005924
vision
research
50th
anniversary
issue
part
40.
krieger
groh
sensorimotor
integration
whisker
system
springer
publishing
company
incorporated
1st
edn
2015
41.
krizhevsky
sutskever
hinton
g.e
imagenet
classiﬁcation
deep
convolutional
neural
networks
pereira
burges
bottou
wein-
berger
eds
advances
neural
information
processing
systems
1097–1105
curran
associates
2012
http
//papers.nips.cc/paper/
4824-imagenet-classification-with-deep-convolutional-neural-networks
pdf
inc.
42.
kurzweil
create
mind
secret
human
thought
re-
vealed
penguin
publishing
group
2012
https
//books.google.com/books
id=
fccxibpurdec
43.
lecun
wrong
deep
learning
http
//www.pamitc.org/
cvpr15/files/lecun-20150610-cvpr-keynote.pdf
2015
online
accessed
20-
november-2015
44.
lecun
bengio
hinton
deep
learning
nature
521
7553
436–444
may
2015
http
//dx.doi.org/10.1038/nature14539
insight
45.
lee
ekanadham
a.y
sparse
deep
belief
net
model
visual
area
advances
neural
information
processing
systems
nips
vol
873–880
2007
46.
vitnyi
p.m.
introduction
kolmogorov
complexity
appli-
cations
springer
publishing
company
incorporated
edn
2008
47.
mackay
d.j.c
information
theory
inference
learning
algorithms
cam-
bridge
university
press
2003
http
//www.cambridge.org/0521642981
48.
memisevic
hinton
g.e
learning
represent
spatial
transformations
factored
higher-order
boltzmann
machines
neural
computation
1473–1492
2010
49.
meyniel
schlunegger
dehaene
sense
conﬁdence
prob-
abilistic
learning
normative
account
plos
comput
biol
e1004305
2015
http
//dx.doi.org/10.1371
2fjournal.pcbi.1004305
50.
meyniel
sigman
mainen
conﬁdence
bayesian
probability
neural
origins
behavior
neuron
78–92
2015/11/25
xxxx
http
//dx
doi.org/10.1016/j.neuron.2015.09.039
51.
mnih
kavukcuoglu
silver
rusu
a.a.
veness
bellemare
m.g.
graves
riedmiller
fidjeland
a.k.
ostrovski
petersen
beattie
sadik
antonoglou
king
kumaran
wierstra
legg
hassabis
human-level
control
deep
reinforcement
learning
nature
518
7540
529–533
feb
2015
http
//dx.doi.org/10.1038/nature14236
letter
52.
modha
d.s.
singh
network
architecture
long-distance
pathways
macaque
brain
proceedings
national
academy
sciences
107
13485–
13490
2010
53.
mohamed
dahl
g.e.
hinton
g.e
deep
belief
networks
phone
recogni-
tion
nips
workshop
deep
learning
speech
recognition
2009
54.
mountcastle
v.b
organizing
principle
cerebral
function
unit
model
distributed
system
edelman
g.m.
mountcastle
v.v
eds
mindful
brain
7–50
mit
press
cambridge
1978
kamil
rocki
55.
man
behind
google
brain
andrew
2013
http
//www.wired.com/2013/05/
http
//www.wired.com/2013/05/neuro-
new
quest
jul
neuro-artificial-intelligence/
artiﬁcial-intelligence/
56.
olshausen
b.a.
field
d.j
sparse
coding
overcomplete
basis
set
strat-
egy
employed
vision
research
3311–3325
1997
57.
resnik
hardisty
gibbs
sampling
uninitiated
tech
rep.
dtic
document
2010
58.
rissanen
modeling
shortest
data
description
automatica
465–471
1978
59.
roe
a.w.
pallas
s.l.
kwon
y.h.
sur
visual
projections
routed
auditory
pathway
ferrets
receptive
ﬁelds
visual
neurons
primary
auditory
cortex
journal
neuroscience
3651–3664
1992
60.
rolfs
jonikaitis
deubel
cavanagh
predictive
remapping
at-
tention
across
eye
movements
nat
neurosci
252–256
feb
2011
http
//dx.doi.org/10.1038/nn.2711
61.
russell
s.j.
norvig
artiﬁcial
intelligence
modern
approach
pearson
ed-
ucation
edn
2003
62.
saul
l.k.
roweis
s.t
think
globally
locally
unsupervised
learning
low
di-
mensional
manifolds
journal
machine
learning
research
119–155
2003
63.
schmidhuber
learning
complex
extended
sequences
using
principle
his-
tory
compression
neural
computation
234–242
1992
64.
schmidhuber
simple
algorithmic
principles
discovery
subjective
beauty
se-
lective
attention
curiosity
creativity
proc
18th
intl
conf
algorithmic
learning
theory
alt
2007
lnai
4754.
32–33
springer
2007
joint
invited
lecture
alt
2007
2007
sendai
japan
2007
65.
schmidhuber
deep
learning
neural
networks
overview
neural
networks
85–117
2015
published
online
2014
based
arxiv:1404.7828
cs.ne
66.
searle
minds
brains
science
reith
lectures
harvard
university
press
1984
https
//books.google.com/books
id=ynjn-_jznw4c
67.
solomonoﬀ
r.j.
formal
theory
inductive
inference
part
information
control
1–22
1964
68.
stanley
k.o.
lehman
greatness
planned
myth
objective
springer
2015
http
//dx.doi.org/10.1007/978-3-319-15524-1
69.
sutskever
hinton
learning
multilevel
distributed
representations
high-
dimensional
sequences
aistats
2007
http
//machinelearning.wustl.edu/
mlpapers/paper_files/aistats07_sutskeverh.pdf
70.
sutskever
hinton
g.e.
taylor
g.w
recurrent
temporal
restricted
boltz-
mann
machine
nips
vol
2008
2008
71.
szegedy
liu
jia
sermanet
reed
anguelov
erhan
vanhoucke
rabinovich
going
deeper
convolutions
2014
72.
villa
johnson
d.r.
connor
bolotin
nellans
luitjens
sakharnykh
wang
micikevicius
scudiero
keckler
s.w.
dally
w.j
scaling
power
wall
path
exascale
proceedings
in-
ternational
conference
high
performance
computing
networking
storage
analysis
830–841
ieee
press
piscataway
usa
2014
http
//dx.doi.org/10.1109/sc.2014.73
73.
watts
d.j.
strogatz
s.h
collective
dynamics
small-world
networks
nature
393
6684
409–10
1998
74.
wiesel
d.h.
hubel
t.n
receptive
ﬁelds
single
neurones
cat
striate
cortex
physiol
148
574–591
1959
towards
machine
intelligence
75.
wiskott
sejnowski
slow
feature
analysis
unsupervised
learning
invari-
ances
neural
computation
715–770
2002
76.
wolpert
d.h.
macready
w.g
free
lunch
theorems
optimization
trans
evol
comp
67–82
apr
1997
http
//dx.doi.org/10.1109/4235.585893
