characterization
neighborhood
behaviours
multi-neighborhood
local
search
algorithm
nguyen
thi
thanh
dang
patrick
causmaecker
leuven
kulak
codes
iminds-itec
nguyenthithanh.dang
patrick.decausmaecker
kuleuven-kulak.be
abstract
consider
multi-neighborhood
local
search
algorithm
large
number
possible
neighborhoods
neighborhood
accompanied
weight
value
represents
probability
chosen
iteration
weights
ﬁxed
algorithm
runs
considered
parameters
algorithm
given
set
instances
oﬀ-line
tuning
algorithm
parameters
done
automated
algorithm
conﬁguration
tools
e.g.
smac
however
large
number
neighborhoods
make
tuning
expensive
dif-
ﬁcult
even
number
parameters
reduced
intuition
work
propose
systematic
method
characterize
neighborhood
behaviours
representing
feature
vector
using
cluster
analysis
form
similar
groups
neighborhoods
novelty
characterization
method
ability
reﬂecting
changes
behaviours
according
hardness
diﬀerent
solution
quality
regions
show
using
neighborhood
clusters
instead
individual
neighbor-
hoods
helps
reduce
parameter
conﬁguration
space
without
mis-
leading
search
tuning
procedure
moreover
method
problem-independent
potentially
applied
similar
contexts
keywords
algorithm
conﬁguration
clustering
multi-neighborhood
lo-
cal
search
introduction
optimization
algorithms
usually
highly
parameterized
algorithm
parameter
tuning/conﬁguration
important
task
given
distribution
problem
instances
need
ﬁnd
parameter
conﬁgurations
optimize
pre-
deﬁned
performance
measure
distribution
mean
optimality
gap
last
ﬁfteen
years
automated
algorithm
conﬁguration
ex-
tensively
studied
general-purpose
automated
algorithm
conﬁguration
tools
smac
irace
successfully
applied
several
studies
work
consider
parameter
tuning
problem
multi-neighborhood
local
search
algorithm
consists
large
number
neighborhoods
algorithm
winner
verolog
solver
challenge
2014
iteration
neighbor
solution
generated
randomly
chosen
neighborhood
probability
deﬁned
weight
value
range
0,1
weights
n.t.t.dang
p.de
causmaecker
neighborhoods
ﬁxed
algorithm
runs
considered
algo-
rithm
parameters
given
set
six
large
instances
provided
challenge
automated
algorithm
conﬁguration
tools
used
tune
algorithm
pa-
rameters
however
large
number
parameters
real
parameters
weight
values
integer
parameters
local
search
might
deteriorate
tuning
tool
eﬃciency
especially
case
run
algo-
rithm
computationally
cheap
600
seconds
per
run
instance
potential
solution
cluster
neighborhoods
groups
assign
common
weight
value
help
reduce
algorithm
conﬁguration
space
hoping
make
use
available
tuning
budgets
eﬃciently
key
question
raised
clustering
characterize
neighborhood
behaviours
set
instances
represent
feature
vector
paper
propose
method
method
problem-independent
depend
speciﬁc
local
search
moreover
done
stages
algorithm
development
e.g.
testing
manual/automated
tuning
paper
organized
follows
describe
tuning
problem
detail
section
method
characterizing
neighborhoods
behaviours
clustering
explained
section
section
shows
advantage
using
clustering
automated
parameter
tuning
experimental
results
finally
section
gives
conclusion
discussion
future
work
parameter
tuning
multi-neighborhood
local
search
algorithm
algorithm
considered
work
developed
codes
group
members
university
leuven
belgium
tackles
swap-body
ve-
hicle
routing
problem
iterated
local
search
algorithm
uses
late
acceptance
hill
climbing
local
search
component
iteration
late
acceptance
hill
climbing
neighborhood
randomly
chosen
large
set
neighborhoods
neighbor
solution
generated
according
probability
neighborhood
chosen
proportional
weight
value
weight
values
ﬁxed
algorithm
run
sum
one
addition
two
integer
parameters
control
late
acceptance
hill
climbing
local
search
component
stopped
number
itw
consecutive
iterations
without
improvement
current
solution
parameter
lalist
represents
size
saved
memory
algorithm
consists
neighborhoods
generated
neighborhood
types
specially
designed
swap-body
vehicle
routing
problem
e.g.
convert-to-sub-route
others
taken
vehicle
routing
problem
literature
e.g.
cheapest-insertion
neighborhood
types
parameterized
sizes
example
size
cheapest-insertion
neighborhood
deﬁned
number
customers
removed
re-inserted
back
current
solution
small
cheapest-insertion
neighborhood
size
large
cheapest-insertion
neighborhood
size
25.
characterization
neighborhood
behaviours
intuition
used
reduce
number
weights
neighbor-
hoods
belong
neighborhood
type
similar
sizes
grouped
one
list
neighborhood
types
groups
sizes
listed
table
parameter
tuning
done
six
large
problem
instances
large
normal
large
large
without
new
normal
new
new
without
provided
competition
algorithm
run
instance
takes
600
sec-
onds
note
algorithm
considered
paper
actually
one
competition
winning
one
multi-threaded
in-
dependent
parallel
runs
one
use
single-threaded
aim
work
beat
winning
algorithm
use
case
study
proof
concept
characterization
method
table
neighborhood
types
neighborhoods
generated
neigh-
borhoods
sizes
line
grouped
one
reduce
number
weight-value
parameters
28.
neighborhood
type
cheapest-insertion
swap
intra-route-two-opt
inter-route-two-opt
change-swap-location
merge-route
split-to-sub-routes
ruin-recreate
remove-route
remove-sub-route
remove-sub-route-with-cheapest-insertion
remove-chain
each-sequence-cheapest-insertion
convert-to-route
convert-to-sub-route
add-sub-route
ejection-chain
sizes
2,5
5,2
4,4
n.t.t.dang
p.de
causmaecker
neighborhood
characterization
clustering
inspired
idea
oscar
automated
approach
online
selection
algorithm
portfolio
characterize
neighborhood
behaviours
instance
based
following
six
observables
probabilities
improves
worsens
nothing
solution
denoted
rimprove
rworsen
rnothing
rimprove
rworsen
rnothing
magnitudes
improvement
worsening
denoted
aimprove
aworsen
running
time
used
tie-breaking
explained
section
3.3
novelty
method
represent
using
estimated
values
observables
diﬀerent
solution
quality
regions
reﬂect
changes
behaviours
according
hardness
solution
dealing
illustration
changes
rimprove
rworsen
rnothing
four
neighborhoods
instance
visualized
figure
x-axis
represents
solution
quality
larger
value
better
corresponding
solution
y-axis
represents
values
three
observables
order
draw
plots
divide
range
solution
quality
intervals
collect
necessary
in-
formation
algorithm
runs
group
every
ten
intervals
one
details
collect
information
visualization
described
sec-
tion
3.1.
figure
see
solution
quality
low
i.e.
local
search
easy-to-improve
region
solution
quality
space
merge-
route
neighborhood
high
probability
improving
solution
tackling
probability
drastically
decreases
neighborhood
reaches
good
solution
quality
region
probability
worsening
current
so-
lution
starts
reaching
one
point
hand
remove-route
neighborhood
figure
shows
similar
behaviour
low-solution
quality
region
however
good-solution
quality
region
neighborhood
tends
preserve
quality
current
solution
rather
worsen
even
neigh-
borhoods
belonging
neighborhood
type
behave
diﬀerently
diﬀerent
regions
shown
figure
small
cheapest-insertion
neighborhood
size
much
smaller
probability
worsening
solu-
tion
hard-to-improve
region
compared
large
cheapest-insertion
neighborhood
size
25.
rest
section
introduce
four
steps
characterize
clus-
ter
neighborhoods
firstly
necessary
information
collected
algorithms
runs
solution
quality
regions
automatically
identiﬁed
next
collected
information
region
aggregated
build
neighborhoods
feature
vectors
finally
carry
cluster
analysis
3.1
collect
necessary
information
algorithm
runs
part
describe
procedure
collecting
necessary
information
characterizing
neighborhood
behaviours
given
problem
instance
assume
characterization
neighborhood
behaviours
cheapest
insertion
size
cheapest
insertion
size
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
remove
route
merge
route
rworsening
rnothing
rimprove
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
fig
visualization
rimprove
rworsen
rnothing
four
neighborhoods
x-axis
represents
solution
quality
larger
value
better
corresponding
solution
y-axis
represents
values
three
observables
upper
bound
lower
bound
optimal
solution
quality
available
since
bounds
need
tight
assumption
hard
satisﬁed
example
upper
bound
could
obtained
random
solution
solution
generated
greedy
algorithm
lower
bound
could
result
solving
linear
programming
relaxation
problem
algorithm
considered
work
initial
solution
instance
produced
creating
one
route
customer
take
solution
value
upper
bound
lower
bound
instance
provided
authors
algorithm
best
solution
obtained
running
best
algorithm
conﬁguration
multi-threaded
version
six
hours
divide
range
upper
bound
lower
bound
large
number
small
intervals
set
1000
higher
quality
solutions
general
harder
improve
let
size
intervals
decrease
exponentially
next
interval
size
0.99
size
previous
interval
every
time
neighborhood
applied
solution
qual-
ity
value
belonging
interval
following
values
accumulatively
collected
pair
niters
number
times
applied
nsn
numbers
times
improves
nothing
worsens
solutions
respectively
sums
amount
improvement
worsening
stime
sum
running
time
since
collection
values
independent
algorithm
conﬁguration
done
algorithm
runs
testing
manual
pa-
rameter
tuning
automated
algorithm
tuning
runs
n.t.t.dang
p.de
causmaecker
better
estimated
values
observables
work
collect
running
two
algorithm
conﬁgurations
instances
runs
per
instance
total
number
algorithm
runs
240.
use
little
bit
longer
running
time
900
seconds
per
run
make
sure
collected
information
cover
hard
parts
solution
quality
space
3.2
identify
solution
quality
regions
frames
intervals
grouped
frames
based
sum
niters
sum
neighbor-
hoods
niters
values
interval
figure
shows
plots
sum
niters
instance
note
lower
bounds
solution
quality
reached
intervals
zero
sum
niters
end
removed
ﬁgure
high
peak
every
plot
representing
interval
algorithm
stays
time
thus
conjecture
local
optima
plateau
lie
interpret
solution
quality
regions
low
sum
niters
values
peak
easy-to-reach
easy-to-escape
whereas
regions
around
peak
easy-to-reach
hard-to-escape
regions
peak
hard-to-reach
smaller
peaks
two
instances
new
large
indicate
sec-
ond
local
optima
plateau
propose
algorithm
grouping
intervals
rames
regions
frames
tries
reﬂect
interpretation
figure
shows
identiﬁed
frames
rames
used
experiments
six
provided
instances
1.5e+07
1.0e+07
5.0e+06
0.0e+00
4e+07
3e+07
2e+07
1e+07
0e+00
4e+07
3e+07
2e+07
1e+07
new_normal
large_normal
1.5e+07
1.0e+07
5.0e+06
100
200
300
interval
new_with
400
500
0.0e+00
4e+07
3e+07
400
200
interval
large_with
2e+07
1e+07
200
interval
400
600
200
400
600
interval
0e+00
new_without
large_without
4e+07
3e+07
2e+07
1e+07
0e+00
0e+00
100
200
interval
300
400
100
200
interval
300
400
fig
sum
niters
instance
x-axis
represents
solution
intervals
y-axis
shows
sum
niters
characterization
neighborhood
behaviours
algorithm
group
intervals
frames
input
array
sum
niters
values
nintervals
number
intervals
removing
empty
ending
intervals
rames
number
frames
output
rames-element
array
element
contains
index
last
interval
frame
/nf
rames
avg
ninterval
i=1
else
0.05
avg
pnintervals
true
0.01
end
else
let
largest
value
j=i
end
contains
less
rames
elements
contains
rames
elements
starting
ﬁrst
frame
combine
every
pair
frames
one
repeat
ﬁrst
new
frame
necessary
rames
elements
left
end
break
end
end
return
1.5e+07
1.0e+07
5.0e+06
0.0e+00
4e+07
3e+07
2e+07
1e+07
new_normal
large_normal
1.5e+07
1.0e+07
5.0e+06
100
200
300
interval
new_with
400
500
0.0e+00
4e+07
3e+07
400
200
interval
large_with
2e+07
1e+07
0e+00
4e+07
3e+07
2e+07
1e+07
200
interval
400
600
200
400
600
interval
0e+00
new_without
large_without
4e+07
3e+07
2e+07
1e+07
0e+00
0e+00
100
200
interval
300
400
100
200
interval
300
400
fig
sum
niters
instance
frame
boundaries
shown
vertical
lines
rames
x-axis
represents
solution
intervals
y-axis
shows
sum
niters
n.t.t.dang
p.de
causmaecker
3.3
characterize
neighborhood
behaviours
feature
vectors
aggregating
collected
information
frames
ﬁrst
three
observables
rimprove
rworsen
rnothing
simply
sum
three
values
nsn
intervals
belonging
frame
divide
sum
niters
get
ratios
two
observables
aimprove
aworsen
aggregation
complicated
sum
values
intervals
get
average
due
fact
values
incomparable
among
diﬀerent
intervals
example
say
amount
improvement
two
intervals
33762
33621
33621
33482
equal
since
hardness
solutions
belonging
probably
therefore
translate
ranks
ag-
gregation
interval
neighborhoods
ranked
based
averages
corresponding
values
ties
happen
e.g.
neighbor-
hoods
might
never
make
improvement
hard
solution
quality
regions
average
value
stime
corresponding
interval
used
tie-breaking
since
intervals
ﬁne
resulting
ranked
lists
possibly
noisy
intervals
niters
neighborhoods
might
small
estimated
values
aimprove
aworsen
might
inaccurate
partial
niters
neighborhoods
might
equal
zero
in-
tervals
i.e.
information
neighborhoods
intervals
therefore
aggregate
using
package
robustrankaggreg
ro-
bust
ranking
aggregation
method
specially
designed
similar
situations
bioinformatics
eventually
neighborhood
feature
vector
com-
posing
150
components
combination
observables
frames
instances
3.4
cluster
analysis
neighborhoods
ﬁrst
three
observables
rimprove
rworsen
rnothing
sum
one
result
corresponding
vector
components
belong
special
class
named
compositional
data
explained
sample
space
compositional
vec-
tors
radically
diﬀerent
real
euclidean
space
associated
uncon-
strained
data
multivariate
statistical
methodology
designed
unconstrained
data
could
applied
directly
convert
back
euclidean
space
apply
isometric
log-ratio
transformation
proposed
trans-
formation
since
three
observables
reduced
two
feature
vector
120-dimensional
start
cluster
analysis
neighborhoods
based
vectors
since
number
dimensions
120
larger
number
individuals
clustering
method
high-dimensional
data
clus-
tering
hddc
implemented
package
hdclassif
used
cluster
analysis
method
two
desirable
properties
ability
dealing
high-dimensional
low-sample
data
optimal
number
clusters
automatically
decided
based
bayesian
information
criterion
end
neighborhoods
grouped
clusters
characterization
neighborhood
behaviours
ejection-chain
remove-chain
remove-sub-route-
with-cheapest-insertion
swap
inter-route-two-opt
cheapest-insertion
each-sequence-cheapest-insertion
2,5
4,4
5,2
remove-chain
cheapest-insertion
change-swap-location
merge-route
add-sub-route
convert-to-sub-route
ejection-chain
remove-chain
intra-route-two-opt
ruin-recreate
convert-to-route
remove-sub-route
remove-route
split-to-sub-route
might
interesting
look
resulting
clusters
two
neighborhoods
merge-route
remove-route
behaves
quite
diﬀerently
second-half
region
shown
figure
indeed
clustered
two
diﬀerent
groups
taking
look
neighborhoods
implementation
know
add-sub-route
convert-to-sub-route
extreme
behaviour
compared
others
add
additional
cost
current
solution
worsen
time
also
seen
plots
observables
similar
ones
shown
figure
say
cluster
analysis
recognize
extremeness
two
neighborhoods
grouped
separated
cluster
addition
reﬂecting
knowledge
guessed
looking
neighborhoods
implementation
cluster
analysis
also
grouping
intuitive
neighborhoods
structure
e.g.
grouping
ejection-chain
10,15,35
intra-route-two-opt
experimental
results
hypothesis
proposed
characterization
method
reﬂect
neigh-
borhood
behaviours
given
set
instances
generated
feature
vectors
correctly
represent
neighborhoods
clusters
ob-
tained
meaningful
test
hypothesis
applied
automated
tuning
tool
smac
two
conﬁguration
scenarios
ﬁrst
one
dubbed
basic
uses
groups
neighborhoods
described
table
second
one
dubbed
clustered
uses
clusters
neighborhoods
generated
characteriza-
tion
method
carried
runs
smac
scenario
one
budget
2000
algorithm
runs
13.9
cpu
days
due
large
cpu
time
smac
run
requires
use
shared-model-mode
oﬀered
smac
cores
walltime
1.39
days
take
conﬁguration
best
training
performance
ﬁnal
one
mean
optimality
gaps
percentage
instance
set
used
tuning
performance
measure
optimality
gap
instance
calculated
optimalitygap
100
solutioncost
lowerbound
/lowerbound
lowerbound
provided
algorithm
authors
best
solu-
tion
cost
obtained
running
multi-threaded
version
algorithm
n.t.t.dang
p.de
causmaecker
corresponding
instance
hours
best
algorithm
conﬁguration
smac
run
evaluated
using
test
performance
mean
op-
timality
gaps
obtained
runs
conﬁguration
instance
set
runs/instance
box-plots
smac
runs
scenario
shown
figure
clustered
scenario
oﬀers
advantage
basic
scenario
paired
t-test
conducted
gives
value
0.009258918
indicating
sta-
tistical
signiﬁcance
2.7
2.6
2.5
2.4
2.3
basic
clustered
fig
test
performance
two
considered
scenarios
hyper-heuristic
community
particular
selection
hyper-heuristic
class
aim
manage
set
low-level
heuristics
search
selecting
one
iteration
simple
random
heuristic
se-
lection
mechanism
often
used
baseline
setting
equivalent
parameter
conﬁguration
identical
weights
neighborhoods
interesting
compare
resulting
conﬁgurations
obtained
oﬀ-line
tuning
scenario
best
tuned
conﬁgurations
taken
neighborhood
weights
inside
set
identical
test
performance
values
shown
basic
identical
weights
clustered
identical
weights
figure
horizontal
line
represents
test
performance
algorithm
conﬁguration
neighborhood
weights
identical
lalist
itiw
set
values
recommended
algorithm
authors
conﬁguration
also
used
default
conﬁguration
smac
runs
men-
tioned
see
versions
scenarios
give
worst
test
performance
paired
t-test
conducted
scenario
basic
basic
identical
weights
p-value
0.07464
clustered
clustered
identical
weights
p-value
0.000459
characterization
neighborhood
behaviours
p-value
second
t-test
indicates
neighborhoods
weights
inﬂuence
algorithm
performance
tests
also
reﬂect
hardness
tuning
weights
basic
tuning
fails
show
signiﬁcant
improvement
identical-weight
conﬁgurations
advantage
clustered
basic
2.7
2.6
2.5
2.4
2.3
basic
basic
identical
weights
clustered
clustered
identical
weights
fig
test
performance
two
considered
scenarios
versions
default
conﬁguration
conclusion
future
work
paper
proposed
systematic
method
characterize
neigh-
borhood
behaviours
multi-neighborhood
local
search
framework
probability
choosing
neighborhood
iteration
chosen
oﬀ-line
manner
characterization
based
probabilities
neighborhood
improve
worsen
nothing
solution
magnitudes
im-
provement
worsening
running
time
observed
characteristics
change
according
hardness
diﬀerent
regions
solution
qual-
ity
space
result
design
method
tries
detect
regions
based
collected
information
represent
neighborhood
behaviours
feature
vectors
cluster
analysis
applied
form
groups
similar
neighborhoods
tuning
experiment
automated
algorithm
con-
ﬁguration
tool
smac
shows
using
clusters
gives
statistically
signiﬁcant
improvement
test
performances
obtained
algorithm
conﬁg-
urations
non-clustered
version
veriﬁes
hypothesis
char-
acterization
method
able
correctly
reﬂect
neighborhood
behaviours
given
instance
set
since
information
used
method
depend
n.t.t.dang
p.de
causmaecker
speciﬁc
problem
characterization
clustering
procedure
potentially
applied
similar
contexts
ﬁrst
version
method
implementa-
tion
available
toolbox
obtained
sending
request
corresponding
author
toolbox
receives
log
ﬁles
containing
neces-
sary
information
collected
algorithm
runs
input
returns
results
cluster
analysis
well
box-plots
graphs
visualization
observables
solution
quality
regions
future
work
multi-level
tuning
might
interesting
firstly
post-
analysis
importance
cluster
using
fanova
eﬃ-
cient
approach
quantify
eﬀect
algorithm
parameters
applied
ﬁner
tuning
neighborhoods
belong
important
clusters
done
addition
since
current
method
limited
small
number
instances
seeking
possibility
extension
large
set
instances
might
want
exploit
problem-speciﬁc
expert
knowledge
e.g.
instance
features
case
acknowledgement
work
funded
comex
project
p7/36
belspo/iap
programme
would
like
thank
t´ulio
toﬀolo
great
support
course
research
thomas
st¨utzle
jan
verwaeren
valuable
remarks
computational
resources
services
used
work
provided
vsc
flemish
supercomputer
center
funded
hercules
foundation
flemish
government
department
ewi
references
holger
hoos
automated
algorithm
conﬁguration
parameter
tuning
autonomous
search
pages
37–71
springer
2012
frank
hutter
holger
hoos
kevin
leyton-brown
sequential
model-based
optimization
general
algorithm
conﬁguration
learning
intelligent
op-
timization
pages
507–523
springer
2011
manuel
l´opez-ib´anez
j´er´emie
dubois-lacoste
thomas
st¨utzle
mauro
bi-
rattari
irace
package
iterated
race
automatic
algorithm
conﬁguration
technical
report
citeseer
2011
tony
wauters
t´ulio
toﬀolo
jan
christiaens
sam
van
malderen
win-
ning
approach
verolog
solver
challenge
2014
swap-body
vehicle
routing
problem
proceedings
orbel29
2015
heid
hasle
vigo
verolog
solver
challenge
2014–vsc2014
problem
description
verolog
euro
working
group
vehicle
routing
logistics
optimization
ptv
group
pages
1–6
2014
helena
louren¸co
olivier
martin
thomas
st¨utzle
iterated
local
search
handbook
metaheuristics
pages
363–397
framework
applications
springer
2010
edmund
burke
yuri
bykov
late
acceptance
strategy
hill-climbing
exam
timetabling
problems
patat
2008
conference
montreal
canada
2008.
characterization
neighborhood
behaviours
mustafa
mısır
stephanus
daniel
handoko
hoong
chuin
lau
oscar
online
selection
algorithm
portfolios
case
study
memetic
algorithms
learning
intelligent
optimization
page
raivo
kolde
sven
laur
priit
adler
jaak
vilo
robust
rank
aggregation
gene
list
integration
meta-analysis
bioinformatics
:573–580
2012
10.
john
aitchison
concise
guide
compositional
data
analysis
2nd
composi-
tional
data
analysis
workshop
codawork05
2005
11.
juan
jos´e
egozcue
vera
pawlowsky-glahn
oria
mateu-figueras
carles
barcelo-vidal
isometric
logratio
transformations
compositional
data
analysis
mathematical
geology
:279–300
2003
12.
charles
bouveyron
st´ephane
girard
cordelia
schmid
high-dimensional
data
clustering
computational
statistics
data
analysis
:502–519
2007
13.
laurent
berg´e
charles
bouveyron
st´ephane
girard
hdclassif
package
model-based
clustering
discriminant
analysis
high-dimensional
data
2012
14.
edmund
burke
michel
gendreau
matthew
hyde
graham
kendall
gabriela
ochoa
ender
¨ozcan
rong
hyper-heuristics
survey
state
art
journal
operational
research
society
:1695–1724
2013
15.
frank
hutter
holger
hoos
kevin
leyton-brown
eﬃcient
approach
proceedings
31st
international
assessing
hyperparameter
importance
conference
machine
learning
icml-14
pages
754–762
2014
