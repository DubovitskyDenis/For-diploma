coco
experimental
procedure
nikolaus
hansen1,2
tea
tu≈°ar3
olaf
mersmann4
anne
auger1,2
dimo
brockhoff3
1inria
research
centre
saclay
france
2universit√©
paris-saclay
lri
france
3inria
research
centre
lille
france
4tu
dortmund
university
chair
computational
statistics
germany
abstract
present
budget-free
experimental
setup
procedure
benchmarking
numeri-
cal
optimization
algorithms
black-box
scenario
procedure
applied
coco
benchmarking
platform
describe
initialization
input
algorithm
touch
upon
relevance
termination
restarts
contents
introduction
1.1
terminology
conducting
experiment
2.1
initialization
input
algorithm
2.2
budget
termination
criteria
restarts
parameter
setting
tuning
algorithms
time
complexity
experiment
introduction
based
han2009
han2010
describe
comparatively
simple
experimental
setup
black-box
optimization
benchmarking
recommend
use
procedure
within
coco
platform
han2016co
central
measure
performance
experimental
procedure
adapted
number
calls
objective
function
reach
certain
solution
quality
function
value
ùëì-value
indicator
value
also
denoted
runtime
1.1
terminology
function
talk
objective
function
parametrized
mapping
scal-
able
input
space
yet
determined
usually
functions
parametrized
different
instances
function
available
e.g
trans-
lated
shifted
versions
problem
talk
problem
coco_problem_t
speciÔ¨Åc
function
instance
optimization
algorithm
run
speciÔ¨Åcally
problem
described
triple
dimension
function
instance
problem
evaluated
returns
ùëì-value
-vector
context
performance
assessment
target
indicator-value
attached
problem
target
value
added
triple
deÔ¨Åne
single
problem
case
runtime
deÔ¨Åne
runtime
run-length
hoo1998
number
evaluations
conducted
given
problem
also
referred
number
function
evaluations
central
performance
measure
runtime
given
target
value
hit
han2016perf
suite
test-
benchmark-suite
collection
problems
typically
twenty
hun-
dred
number
objectives
Ô¨Åxed
conducting
experiment
optimization
algorithm
benchmarked
run
problem
given
test
suite
problem
algorithm
parameter
setting
initialzation
procedure
budget
termination
and/or
restart
criteria
etc
used
prescribed
minimal
maximal
allowed
budget
benchmarking
setup
budget-free
longer
experiment
data
available
assess
performance
accurately
see
also
section
budget
termination
criteria
restarts
coco
platform
provides
several
single
bi-objective
test
suites
collection
black-box
optimiza-
tion
problems
different
dimensions
minimized
coco
automatically
collects
relevant
data
display
performance
results
post-processing
applied
2.1
initialization
input
algorithm
algorithm
use
following
input
information
problem
time
input
output
dimensions
deÔ¨Åning
interface
problem
speciÔ¨Åcally
search
space
input
dimension
via
coco_problem_get_dimension
number
objectives
via
coco_problem_get_number_of_objectives
output
dimension
coco_evaluate_function
functions
single
benchmark
suite
number
objectives
currently
either
one
two
number
constraints
via
coco_problem_get_number_of_constraints
output
dimension
coco_evaluate_constraint
problems
single
benchmark
suite
either
constraints
one
constraints
search
domain
interest
deÔ¨Åned
coco_problem_get_largest_values_of_interest
coco_problem_get_smallest_values_of_interest
optimum
extremal
solution
pareto
set
lies
within
search
domain
interest
optimizer
operates
bounded
domain
domain
interest
interpreted
lower
upper
bounds
feasible
initial
solution
provided
coco_problem_get_initial_solution
initial
state
optimization
algorithm
parameters
shall
based
input
values
initial
algorithm
setting
considered
part
algorithm
must
therefore
follow
procedure
problems
suite
problem
identiÔ¨Åer
positioning
problem
suite
known
characteristics
problem
allowed
input
algorithm
see
also
section
parameter
setting
tuning
algorithms
optimization
run
following
new
information
available
algorithm
result
i.e.
ùëì-value
evaluating
problem
given
search
point
via
coco_evaluate_function
result
evaluating
constraints
problem
given
search
point
via
coco_evaluate_constraint
result
coco_problem_final_target_hit
used
terminate
run
conclusively
without
changing
performance
assessment
way
currently
number
objectives
function
returns
always
zero
number
evaluations
problem
and/or
constraints
search
costs
also
referred
runtime
used
performance
assessment
algorithm.2
coco_problem_get_evaluations
const
coco_problem_t
problem
convenience
func-
tion
returns
number
evaluations
done
problem
information
available
optimization
algorithm
anyway
convenience
function
might
used
additionally
2.2
budget
termination
criteria
restarts
algorithms
and/or
setups
budget
function
evaluations
eligible
benchmarking
setup
budget-free
consider
termination
criteria
part
benchmarked
algorithm
choice
termination
relevant
part
algorithm
one
hand
allowing
larger
number
function
evaluations
increases
chance
Ô¨Ånd
solutions
better
quality
hand
timely
termination
stagnating
runs
improve
performance
evaluations
used
effectively
exploit
large
number
function
evaluations
effectively
encourage
use
independent
restarts3
particular
algorithms
terminate
naturally
within
comparatively
small
bud-
get
independent
restarts
natural
way
approach
difÔ¨Åcult
optimization
problems
change
central
performance
measure
used
coco
hence
budget-free
however
inde-
pendent
restarts
improve
reliability
comparability4
precision
visibility
measured
results
moreover
multistart
procedure
relies
interim
termination
algorithm
encouraged
multistarts
may
independent
feature
parameter
sweep
e.g.
increasing
population
size
har1999
aug2005
based
outcome
previous
starts
and/or
feature
systematic
change
initial
conditions
algorithm
multistart
procedure
established
recommended
procedure
use
budget
proportional
dimension
run
repeated
experiments
increase
e.g
like
100
300
good
compromise
availability
latest
results
computational
overhead
algorithm
conclusively
terminated
coco_problem_final_target_hit
returns
1.5
saves
cpu
cycles
without
affecting
performance
assessment
target
left
hit
parameter
setting
tuning
algorithms
tuning
algorithm
parameters
test
suite
described
approximate
overall
number
tested
parameter
settings
algorithm
variants
approximate
overall
invested
budget
given
recommended
tuning
procedure
veriÔ¨Åcation
termination
conditions
algorithm
suited
given
testbed
case
tuning
termination
parameters.6
early
coco
platform
provides
example
code
implementing
independent
restarts
algorithms
comparable
smallest
budget
given
bbob-biobj
suite
however
currently
never
case
example
single
objective
case
care
taken
apply
termination
conditions
allow
hit
Ô¨Ånal
target
basic
functions
like
sphere
function
problems
360
720
1080
1440
1800
bbob
suite
experience
numerical
optimization
software
frequently
terminates
early
default
evolutionary
computation
software
often
terminates
late
default
late
termination
identiÔ¨Åed
adjusted
comparatively
easy
also
useful
prerequisite
allowing
restarts
become
effective
functions
parameter
setting
must
used
might
well
depend
dimensionality
see
section
initialization
input
algorithm
means
priori
use
function-dependent
parameter
settings
prohibited
since
2012
function
func-
tion
characteristics
like
separability
multi-modality
...
considered
input
parameter
algorithm
hand
benchmarking
different
parameter
settings
different
algorithms
entire
test
suite
encouraged
time
complexity
experiment
order
get
rough
measurement
time
complexity
algorithm
wall-clock
cpu
time
measured
running
algorithm
benchmark
suite
chosen
setup
reÔ¨Çect
realistic
average
scenario
time
divided
number
function
evaluations
shall
presented
separately
dimension
chosen
setup
coding
language
compiler
computational
architecture
conducting
experiments
given
acknowledgments
authors
would
like
thank
raymond
ros
steffen
finck
marc
schoenauer
petr
posik
dejan
tu≈°ar
many
invaluable
contributions
work
work
support
grant
anr-12-monu-0009
numbbo
french
national
research
agency
references
aug2005
auger
hansen
restart
cma
evolution
strategy
increasing
pop-
ulation
size
proceedings
ieee
congress
evolutionary
computation
cec
2005
pages
1769‚Äì1776
ieee
press
2005
han2016perf
hansen
auger
brockhoff
tu≈°ar
tu≈°ar
coco
performance
assessment
arxiv
e-prints
arxiv:1605.03560
2016
example
experiment
code
provides
timing
output
measured
problems
single
dimension
default
also
used
make
record
timing
experiment
pure
random
search
serve
additional
base-line
data
bbob
test
suite
also
Ô¨Årst
instance
rosenbrock
function
used
experiment
previously
suite
indices
105
465
825
1185
1545
1905
han2009
han2010
hansen
auger
finck
ros
real-parameter
black-box
optimiza-
tion
benchmarking
2009
experimental
setup
inria
research
report
rr-6828
http
//hal.inria.fr/inria-00362649/en
2009.
hansen
auger
finck
ros
real-parameter
black-box
optimiza-
tion
benchmarking
2010
experimental
setup
inria
research
report
rr-7215
http
//hal.inria.fr/inria-00362649/en
2010
han2016co
hansen
auger
mersmann
tu≈°ar
brockhoff
coco
platform
comparing
continuous
optimizers
black-box
setting
arxiv
e-prints
arxiv:1603.08785
2016.
g.r
harik
f.g.
lobo
parameter-less
genetic
algorithm
proceedings
genetic
evolutionary
computation
conference
gecco
volume
pages
258-265.
acm
1999
har1999
hoo1998
h.h
hoos
st√ºtzle
evaluating
las
vegas
algorithms
pitfalls
reme-
dies
proceedings
fourteenth
conference
uncertainty
artiÔ¨Åcial
intelligence
uai-98
pages
238-245
1998
