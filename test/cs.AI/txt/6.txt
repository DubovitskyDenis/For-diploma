computational
power
dynamic
bayesian
networks
joshua
brulé∗
abstract
paper
considers
computational
power
constant
size
dy-
namic
bayesian
networks
although
discrete
dynamic
bayesian
networks
powerful
hidden
markov
models
dynamic
bayesian
networks
continuous
random
variables
discrete
children
con-
tinuous
parents
capable
performing
turing-complete
computation
modiﬁed
versions
existing
algorithms
belief
propagation
simulation
carried
real
time
result
suggests
dynamic
bayesian
networks
may
powerful
previously
con-
sidered
relationships
causal
models
recurrent
neural
networks
also
discussed
introduction
bayesian
networks
probabilistic
graphical
models
represent
set
ran-
dom
variables
conditional
dependencies
via
directed
acyclic
graph
explicitly
modeling
conditional
dependencies
random
variables
per-
mit
eﬃcient
algorithms
perform
inference
learning
network
causal
bayesian
networks
additional
requirement
edges
network
model
causal
relationship
dynamic
bayesian
networks
time-generalization
bayesian
net-
works
relate
variables
adjacent
time
steps
dynamic
bayesian
networks
unify
extend
number
state-space
models
including
hidden
markov
models
hierarchical
hidden
markov
models
kalman
ﬁlters
dynamic
bayesian
networks
also
seen
natural
extension
acyclic
causal
models
models
permit
cyclic
causal
relationships
avoiding
problems
causal
models
try
model
temporal
relationships
atemporal
description
natural
question
expressive
power
networks
result
paper
shows
although
discrete
dynamic
bayesian
networks
sub-turing
computational
power
introducing
continuous
random
variables
discrete
children
suﬃcient
model
turing-complete
computation
∗department
computer
science
university
maryland
college
park
20742.
jbrule
cs.umd.edu
addition
distributions
used
construction
marginal
posterior
probabilities
random
variables
network
eﬀectively
computed
modiﬁed
versions
existing
algorithms
ignoring
overhead
arbitrary
precision
arithmetic
simulation
conducted
constant
time
penalty
model
main
results
bayesian
network
consists
directed-acyclic
graph
set
vertices
probability
distribution
set
vari-
ables
correspond
vertices
bayesian
network
factorizes
probability
distribution
variables
requiring
variable
conditionally
independent
non-descendants
given
parents
denoted
markov
condition
xi|pai
dynamic
bayesian
networks
dbn
extend
bayesian
networks
model
probability
distribution
semi-inﬁnite
collection
random
variables
collection
random
variables
modeling
system
point
time
following
conventions
collections
denoted
variables
partitioned
represent
input
hidden
output
variables
state
space
model
network
dynamic
sense
model
dynamic
system
network
topology
changes
time
dbn
deﬁned
pair
bayesian
network
deﬁnes
prior
two-slice
temporal
bayes
net
2tbn
deﬁnes
zt|zt−1
via
directed
acyclic
graph
zt|zt−1
i=1
t|pa
ith
node
time
graph
parents
node
either
time
slice
previous
time
slice
i.e
model
ﬁrst-order
markov
parents
semantics
dbn
deﬁned
unrolling
2tbn
time-slices
joint
distribution
given
t=1
i=1
t|pa
analyzing
computational
power
dbn
requires
deﬁning
means
dbn
accept
halt
reject
input
deﬁne
input
sequence
bernoulli
random
variables
model
binary
input
sim-
ilarly
deﬁne
output
sequence
run
halt0
halt1
represent
whether
machine
halted
answer
gives
given
input
in1
in2
int
decision
problem
machine
modeled
dbn
halted
accepted
time
halt1|u1
in1
int
0.5
halted
rejected
halt0|u1
in1
int
0.5
2.1
discrete
dynamic
bayesian
networks
turing-
complete
discrete
bayesian
networks
bayesian
networks
random
variables
ﬁnite
number
outcomes
i.e
bernoulli
categorical
random
vari-
ables
dynamic
bayesian
networks
permitted
increase
number
random
variables
time
simulating
turing-machine
becomes
trivial
simply
add
new
variable
time
step
model
newly
reachable
cell
turing
machine
tape
however
requires
ﬁrst-order
features
language
used
specify
network
computational
eﬀort
required
step
simulation
grow
without
bound
ﬁxed
number
random
variables
time
step
property
dbns
ﬁrst-order
markov
computational
eﬀort
per
step
remains
constant
however
discrete
dbns
sub-turing
computational
power
in-
tuitively
discrete
dbn
possibly
simulate
turing
machine
since
way
store
contents
machine
tape
formally
discrete
bayesian
network
converted
hidden
markov
model
done
collapsing
hidden
variables
dbn
single
random
variable
taking
cartesian
product
sample
space
collapsed
dbn
models
probability
distribution
exponentially
larger
still
ﬁnite
sample
space
hidden
markov
models
equivalent
probabilistic
ﬁnite
automata
recognize
stochastic
languages
stochastic
languages
rp-complexity
class
thus
discrete
dbns
turing
complete
2.2
dynamic
bayesian
network
continuous
dis-
crete
variables
2tbn
constructed
simulate
transitions
two
stack
push-
automaton
pda
equivalent
standard
one
tape
turing
machine
two
stack
pda
consists
ﬁnite
control
two
unbounded
binary
stacks
input
tape
step
computation
machine
reads
advances
input
tape
reads
top
element
stack
either
push
new
element
pop
top
element
leave
stack
unchanged
state
control
change
function
previous
state
read
symbols
control
reaches
one
two
possible
halt
states
halt0
halt1
machine
stops
output
decision
problem
computing
deﬁned
halt
states
stops
key
part
construction
using
dirac
distribution
simulate
stack
dirac
distribution
centered
deﬁned
limit
normal
distributions
lim
σ↓0
σ√2π
single
dirac
distributed
random
variable
suﬃcient
simulate
stack
stack
construction
adapted
encodes
binary
string
ω1ω2
number
i=1
2ωi
note
string
begins
value
value
least
3/4
string
begins
less
1/2
never
need
distinguish
among
two
close
numbers
read
signiﬁcant
digit
addition
empty
string
encoded
non-empty
string
value
least
1/4
random
variables
except
stack
random
variables
categorically
distributed
thus
conditional
probabilities
densities
represented
using
standard
conditional
probability
tables
extracting
top
value
stack
requires
conditional
probability
distribution
bernoulli
random
variable
given
dirac
stack
distributed
parent
heavyside
step
function
meets
re-
quirement
deﬁned
limit
logistic
functions
generally
softmax
functions
centered
1/2
lim
k→∞
e−k
x−1/2
linear
operation
transfers
range
least
top
element
stack
top
element
stack
conditional
probability
density
function
op|stack
yields
whenever
top
element
stack
whenever
top
element
stack
similarly
conditional
probability
distribution
deﬁned
bernoulli
random
variable
empty
empty|stack
check
stack
empty
finally
linear
operations
push
pop
respectively
stack
conditional
probability
density
stack
time
t+1
given
stack
time
top
stack
time
action
performed
stack
actiont
push0
push1
pop
noop
fully
described
follows
2b+1
stackt+1|t
opt
stackt
actiont
push0
q/4
1/4
stackt+1|t
opt
stackt
actiont
push1
q/4
3/4
stackt+1|t
opt
stackt
actiont
pop
stackt+1|t
opt
stackt
actiont
noop
since
two
stacks
full
construction
labeled
time
stacka
stackb
rest
construction
straightforward
statet
actiona
actionb
functions
statet−1
opa
emptya
opb
emptyb
int
since
discrete
random
variables
conditional
prob-
ability
densities
simply
transition
function
pda
written
stochastic
matrix
expected
halti|state
state
halt
state
otherwise
finally
priors
dynamic
bayesian
network
simply
stacka,1
stackb,1
state1
initial
state
described
construction
somewhat
abuse
term
prob-
abilistic
graphical
model
probability
mass
concentrated
single
event
every
random
variable
system
every
time
step
however
easy
see
construction
faithfully
simulates
two
stack
machine
random
variable
construction
corresponds
exactly
component
simulated
automaton
2.3
exact
inference
continuous-discrete
bayesian
net-
works
construction
requires
continuous
random
variables
raise
concerns
whether
marginal
posterior
probabilities
eﬀectively
computed
original
junction
tree
algorithm
cut-set
conditioning
approaches
belief
propagation
compute
exact
marginals
arbitrary
dags
require
dis-
crete
random
variables
lauritzen
algorithm
conducts
inference
mixed
graphical
models
limited
conditional
linear
gaussian
clg
continu-
ous
random
variables
clg
model
let
continuous
node
discrete
parents
continuous
parents
x|a
wa,0
i=1
iyi
lauritzen
algorithm
conduct
approximate
inference
since
true
posterior
marginals
may
multimodal
mix
gaussians
algorithm
supports
clg
random
variables
however
algorithm
exact
sense
computes
exact
ﬁrst
second
moments
posterior
marginals
suﬃcient
turing
machine
simulation
laurientz
algorithm
permit
discrete
random
variables
chil-
dren
continuous
random
variables
lerner
algorithm
extends
lau-
ritzen
algorithm
support
softmax
conditional
probability
densities
dis-
crete
children
continuous
parents
let
discrete
node
possible
values
let
parents
ai|y1
exp
l=1
j=1
exp
l=1
like
lauritzen
algorithm
lerner
algorithm
computes
approximate
pos-
terior
marginals
relying
observation
product
softmax
gaussian
approximately
gaussian
exact
ﬁrst
second
moments
errors
numerical
integration
used
compute
best
gaussian
approximation
product
gaussian
softmax
calculation
actually
simpler
case
softmax
replaced
heavyside
lerner
algorithm
run
essentially
unmodiﬁed
mixture
heavyside
softmax
conditional
probability
densities
case
dirac-distributed
parents
heavyside
conditional
probability
densities
numeric
integration
unnecessary
errors
introduced
computing
ﬁrst
second
moments
posterior
distribution
non-zero
variance
continuous
variables
leak
probability
values
stack
random
variables
turing
machine
simulation
eventually
leading
errors
lauritzen
original
algorithm
assumes
positive-
deﬁnite
covariance
matrices
continuous
random
variables
extend
handle
degenerate
gaussians
summary
posterior
marginals
turing
machine
simulation
computed
exactly
using
modiﬁed
version
lerner
algorithm
restricted
dirac
distributed
continuous
random
variables
heavside
conditional
probability
densities
gaussian
random
variables
softmax
conditional
probability
densities
also
intro-
duced
ﬁrst
second
moments
posterior
marginals
computed
exactly
errors
numerical
integration
although
slowly
degrade
quality
turing
machine
simulation
later
time
steps
inference
bayesian
networks
np-hard
however
assuming
arithmetic
operations
computed
unit
time
arbitrary-precision
numbers
e.g
real
ram
model
work
necessary
time
step
constant
thus
dynamic
bayesian
networks
simulate
turing-machines
constant
time
overhead
real
ram
model
slowdown
proportional
time
complexity
arbitrary
precision
arithmetic
otherwise
discussion
result
suggests
causal
bayesian
networks
may
richer
language
modeling
causality
currently
appreciated
halpern
suggests
general
causal
reasoning
richer
language
including
some-ﬁrst
order
features
may
needed
first-order
features
likely
useful
causal
model-
ing
practice
turing-complete
power
dynamic
bayesian
networks
suggests
ﬁrst-order
features
may
unnecessary
result
dynamic
bayesian
networks
analogous
siegelmann
sontag
proof
recurrent
neural
network
simulate
turing
machine
real
time
fact
neural
networks
bayesian
networks
turn
similar
expressive
power
single
perceptron
gaussian
naive
bayes
logistic
regression
multilayer
perceptron
full
bayesian
network
universal
function
ap-
proximation
recurrent
neural
network
dynamic
bayesian
network
turing
complete
interesting
gap
decidability
takes
little
turn
sub-turing
framework
modeling
turing-complete
one
case
neural
networks
single
recurrent
layer
arbitrary-precision
rational
weights
saturating
linear
transfer
function
suﬃcient
dynamic
bayesian
networks
two
time-slices
continuous-valued
random
variables
combination
linear
step
function
conditional
probability
densities
suﬃcient
although
simple
recurrent
neural
network
theoretically
capable
performing
arbitrary
computations
practical
extensions
include
higher-order
connections
gates
long
short-term
memory
even
connections
external
turing
machine
additions
enrich
capabilities
standard
neural
networks
make
easier
train
complex
algo-
rithmic
tasks
interesting
question
degree
dynamic
bayesian
networks
similarly
extended
core
dynamic
bayesian
network
ca-
pable
turing-complete
computation
aﬀects
overall
performance
networks
acknowledgements
would
like
thank
james
reggia
william
gasarch
brendan
good
discussions
helpful
comments
early
drafts
paper
references
poole
crowley
cyclic
causal
models
discrete
variables
markov
chain
equilibrium
semantics
sample
ordering
proceed-
ings
twenty-third
international
joint
conference
artiﬁcial
in-
telligence
1060–1068
aaai
press
2013
pearl
bayesian
networks
model
self-activated
memory
evi-
dential
reasoning
proceedings
7th
conference
cognitive
science
society
university
california
irvine
329–334
aug.
1985
bareinboim
brito
pearl
graph
structures
knowledge
rep-
resentation
reasoning
second
international
workshop
gkr
2011
barcelona
spain
july
2011.
revised
selected
papers
local
char-
acterizations
causal
bayesian
networks
1–17
berlin
heidelberg
springer
berlin
heidelberg
2012
dean
kanazawa
model
reasoning
persistence
causation
comput
intell.
vol
142–150
dec.
1989
murphy
dynamic
bayesian
networks
representation
inference
learning
phd
thesis
university
california
berkeley
2002
dupont
denis
esposito
links
probabilistic
au-
learning
tomata
hidden
markov
models
probability
distributions
models
induction
algorithms
pattern
recognition
vol
1349
1371
2005.
grammatical
inference
siegelmann
sontag
computational
power
neural
nets
journal
computer
system
sciences
vol
132–150
1995
lauritzen
spiegelhalter
local
computations
prob-
abilities
graphical
structures
application
expert
systems
journal
royal
statistical
society
series
methodological
157–
224
1988
pearl
probabilistic
reasoning
intelligent
systems
networks
plau-
sible
inference
morgan
kaufmann
publishers
inc.
1988
lauritzen
propagation
probabilities
means
variances
mixed
graphical
association
models
journal
american
statistical
association
vol
420
1098–1108
1992
lerner
segal
koller
exact
inference
networks
dis-
crete
children
continuous
parents
proceedings
seventeenth
conference
uncertainty
artiﬁcial
intelligence
319–328
morgan
kaufmann
publishers
inc.
2001
raphael
bayesian
networks
degenerate
gaussian
distributions
methodology
computing
applied
probability
vol
235–
263
2003
cooper
computational
complexity
probabilistic
inference
us-
ing
bayesian
belief
networks
artiﬁcial
intelligence
vol
393–
405
1990
halpern
axiomatizing
causal
reasoning
journal
artiﬁcial
in-
telligence
research
317–337
2000
jordan
discriminative
vs.
generative
classiﬁers
com-
parison
logistic
regression
naive
bayes
advances
neural
infor-
mation
processing
systems
vol
841
2002
cybenko
approximation
superpositions
sigmoidal
function
mathematics
control
signals
systems
vol
303–314
1989
varando
bielza
larrañaga
expressive
power
binary
relevance
chain
classiﬁers
based
bayesian
networks
multi-label
classiﬁcation
probabilistic
graphical
models
519–534
springer
2014
pineda
generalization
back
propagation
recurrent
higher
order
neural
networks
neural
information
processing
systems
602–
611
1988
hochreiter
schmidhuber
long
short-term
memory
neural
com-
putation
vol
1735–1780
1997
graves
wayne
danihelka
neural
turing
machines
arxiv
preprint
arxiv:1410.5401
2014
