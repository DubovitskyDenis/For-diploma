distributional
framework
emergent
knowledge
acquisition
application
automated
document
annotation
v´ıt
nov´aˇcek
digital
enterprise
research
institute
deri
national
university
galway
ireland
nuig
ida
business
park
lower
dangan
galway
ireland
e-mail
vit.novacek
deri.org
september
2018
abstract
paper
introduces
framework
representation
acquisition
knowl-
edge
emerging
large
samples
textual
data
utilise
tensor-based
dis-
tributional
representation
simple
statements
extracted
text
show
one
use
representation
infer
emergent
knowledge
patterns
tex-
tual
data
unsupervised
manner
examples
patterns
investigate
paper
implicit
term
relationships
conjunctive
if-then
rules
evaluate
practical
relevance
approach
apply
annotation
life
science
ar-
ticles
terms
mesh
controlled
biomedical
vocabulary
thesaurus
introduction
ubiquity
methods
digital
content
publishing
processing
sharing
led
lot
data
made
globally
available
every
day
unprecedented
world-
wide
availability
content
generally
beneﬁcial
yet
also
poses
big
challenges
instance
dynamic
voluminous
domains
life
sciences
virtually
impos-
sible
users
utilise
available
relevant
knowledge
comprehensive
timely
manner
mitigation
problem
special
focus
biomedical
literature
served
main
motivation
research
presented
paper
seen
instance
popular
way
tackling
information
overload
context
biomedical
literature
annotation
articles
terms
standardised
biomedical
vocabularies
annotations
turn
make
retrieval
relevant
documents
much
efﬁcient
however
providing
necessary
annotations
manually
expensive
automated
methods
desired
going
address
technical
contribution
presented
work
two-fold
firstly
introduce
general
framework
automated
acquisition
knowledge
textual
collections
proposed
framework
builds
principles
distributional
emergent
semantics
allows
inference
complex
knowledge
patterns
within
simple
co-
occurrence
statements
extracted
articles
second
contribution
show
knowledge
inferred
text
applied
unsupervised
parameter-free
annotation
biomedical
articles
rest
paper
organised
follows
section
gives
overview
re-
lated
work
framework
emergent
knowledge
representation
acquisition
described
section
application
framework
document
annotation
detailed
section
also
discuss
experiment
performed
evaluate
approach
section
concludes
paper
outlines
future
work
related
work
approach
builds
shares
lot
similarities
recent
works
emer-
gent
distributional
semantics
however
quite
restrictive
applies
notion
emergence
merely
complex
patterns
arising
simple
inter-
actions
autonomous
agents
distributed
systems
like
p2p
networks
general
focusing
rather
inference
analysis
complex
patterns
emerging
within
large
amounts
simple
statements
extracted
directly
data
accor-
dance
recent
approach
distributional
semantics
presented
employ
similar
tensor-based
structures
representation
data
analysis
knowledge
emerging
yet
also
augment
work
explicit
representation
data
provenance
importantly
method
mining
rules
distributional
representations
latter
related
associative
rule
mining
intro-
duced
however
generalise
state
art
method
make
use
distributional
essentially
vector-based
representation
data
regarding
application
framework
annotation
biomedical
articles
body
less
recent
works
like
exists
sec-
ond
third
ﬁfth
approaches
either
used
considered
use
support
service
professional
annotators
articles
pubmed
biomedical
lit-
erature
repository
state
art
methods
however
often
require
least
indirect
input
human
users
produce
annotations
new
articles
automatically
instance
require
large
corpus
previously
annotated
articles
learning
ranking
possible
annotations
new
resources
methods
like
require
rather
sophisticated
tuning
e.g.
experimenting
parameter
set-
tings
processing
pipeline
composition
optimum
performance
new
data
case
approach
work
purely
unsupervised
manner
off-the-shelf
distributional
framework
emergent
knowledge
acquisition
section
ﬁrst
describes
one
represent
knowledge
emerging
textual
documents
various
levels
complexity
simple
term
co-occurrence
statements
within
documents
integral
view
statements
across
document
cor-
pus
different
perspectives
corpus-wide
view
analysing
various
types
emergent
semantic
phenomena
levels
representation
based
compact
tensor
structures
tensor
generalisation
scalar
vector
matrix
notions
see
http
//en.wikipedia.org/wiki/tensor
detailed
overview
rest
section
deals
analysis
two
particular
types
emergent
semantic
phenomena
relevant
document
annotation
motivating
use
case
3.1
source
representation
ﬁrst
layer
consists
called
source
representation
captures
co-occurrence
terms
across
set
documents
concrete
way
extracting
co-
occurrence
relationships
presented
section
let
sets
representing
left
right
arguments
binary
co-occurrence
relationships
i.e.
statements
types
relationships
furthermore
let
set
representing
provenances
par-
ticular
relationships
i.e.
document
identiﬁers
deﬁne
source
representation
4-ary
labeled
tensor
r|al|×|l|×|ar|×|p
four-dimensional
array
struc-
ture
indexed
argument-link-argument-provenance
tuples
values
reﬂecting
weight
e.g.
frequency
statements
context
particular
sources
state-
ment
occur
source
instance
statement
protein
different
gene
occurs
two
times
source
element
gprotein
dif
erent
rom
gene
details
given
example
example
let
consider
documents
following
terms
occurring
protein
domain
protein
domain
gene
internal
tandem
duplications
mutations
jux-
tamembrane
extracellular
domains
abbreviated
p.d.
i.t.d.
e.d.
respectively
following
let
assume
following
statements
extracted
documents
p.d.
p.d.
i.t.d.
i.t.d.
i.t.d.
e.d
p.d.
abbreviations
relation
terms
different
type
omitting
zero
values
representing
four-dimensional
tensor
two-dimensional
table
three
ﬁrst
columns
tensor
indices
fourth
one
corresponding
tensor
value
represent
source
statements
follows
using
statement
frequencies
values
p.d
p.d
i.t.d
i.t.d
i.t.d
p.d
e.d
3.2
corpus
representation
source
tensor
low-level
data
representation
merely
preserving
association
statements
provenance
contexts
allowing
actual
distributional
analysis
data
transformed
compact
structure
call
cor-
pus
representation
r|al|×|l|×|ar|
ternary
three-dimensional
labeled
tensor
providing
universal
compact
distributional
representation
simple
state-
ments
extracted
source
documents
corpus
constructed
source
representation
using
functions
element
pd∈p
element
source
tensor
functions
act
follows
assigns
relevance
degree
document
reﬂects
relevance
statement
elements
e.g.
mutual
information
score
subject
object
within
source
aggregates
result
functions
application
way
con-
structing
elements
corpus
tensor
source
representation
aggregates
occurrences
statements
within
input
data
reﬂecting
also
two
important
things
relevance
particular
sources
via
function
relevance
state-
ments
via
function
speciﬁc
implementation
functions
left
applications
alternatives
include
limited
ranking
statement
document
level
statistical
analysis
statements
within
input
data
example
corpus
corresponding
source
tensor
example
represented
depicted
values
sources
aggregated
source
values
using
relative
frequency
data
set
containing
statements
total
p.d
p.d
i.t.d
i.t.d
i.t.d
e.d
1/7
2/7
1/7
1/7
1/7
1/7
3.3
corpus
perspectives
elegance
corpus
representation
lays
compactness
universality
however
yields
many
diverse
possibilities
underlying
data
analysis
analysis
enabled
process
called
matricisation
corpus
tensor
essentially
matricisation
process
representing
higher-order
tensor
using
dimensional
matrix
perspective
done
ﬁxing
one
tensor
index
one
matrix
dimension
generating
possible
combinations
tensor
indices
within
remaining
matrix
dimension
following
illustrate
process
cor-
pus
tensor
example
example
ﬁxing
subjects
set
members
corpus
tensor
example
one
get
following
matricised
perspective
rows
columns
zero
values
omitted
s/hp
p.i
d.i
m.i
j.i
e.d.i
p.d
i.t.d
1/7
1/7
2/7
1/7
1/7
1/7
row
column
index
abbreviations
correspond
example
one
see
trans-
formation
lossless
original
tensor
easily
reconstructed
matrix
ap-
propriate
re-grouping
indices
corpus
tensor
matricisations
correspond
vector
spaces
consisting
elements
deﬁned
particular
rows
matrix
perspectives
row
vector
name
corresponding
matrix
row
index
set
features
matrix
column
indices
features
represent
distributional
attributes
entity
associated
vector
name
contexts
aggregated
across
whole
corpus
used
various
types
analysis
inference
complex
semantic
features
emerging
within
simple
statements
extracted
source
data
following
sections
describe
two
particular
types
analysis
relevant
motivating
use
case
paper
computation
related
semantically
close
terms
mining
conjunctive
if-then
rules
data
3.4
computing
related
terms
comparing
row
vectors
corpus
tensor
matricisations
one
essentially
compares
meaning
corresponding
label
terms
emerging
underlying
data
exploring
matricised
perspectives
one
use
linear
algebra
methods
proven
work
countless
successful
applications
vector
space
analysis
last
couple
decades
large
feature
spaces
reliably
reduced
manageable
less
noisy
number
dimensions
techniques
like
singular
value
decomposition
random
indexing
see
http
//en.wikipedia.org/wiki/dimension_reduction
optional
dimensionality
reduction
perspective
vectors
compared
well-founded
manner
measures
like
cosine
similarity
see
http
//en.wikipedia.org/wiki/cosine_similarity
illustrated
example
example
let
add
one
matrix
perspective
s/hp
one
provided
example
represents
distributional
features
right
arguments
based
contexts
relation
terms
left
arguments
tend
co-occur
corpus
o/hp
p.d.i
p.d.i
g.i
i.t.d.i
i.t.d.i
e.d
1/7
2/7
1/7
1/7
1/7
1/7
vector
spaces
induced
matrix
perspectives
s/hp
o/hp
used
ﬁnding
similar
terms
comparing
corresponding
vectors
using
cosine
vec-
tor
similarity
one
ﬁnds
sims/hp
p.d.
0.2972
1/7
1/7
non-zero
similarities
among
simo/hp
e.d
terms
present
corpus
corresponds
intuitive
interpretation
data
repre-
sented
initial
statements
example
protein
domains
genes
seem
different
proteins
yet
protein
domain
type
domain
gene
therefore
share
similarities
completely
equal
according
data
juxtamembranes
extra-
cellular
domains
places
internal
tandem
duplications
occur
information
available
deemed
equal
data
comes
1/7
2/7
1/7
1/7
1/7
1/7
1/7
easily
seen
computation
related
terms
relevant
anno-
tation
use
case
motivated
paper
computing
mesh
terms
related
content
article
i.e.
terms
extracted
one
get
annota-
tions
semantically
related
article
even
present
and/or
linked
explicit
way
3.5
rule
mining
another
type
emergent
semantic
pattern
infer
matricised
corpus
perspectives
if-then
rules
rules
useful
motivating
use
case
due
applicability
extension
basic
article
annotations
know
article
annotations
conform
rule
antecedent
also
add
annotations
present
rule
consequent
simplify
presentation
let
consider
conjunctive
if-then
rules
type
∧···∧
lk+1
rk+1
fol-
lowing
variable
concrete
relation
right
argument
terms
example
rule
type
domain
diﬀerent
protein
says
everything
type
domain
protein
rule
mining
consists
two
steps
using
matrix
perspective
oi/s
ﬁnding
candidate
sets
hli
rii
tuples
form
rules
using
matrix
per-
spective
s/hp
pruning
generated
rules
based
conﬁdence
note
types
single-variable
conjunctive
if-then
rules
i.e.
ones
variable
occurring
second
third
position
rule
statements
computed
way
using
different
perspectives
ﬁrst
step
corresponds
ﬁnding
frequent
itemsets
database
described
row
vectors
oi/s
matrix
essentially
items
features
rules
i.e.
concrete
tuples
grouping
close
vectors
discover
related
features
may
possibly
form
rules
perhaps
simplest
way
k-means
clustering
based
euclidean
distance
applied
oi/s
matrix
parameter
set
sizes
generated
clusters
correspond
desired
maximum
number
statements
present
rule
practice
recom-
mend
apply
dimensionality
reduction
columns
matrix
makes
clustering
faster
also
leading
noise
reduction
better
representation
features
meaning
sense
described
approach
effectively
replaces
process
ﬁnding
frequent
itemsets
using
distributional
representation
ﬁnd
promising
itemsets
via
support
discrete
data
transactions
exploiting
continuous
latent
semantics
second
step
involves
pruning
previously
generated
rules
using
measures
support
supp
conﬁdence
conf
rules
sufﬁciently
high
conﬁdence
kept
result
mining
process
measures
computed
matrix
transpose
one
used
generating
rules
s/hp
case
discussed
type
rules
keep
original
dimensions
matrix
time
check
conﬁdence
rules
using
actual
data
without
transformations
base
rule
pruning
deﬁnitions
support
conﬁdence
provided
however
generalise
support
fully
exploit
power
distributional
representation
classic
deﬁnition
supp
itemset
set
features
form
rule
statements
relative
frequency
rows
data
contain
items
due
fact
data
representation
classical
rule
mining
crisp
rows
transactions
contain
zeros
ones
indicate
lack
presence
item
transaction
respectively
data
representa-
tion
general
zeros
matrix
still
mean
lack
item
given
row
however
actual
presence
items
represented
ﬂuid
way
real-valued
weights
therefore
deﬁne
generalised
support
function
supp
set
rule
features
i.e.
column
labels
corpus
perspective
matrix
rules
tested
s/hp
type
rules
discussed
support
feature
set
perspective
matrix
computed
supp
set
row
indices
matrix
features
present
i.e.
non-zero
value
element
matrix
indices
||m||
matrix
norm
i.e.
size
deﬁned
k|k∈j∧mi
k6=0
sets
row
column
indices
||m||
pi∈i
respectively
conﬁdence
rule
computed
deﬁned
i.e.
conf
supp
x∪y
using
generalised
support
process
rule
mining
illustrated
example
end
section
√pj∈x
||m||
pi∈ix
pj∈j
supp
|x|
proposed
deﬁnition
support
essentially
computes
weighted
relative
frequency
input
feature
set
matrix
rows
rows
contain
features
con-
tribute
absolute
frequency
count
actual
contribution
computed
nor-
malised
euclidean
size
row
vector
restricted
column
indices
normalising
factor
size
feature
set
make
support
value
independent
size
absolute
weighted
frequency
feature
set
divided
||m||
get
relative
frequency
analogically
classical
def-
inition
support
||m||
reﬂects
size
real-valued
data
set
sum
weights
matrix
normalised
number
non-zero
elements
per
row
makes
also
norm
independent
size
potential
feature
sets
one
easily
check
values
matrix
zeros
ones
traditional
data
representation
used
support
becomes
classical
one
example
building
previous
examples
training
matrix
rule
mining
transpose
one
given
example
oi/s
p.i
d.i
m.i
j.i
e.d.i
p.d
1/7
2/7
1/7
i.t.d
1/7
1/7
1/7
dimension
reduction
applied
due
simplicity
example
testing
matrix
original
one
i.e.
s/hp
example
euclidean
distance
two
last
three
vectors
training
matrix
oi/s
distance
ﬁrst
two
vectors
d1,2
1/7
2/7
1/7
distances
ﬁrst
second
last
three
vectors
d1,3−5
1/7
1/7
1/7
d2,3−5
2/7
1/7
respectively
minimum-distance
grouping
vectors
clusters
containing
least
two
elements
thus
follows
p.i
d.i
m.i
j.i
e.d.i
let
abbreviate
rule
statements
corresponding
training
matrix
fol-
lows
e.d.
groups
generate
rules
r1−r2
r3−r5
r6−r8
r9−r14
testing
matrix
s/hp
see
example
size
0.5.
corresponding
supports
relevant
sets
rule
statements
supp
supp
supp
supp
supp
supp
supp
thus
conﬁdences
rules
supp
conf
conf
conf
conf
conf
conf
conf
conf
conf
conf
r10
···
conf
r14
setting
conﬁdence
threshold
0.5
rules
r1−r5
discarded
supp
supp
2√3
automated
document
annotation
section
illustrates
practical
potential
general
framework
introduced
far
first
describe
application
unsupervised
annotation
biomedical
articles
terms
mesh
thesaurus
present
evaluation
approach
discuss
results
obtained
4.1
data
method
corpus
documents
annotation
employed
003
articles
pubmed
repository
http
//www.ncbi.nlm.nih.gov/pubmed/
fulltexts
available
pubmed
central
http
//www.ncbi.nlm.nih.gov/pmc/
articles
selected
article
present
corpus
also
contained
corre-
sponding
related
articles
offered
pubmed
related
articles
service
fact
important
evaluation
later
article
annotation
used
mesh
2011
version
obtained
http
//www.nlm.nih.gov/mesh/filelist.html
processed
data
using
following
high-level
pipeline
extraction
statements
articles
mesh
incorporation
extracted
state-
ments
two
separate
knowledge
bases
pubmed
articles
mesh
thesaurus
construction
basic
mesh
annotation
sets
article
mining
rules
mesh
knowledge
base
rule-based
extension
basic
annotation
sets
evaluation
initial
extended
sets
annotations
extraction
step
focusing
simple
binary
co-occurrence
statements
tokenized
article
text
sentences
applied
part
speech
tagging
shallow
parsing
order
determine
noun
phrases
two
noun
phrases
occurring
sentence
formed
statement
stands
following
related
relationship
expressing
general
relat-
edness
left
right
arguments
synonyms
mesh
terms
statements
converted
corresponding
preferred
mesh
headings
order
lexically
unify
data
379
235
statements
generated
003
articles
way
mesh
data
set
generated
statements
terms
i.e.
headings
parent
child
sibling
mesh
hierarchy
led
632
statements
note
data
sets
considered
relation
symmetric
effectively
made
s/hp
o/hs
perspectives
equivalent
consequent
steps
adopted
model
co-occurrence
limited
single
general
relationship
may
seem
restrictive
however
chose
able
link
semantics
data
extracted
articles
semantics
mesh
general
sense
applicable
apart
suggest
settings
similar
ﬂattened
semantics
actually
perform
better
model
multiple
relations
second
step
experimental
pipeline
incorporation
extracted
statements
knowledge
bases
i.e.
source
corpus
perspective
structures
de-
scribed
section
incorporation
done
way
pubmed
mesh
data
source
values
set
elements
statement
occurred
document
values
get
corpus
tensor
values
multiplied
frequency
triples
i.e.
pd∈p
point-wise
mutual
information
score
tuple
see
http
//en.wikipedia.org/wiki/pointwise_mutual_information
annotations
article
computed
using
article
knowledge
base
follows
first
constructed
set
terms
extracted
absolute
frequency
term
tuple
computed
another
set
relt
sims/hp
|sims/hp
rephrased
prose
relt
sets
contained
tuples
terms
similar
actual
similarities
multiplied
fre-
quency
frequent
terms
generally
produce
terms
higher
relatedness
value
sims/hp
similarity
function
deﬁned
example
eventually
collated
particular
term
relatedness
values
across
whole
document
overall
relatedness
rel
pw∈wt′
wt′
st∈d
relt
sum
relatedness
values
occurring
st∈d
relt
union
ﬁnal
output
step
document
set
related
terms
mesh
rel
values
used
ranking
set
mesh
annotations
taking
top
ones
necessary
rule
mining
part
experimental
pipeline
executed
iteratively
dif-
ferent
random
initialisations
clusters
new
rules
added
least
recent
iterations
obtained
384
rules
conﬁdence
least
0.5
way
rules
used
extending
basic
article
annotation
sets
fol-
lows
let
assume
article
annotations
rule
···
ek+1
ek+2
···
used
consequent
set
ek+1
ek+2
extended
annotations
article
relatedness
mea-
sure
extensions
computed
set
conﬁ-
dences
rules
contributed
extension
sum
conﬁdences
across
extensions
computed
similarly
basic
annotation
sets
relatedness
extensions
used
ranking
possible
restriction
top-scoring
ones
pw∈ce
note
data
working
well
library
scripts
implemented
experiment
available
reference
http
//dl.dropbox.com/u/21379226/aaai2012_761.zip
4.2
evaluation
discussion
evaluate
annotation
sets
produced
experimental
pipeline
used
two
methods
firstly
measured
precision
recall
basic
extended
annota-
tion
sets
based
comparison
manually
provided
mesh
annotations
corresponding
articles
available
pubmed
entrez
api
article
computed
average
precision
precision
recall
computed
annotations
also
top
ones
number
human
annotations
given
article
second
evaluation
method
focused
utility
computed
annota-
tions
namely
task
ﬁnding
related
articles
used
standard
vector
space
model
determining
relatedness
documents
features
formed
sets
computed
manually
assigned
article
annotations
document
computed
different
sets
related
documents
based
human
annotations
basic/extended
ones
generated
framework
determine
precision
recall
computed
sets
compared
corresponding
sets
related
articles
provided
dedicated
pubmed
service1
similarly
evaluation
annota-
tions
measured
average
precision
precision
recall
top
related
articles
computed
number
related
articles
gold
standard
results
evaluation
summarised
tables
mean
average
precision
map
precision
recall
lines
tables
computed
arithmetic
mean
across
particular
values
003
articles
experimental
corpus
f-score
particular
computed
mean
precision/recall
values
columns
tables
correspond
types
article
annotation
sets
described
base
ext
refer
basic
extended
annotations
top
refer
complete
top-h
annotation
sets
note
include
ext./top
annotations
result
summaries
since
performing
signiﬁcantly
worse
ones
measured
categories
comparison
manually
curated
mesh
annotations
table
look
particularly
impressive
highest
precision
recall
values
16.4
12.7
respectively
hand
automatically
computed
annotations
per-
1the
service
based
algorithms
described
obviously
less
desirable
gold
standard
designed
solely
human
experts
however
gold
standard
readily
available
pubmed
articles
processed
lacked
manpower
create
situation
considered
state
art
service
currently
endorsed
pubmed
staff
millions
users
reasonable
alternative
hand-crafted
gold
standard
base
5.5
14.9
12.2
13.4
top
5.5
16.4
11.7
13.7
ext
5.3
9.3
12.7
10.7
map
prec
rec
f-sc
table
evaluation
results
article
annotation
formed
much
better
manual
ones
using
features
ﬁnding
related
articles
seen
table
substantial
improvement
namely
base-all
top
21.7
21.6
64.7
51.8
57.7
57.7
base-top
top
22.3
14.6
79.2
49.9
36.3
58.3
53.8
49.8
ext-all
human
36.9
91.1
41.8
57.3
top
36.9
91.1
41.8
57.3
21.1
45.3
68.5
54.5
top
21.9
46.9
67.9
55.5
map
prec
rec
f-sc
table
evaluation
results
annotation
utility
regarding
precision
overall
f-score
measure
manually
curated
annotations
perform
slightly
1.1-times
better
next-best
automated
method
recall
especially
notable
difference
precision
extended
annotations
achieve
two-times
better
human
ones
results
obtain
may
several
interpretations
believe
one
plausible
ones
related
nature
manually
provided
mesh
annota-
tions
mentioned
instance
goal
pubmed
annotators
provide
best
mesh
tags
purpose
indexing
digital
library
collections
thus
motivated
select
annotations
better
discriminate
papers
may
however
rather
detrimental
task
identify
related
papers
using
annotations
features
used
identifying
relatedness
i.e.
similarity
often
dual
features
used
discrimination
entities
reasoning
turn
explain
automatically
computed
article
annotations
apparently
different
manually
curated
ones
perform
signiﬁcantly
better
used
features
ﬁnding
related
articles
better
performance
especially
case
precision
extended
annotations
may
indicate
automatically
computed
annotations
selected
ﬁne-grained
manner
varied
vocabulary
ones
provided
human
annotators
hardly
grasp
scale
hypothet-
ically
available
annotations
addition
different
motivations
mentioned
say
either
kind
annotations
worse
much
rather
means
simply
serve
slightly
different
purposes
conclude
discussion
believe
despite
low
performance
approach
terms
comparison
manually
curated
mesh
annotations
still
offer
potentially
beneﬁcial
results
especially
case
annotations
augmented
emergent
rules
holds
particularly
use
cases
annotations
supposed
produced
scalable
economical
way
order
determine
sim-
ilarities
articles
examples
use
cases
include
identiﬁcation
related
documents
also
question
answering
automated
linking
publica-
tions
supplementary
data
e.g.
biomedical
data
rdf
format
provided
http
//linkedlifedata.com/sources
easily
incorporate
implied
conclusions
future
work
presented
approach
acquisition
complex
knowledge
patterns
emerging
within
simple
statements
extracted
textual
data
distinctive
features
approach
uniﬁcation
principles
emergent
distributional
semantics
novel
method
mining
rules
proposed
distributional
representation
demonstrate
practical
relevance
work
applied
annotation
pubmed
articles
terms
mesh
thesaurus
discussing
results
identiﬁed
areas
approach
likely
bring
beneﬁts
users
future
explore
use
cases
investigate
types
knowledge
patterns
e.g.
emergent
formation
new
candidate
concepts
taxonomical
relations
recommended
inclusion
mesh
thesaurus
regarding
presented
use
case
intend
look
possible
combinations
approach
relevant
state
art
namely
ranking-based
methods
like
also
related
deeper
evaluation
work
would
utilise
state
art
approaches
base-line
currently
able
comprehensively
enough
due
lack
publicly
available
applicable
implementations
eventually
want
perform
qualitative
evaluation
annotations
produced
system
assistance
domain
experts
references
rakesh
agrawal
tomasz
imieli´nski
arun
swami
mining
association
rules
sets
items
large
databases
sigmod
rec.
:207–216
1993
aronson
mork
gay
humphrey
rogers
nlm
indexing
initiatives
medical
text
indexer
studies
health
technololgy
informatics
pages
268–272
2004
baroni
lenci
distributional
memory
general
framework
corpus-based
semantics
computational
linguistics
2010
david
chalmers
strong
weak
emergence
oxford
university
press
2006
philippe
cudr´e-mauroux
emergent
semantics
encyclopedia
database
systems
springer
2009
scott
deerwester
susan
dumais
george
furnas
thomas
landauer
indexing
latent
semantic
analysis
journal
richard
harshman
american
society
information
science
:391–407
1990
j.r.
firth
synopsis
linguistic
theory
1930-1955.
studies
ling
anal.
1957
hartigan
wong
algorithm
136
k-means
clustering
al-
gorithm
journal
royal
statistical
society
series
applied
statistics
:100–108
1979
huang
n´ev´eol
recommending
mesh
terms
annotating
biomedical
articles
journal
american
medical
informatics
association
:660–667
2011
kim
aronson
wilbur
automatic
mesh
term
assignment
quality
assessment
proceedings
amia
annual
symposium
pages
319–323
amia
2001
jimmy
lin
john
wilbur
pubmed
related
articles
probabilistic
topic-
based
model
content
similarity
bmc
bioinformatics
2007
christopher
manning
prabhakar
raghavan
hinrich
schuetze
introduc-
tion
information
retrieval
cambridge
university
press
2008
n´ev´eol
shooshan
humphrey
mork
aronson
recent
advance
automatic
indexing
biomedical
literature
journal
biomedical
informatics
:814–823
2009
v´ıt
nov´aˇcek
siegfried
handschuh
stefan
decker
getting
meaning
right
complementary
distributional
layer
web
semantics
proceedings
iswc
springer
2011
ruch
automatic
assignment
biomedical
categories
toward
generic
ap-
proach
bioinformatics
2006
salton
wong
yang
vector
space
model
automatic
index-
ing
commun
acm
:613–620
1975
junichi
tsujii
reﬁne
pathtext
combines
text
mining
pathways
keynote
semantic
enrichment
scientiﬁc
literature
2009
sesl
2009
march
2009
amos
tversky
features
similarity
psychological
review
:327–352
1977
