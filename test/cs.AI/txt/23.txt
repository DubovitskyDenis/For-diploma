optimistic
agents
asymptotically
optimal
peter
sunehag
marcus
hutter
peter.sunehag
anu.edu.au
marcus.hutter
anu.edu.au
research
school
computer
science
australian
national
university
canberra
act
0200
australia
september
2012
abstract
use
optimism
introduce
generic
asymptotically
optimal
rein-
forcement
learning
agents
achieve
arbitrary
ﬁnite
com-
pact
class
environments
asymptotically
optimal
behavior
further-
ﬁnite
deterministic
case
provide
ﬁnite
error
bounds
keywords
reinforcement
learning
optimism
optimality
agents
uncertainty
introduction
article
studies
fundamental
question
artiﬁcial
intelligence
given
set
environments
deﬁne
agent
eventually
acts
optimally
re-
gardless
environments
question
relates
even
fundamental
question
intelligence
hut05
deﬁnes
intelli-
gent
agent
one
act
well
large
range
environments
studies
arbitrary
classes
environments
particular
attention
universal
classes
environments
like
computable
deterministic
environments
lower
semi-computable
stochastic
environments
deﬁnes
aixi
agent
bayesian
reinforcement
learning
agent
universal
hypothesis
class
solomonoﬀ
prior
agent
interesting
optimality
properties
be-
sides
maximizing
expected
utility
respect
priori
distribution
design
also
pareto
optimal
self-optimizing
possible
considered
class
however
shown
ors10
guaranteed
asymptotically
optimal
computable
deterministic
environments
lh11a
shows
surprising
since
least
geometric
discount-
ing
agent
lh11a
also
shows
weaker
average
sense
optimality
achieved
class
computable
environments
using
algorithm
includes
long
exploration
phases
furthermore
simple
realize
bayesian
agents
always
achieve
optimality
ﬁnite
class
deterministic
environments
even
prior
weights
strictly
positive
use
principle
optimism
deﬁne
agent
ﬁnite
class
deterministic
environments
eventually
acts
optimally
extend
results
case
ﬁnite
compact
classes
stochastic
environments
deterministic
case
also
prove
ﬁnite
error
bounds
optimism
previously
used
design
exploration
strategies
discounted
undiscounted
mdps
ks98
sl05
ao06
lh12
though
deﬁne
optimistic
algorithms
ﬁnite
class
environments
related
work
besides
aixi
hut05
discussed
lh11a
in-
troduces
agent
achieves
asymptotic
optimality
average
sense
class
deterministic
computable
environments
however
time
step
optimal
every
time
step
due
inﬁnite
number
long
exploration
phases
introduce
agent
ﬁnite
classes
environments
eventually
achieve
optimality
every
time
step
stochastic
case
agent
achieves
given
probability
optimality
within
simple
agent
relying
elegantly
principle
optimism
used
previously
restrictive
mdp
case
discounting
ks98
sl05
lh12
without
ao06
instead
indeﬁnite
number
explicitly
enforced
bursts
exploration
rh08
also
introduces
agent
relies
bursts
exploration
aim
achieving
asymptotic
optimality
asymptotic
optimality
guarantees
restricted
setting
environments
satisfy
certain
restrictive
value-preservation
property
edkm05
studied
learning
general
partially
observable
markov
decision
pro-
cesses
pomdps
though
pomdps
constitute
general
reinforcement
learning
setting
interested
agents
given
deterministic
stochastic
class
environments
successfully
utilize
knowledge
true
environment
lies
class
background
consider
agent
rn10
hut05
interacts
environment
performing
actions
ﬁnite
set
receives
observations
ﬁnite
set
rewards
ﬁnite
set
let
set
histories
return
a1o1r1a2o2r2
...
anonrn
j=1
obvious
extension
inﬁnite
sequences
function
called
deterministic
environment
studied
section
function
called
policy
agent
deﬁne
value
function
i=t
γi−tri
sequence
rewards
achieved
following
time
step
onwards
environment
seen
ht−1
ht−1
instead
viewing
environment
function
equivalently
write
function
ht−1
write
r|h
function
value
equals
zero
ﬁrst
formulation
sent
case
stochastic
environments
study
section
instead
function
r|h
furthermore
deﬁne
ht|π
or1
t|π
i=1ν
oiri|ai
hi−1
hi−1
·|π
probability
measure
strings
sequences
discussed
next
section
deﬁne
·|π
ht−1
conditioning
·|π
ht−1
deﬁne
·|π
ht−1
ν-expected
return
policy
special
case
environment
markov
decision
process
mdp
sb98
classical
setting
reinforcement
learning
case
environment
depend
full
history
latest
ob-
servation
action
therefore
function
situation
one
often
refers
observations
states
since
latest
observation
tells
everything
need
know
situation
optimal
policy
represented
function
state
set
need
base
decision
latest
observation
several
al-
gorithms
ks98
sl05
lh12
devised
solving
discounted
mdps
one
prove
pac
probably
approximately
correct
bounds
ﬁnite
time
bounds
hold
high
probability
depend
polynomially
number
states
actions
discount
factor
methods
relying
optimism
method
making
agent
suﬃciently
explorative
optimism
roughly
means
one
high
expectations
one
yet
know
optimism
also
used
prove
regret
bounds
undiscounted
mdps
ao06
extended
feature
mdps
mmr11
note
methods
restricted
mdps
make
markov
ergodicity
stationarity
etc
assumptions
environments
size
class
outline
article
deﬁne
optimistic
agents
far
general
setting
mdps
prove
asymptotic
optimality
results
question
mere
existence
already
non-trivial
hence
asymptotic
results
deserve
attention
section
consider
ﬁnite
classes
deterministic
environments
introduce
simple
optimistic
agent
guaranteed
eventually
act
optimally
also
provide
ﬁnite
error
bounds
section
generalize
ﬁnite
classes
stochastic
environments
section
compact
classes
finite
classes
deterministic
environments
given
ﬁnite
class
deterministic
environments
...
deﬁne
algorithm
unknown
environment
eventually
achieves
optimal
behavior
sense
exists
maximum
reward
achieved
time
onwards
algorithm
chooses
optimistic
hypothesis
sense
picks
environment
one
achieve
highest
reward
case
tie
choose
environment
comes
ﬁrst
enumeration
policy
optimal
environment
followed
hypothesis
contradicted
feedback
environ-
ment
new
optimistic
hypothesis
picked
environments
still
consistent
technique
important
consequence
hypothesis
contradicted
still
acting
optimally
optimizing
incorrect
hypothesis
require
finite
class
deterministic
environments
repeat
arg
maxπ∈π
ν∈mt−1
repeat
ht−1
ht−1
perceive
otrt
environment
ht−1atotrt
remove
inconsistent
environments
mt−1
hπ◦
mt−1
empty
algorithm
optimistic
agent
deterministic
environments
let
environment
called
consistent
hπ◦
history
time
generated
policy
environment
particular
let
hπ◦
history
generated
algorithm
policy
interacting
actual
true
environment
end
cycle
know
let
environments
consistent
algorithm
needs
check
whether
oπ◦
mt−1
since
previous
cycles
ensure
hπ◦
maximization
algorithm
deﬁnes
optimism
time
performed
set
consistent
hypotheses
time
πall
class
deterministic
policies
t−1
ht−1
trivially
aπ◦
rπ◦
theorem
optimality
finite
deterministic
class
use
algorithm
environment
max
key
proving
theorem
time-consistency
lh11b
geometric
dis-
counting
following
lemma
tells
act
optimally
respect
chosen
optimistic
hypothesis
remains
optimistic
contradicted
lemma
time-consistency
suppose
arg
maxπ∈π
ν∈mt
act
according
time
time
˜t−1
still
consistent
time
arg
maxπ∈π
ν∈m˜t
proof
suppose
˜t−tv
h˜t
accumulated
reward
let
h˜t
holds
h˜t
h˜t
policy
equals
equals
follows
˜t−tv
contradicts
assumption
arg
maxπ∈π
ν∈mt
h˜t
therefore
h˜t
˜t−tv
h˜t
h˜t
proof
theorem
time
know
mt−1
inconsistent
i.e
hπ◦
gets
removed
i.e
mt′
since
ﬁnite
inconsistencies
happen
ﬁnitely
often
i.e
onwards
since
hπ◦
know
assume
henceforth
optimistic
hypothesis
change
optimistic
hypothesis
true
environment
point
obviously
chosen
true
optimal
policy
general
optimistic
hypothesis
never
con-
tradicted
actions
taken
according
hence
change
anymore
implies
max
ν∈mt
max
π∈π
max
π∈π
ﬁrst
equality
follows
equals
on-
wards
second
equality
follows
consistency
third
equality
follows
optimism
constancy
time-consistency
geometric
discounting
lemma
last
inequality
follows
reverse
inequality
follows
therefore
acting
optimally
times
maxπ
besides
eventual
optimality
guarantee
also
provide
bound
number
time
steps
value
following
algorithm
certain
less
optimal
reason
bound
true
suboptimality
certain
number
time
steps
point
current
hypothesis
becomes
inconsistent
number
inconsistency
points
bounded
number
environments
theorem
finite
error
bound
following
algorithm
max
π∈π
|m|
log
1−γ
γ−1
time
steps
proof
consider
ℓ-truncated
value
t+ℓ
i=t+1
γi−t−1ri
sequence
rewards
achieved
following
time
seeing
letting
log
1−γ
positive
due
negativity
numerator
denominator
achieve
1−γ
let
policy-environment
pair
selected
algorithm
cycle
log
let
ﬁrst
assume
hπ◦
t+1
t+ℓ
hπ◦
t+1
t+ℓ
change
...
inner
loop
algorithm
consistent
t+1
t+ℓ
i.e
hence
1−γ
bound
extra
terms
drop
terms
ht+1
t+ℓ
max
π∈π
max
ν∈mt
1−γ
def
ht+1
t+ℓ
max
π∈π
let
...
times
currently
selected
inconsistent
i.e
...
hπ◦
t+1
t+ℓ
times
maxπ∈π
except
possibly
finally
i=1
ti−ℓ
...
ti−1
implies
therefore
gets
t+1
t+ℓ
|t×|
ℓ·k
ℓ·|m|
log
log
|m|
|m|
log
refer
algorithm
conservative
agent
since
sticks
model
long
corresponding
liberal
agent
reevaluates
optimistic
hypothesis
every
time
step
switch
diﬀerent
optimistic
policies
time
algorithm
actually
special
case
shown
lemma
liberal
agent
really
class
algorithms
larger
class
algorithms
consists
exactly
algorithms
optimistic
every
time
step
without
restrictions
conservative
agent
subclass
algorithms
switch
hypothesis
previous
contra-
dicted
results
conservative
agent
extended
liberal
one
omit
space
reasons
stochastic
environments
stochastic
hypothesis
may
never
become
completely
inconsistent
sense
assigning
zero
probability
observed
sequence
still
assigning
diﬀerent
probabilities
true
environment
therefore
exclude
based
threshold
probability
assigned
generated
history
unlike
deterministic
case
hypothesis
cease
optimistic
one
without
excluded
therefore
consider
algorithm
reeval-
uates
optimistic
hypothesis
every
time
step
algorithm
speciﬁes
procedure
theorem
states
asymptotically
optimal
require
finite
class
stochastic
environments
threshold
repeat
ht−1
arg
maxπ
ν∈mt
ht−1
perceive
otrt
environment
ht−1atotrt
mt−1
end
time
algorithm
optimistic
agent
stochastic
finite
class
max
˜ν∈m
ht|a1
ht|a1
theorem
optimality
finite
stochastic
class
deﬁne
using
algo-
rithm
threshold
ﬁnite
class
stochastic
envi-
ronments
containing
true
environment
probability
z|m
exists
every
number
max
borrow
techniques
hut09
introduced
merging
opinions
result
generalized
classical
theorem
bd62
classical
result
says
suﬃcient
true
measure
inﬁnite
sequences
absolutely
continuous
respect
chosen
priori
distribution
guarantee
almost
surely
merge
sense
total
variation
distance
generalized
version
given
lemma
combine
policy
environment
letting
actions
taken
policy
deﬁned
measure
denoted
·|π
space
inﬁnite
sequences
ﬁnite
alphabet
denote
sample
sequence
elements
σ-algebra
generated
cylinder
sets
γy1
ω|ω1
measure
determined
values
sets
simplify
notation
next
lemmas
write
·|π
meaning
ht|a1
ojrj
hj−1
furthermore
·|ht
·|ht
deﬁnition
total
variation
distance
total
variation
distance
two
measures
inﬁnite
sequences
elements
ﬁnite
alphabet
deﬁned
sup
previously
speciﬁed
σ-algebra
generated
cylinder
sets
results
hut09
based
fact
martin-
gale
sequence
true
measure
therefore
converges
probability
doo53
crucial
question
limit
strictly
positive
following
lemma
shows
probability
either
case
limit
case
·|ω1
·|ω1
say
environments
merge
·|π
·|π
lemma
generalized
merging
opinions
hut09
measures
holds
·|ω1
·|ω1
lemma
value
convergence
merging
environments
given
policy
environments
follows
·|ht
·|ht
proof
lemma
follows
general
inequality
cid:12
cid:12
cid:12
cid:12
sup
sup
cid:12
cid:12
cid:12
cid:12
inserting
·|ht
·|ht
using
following
lemma
replaces
property
deterministic
environments
either
consistent
indeﬁnitely
probability
generated
history
becomes
lemma
merging
environments
suppose
given
two
environments
true
one
policy
deﬁned
e.g
algorithm
let
·|π
·|π
probability
lim
t→∞
lim
t→∞
proof
follows
combination
lemma
lemma
next
lemma
tells
happens
environments
removed
removed
state
time
notational
simplicity
lemma
optimism
nearly
optimal
suppose
ﬁnite
inﬁnite
class
possibly
stochastic
environments
containing
true
en-
vironment
also
suppose
none
environments
excluded
time
algorithm
inﬁnite
history
generated
running
given
max
∀ν1
proof
theorem
given
policy
let
·|π
true
environment
·|π
let
outcome
sequence
sequence
o1r1
o2r2
...
denoted
follows
doob
martingale
inequality
doo53
sup
1/z
implies
inf
proves
using
union
bound
probability
algorithm
ever
excluding
true
environment
less
z|m
limits
ht|π◦
ht|π◦
converge
almost
surely
argued
using
martin-
gale
convergence
theorem
lemma
tells
given
environment
probability
one
eventually
excluded
permanently
included
merge
true
one
remaining
environments
according
sense
lemma
merge
true
environment
lemma
tells
diﬀerence
value
functions
policy
merg-
ing
environments
converges
zero
since
ﬁnitely
many
environments
ones
remain
indeﬁnitely
merge
true
environment
every
following
holds
∀ν1
proof
concluded
lemma
case
true
environment
remains
indeﬁnitely
included
happens
probability
z|m
compact
classes
section
discuss
inﬁnite
compact
classes
stochastic
environ-
ments
first
note
without
assumptions
asymptotic
optimality
impossible
achieve
even
countably
inﬁnite
deterministic
environments
lh11a
consider
classes
compact
respect
total
variation
distance
precisely
respect
max
·|h
·|h
total
variation
distance
section
example
class
markov
decision
processes
pomdps
certain
number
states
algorithm
need
modiﬁcation
achieve
asymptotic
optimality
compact
case
alternative
modifying
algorithm
satisﬁed
reaching
optimality
within
pre-chosen
achieved
ﬁrst
choosing
ﬁnite
covering
balls
total
variation
radius
less
use
algorithm
centers
balls
algorithm
eventually
achieves
optimality
within
demanding
task
need
able
say
true
environment
remain
indeﬁnitely
considered
class
given
conﬁdence
purpose
introduce
conﬁdence
radius
inspired
mdp
solving
algorithms
like
mbie
sl05
ucrl
ao06
still
use
notation
algorithm
deﬁne
algorithm
based
replacing
larger
˜mt
true
environment
likely
excluded
deﬁnition
conﬁdence
radius
denote
environments
within
˜mt
∃˜ν
given
say
almost
surely
true
environment
˜mt
probability
p-conﬁdence
radius
sequence
deﬁnition
algorithm
given
class
environments
compact
total
variation
distance
deﬁne
algorithm
algorithm
replaced
˜mt
deﬁnition
radon-nikodym
diﬀerentiable
class
suppose
class
true
environment
policy
holds
probability
one
ht|π
ht|π
converges
random
variables
call
class
radon-nikodym
diﬀerentiable
property
holds
respect
speciﬁc
policy
say
class
rn-diﬀerentiable
respect
remark
13.
every
countable
class
rn-diﬀerentiable
class
mdps
certain
number
states
mbie
sl05
ucrl
ao06
algorithms
based
fact
one
deﬁne
conﬁdence
radiuses
mdps
though
bounds
need
separate
intervals
state-action
pair
depending
number
visits
ergodic
mdp
state-action
pairs
almost
surely
seen
inﬁnitely
often
max
length
intervals
tend
zero
therefore
one
deﬁne
radius
based
maximum
length
alternatively
one
easily
allow
algorithm
run
rectangular
sets
instead
theorem
optimality
compact
stochastic
class
suppose
use
algo-
rithm
threshold
compact
total
variation
rn-diﬀerentiable
class
respect
enough
stochastic
environments
denote
resulting
policy
conﬁdence
radius
sequence
true
environment
probability
every
tim
max
lemma
uniform
exclusion
let
·|π◦
·|π◦
true
environment
policy
deﬁned
algorithm
outcome
sequence
let
closed
subset
every
every
subset
proof
since
compact
subset
question
closed
follows
also
compact
using
arzel
a-ascoli
theorem
rud76
conclude
subsequence
converges
uniformly
means
let
min
proof
theorem
strategy
use
environment
excluded
lie
within
certain
distance
environment
merges
true
one
excluded
certain
ﬁnite
time
say
remaining
environments
value
functions
diﬀer
certain
amount
apply
lemma
probability
one
say
hold
ht|π◦
ht|π◦
converges
environment
·|ht
·|ht
compact
total
variation
distance
topology
since
closed
subset
topology
deﬁned
compact
set
˜ε1
following
consider
total
variation
ball
radius
˜ε1/4
note
˜ε1/2
whenever
collection
balls
induces
open
cover
compact
set
follows
ﬁnite
subcover
consider
balls
ﬁnite
cover
intersect
let
union
ﬁnitely
many
open
balls
let
closed
subset
want
say
ﬁnite
time
environments
excluded
˜mt
happens
deﬁned
union
closed
balls
radius
every
point
excluded
large
enough
also
closed
subset
lemma
tells
environments
excluded
ﬁnite
amount
time
therefore
environments
excluded
˜mt
thus
˜mt
particular
optimistic
hypothesis
let
optimistic
hypothesis
time
optimistic
policy
parameter
particular
lies
within
ball
center
lies
within
point
hence
due
uniform
merging
environments
˜ε1/2
∀ν1
˜ε1
∀ν1
since
˜mt
conclude
˜ε1/2
˜ε1
∀ν1
˜mt
lemma
know
picked
˜ε1
small
enough
know
ε/2
˜mt
furthermore
picking
˜ε1
suﬃciently
small
ensure
˜mt
ε/2
given
true
environment
remains
indeﬁnitely
˜mt
happens
least
probability
follows
max
conclusions
introduced
optimistic
agents
ﬁnite
compact
classes
arbitrary
en-
vironments
proved
asymptotic
optimality
deterministic
case
also
bound
number
time
steps
value
following
algorithm
certain
amount
lower
optimal
future
work
includes
inves-
tigating
ﬁnite-error
bounds
classes
stochastic
environments
acknowledgement
work
supported
arc
grant
dp120100950
authors
grateful
feedback
tor
lattimore
wen
shao
references
ao06
auer
ortner
logarithmic
online
regret
bounds
undis-
counted
reinforcement
learning
proceedings
nips
2006
pages
49–56
2006
bd62
blackwell
dubins
merging
opinions
increasing
in-
formation
annals
mathematical
statistics
:882–886
1962
doo53
doob
stochastic
processes
wiley
new
york
1953
edkm05
even-dar
kakade
mansour
reinforcement
learning
proceedings
ijcai-05
pages
690–695
pomdps
without
resets
2005
hut05
hutter
universal
articial
intelligence
sequential
decisions
based
algorithmic
probability
springer
berlin
2005
hut09
hutter
discrete
mdl
predicts
total
variation
advances
neural
information
processing
systems
nips
2009
pages
817–825
2009
ks98
kearns
singh
near-optimal
reinforcement
learning
polynomial
time
proceedings
15nd
international
conference
machine
learning
icml
1998
pages
260–268
1998
lh11a
lattimore
hutter
asymptotically
optimal
agents
proc
algorithmic
learning
theory
alt
2011
volume
6925
lecture
notes
computer
science
pages
368–382
springer
2011
lh11b
lattimore
hutter
time
consistent
discounting
proc
22nd
international
conf
algorithmic
learning
theory
alt
vol-
ume
6925
lnai
pages
383–397
espoo
finland
2011.
springer
berlin
lh12
lattimore
hutter
pac
bounds
discounted
mdps
proc
23rd
international
conf
algorithmic
learning
theory
alt
volume
7568
lnai
lyon
france
2012.
springer
berlin
mmr11
o.-a
maillard
munos
ryabko
selecting
state-
representation
reinforcement
learning
advances
neural
informa-
tion
processing
systems
nips
2011
pages
2627–2635
2011
ors10
orseau
optimality
issues
universal
greedy
agents
static
priors
proc
algorithmic
learning
theory
alt
2010
volume
6331
lecture
notes
computer
science
pages
345–359
springer
2010
rh08
ryabko
hutter
possibility
learning
reactive
en-
vironments
arbitrary
dependence
theor
c.s.
405
:274–284
2008
rn10
russell
norvig
artiﬁcial
intelligence
modern
approach
prentice
hall
englewood
cliﬀs
3nd
edition
2010
rud76
rudin
principles
mathematical
analysis
mcgraw-hill
1976
sb98
sutton
barto
reinforcement
learning
mit
press
1998
sl05
strehl
littman
theoretical
analysis
model-based
interval
estimation
proceedings
icml
2005
pages
856–863
2005
