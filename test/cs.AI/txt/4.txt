optimal
sensing
via
multi-armed
bandit
relaxations
mixed
observability
domains
mikko
lauri
risto
ritala
abstract—
sequential
decision
making
uncertainty
studied
mixed
observability
domain
goal
maximize
amount
information
obtained
partially
observable
stochastic
process
constraints
imposed
fully
observable
internal
state
upper
bound
optimal
value
function
derived
relaxing
constraints
identify
conditions
relaxed
problem
multi-
armed
bandit
whose
optimal
policy
easily
computable
upper
bound
applied
prune
search
space
original
problem
effect
solution
quality
assessed
via
simulation
experiments
empirical
results
show
effective
pruning
search
space
target
monitoring
domain
introduction
deploying
autonomous
agents
robots
equipped
appropriate
set
sensors
allows
automated
ex-
ecution
various
information
gathering
tasks
tasks
include
monitoring
identiﬁcation
spatio-temporal
processes
automated
exploration
data
collection
campaigns
environments
human
presence
unde-
sired
infeasible
robots
mobile
sensor
platforms
whose
actions
optimized
maximize
informativeness
measurement
data
target
state
known
probability
den-
sity
function
pdf
state
called
belief
state
maintained
information
conveyed
measurement
data
incorporated
belief
state
bayesian
ﬁltering
as-
suming
markovian
dynamics
conditional
independence
measurement
data
given
system
state
problem
partially
observable
markov
decision
process
pomdp
optimal
information
gathering
studied
context
sensor
management
review
apply-
ing
pomdps
sensor
management
presented
problem
formulated
decision
process
uncertainty
goal
ﬁnd
control
policy
mapping
belief
states
actions
followed
maximizes
expected
sum
discounted
rewards
horizon
time
reward
associated
action
may
depend
either
true
state
system
belief
state
former
encode
objectives
reaching
favorable
state
avoiding
costly
ones
useful
e.g
navigation
obstacle
avoidance
latter
option
allows
information
theoretic
rewards
mutual
information
applied
various
sequential
information
gathering
problems
robotics
see
e.g
indeﬁnite-horizon
problems
terminate
lauri
ritala
department
automation
sci-
ence
engineering
tampere
university
technology
p.o
box
692
fi-33101
tampere
finland
email
mikko.lauri
tut.fi
risto.ritala
tut.fi
special
stopping
action
executed
natural
model
tasks
may
stopped
certain
level
conﬁdence
state
reached
finding
optimal
policies
pomdps
computationally
hard
several
approximate
methods
sug-
gested
point-based
algorithms
track
so-called
alpha
vectors
set
points
belief
space
alpha
vectors
may
used
approximate
optimal
policy
belief
state
online
planning
methods
ﬁnd
optimal
action
current
belief
state
instead
representation
optimal
policy
problem
cast
search
tree
belief
states
reachable
current
belief
state
various
action-observation
histories
combining
online
methods
monte
carlo
simulations
evaluate
utility
actions
lead
approximate
algorithms
able
handle
problems
1052
states
mixed
observability
domains
part
state
space
fully
observable
belief
space
union
low-
dimensional
subspaces
one
value
fully
ob-
servable
state
variable
robotic
systems
often
exhibit
mixed
observability
may
exploited
derive
efﬁcient
pomdp
algorithms
multi-armed
bandit
mab
model
sequential
decision-making
also
applied
sensor
management
decision-maker
plays
one
arm
mab
collects
reward
depending
state
arm
arm
randomly
transitions
new
state
arms
remain
stationary
solutions
mabs
index
policies
easier
compute
solutions
general
pomdps
aforementioned
research
applies
reward
func-
tions
depend
true
state
action
expectation
reward
linear
belief
state
feature
leveraged
many
solution
algorithms
information
theoretic
quantities
entropy
mutual
information
would
useful
reward
functions
optimal
sensing
problems
nonlinear
belief
state
classical
pomdp
algorithms
applied
solve
problems
paper
study
pomdps
mixed
observability
mutual
information
reward
function
approach
especially
suited
optimal
sensing
problems
robotics
domains
remove
constraints
available
actions
obtain
relaxed
problem
optimal
value
relaxed
problem
obtained
upper
bound
optimal
value
pomdp
identify
conditions
relaxed
problem
mab
easily
computable
optimal
solution
upper
bound
applied
online
planning
algorithm
prune
search
space
paper
organized
follows
section
mixed-
observability
pomdp
deﬁned
section
iii
methods
solving
problem
discussed
section
two
relaxations
derived
provide
upper
bounds
optimal
value
function
section
determines
conditions
relaxations
mabs
empirical
results
provided
section
section
vii
concludes
paper
mixed
observability
pomdp
notation
denote
random
variables
sets
up-
percase
letters
realizations
random
variables
members
sets
lowercase
letters
time
instants
distinguished
writing
e.g
realizations
time
respectively
agent
e.g
robot
another
sensor
platform
internal
state
captures
dynamics
con-
straints
operating
on-board
sensors
devices
internal
state
evolves
according
deterministic
dynamics
model
deﬁned
control
action
ﬁnite
set
actions
allowed
internal
state
let
denote
set
random
inference
variables
agent
wishes
obtain
infor-
mation
dynamics
variables
governed
stochastic
model
deﬁned
markov
chain
complete
state
system
problem
features
mixed
observability
in-
ternal
state
fully
observable
inference
variables
partially
observable
agent
observations
follow
observation
model
deﬁned
agent
maintains
belief
state
consisting
deterministic
fully
observable
internal
state
pdf
initial
belief
state
given
given
belief
state
action
observation
belief
state
next
time
instant
given
belief
update
equation
pdf
inference
variables
obtained
bayesian
ﬁlter
py∈y
predictive
pdf
normalization
factor
denoting
prior
probability
observing
given
sequence
actions
observations
uncertainty
resulting
internal
state
thus
equivalently
deﬁne
set
allowed
actions
via
belief
state
agent
objective
encoded
reward
function
objective
maximize
expected
sum
discounted
rewards
horizon
decisions
discount
factor
consider
belief-dependent
reward
functions
let
i.e
mutual
information
posterior
state
observation
deﬁned
entropy
predictive
pdf
second
term
expected
entropy
posterior
pdf
prior
pdf
problem
×dy
instance
pomdp
bellman
principle
optimality
solution
may
found
via
backward
time
recursion
procedure
known
value
iteration
optimal
value
function
maps
belief
state
maximum
expected
sum
discounted
rewards
optimal
policy
followed
next
decisions
optimal
value
functions
computed
γxz∈z
max
a∈ab
t−1
starting
optimal
policy
remaining
decisions
found
extracting
argument
maximizing
recursion
continued
iii
solving
pomdps
belief-dependent
rewards
pomdps
reward
function
state-dependent
expectation
linear
belief
state
ﬁnite-
horizon
optimal
value
function
ﬁnite
representa-
tion
convex
hull
set
hyperplanes
belief
space
many
exact
approximate
ofﬂine
algorithms
pomdps
rely
piecewise
lin-
earity
convexity
value
function
reward
functions
mutual
information
entropy
useful
optimal
sensing
problems
nonlinear
belief
state
thus
ofﬂine
algorithms
applicable
solve
recursion
belief-dependent
reward
function
online
planning
methods
ﬁnd
optimal
action
current
belief
state
instead
closed
form
representation
optimal
policy
explicit
representations
policies
required
nonlinear
belief-dependent
reward
func-
tion
constitute
additional
difﬁculty
online
planning
tree
graph
belief
states
reachable
current
belief
state
constructed
current
belief
state
root
tree
belief
states
computed
via
added
child
nodes
node
desired
search
depth
reached
values
leaves
tree
propagated
back
root
according
suboptimal
actions
may
sometimes
pruned
search
tree
branch-and-bound
pruning
optimal
value
executing
action
belief
state
upper
bound
lower
bound
given
action
suboptimal
successor
nodes
may
pruned
tree
bounds
may
similarly
propagated
via
number
belief
states
search
tree
reduced
alternatives
online
tree
search
include
e.g
specialized
approximate
methods
however
limited
small
prob-
lems
open-loop
approximation
applied
receding
horizon
control
principle
reduced
value
iteration
gaussian
beliefs
mixed-observability
case
theoretical
treatment
nonlinear
convex
reward
functions
pomdps
refer
reader
bounds
value
function
optimal
policy
attains
optimal
value
belief
states
policy
achieves
value
lower
bound
optimal
value
simple
choice
set
greedy
one-step
look-ahead
policy
argmaxa∈ab
options
include
random
policies
blind
policies
always
executing
single
ﬁxed
action
upper
bounds
found
deriving
two
relaxed
versions
original
pomdp
problem
removing
constraints
applicable
actions
set
internal
states
reachable
subset
single
time
step
∀x∈xs
∀a∈a
set
internal
states
reachable
steps
ﬁrst
relaxation
obtained
removing
constraints
imposed
internal
state
follows
times
universal
sensor
relaxation
given
pomdp
problem
universal
sensor
relax-
ation
sx∈x
contains
actions
stochastic
part
replaced
consider
actions
applicable
internal
states
reachable
within
decisions
current
time
step
obtain
k-step
sensor
relaxation
k-step
sensor
relaxation
given
pomdp
problem
k-step
sensor
relaxation
ˆak
ˆak
=si∈f
set
actions
possible
internal
states
reachable
within
time
steps
current
internal
state
ˆak
optimal
value
either
relaxed
problem
greater
equal
optimal
value
original
problem
let
denote
optimal
value
functions
respectively
let
denote
value
function
greedy
policy
given
one
machine
played
agent
per
action
state
machine
evolves
agent
may
affect
machines
played
remain
current
state
machines
independent
machines
played
contribute
reward
gittins
showed
optimal
policies
mabs
so-called
greedy
index
allocation
policies
arm
allocation
index
known
gittins
index
calculated
optimal
selection
yielding
highest
index
value
index
policies
optimal
actions
irrevocable
action
available
stage
may
chosen
later
stage
reward
excluding
effect
discount
factor
index
policies
usually
much
easier
compute
backward
induction
solutions
pomdps
index
policy
general
optimal
mixed-
observability
pomdp
section
actions
irrevocable
due
constraints
imposed
internal
state
however
relaxations
ﬁxed
action
space
following
three
properties
required
relaxations
mabs
results
derived
hold
restricted
case
well
property
related
property
given
conditional
values
subset
inference
variables
stationary
i.e
yyi∈ya
yyi∈y
\ya
dirac
delta
function
property
observation
conditional
i.e
i.e
prior
corollary
qa∈
inference
variables
independent
subsets
properties
hold
independence
preserved
posterior
yk∈
furthermore
holds
optimal
value
bounds
multi-armed
bandit
index
policies
pomdp
relaxations
relaxations
deﬁned
pomdps
solving
even
relaxed
problems
may
thus
computa-
tionally
intractable
task
motivates
identifying
pomdps
whose
relaxations
easily
computable
optimal
policies
multi-armed
bandit
mab
problem
decision-maker
plays
one
arm
mab
collects
reward
depending
state
arm
four
requirements
distinguish
mabs
among
general
stochastic
control
problems
exactly
equation
seen
hold
applying
given
prior
models
satisfying
equation
seen
hold
two
steps
first
due
independence
struc-
ture
prior
posterior
=pk∈
similarly
second
see
applying
steps
leads
state
main
result
determining
conditions
pomdp
relaxation
mab
proposition
mab
equivalence
pomdp
relaxations
properties
1-3
fulﬁlled
prior
relaxations
multi-
qa∈
armed
bandit
problems
proof
sketch
consider
four
requirements
mabs
introduced
property
establishes
arms
bandit
partly
satisfying
requirement
rest
require-
ments
satisﬁed
properties
establish
states
bandit
arms
requirement
satisﬁed
independence
properties
ﬁrst
part
corollary
latter
part
corollary
shows
requirement
satisﬁed
proposition
holds
optimal
policies
respectively
optimal
values
thus
greedy
index
policies
values
much
easier
compute
general
pomdps
let
consider
following
example
problem
monitoring
reactive
targets
agent
located
every
time
step
agent
may
either
stay
move
one
neighboring
locations
applicable
actions
let
assuming
value
target
present
location
target
reacts
agent
presence
agent
records
measurements
according
implements
online
search
belief
states
reachable
current
belief
state
applies
lower
upper
bounds
prune
suboptimal
actions
applied
greedy
lower
bound
section
upper
bounds
section
compared
approach
exhaustive
search
reachable
belief
states
equivalent
using
lower
upper
bounds
pomcp
algorithm
gives
recommendation
next
action
execute
based
series
monte
carlo
simulations
case
properties
1-3
satisﬁed
deﬁned
proposition
holds
two-state
denotes
markov
chains
parameters
transitions
probability
sampled
uniformly
random
0.8
1.0
set
1000
initial
belief
states
0.0
0.2
satisfying
independence
assumption
inference
variables
sampled
uniformly
random
greedy
mab
policies
give
valid
upper
bounds
see
applying
rtbss
bounds
hence
always
ﬁnds
optimal
solution
veriﬁed
simulations
number
visited
nodes
search
tree
1000
belief
states
shown
fig
tighter
applying
upper
bounds
since
bound
results
lower
equal
number
visited
nodes
comparison
average
number
visited
nodes
exhaustive
search
shown
table
note
applying
either
bound
greatly
reduces
number
visited
nodes
cases
order
magnitude
although
reduction
number
visited
nodes
substantial
evaluating
bounds
computational
cost
must
balanced
savings
visiting
fewer
nodes
point
discussed
detail
next
subsection
0.5
0.5
false
negative
positive
probabilities
respectively
1−p
reward
function
consider
relaxations
problem
property
immediately
seen
satisﬁed
property
satisﬁed
i.e
targets
remain
stationary
agent
present
may
chosen
freely
property
satisﬁed
observation
depends
value
empirical
evaluation
ran
simulation
experiments
monitoring
problem
deﬁned
inference
variables
arranged
rectangular
two-dimensional
four-
connected
grid
agent
allowed
move
grid
sense
targets
examined
two
cases
ﬁrst
case
properties
1-3
satisﬁed
second
case
relaxed
property
allowing
inference
variables
change
state
cases
optimization
horizon
varied
decisions
parameters
0.05
0.05
0.95.
implemented
real-time
belief
space
search
rtbss
algorithm
presented
rtbss
105
104
103
102
101
105
104
103
102
101
101
103
102
104
nodes
expanded
105
101
103
102
104
nodes
expanded
105
fig
number
search
tree
nodes
expanded
rtbss
upper
bound
x-axis
y-axis
diagonal
line
shows
two
values
equal
pomcp
recommendations
coincide
optimal
ac-
tion
reliably
number
simulations
in-
creased
optimization
horizon
short
see
table
compared
values
optimal
actions
recom-
mended
pomcp
two
differed
difference
table
table
average
number
nodes
expanded
exhaustive
search
percentage
rtbss
solutions
agreeing
optimal
nodes
4.0
102
3.8
103
3.6
104
3.5
105
solution
property
satisfied
dynamics
bound
percentage
pomcp
recommendations
agreeing
table
optimal
simulations
101
102
103
104
40.7
57.4
69.5
75.0
36.8
50.1
62.2
74.9
31.3
45.3
54.0
69.1
28.1
43.2
47.6
63.5
29.2
40.1
46.8
59.1
two
values
performance
loss
computed
mean
values
worst-case
maximum
values
results
shown
table
iii
performance
loss
tends
greater
fewer
simulations
greater
optimization
horizon
number
simulations
increases
mean
performance
loss
low
indicating
average
pomcp
performs
well
compared
optimal
solution
however
even
mean
performance
loss
low
worst
case
performance
loss
following
pomcp
recommendations
may
signiﬁcantly
greater
problems
suboptimal
actions
may
lead
unacceptable
performance
loss
methods
rtbss
valid
bounds
may
preferable
pomcp
case
property
satisﬁed
next
examined
case
property
satisﬁed
set
dynamics
models
two-state
markov
chain
considered
three
subcases
distinguished
rate
state
transitions
slow
medium
fast
slow
dynamics
parameters
sampled
uniformly
random
slow
0.8
1.0
medium
dy-
namics
med
0.6
0.8
fast
dynamics
ast
0.4
0.6
experiment
repeated
1000
randomly
sampled
initial
belief
states
dynamics
models
beliefs
satisﬁed
independence
assumption
inference
variables
0.2
0.4
med
0.0
0.2
slow
0.4
0.6
ast
problem
quite
similar
one
subsec-
tion
vi-a
pomcp
performance
also
observed
good
average
mab
equivalence
proposition
satisﬁed
relaxed
problems
thus
upper
bounds
approximate
optimality
rtbss
guaranteed
examined
effect
solutions
provided
rtbss
results
summarized
table
table
shows
percentage
solutions
equal
optimal
solution
case
slow
medium
fast
dynamics
either
universal
sensor
upper
bound
k-step
sensor
upper
bound
optimal
solutions
found
majority
cases
percentage
decreasing
optimization
horizon
slow
medium
fast
100
100
100
100
100
100
100
100
99.9
99.9
99.6
99.6
100
100
100
100
99.8
99.7
99.1
98.8
99.9
100
100
100
99.9
99.7
99.0
98.9
100
100
greater
rate
dynamics
faster
since
often
likely
bound
obtained
universal
sensor
relaxation
overestimate
optimal
value
consequently
better
agreement
optimal
solution
observed
results
suggest
may
still
reasonable
approximate
upper
bounds
value
greedy
policies
relaxed
problems
even
optimality
guaranteed
efﬁciency
pruning
search
tree
affected
signiﬁcantly
compared
case
previous
subsection
applying
either
bound
dramatically
reduced
number
visited
nodes
search
tree
examined
mean
time
required
ﬁnd
solution
belief
state
either
exhaus-
tive
search
branch-and-bound
pruning
representative
comparison
presented
fig
case
medium
dynamics
exhaustive
search
performs
fastest
computational
burden
computing
bounds
outweighs
savings
visiting
fewer
nodes
search
advantages
pruning
search
tree
become
apparent
best
applying
pruning
order
magnitude
faster
exhaustive
search
upper
bound
fastest
using
upper
bound
faster
exhaustive
search
comparing
computation
times
pomcp
rtbss
meaningful
experiments
run
different
computer
platforms
different
implementations
e.g
search
trees
104
103
102
101
100
10−1
10−2
case
medium
dynamics
exhaustive
optimization
horizon
fig
mean
runtime
per
decision
milliseconds
function
optimization
horizon
exhaustive
branch-and-bound
search
applying
upper
bounds
performance
loss
pomcp
compared
optimal
table
iii
simulations
mean
0.0749
0.0298
0.0122
0.0064
101
102
103
104
max
0.3930
0.2296
0.1025
0.0503
mean
0.0948
0.0518
0.0294
0.0168
max
0.5278
0.3299
0.1751
0.0893
mean
0.1061
0.0584
0.0402
0.0261
max
0.5283
0.2953
0.2035
0.1399
mean
0.1084
0.0642
0.0487
0.0317
max
0.5587
0.3251
0.3639
0.1816
mean
0.1175
0.0670
0.0518
0.0380
max
0.6278
0.3968
0.2839
0.2020
vii
conclusions
optimal
sensing
problem
mixed-observability
domain
internal
state
fully
observable
set
inference
variables
partially
observable
formulated
pomdp
objective
sequential
maximization
mutual
information
inference
variables
obser-
vations
upper
bounds
optimal
value
function
found
relaxing
constraints
original
problem
three
conditions
fulﬁlled
relaxed
problems
mabs
first
action
related
unique
subset
inference
variables
secondly
inference
variables
subset
corresponding
current
action
evolve
inference
variables
remain
stationary
finally
observations
depend
inference
variables
subset
related
current
action
optimal
solution
mab
problem
greedy
index
allocation
policy
much
easier
ﬁnd
solving
general
pomdp
pomdp
solved
branch-and-bound
search
effectiveness
bounds
pruning
search
space
empirically
veriﬁed
target
monitoring
problem
finding
optimal
action
requires
searching
fraction
reachable
belief
states
compared
exhaustive
search
computation
time
best
order
magnitude
smaller
applying
pruning
computational
savings
become
apparent
savings
due
reduced
search
space
size
exceed
additional
cost
computing
bounds
future
work
includes
studying
applicability
method-
ology
wider
range
mixed
observability
domains
mo-
tivated
positive
results
optimality
greedy
policies
restless
bandit
problem
believe
may
exist
classes
stochastic
control
problems
currently
known
greedy
policy
optimal
identifying
classes
would
expand
applicability
results
references
kaelbling
littman
cassandra
planning
acting
partially
observable
stochastic
domains
artiﬁcial
intelligence
vol
101
1-2
99–134
1998
hero
casta˜n´on
cochran
kastella
eds.
foun-
dations
applications
sensor
management
new
york
springer
2007
chong
kreucher
hero
partially
observable
markov
decision
process
approximations
adaptive
sensing
discrete
event
dynamic
systems
vol
377–422
may
2009
charrow
kumar
michael
approximate
representations
multi-robot
control
policies
maximize
mutual
information
autonomous
robots
vol
383–400
aug.
2014
atanasov
daniilidis
pappas
information
acquisition
sensing
robots
algorithms
error
bounds
ieee
int
conf
robotics
automation
icra
hong
kong
china
june
2014
6447–6454
lauri
ritala
stochastic
control
maximizing
mutual
information
active
sensing
icra
2014
workshop
robots
homes
industry
look
first
hong
kong
china
june
2014
hansen
indeﬁnite-horizon
pomdps
action-based
ter-
mination
proceedings
national
conference
artiﬁcial
intelligence
vancouver
canada
july
2007
1237–1242
papadimitriou
tsitsiklis
complexity
markov
decision
processes
mathematics
operations
research
vol
441–450
1987
spaan
vlassis
perseus
randomized
point-based
value
iteration
pomdps
journal
artiﬁcial
intelligence
re-
search
vol
195–220
2005
pineau
gordon
thrun
anytime
point-based
approxima-
tions
large
pomdps
journal
artiﬁcial
intelligence
research
vol
335–380
2006
ross
pineau
paquet
chaib-draa
online
planning
algorithms
pomdps
journal
artiﬁcial
intelligence
research
vol
663–704
2008
silver
veness
monte-carlo
planning
large
pomdps
advances
neural
information
processing
systems
vancouver
canada
dec.
2010
2164–2172
ong
png
hsu
lee
planning
uncertainty
robotic
tasks
mixed
observability
international
journal
robotics
research
vol
1053–1068
2010
hero
cochran
sensor
management
past
present
future
ieee
sensors
journal
vol
3064–3075
2011
bellman
dynamic
programming
princeton
new
jersey
prince-
ton
university
press
1957
smallwood
sondik
optimal
control
partially
observable
markov
processes
ﬁnite
horizon
operations
re-
search
vol
1071–1088
1973
lovejoy
survey
algorithmic
methods
partially
observed
markov
decision
processes
annals
operations
research
vol
47–65
1991
hauskrecht
value-function
approximations
partially
observ-
able
markov
decision
processes
journal
artiﬁcial
intelligence
research
vol
33–94
2000
krishnamurthy
djonin
structured
threshold
policies
dynamic
sensor
scheduling
partially
observed
markov
decision
process
approach
ieee
transactions
signal
processing
vol
4938–4957
oct.
2007
araya
buffet
thomas
charpillet
pomdp
extension
belief-dependent
rewards
advances
neural
information
processing
systems
vancouver
canada
dec.
2010
64–72
gittins
bandit
processes
dynamic
allocation
indices
journal
royal
statistical
society
series
methodological
148–177
1979
paquet
chaib-draa
ross
hybrid
pomdp
algorithms
proceedings
aamas
workshop
multi-agent
sequential
decision
making
uncertain
domains
hakodate
japan
may
2006
133–147
ahmad
liu
javidi
zhao
krishnamachari
optimality
myopic
sensing
multichannel
opportunistic
access
ieee
transactions
information
theory
vol
4040–
4050
2009
