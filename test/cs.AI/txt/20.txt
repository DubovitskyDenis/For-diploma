algorithms
batch
hierarchical
reinforcement
learning
tiancheng
zhao
mohammad
gowayyed
language
technologies
institute
carnegie
mellon
university
tianchez
gowayyed
cs.cmu.edu
abstract
hierarchical
reinforcement
learning
hrl
ex-
ploits
temporal
abstraction
solve
large
markov
decision
processes
mdp
provide
transfer-
able
subtask
policies
paper
introduce
off-policy
hrl
algorithm
hierarchical
q-value
iteration
hqi
show
possible
effec-
tively
learn
recursive
optimal
policies
valid
hierarchical
decomposition
original
mdp
given
ﬁxed
dataset
collected
ﬂat
stochastic
behavioral
policy
ﬁrst
formally
prove
con-
vergence
algorithm
tabular
mdp
experiments
taxi
domain
show
hqi
converges
faster
ﬂat
q-value
iteration
enjoys
easy
state
abstraction
also
demonstrate
algorithm
able
learn
optimal
poli-
cies
different
hierarchical
structures
ﬁxed
dataset
enables
model
compar-
ison
without
recollecting
data
introduction
conventional
tabular
reinforcement
learning
bottle-necked
curse
dimensionality
practical
applications
number
parameters
needs
trained
grows
exponen-
tially
respect
size
states
actions
order
make
reinforcement
learning
practically
tractable
one
line
research
hierarchical
reinforcement
learning
hrl
develops
principled
ways
temporal
state
abstraction
reduce
dimensionality
sequential
decision
making
basic
idea
temporal
abstraction
develop
macro-
actions
take
several
steps
terminate
returning
usually
good
macro-actions
aim
solve
sub-goals
multiple
macro-actions
divide
difﬁcult
tasks
several
sim-
pler
ones
addition
state
abstraction
tries
reduce
dimensionality
removing
irrelevant
state
variables
de-
cision
making
reducing
cardinality
state
space
helping
tackling
over-ﬁtting
two
techniques
lead
natural
hierarchical
control
architecture
intuitively
resembles
humans
solve
complex
tasks
another
area
research
closely
related
work
batch
reinforcement
learning
batch
reinforcement
learning
aims
learn
best
policy
ﬁxed
set
prior-known
samples
compared
on-policy
algorithms
batch
reinforce-
ment
learning
enjoys
stability
data-efﬁciency
im-
portantly
allows
apply
reinforcement
learning
prac-
tical
problem
expensive
collecting
new
samples
education
spoken
dialog
system
medical
sys-
tems
well-known
algorithms
batch
reinforcement
learn-
ing
include
least
square
policy
iteration
lspi
fitted
iteration
fqi
neural
fitted
iteration
nfq
etc
paper
combine
batch
learning
hierarchi-
cal
reinforcement
learning
order
achieve
faster
learning
speed
data
efﬁciency
model
comparison
related
work
three
major
approaches
developed
relatively
inde-
pendently
aiming
formalize
idea
abstraction
reinforcement
learning
three
approaches
op-
tion
framework
hierarchies
abstract
machines
hams
maxq
framework
option
framework
developers
augment
original
action
set
options
macro
actions
predeﬁned
policy
termination
state
ac-
tive
state
sutton
shown
system
semi-markov
decision
process
smdp
converges
unique
hierarchical
optimal
solution
using
modiﬁed
learning
algorithm
ham
framework
rather
giving
entire
policy
macro
actions
developers
need
specify
partial
program
speciﬁes
part
policy
using
hamq
learning
ham
also
converge
hierarchical
optimal
solution
last
maxq
framework
provides
elegant
formulation
decomposes
original
mdp
several
subroutines
hierarchy
algorithm
learn
policies
recursively
subroutines
therefore
maxq
framework
need
specify
policy
macro-actions
however
dietterich
shows
achieve
recursive
optimal
solution
extreme
case
arbitrarily
worse
hierarchical
optimal
solution
work
assumes
agent
interact
world
learning
however
real-world
appli-
cations
needs
hrl
usually
expensive
collect
data
terrible
failures
allowed
operation
forbids
usage
online
learning
algorithms
could
po-
tentially
preform
horribly
early
learning
stage
best
knowledge
little
prior
work
developing
batch
learning
algorithms
allow
hierarchical
smdp
trained
existing
data
set
collected
stochastic
behavior
policy
believe
algorithms
valuable
applying
hrl
complex
practical
domains
batch
learning
hsmdp
3.1
deﬁnitions
mostly
follow
deﬁnitions
maxq
framework
however
notation
simplicity
also
borrow
nota-
tions
option
framework
3.2
markov
decision
process
mdp
described
primitive
action
state
state
space
set
primitive
actions
available
cid:48
deﬁnes
transition
probability
executing
cid:48
reward
function
deﬁned
3.3
hierarchical
decomposition
mdp
decomposed
ﬁnite
set
sub-
tasks
...
convention
root
subtask
i.e
solving
solves
entire
original
mdp
semi-markov
decision
process
smdp
shares
extra
tu-
ple
termination
predicate
subtask
par-
tition
set
active
states
set
terminal
states
enters
state
subtasks
exit
immediately
i.e
otherwise
nonempty
set
actions
performed
actions
either
primitive
actions
subtask
cid:54
refer
children
subtask
evident
valid
hierarchical
decomposition
forms
direct
acyclic
graph
dag
non-terminal
node
corresponds
subtask
terminal
node
corresponds
primitive
action
later
discussion
use
hierar-
chical
decomposition
dag
interchangeably
3.4
hierarchical
policy
hierarchical
policy
set
policies
sub-
task
...
terminology
option
framework
subtask
policy
deterministic
option
otherwise
3.5
recursive
optimality
recursive
optimal
policy
mdp
hierarchical
de-
composition
hierarchical
policy
...
subtask
corresponding
policy
optimal
smdp
deﬁned
set
states
set
ac-
tions
state
transition
probability
cid:48
n|s
rewards
function
cid:48
cid:48
algorithm
problem
formulation
following
given
ﬁnite
set
samples
cid:8
...
cid:9
valid
hierarchical
decomposition
original
mdp
wish
learn
recursive
optimal
hierarchical
policy
propose
hierarchical
q-value
iteration
hqi
prove
converges
recursive
optimal
solution
hierarchical
decomposition
given
batch
sam-
ple
distribution
sufﬁcient
state
action
exploration
ba-
sic
idea
train
every
subtask
using
subtask
q-value
itera-
tion
sqi
bottom
fashion
training
prerequisite
sqi
speciﬁc
subtask
children
converged
greedy
optimal
policies
order
fulﬁl
constraint
hqi
ﬁrst
topologically
sorts
dag
run-
ning
sqi
subtasks
whose
children
primitive
actions
subtasks
converge
optimal
policy
algorithm
continues
subtasks
whose
children
either
converged
primitive
actions
show
always
exist
ordering
training
every
subtask
valid
dag
fulﬁlls
prerequisite
sqi
one
challenge
training
subtask
subtask
children
use
optimal
smdp
bellman
equation
described
maxq
framework
q-value
function
subtask
state
action
cid:48
cid:48
cid:48
n|s
cid:88
cid:48
cid:26
maxu
cid:48
cid:48
cid:80
subtask
cid:48
cid:48
cid:48
primitive
main
problem
equation
order
es-
timate
q-value
subtask
children
parent
needs
estimate
transition
probability
cid:48
n|s
distribution
exit
state
number
primitive
steps
needed
reach
termination
al-
though
termination
states
child
given
difﬁcult
estimate
joint
distribution
termination
steps
follows
policy
different
behavior
policy
without
recollecting
new
samples
since
behavior
policy
usually
random
poor
performance
collected
samples
provide
in-
formation
many
steps
subtask
would
take
terminate
following
different
optimal
policy
therefore
instead
using
bellman
equation
updates
table
parent
child
exits
use
intra-option
bellman
equation
proposed
option
framework
a|s
γvi
cid:48
cid:20
a|s
cid:48
cid:48
cid:88
cid:48
cid:21
cid:88
cid:88
a∈a
a∈a
max
cid:48
∈ui
cid:48
estimate
two
terms
cid:80
cid:80
m=1
cid:48
equation
also
yeilds
contraction
max
norm
able
learn
table
observing
every
new
reward
cid:48
n|s
an-
eliminates
need
estimate
key
beneﬁt
use
ﬂat
samples
estimate
one
step
transition
probability
rewards
equation
makes
algorithm
independent
hierarchi-
cal
decomposition
able
learn
optimal
polices
dif-
ferent
structures
dataset
speciﬁcally
cid:48
cid:48
cid:48
m=1
number
experiences
cid:48
respectively
last
since
assume
converged
subtasks
follow
deterministic
greedy
policy
a|s
greedy
primitive
action
subtask
would
take
state
a|s
otherwise
step
fact
cru-
cial
hqi
learn
optimal
policy
allows
subtask
discard
samples
following
optimal
behavior
children
cid:48
cid:80
hqi
algorithm
summarized
algorithm
sqi
summarized
algorithm
dataset
used
every
iteration
sqi
initial
data
sufﬁcient
cover
important
state-action
space
dataset
able
train
subtasks
dag
4.1
extension
function
approximation
state
abstraction
note
trivial
extend
sqi
fitted-sqi
uses
function
approximator
model
q-value
function
subtask
end
iteration
direct
advan-
tage
using
function
approximation
incorporate
powerful
supervised
regression
methods
gaussian
processes
neural
networks
scale
large-scale
continuous
mdps
although
using
function
approximations
usually
compromises
theoretical
convergence
guarantee
tabular
mdp
experiments
shows
fitted-hqi
able
converge
unique
optimal
solution
fitted-
sqi
summarized
algorithm
furthermore
state
abstraction
means
ﬁnding
subset
state
variables
informative
subtask
good
hierarchical
decomposition
decomposes
original
mdp
several
simpler
ones
agent
needs
care
small
set
features
task
therefore
good
structure
create
easy
opportunity
state
ab-
straction
level
hierarchy
many
techniques
explored
non-hierarchical
batch
reinforce-
ment
learning
achieve
state
abstractions
methods
directly
applied
ﬁtting
step
fitted-sqi
subtask
learn
sparse
state
representation
due
space
limit
conduct
simple
manual
state
abstraction
subtask
paper
leave
study
analyz-
ing
automatic
feature
selection
techniques
future
works
proof
convergence
section
prove
hqi
tabular
case
converges
recursive
optimal
policy
assume
policy
subtask
ordered
break
ties
deterministically
e.g
favor
left
right
deﬁnes
unique
recursive
optimal
hierarchical
policy
corresponding
algorithm
hierarchical
q-value
iteration
hqi
require
rain
primitive
children
done
rain
cid:54
empty
rain
sqi
done.add
end
rain
done
done
end
algorithm
subtask
q-value
iteration
sqi
require
maxiter
cid:48
cid:48
else
qk−1
greedypolicy
cid:48
qk−1
+βu
cid:48
maxu
cid:48
∈uiqk−1
qk−1
end
cid:48
cid:48
cid:48
end
end
end
end
algorithm
fitted
subtask
q-value
iteration
fitted
sqi
require
maxiter
cid:48
cid:48
else
greedypolicy
cid:48
qk−1
+βu
cid:48
maxu
cid:48
∈uiqk−1
cid:48
cid:48
cid:48
end
end
x.add
y.add
end
end
end
algorithm
greedypolicy
require
return
argmaxu
cid:48
∈uu
cid:48
return
greedypolicy
else
end
show
hqi
subscript
refers
recursive
recursive
optimal
function
converge
optimality
5.1
proof
want
prove
mdp
hierarchical
decomposition
..on
hqi
con-
verges
recursive
optimal
policy
hierarchical
policy
ﬁrst
prove
subtask
chil-
dren
converged
recursive
optimal
policies
inﬁnity
amount
batch
data
algorithm
sqi
converge
optimal
q-value
function
inﬁnity
number
iterations
limk−
show
hqi
provides
order
training
subtasks
dag
graph
training
subtask
children
already
converged
optimal
recursive
policies
5.2
proof
sketch
step
begin
base
case
subtask
whose
chil-
dren
primitive
actions
notice
equation
falls
back
traditional
bellman
operator
ﬂat
mdp
be-
cause
primitive
action
always
terminate
one
step
cid:48
max
cid:48
∈ui
cid:48
cid:88
cid:48
therefore
subtask
primitive
children
sqi
equivalent
ﬂat
q-value
iteration
guaranteed
converge
optimal
policy
given
sufﬁcient
data
subtasks
subtask
children
deﬁni-
tion
run
sqi
children
con-
verged
unique
deterministic
optimal
recursive
policy
means
every
action
deterministic
deter-
ministic
markov
option
deﬁned
option
framework
proved
set
deterministic
markov
options
one
step
intra-option
q-learning
converges
w.p
optimal
q-values
every
option
regardless
op-
tions
executed
learning
provided
every
primitive
action
gets
executed
every
state
inﬁnitely
often
refer
detailed
proof
step
deﬁnition
hierarchical
decomposition
di-
rected
acyclic
graph
dag
edges
parents
children
proof
ﬁrst
reverse
edges
children
parents
also
know
graph
theory
directed
acyclic
graph
least
one
topo-
logical
sort
every
edge
comes
figure
taxi
domain
ordering
therefore
topologically
sort
hi-
erarchical
decomposition
reversed
edges
sqi
always
train
children
parents
also
deﬁnition
topological
sort
ensures
initial
condition
least
one
subtask
prim-
itive
children
therefore
conclude
dag
hqi
traverse
subtasks
conditions
sqi
convergence
met
hqi
converges
subtasks
experiments
6.1
experimental
setup
applied
algorithm
taxi
domain
described
simple
grid
world
contains
taxi
pas-
senger
four
specially-designated
locations
labeled
starting
state
taxi
randomly-chosen
cell
grid
passenger
one
four
special
locations
passenger
desired
destination
he/she
wishes
reach
job
taxi
passen-
ger
pick
him/her
passenger
destination
drop
passenger
taxi
six
primitive
actions
avail-
able
move
one
step
one
four
directions
north
south
east
west
pick
passenger
put
passenger
make
task
difﬁcult
move
actions
deterministic
chance
moving
one
directions
also
every
move
grid
cost
reward
attempting
pick
drop
passenger
wrong
location
cause
−10
reward
last
successfully
ﬁnish
task
reward
grid
described
ﬁgure
therefore
possible
state
destination
possible
state
passenger
location
car
possible
locations
results
500∗
3000
parameters
q-table
needs
learned
denote
state
variable
dest
pass
later
discussion
dataset
run
collected
advance
choosing
actions
uniformly
random
different
sizes
evaluate
performance
algorithms
running
greedy
execution
100
times
obtain
average
discounted
return
every
5000
new
samples
000
samples
re-
peat
experiments
times
evaluate
inﬂuence
different
sample
distribution
discounting
factor
set
0.99.
figure
dag
table
dag-1
state
abstraction
subtask
root
get
put
navi
get
navi
put
active
states
pass
pass
dest
pass
dest
figure
average
discounted
reward
postﬁx
means
state
abstraction
error
bar
one
standard
deviation
runs
6.2
results
conducted
three
sets
experiments
comparison
hqi
ﬂat
q-value
iteration
effect
state
abstrac-
tion
learning
polices
different
dags
dataset
learning
policy
using
fitted-hqi
random
forest
function
approximator
ﬁrst
experiment
compares
hqi
ﬂat
q-value
it-
eration
fqi
also
pointed
state
abstraction
essential
maxq
fast
learning
speed
compared
ﬂat
learning
result
manually
conduct
state
ab-
straction
subtask
dag
however
different
aggressive
state
abstraction
described
every
subtask
child
pair
different
set
state
variables
conduct
simple
state
abstraction
subtask
level
i.e
children
subtask
state
abstraction
ﬁnal
state
abstraction
listed
table
described
run
independent
runs
different
random
sam-
ples
different
sizes
report
mean
average
discounted
return
ﬁve
runs
figure
well
best
average
discounted
reward
ﬁve
runs
figure
results
show
hqi
without
state
abstrac-
tion
consistently
outperforms
fqi
limited
training
data
dataset
large
enough
con-
verge
optimal
performance
around
1.0.
also
notice
occasionally
hqi
state
abstraction
learn
optimal
performance
state
abstraction
limited
samples
i.e
5000
samples
demonstrates
proper
hierarchy
constraints
good
behavioral
policy
hqi
generalize
much
faster
fqi
moreover
even
hqi
without
state
abstraction
consistently
outperforms
fqi
terms
sample
efﬁciency
different
be-
havior
on-policy
maxq-q
algorithm
reported
needs
state
abstraction
order
learn
faster
learning
argue
hqi
without
state
abstraction
sample
efﬁcient
fqi
following
reasons
hqi
uses
applicable
primitive
samples
update
q-table
every
subtask
maxq-q
updates
subtask
executes
particular
action
upper
level
subtask
figure
best
performance
comparison
maxq-q
needs
wait
children
gradually
converges
greedy
optimal
policy
good
estimate
cid:48
n|s
hqi
limi-
tation
second
experiment
running
hqi
different
vari-
ations
hierarchical
decomposition
original
mdp
figure
figure
show
two
different
valid
dags
could
also
solve
original
mdp
figure
demonstrates
sufﬁcient
data
three
dag
converge
recursive
optimal
solution
conﬁrms
hqi
able
converge
different
hierarchies
terms
sample
efﬁciency
three
structures
demonstrate
slight
different
behavior
no-
tice
dag
learns
particularly
slower
two
argue
poor
decomposition
original
mdp
based
problem
settings
pick
drop
risky
actions
illegal
execution
lead
−10
reward
dag
two
actions
mixed
low-cost
move
actions
two
dags
isolated
higher
level
decision
making
therefore
designing
good
hierarchy
crucial
obtain
performance
gain
versus
ﬂat
approaches
emphasizes
importance
off-
policy
nature
hqi
allows
developers
experiment
different
dag
structures
without
collecting
new
sam-
ples
effectively
evaluate
performance
particu-
lar
hierarchical
decomposition
without
using
simulator
part
future
research
last
experiment
utilizes
random
forests
func-
figure
dag
figure
dag
tion
approximator
model
q-value
function
dag
main
purpose
demonstrate
convergence
fit-
ted
hqi
subtask
q-value
function
modelled
random
forest
dest
pass
in-
put
feature
since
dest
pass
categorical
variables
represent
one-hot
vector
transforms
state
variable
dimension
vector
destination
passenger
coordinate
report
mean
average
discounted
rewards
independent
runs
dif-
ferent
random
samples
different
sizes
figure
shows
fitted-hqi
achieves
similar
performance
compared
tabu-
lar
hqi
figure
comparison
different
dags
error
bar
one
standard
deviation
runs
figure
comparison
ﬁtted-hq
hqi
dag-
error
bar
one
standard
deviation
runs
6.3
comparison
maxq-q
intra-option
learning
compared
maxq-q
hqi
enjoys
sample
efﬁciency
ability
off-policy
advantage
off-policy
require
hyper-parameter
tuning
explo-
ration
rate
since
high
level
subtask
maxq-q
needs
wait
children
converge
ﬁrst
developers
usually
set
faster
exploration
decay
rate
lower
level
subtasks
extra
hyperparameter
needs
tuning
limitation
hqi
maintains
independent
table
subtask
maxq-q
allows
part
parent
value
function
recursively
retrieved
children
technique
known
value
function
decomposition
allows
compact
memory
usage
accelerates
learning
parents
share
value
function
off-policy
setting
future
research
topic
intra-option
learning
option
framework
main
ad-
vantage
hqi
require
developers
fully
deﬁne
policy
every
options
instead
one
needs
deﬁne
dag
terminal
predicate
node
graph
argue
general
easier
deﬁne
task
hierarchy
giving
full
policy
macro-actions
there-
fore
hqi
combines
strength
intra-option
off-policy
learning
maxq
provides
method
training
op-
tions
off-policy
fashion
compared
hqi
advantage
learning
subtasks
ﬂat
batch
dataset
algorithm
require
task
dag
priory
col-
lecting
data
manual
deﬁnition
option
policies
conclusion
future
work
paper
introduced
off-policy
batch
learning
al-
gorithm
hierarchical
showed
possible
blindly
collect
data
using
random
ﬂat
policy
use
data
learn
different
structures
data
collection
aware
experiments
taxi
domain
show
converges
faster
fqi
optimal
policy
also
shows
different
dag
structures
able
learn
ﬂat
data
different
speeds
every
dag
structure
number
parameters
suggests
possible
line
research
try
minimize
number
parameters
hierarchy
future
work
include
comparing
different
feature
selections
techniques
fitted-sqi
applying
algorithm
large-scale
complex
domains
references
andrew
barto
sridhar
mahadevan
recent
ad-
vances
hierarchical
reinforcement
learning
discrete
event
dynamic
systems
1-2
:41–77
2003
mitchell
keith
bloch
reducing
commitment
tasks
learning
off-policy
hierarchical
reinforcement
arxiv
preprint
arxiv:1104.5059
2011
thomas
cormen
charles
leiserson
ronald
rivest
clifford
stein
section
22.4
topological
introduction
algorithms
2nd
mit
press
sort
mcgraw-hill
pages
549–552
2001
thomas
dietterich
hierarchical
reinforcement
learn-
ing
maxq
value
function
decomposition
ar-
tif
intell
res
jair
13:227–303
2000
thomas
dietterich
overview
maxq
hierarchi-
cal
reinforcement
learning
abstraction
reformula-
tion
approximation
pages
26–44
springer
2000
damien
ernst
pierre
geurts
louis
wehenkel
tree-based
batch
mode
reinforcement
learning
jour-
nal
machine
learning
research
pages
503–556
2005
alborz
geramifard
thomas
walsh
nicholas
roy
jonathan
batch-ifdd
representation
expansion
large
mdps
arxiv
preprint
arxiv:1309.6831
2013
michail
lagoudakis
ronald
parr
least-squares
policy
iteration
journal
machine
learning
re-
search
4:1107–1149
2003
christopher
painter-wakeﬁeld
ronald
parr
greedy
arxiv
algorithms
sparse
reinforcement
learning
preprint
arxiv:1206.6485
2012
ronald
parr
stuart
russell
reinforcement
learning
hierarchies
machines
advances
neural
infor-
mation
processing
systems
pages
1043–1049
1998
zhiwei
qin
weichang
firdaus
janoos
sparse
reinforcement
learning
via
convex
optimization
pro-
ceedings
31st
international
conference
ma-
chine
learning
icml-14
pages
424–432
2014
martin
riedmiller
neural
ﬁtted
iteration–ﬁrst
expe-
riences
data
efﬁcient
neural
reinforcement
learn-
ing
method
machine
learning
ecml
2005
pages
317–328
springer
2005
richard
sutton
doina
precup
satinder
singh
mdps
semi-mdps
framework
tem-
poral
abstraction
reinforcement
learning
artiﬁcial
intelligence
112
:181–211
1999
