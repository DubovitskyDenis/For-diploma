controlling
search
large
commonsense
knowledge
bases
machine
learning
approachÔÄ†
abhishek
sharma1
michael
witbrock2
keith
goolsbey1
1cycorp
inc.
7718
wood
hollow
drive
suite
250
austin
78731
2lucid
600
congress
avenue
austin
78701
abhishek
cyc.com
witbrock
lucid.ai
goolsbey
cyc.com
abstract
large
commonsense
knowledge
bases
kbs
often
thousands
millions
axioms
relatively
relevant
answering
given
query
large
number
irrelevant
axioms
easily
overwhelm
resolution-based
theorem
provers
therefore
methods
help
reasoner
identify
useful
inference
paths
form
essential
part
large-scale
reasoning
systems
paper
describe
two
ordering
heuristics
optimization
reasoning
systems
first
discuss
decision
trees
used
select
inference
steps
likely
succeed
second
identify
small
set
problem
instance
features
suffice
guide
searches
away
intractable
regions
search
space
show
efficacy
techniques
via
experiments
thousands
queries
cyc
results
show
methods
lead
order
magnitude
reduction
inference
time
introduction
commonsense
reasoning
always
core
problem
artificial
intelligence
systems
effective
reasoning
external
world
often
involves
drawing
deductively
valid
conclusions
known
facts
unfortunately
given
combinatorial
explosiveness
reasoning
expressive
knowledge-based
systems
kbs
even
simple
queries
might
get
lost
millions
seemingly
relevant
inference
paths
efficient
reasoning
systems
critical
building
large-scale
systems
ordering
heuristics
play
important
role
optimization
reasoning
kbs
least
two
reasons
first
inference
algorithms
kbs
e.g.
backward
chaining
russell
norvig
2003
cyc
tableaux
algorithms
description
logic
typically
represent
search
space
graph
structure
determined
rules
applicable
given
node
graph
generally
many
rules
might
simultaneously
apply
given
vertex
order
rule
expansion
significant
effect
efficiency
tsarkov
horrocks
2005
second
researchers
used
first-order
logic
fol
theorem
provers
tools
inference
expressive
languages
e.g.
owl
semantic
web
rule
language
swrl
reasoning
complete
language
beyond
scope
existing
algorithms
language
correspond
decidable
fragment
fol
tsarkov
2004
horrocks
voronkov
2006
large
fol
systems
often
thousands
millions
axioms
relevant
answering
given
query
hoder
voronkov
2011
tsarkov
2004
since
hundreds
thousands
axioms
irrelevant
given
query
might
overwhelm
resolution-based
theorem
provers
reasoner
expected
assess
utility
expanding
numerous
incomplete
inference
paths
na√Øve
ordering
paths
lead
potentially
infinite
subtrees
cause
unproductive
backtracking
make
inferences
efficient
paper
suggests
two
types
ordering
heuristics
first
discuss
implausible
search
paths
created
domain-specific
axioms
used
prove
queries
involving
fairly
general
predicates
argue
decision
trees
used
represent
semantic
context
rule
likely
contribute
proof
show
ordering
nodes
help
decision
trees
helps
guiding
search
toward
solution
second
key
impediment
development
fast
broad-application
first-order
reasoning
systems
insufficient
understanding
makes
problems
difficult
propose
comprehensive
set
features
correlate
answerability
nodes
run
inference
engine
large
number
queries
sample
nodes
resulting
search
graphs
record
values
instance
features
use
statistical
regression
methods
derive
model
predicting
answerability
nodes
use
resulting
model
order
nodes
search
demonstrate
improves
search
performance
paper
organized
follows
start
discussing
relevant
previous
work
decision
tree
algorithm
statistical
regression
methods
discussed
next
conclude
discussing
results
plans
future
work
related
work
prior
research
examined
use
machine
learning
identify
best
heuristics1
problems
bridge
2014
select
small
set
axioms/lemmas
relevant
answering
set
queries
hoder
voronkov
2011
sharma
forbus
2013
meng
paulson
2009
kaliszyk
2015
kaliszyk
urban
2015
alama
2014
contrast
focus
ordering
heuristics
enable
inference
algorithms
reason
axioms
taylor
2007
authors
use
reinforcement
learning
guide
inference
whereas
tsarkov
horrocks
2005
authors
study
different
types
rule-ordering
heuristics
e.g.
preference
rules
expansion-ordering
heuristics
e.g.
descending
order
frequency
usage
concepts
disjunction
paper
proposes
rule-
ordering
heuristics
based
search
state
use
regression-based
model
learn
effects
different
features
answerability
nodes
work
fields
e.g.
database
community
chaudhuri
1998
sat
reasoning
hutter
2014
answer
set
programming
brewka
2011
less
relevant
studies
address
complexity
deep
cyclic
search
graphs
arise
expressive
first-order
reasoning
best
knowledge
work
community
used
decision
trees
statistical
regression-based
methods
control
inference
large
commonsense
reasoning
systems
background
assume
familiarity
cyc
representation
language
lenat
guha
1990
matuszek
2006
taylor
2007
cyc
concept
hierarchies
represented
genls
relation
instance
genls
person
mammal
holds
backward
inference
rule
used
transform
query
like
link
type
transformation
link
node
like
performedby
johnmccarthy-computerscientist
isa
buying
leads
sub-goals
like
isa
johnmccarthybuysabook-012
buying
isa
johnmccarthywritesapaper-087
buying
may
satisfiable
links
examples
restriction
links
transformation
restriction
links
play
major
role
determining
out-
degree
nodes
every
node
search
graph
timestamped
node
called
successor
path
consisting
transformation
links
node
parent
transformation
link
exists
parents
successors
denote
sets
parents
successors
node
respectively
let
set
nodes
search
graph
transformation
link
set
set
transformation
links
transform
initial
state
intermediate
state
rule
substitutions
denote
rule
bindings
associated
transformation
link
transitive
inference
well
supported
cyc
query
genls
person
6,700
answers
predicate
genls
allows
transitive
inference
first
argument
position
aforementioned
query
one
open
transitive
argument
position
reasoning
cyc
difficult
due
sheer
size
expressiveness
cycl
representation
language
default
inference
mode
cyc
inference
engine
uses
following
types
axioms/facts
backward
inference
21,743
role
inclusion
axioms
e.g.
2,601
inverse
role
axioms
e.g.
iii
365,593
concepts
986,965
concept
inclusion
axioms
i.e.
genls
facts
817
transitive
roles
99,238
complex
role
inclusion
axioms
e.g.
31,897
binary
roles
7,980
roles
arities
greater
two
21.7
million
assertions
652,037
individuals
control
search
large
kbs
inference
algorithms
often
use
different
control
strategies
distinguish
set
clauses
known
set
support2
define
imporant
facts
problem
set
usable
axioms
outside
set
support
e.g.
see
otter
theorem
prover
russell
norvig
2003
every
step
theorem
provers
resolve
element
set
support
one
usable
axioms
perform
best-first
search
heuristic
control
strategy
mesures
weight
clause
set
support
picks
best
clause
adds
set
support
immediate
consequences
resolving
elements
usable
list
russell
norvig
2003
cyc
uses
set
heuristic
modules
identify
best
clause
set
support
heuristic
module
tuple
function
assesses
quality
node
weight
net
score
node
œÉiwifi
node
highest
score
selected
expansion
next
two
sections
discuss
two
heuristic
modules
focusing
search
decision
trees
focused
search
basic
idea
behind
approach
best
explained
examples
consider
rules
shown
below3
sittypeisspecwithtyperestrictiononroleplayer
absorption
photonabsorption
absorber
type
sittypeisspecwithtyperestrictiononroleplayer
excitation
chemicalobjectexcitation
objectofstatechange
type
cotemporalpropersubeventtypes
absorption
excitation
rule
objectfoundinlocation
arg1
arg2
geopoliticalsubdivision
arg2
objectfoundinlocation
arg1
rule
answer
query
propersubeventtypes
birthdayparty
inference
engine
would
backchain
rule
see
footnote
examples
heuristics
strategies
include
give
priority
axioms
clause
selection
sort
symbols
inverse
frequency
instance
negated
query
often
used
set
sentence
form
sittypeisspecwithtyperestrictiononroleplayer
spec
sit-type
role
type
means
spec
unique
specialization
sit-
type
specialization
situation
objects
play
role
instances
spec
instances
type
support
trees
cotemporalpropersubeventtypes
type/concept
based
decision
sub-role
propersubeventtypes
transformation
would
lead
reason
photon
absorption4
similarly
would
backchain
rule
answer
query
objectfoundinlocation
mesophyllcell-001
search
paths
unlikely
succeed
general
knowledge
bases
often
heavily
used
predicates
hundreds
specializations
specializations
partition
space
several
domains
example
rule
expected
useful
na√Øve
physics
expected
useful
reasoning
geographic
sites
implausible
search
paths
arise
mismatch
exists
query
implied
context
axiom
likely
work
paper
suggest
right
representation
choice
problem
rules
expected
fire
certain
class
type
things
therefore
associate
restrictive
information
variables
axioms
although
variables
expected
range
entire
domain
restrictive
information
specifies
subset
domain
rule
observed
work
restrictions
specified
terms
sorts
concepts
derive
results
successful
uses
given
rule
small
set
successful
bindings
rule
shown
table
fact
minneapolis
anaheim
rochester
cities
helps
derive
sorted
generalization
page
frisch
1992
arg2
likely
range
set
uscity
formally
restriction
condition
pair
variable
concept
let
denote
set
sentences
represent
relationships
among
concepts
substitution
satisfies
restriction
condition
maps
ground
term
jth
restriction
condition
axiom
represented
vars
set
variables
disjunction
constraints
specified
decision
trees
natural
representation
constraints
algorithm
constructing
decision
tree
set
successful
rule
bindings
shown
figure
compact
decision
tree
induced
1900
bindings
rule
simply
arg2
geopoliticalentity
arg1
terrestrialfunctioningobject
geographicalregion
ùëâùëéùëüùë†
arg2
arg1
minnesota-
cityofminneapolismn
univofminnesota
state
newyork-
state
cityofrochesterny
ginna-nuclearpowerplant
california-
cityofanaheimca
angelstadiumofanaheim
state
table
partial
training
set
rule
algorithm
createtree
shown
figure
takes
input
training
set
variables
occur
rule
training
set
generated
querying
antecedent
readers
might
wonder
domain
constraints
first
argument
sittypeisspecwithtyperestrictiononroleplayer
expected
specialization
situation
concept
birthdayparty
satisfies
condition
generality
domain
constraints
ensures
difficult
identify
implausible
sub-queries
rule
fixed
duration
time
bindings
returned
query
results
form
trainingset
given
tuple
training
set
see
table
compute
generalizations
bindings.5
step
algorithm
membership
specific
maximally
covering
generalization
chosen
branching
test
tuple
satisfies
test
explore
constraints
variables
branch
step
otherwise
values
variable
considered
branch
step
stop
growing
tree
step
number
unexplained
training
examples
less
pre-determined
fraction
full
training
set
complexity
top-down
decision
tree
induction
m2.n
number
attributes
size
training
set
kent
hirschberg
1996
use
decision
trees
search
define
heuristic
module
following
function
fdt
assesing
quality
nodes
ùëÜùë¢ùëèùë†ùë°ùëñùë°ùë¢ùë°ùëñùëúùëõùë†
ùëáùëüùëíùëí
ùëÖùë¢ùëôùëí
ùëù‚ààùêø
|ùëù|
expanded
set
transformation
link
sets
node
root
tree
substitution
satisfies
restriction
conditions
specified
tree
otherwise6
module
prioritize
search
paths
satisfy
restriction
conditions
instance
path
uses
rule
answer
query
objectfoundinlocation
mesophyllcell-001
would
late
mesophyllcell-001
transitively
geographicalregion
see
constraint
variable
helps
early
evaluation
inference
steps
use
rules
domains
pertinent
given
query
however
since
might
thousands
rules
relevant
query
also
need
ways
steer
search
toward
productive
states
next
section
propose
statistical
approach
solving
problem
input
trainingset
set
tuples
successful
bindings
rule
output
decision
tree
rule
create
root
node
tree
var
value
variable
value
provide
best
cov-
ering
generalization
add
leftchild
new
branch
root
corresponding
add
rightchild
new
branch
root
corresponding
accountedset
subset
trainingset
var
value
rightchild
createtree
unaccountedset
listofvars
leftchild
createtree
accountedset
listofvars
var
unaccountedset
trainingset
accountedset
stopping
criterion
reached
return
root
figure
createtree
algorithm
listofvars
list
variables
used
rule
test
var
value
10.
return
root
test
var
value
generalization
substitution
set
gen
isa
instance
populatedplace
generalization
minnesota-state
tree
function
returns
decision
tree
given
rule
decide
inference
engine
1-8
problem
size
type
features
cheap
number
variables
literals
fully
unbound
literals
single/multi
literal
query
fully/partially
bound
query
fully
unbound
single
literal
query
9-13
problem
state
features
cheap
depth
number
transformation
restriction
links
potential
fan-out
score
number
rules
used
recursively
reaching
state
14-18
knowledge
level
features
moderate
generality
estimate
unbound
literals
min
terms
termgenerality
number
gafs
predicate
single
literal
query
min
numgafs
generality
estimate
predicate
fully
unbound
single
literal
query
19-21
transitivity
features
moderate
number
open
transitive
argument
positions
number
open
transitive
argument
positions
queries
multiple
variables
number
open
argument
positions
genls
disjointwith
literals
22-29
probing
features
expensive
number
transformation
links
mean
number
literals
mean
degree
nodes
median
max
number
variables
median
ùë∫ùíñùíÑùíÑùíÜùíîùíîùíêùíìùíî
depth
median
knuth
tree
size
estimate
30-32
problem
balance
features
cheap
ratio
number
variables
literals
ratio
number
positive
negative
literals
|number
positive
literals
-1|
33-38
quadratic
terms
cheap
number
literals
number
free
variables
depth2
number
transformation
links
ratio
number
ùë∫ùíñùíÑùíÑùíÜùíîùíîùíêùíìùíî
39-44
number
transformation
links
depth*|
depth
knuth
tree
size
estimate
single
literal
query
number
open
transitive
argument
positions
min
terms
termgenerality
single
literal
query
generality
fully
unbound
literals
*multi
literal
query
misc
cheap
single
literal
query
procedural
support
result
feature
cheap
number
answers
statistical
meta-search
learning
possible
predict
whether
inference
engine
able
solve
arbitrary
node
generated
search
section
show
supervised
machine
learning
methods
used
build
models
predict
number
answers
problem
instance
models
used
allocate
computational
resources
moreover
shedding
light
sources
hardness
problem
instances
help
improving
knowledge
representation
fuel
development
new
algorithms
build
models
take
following
steps
identification
features
first
identify
key
parameters
represent
known
relevant
features
problem
instances
data
collection
next
run
inference
engine
large
set
queries
sample
nodes
generated
search
graph
sampled
node
number
answers
set
feature
values
recorded
iii
learning
finally
learn
model
maps
instance
features
inference
engine
performance
evaluate
test
set
queries
introducing
notation
used
figure
discuss
steps
detail
notation
let
terms
denote
set
predicates
terms
mentioned
query
respectively
predicate
let
numgafs
numrules
denote
number
ground
atomic
formulas
number
relevant
rules
moreover
cyc
maintains
estimate
generality
features
selected
exhaustive
subset
selection
shown
bold
figure
list
features
cost
computation
moderate
depth
ùë∫ùíñùíÑùíÑùíÜùíîùíîùíêùíìùíî
interaction
terms
ùíë‚ààùë∑ùíÇùíìùíÜùíèùíïùíî
ùíèùíêùíÖùíÜ
literals
variables
ùíë‚ààùë∑ùíÇùíìùíÜùíèùíïùíî
ùíèùíêùíÖùíÜ
ùíë‚ààùë∑ùíÇùíìùíÜùíèùíïùíî
ùíèùíêùíÖùíÜ
position
ontology
let
term
based
termgenerality
denote
generality
term
feature
identification
features
cost
computation
shown
figure
broadly
divided
ten
groups
first
group
includes
well-
understood
problem
size
type
features
including
number
literals
number
variables
second
group
contains
attributes
involve
examining
path
led
node
includes
important
features
help
maintaining
right
shape
search
space
feature
depth
critical
ensuring
inference
engine
trapped
depth-first
infinite
regress
feature
number
transformation
links
helps
control
out-
degree
nodes
figures
show
trade-off
depth-first
breadth-first
search
see
conditional
probability
success
node
decreases
rapidly
depth
similarly
figure
shows
successful
transformation
links
added
initial
phase
utility
adding
additional
transformation
link
drops
rapidly
table
shows
conditional
probability
success
nodes
function
number
literals
figure
likelihood
success
function
depth
figure
x-axis
shows
index
range
transformation
links
y-axis
shows
number
successful
transformation
links
given
range
number
literals
prob
success
0.940
0.009
0.001
0.003
0.003
0.003
table
conditional
probability
success
function
number
literals
00.050.10.150.20.250.30.350.41234567891011depthp
success|
depth
050010001500200025003000350040001-56-1011-1516-2021-2526-3031-100transformation
link
indexrange
number
successful
t-links
range
potential
fan-out
score
node
function
number
rules
potentially
used
formally
log10
1+numrules
next
group
includes
features
encode
level
knowledge
predicates
terms
mentioned
query
generality
estimate
mentioned
third
group
defined
log10
1+termgenerality
fourth
group
include
attributes
understanding
cost
transitive
queries
probing
feature
special
kind
feature
examines
neighborhood
node
assess
quality
since
locally
available
information
node
insufficient
gauging
complexity
search
space
fifth
group
include
features
pick
random
path
originating
given
node
record
descriptive
statistics
various
properties
interest
e.g.
number
literals
out-degree
nodes
example
third
feature
fifth
group
computed
finding
median
degree
nodes
encountered
randomly
selected
path.the
sixth
group
captures
balance
node
two
ways
measure
ratio
number
variables
literals
ratio
number
positive
negative
literals
expect
disjunctive
queries
difficult
also
note
whether
non-horn
axioms
used
deriving
state
seventh
eighth
group
included
quadratic
interaction
terms
salient
features
data
collection
cyc
contains
thousands
stored
queries
various
level
difficulty
gathered
large
amount
data
sampling
running
queries
forty
percent
nodes
resulting
space
sampled
values
features
shown
figure
recorded
produced
2.5
million
data
points
data
transformation
learning
recall
number
answers
performance
measure
features
shown
figure
predictor
features
performed
z-score
normalization
predictor
variables
subtracting
mean
dividing
difference
standard
deviation7
given
extreme
variability
number
answers
use
log-transformation
result
feature
i.e.
predict
log10
number
answers
experimented
initial
classification
logistic
regression
predict
likelihood
success
node
since
results
encouraging
switched
multiple
linear
regression
linear
regression
aim
learn
function
form
fsl
wigi
weight
ith
feature
node
function
fsl
used
heuristic
module
assess
quality
nodes
values
determined
minimizing
metric
root
mean
squared
error
rmse
software
used
estimate
values
core
team
2015
used
repeated
random
sub-sampling
approach
10-
fold
cross
validation
validate
model
using
former
method
data
selected
random
unpublished
work
e.g.
na√Øve
bayes
techniques
missing
feature
values
ignored
normalization
set
zero
training
ensures
minimally
informative
equal
mean
distribution
hutter
2014
training
rest
used
test
set
process
repeated
times
mean
rmse
two
validation
methods
0.47
0.76
respectively
multiple
adjusted
model
equal
0.76
f-statistic
1.8*105.
variety
reasons
features
uninformative
correlated
redundant
therefore
use
feature
selection
methods
identify
small
set
features
explains
variance
data
well
full
set
features8
analysis
helps
identify
properties
nodes
strongly
affect
empirical
performance
set
best
features
identified
exhaustive
subset
selection
method
shown
bold
figure
value
subset
models
features
converged
models
inputs
presence
features
knuth
tree
size
estimate
knuth
1975
ratio
number
positive
negative
literals
number
transformation
links
selected
list
suggests
following
locally
available
information
insufficient
predicting
complexity
search
space
probing
features
play
important
role
guiding
search
ensuring
search
graph
right
shape
critical
importance
reasoners
need
find
balance
depth-first
breadth-first
search
iii
negated
literals
disjunctive
queries
difficult
answer
next
section
evaluate
heuristics
help
inference
engine
answering
queries
experimental
results
selection
benchmark
instances
testing
efficacy
heuristics
important
factor
empirical
analysis
selection
problem
instances
guided
two
principles
benchmark
set
consist
queries
intrinsically
difficult
solve
inference
engine
therefore
excluded
simple
queries
answered
without
backchaining
milliseconds
e.g.
isa
marvinminsky
person
genls
dog
carnivore
focused
queries
needed
several
transformations
i.e.
depth
rule
back-chaining
answered
artificially
crafted
randomly
generated
problem
instances
useful
understanding
syntactic
properties
affect
behavior
algorithms
right
methodology
generating
instances
received
much
attention
commonsense
reasoning
community
therefore
work
focused
problems
real-world
applications
cyc
thousands
queries
created
knowledge
engineers
programmers
various
projects
e.g.
project
halo
friedland
2004
hpkb
project
cohen
1998
testing
question-answering
capability
system
queries
varying
levels
difficulty
need
one
transformation
others
required
inference
engine
back-chain
heavily
used
predicates
lead
huge
fan-out
high
search
experimented
forward
backward
exhaustive
subset
selection
methods
three
methods
lead
similar
set
selected
features
307
307
0.8
100
307
4.3
307
1.17
11.7
13.7
3.18
55.0
1705
1705
16.61
dt+sl
method
baseline
baseline
speedup
test
set
queries
q/a
imp
time
hours
answ
-ered
cost
ensured
queries
types
well
represented
test
sets9
based
terms
mentioned
queries
divided
three
test
sets
test
set
military
asymmetrical
warfare
domain
test
set
biology
domain
iii
test
set
others
e.g.
commonsense
queries
english
translation
query
test
set
shown
causes
decline
mpf
activity
phase
question
shown
would
lead
query10
cause
causes-sittypesittype
mpfactivitydroppinginmphase
recall
inference
engine
uses
set
heuristics
ordering
nodes
search
net
score
node
written
w0+
w1.fdt
w2.fsl
fdt
fsl
refer
scores
returned
decision
tree
statistical
learning
models
discussed
first
term
score
returned
heuristics
discussed
paper
table
baseline
version
obtained
setting
zero
setting
assess
efficacy
decision
tree
heuristics
rows
labeled
table
similarly
study
utility
statistical
learning
models
setting
rows
labeled
table
net
contribution
methods
shown
rows
labeled
dt+sl.
experimental
data
collected
4-core
3.40
ghz
intel
processor
ram
used
18,383
decision
trees
ten
best
features
identified
subset
selection
experiments
due
table
experimental
results
baseline
dt+sl
dt+sl
13.50
100.8
14.69
1705
1705
1736
1736
1736
1736
22.6
17.5
7.20
6.24
4.45
5.76
211
230
253
7.6
8.8
4.0
6.8
difficulty
level
queries
gauged
looking
average
time
requirements
baseline
version
table
initially
queries
test
set
could
answered
minutes
imp
improvement
shows
reasoning
large
time
requirements
queries
restricted
cutoff
time
query
minutes
table
contains
results
three
test
sets
see
decision
trees
multiple
regression
based
models
led
significant
speedups
average
speedup
factor
14.
since
heuristics
steer
inference
engine
towards
productive
parts
search
space
improve
question-answering
q/a
performance
fifth
column
table
labeled
q/a
q/a
performance
respect
baseline
conclusion
deep
deductive
large
commonsense
knowledge
bases
critical
modern
systems
intractability
first-order
logic
presented
interesting
research
opportunities
understanding
causes
problem
hardness
developing
new
algorithms
surmounting
article
described
two
techniques
make
reasoning
efficient
first
uses
decision
trees
guide
search
toward
germane
rules
representing
semantic
context
rule
expected
produce
results
second
uses
statistical
regression
techniques
provide
estimate
number
answers
node
expected
provide
based
search
meta-features
inference
engine
uses
heuristics
order
nodes
search
experimental
results
thousands
queries
show
order
magnitude
speedup
results
suggest
several
lines
future
work
first
need
test
heuristics
even
larger
set
queries
understand
dynamics
second
want
extend
decision
tree
implementation
make
probabilistic
assessments
next
would
like
experiment
statistical
models
e.g.
regression
splines
random
forests
improve
model
quality
finally
believe
coupling
approach
decision-theoretic
model
smith
1989
greiner
1991
could
yield
complete
theoretical
model
making
reasoning
efficient
references
alama
2014
alama
heskes
kulhwein
tsivtsivadze
urban
premise
selection
mathematics
corpus
analysis
kernel
methods
journal
automated
reasoning
:191‚Äì213
2014
brewka
2011
brewka
eiter
truszcynski
answer
set
programming
glance
communications
acm
pages
91-103
2011
bridge
2014
bridge
holden
paulson
machine
learning
first-order
theorem
causes-sittypesittype
means
instance
normally
cause
instance
situation
type
proving
journal
automated
reasoning
:141‚Äì
172
2014
chaudhuri
1998
chaudhuri
overview
query
optimization
relational
systems
proceedings
pods
pages
34-43
1998
cohen
1998
cohen
al..
darpa
high-
performance
knowledge
bases
project
magazine
pages
25-48
1998
friedland
2004
friedland
allen
matthews
witbrock
curtis
shepard
al..
project
halo
towards
digitial
aristotle
magazine
pages
29-47
2004
greiner
1991
greiner
finding
optimal
derivation
strategies
redundant
knowledge
bases
artificial
intelligence
pages
95-115
1991
hoder
voronkov
2011
hoder
voronkov
sine
qua
non
large
theory
reasoning
proceedings
cade-23
pages
299-314
2011
horrocks
voronkov
2006
horrocks
voronkov
reasoning
support
expressive
ontology
languages
using
theorem
prover
foundations
information
knowledge
systems
pages
201-218
springer
2006
hutter
2014
algorithm
runtime
prediction
methods
evaluation
artificial
intelligence
206
pages
79-111
2014
page
frisch
1992
page
frisch
generalization
learnability
study
constrained
atoms
inductive
logic
programming
academic
press
1992
core
team
2015
core
team
language
environment
scientific
computing
foundation
statistical
computing
vienna
austria
2015
russell
norvig
2003
stuart
russell
peter
norvig
artificial
intelligence-
modern
approach
pearson
education
2003
sharma
forbus
2013
sharma
forbus
automatic
extraction
efficient
axiom
sets
large
knowledge
bases
proceedings
aaai
2013
smith
1989
smith
controlling
backward
inference
aritificial
intelligence
:145-208
1989
taylor
2007
taylor
matuszek
smith
witbrock
guiding
inference
policy
search
reinforcement
learning
proceedings
flairs
pages
146-151
2007
tsarkov
horrocks
2005
tsarkov
horrocks
ordering
heuristics
description
logic
reasoning
proceedings
ijcai
pages
609-614
2005
tsarkov
2004
tsarkov
riazanov
bechhofer
horrocks
using
vampire
reason
owl
proceedings
semantic
web-iswc
pages
471-485
2004
kaliszyk
2015
kaliszyk
urban
vyskocil
efficient
semantic
features
automated
reasoning
large
theories
proceedings
ijcai
2015
kaliszyk
urban
2015
kaliszyk
urban
learning
assisted
theorem
proving
millions
lemmas
journal
symbolic
computation
69:109-128
2015
kent
hirschberg
1996
kent
hirschberg
complexity
learning
decision
trees
international
symposium
artificial
intelligence
mathematics
pages
112-115
1996
knuth
1975
knuth
estimating
efficiency
backtrack
programs
mathematics
computation
129
:121-136
1975
lenat
guha
1990
lenat
guha
buliding
knowledge-based
systems
representation
inference
cyc
project
addison
wesley
1990
matuszek
2006
matuszek
cabral
witbrock
deoliveira
introduction
syntax
content
cyc
aaai
spring
symposium
formalizing
compiling
background
knowledge
applications
knowledge
representation
question
answering
pages
44-49
2006
meng
paulson
2009
meng
paulson
lightweight
relevance
filtering
machine-generated
resolution
problems
journal
applied
logic
:41-
2009
