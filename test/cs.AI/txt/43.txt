eﬃcient
message-passing
algorithm
m-best
map
problem
dhruv
batra
tti-chicago
dbatra
ttic.edu
abstract
much
eﬀort
directed
algorithms
obtaining
highest
probability
conﬁgu-
ration
probabilistic
random
ﬁeld
model
known
maximum
posteriori
map
inference
problem
many
situations
one
could
beneﬁt
single
solution
top
probable
solu-
tions
known
m-best
map
problem
paper
propose
eﬃcient
message-passing
based
algorithm
solving
m-best
map
problem
speciﬁcally
algorithm
solves
recently
proposed
lin-
ear
programming
formulation
best
map
orders
magni-
tude
faster
generic
lp-solver
ap-
proach
relies
studying
particular
partial
lagrangian
relaxation
m-best
map
exposes
natural
combinatorial
structure
problem
exploit
introduction
large
number
problems
computer
vision
nat-
ural
language
processing
computational
biology
formulated
search
probable
state
discrete
probabilistic
graphical
model
known
map
inference
problem
number
applications
one
beneﬁt
single
best
solution
rather
list
m-best
hypotheses
example
sentences
of-
ten
ambiguous
machine
translation
systems
ben-
eﬁt
working
multiple
plausible
parses
sentence
computational
biology
practitioners
often
interested
computing
top
stable
conﬁgurations
protein
structure
moreover
com-
puting
set
m-best
hypotheses
useful
assessing
sensitivity
model
w.r.t
variations
input
and/or
parameters
model
graphical
models
literature
problem
known
m-best
map
problem
inter-
estingly
perhaps
understandably
algorithms
m-best
map
problem
closely
followed
de-
velopment
algorithms
solving
map
prob-
lem
similar
map
ﬁrst
family
algorithms
m-best
map
18,20
junction-tree
based
exact
al-
gorithms
feasible
low-treewidth
graphs
high-treewidth
models
belief
propagation
typically
used
perform
approximate
map
infer-
ence
yanover
weiss
showed
pseudo-
max-marginals
produced
may
used
com-
pute
approximate
m-best
maps
however
development
linear
program-
ming
relaxations
map
concurrence
be-
tween
map
m-best
map
longer
true
message-passing
algorithms
solving
map
available
soon
lps
studied
algorithm
known
solv-
ing
m-best
map
discrepancy
merely
theoretical
concern
large-scale
empiri-
cal
comparisons
found
message-passing
algorithms
map
signiﬁcantly
outperform
com-
mercial
solvers
like
cplex
importantly
message-passing
algorithms
applied
large-
scale
problems
solvers
like
cplex
simply
would
scale
thus
apply
m-best
map
real
instances
appearing
computational
biology
computer
vision
nlp
must
develop
scalable
distributed
message-passing
algorithms
overview
principal
contribution
paper
develop
eﬃcient
message-passing
algorithm
m-best
map
problem
discrete
undirected
graphical
models
speciﬁcally
markov
random
fields
mrfs
approach
studies
particular
partial
la-
grangian
relaxation
m-best
map
exposes
natural
modular
structure
problem
graphs
cycles
lagrangian
relaxation
in-
volves
exponentially
large
set
dual
variables
use
dynamic
subgradient
method
dsm
solving
lagrangian
dual
dsms
recently
for-
malized
class
methods
interleave
separation
oracle
procedure
selects
subset
active
dual
variables
dual
update
procedure
takes
step
direction
subgradient
high-level
algorithm
brings
map
m-best
map
equal
footing
vis-a-vis
message-passing
algorithm
solving
corresponding
lp-relaxation
importantly
algorithm
retains
guarantees
formulation
fromer
globerson
orders
magnitude
faster
similar
observations
map
ﬁnd
algo-
rithm
enables
solving
m-best
map
large
instances
unsolvable
generic
solvers
outline
begin
brief
history
m-best
map
problem
section
present
preliminaries
background
section
revisit
m-best
map
formulation
fromer
globerson
section
study
lagrangian
dual
present
message-passing
dual
ascent
algorithm
tree-mrf
section
general
mrfs
section
previous
work
m-best
map
problem
ﬁnding
top
solutions
general
combinatorial
optimization
problem
inference
mrfs
typically
studied
context
k-shortest
paths
search
graph
lawler
proposed
general
algorithm
compute
top
solutions
large
family
discrete
optimization
problems
ideas
used
lawler
algorithm
form
basis
algorithms
m-best
map
problem
complexity
ﬁnding
best
solution
number
variables
lawler
algorithm
solves
new
problems
best
solution
among
problems
second
best
solution
original
problem
thus
multiplica-
tive
overhead
top
solutions
iteratively
found
hamacher
queyranne
reduced
overhead
assuming
access
algorithm
compute
ﬁrst
second
best
solu-
tions
dechter
colleagues
recently
provided
dynamic-programming
algorithms
m-best
map
exponential
treewidth
yanover
weiss
proposed
algorithm
requires
access
max-marginals
thus
certain
classes
mrfs
allow
eﬃcient
exact
computation
max-marginals
e.g
binary
pairwise
supermodular
mrfs
m-best
solutions
found
arbi-
trary
treewidth
graphs
moreover
approximate
best
solutions
may
found
approximating
max-marginal
computation
e.g
via
loopy
recently
fromer
globerson
provided
view
m-best
map
problem
proposed
algorithm
stripes
repeatedly
partitions
space
solutions
solves
2nd-best
map
within
partition
generic
lp-solver
revisit
detail
section
describe
sig-
niﬁcantly
eﬃcient
message-passing
algorithm
solving
remainder
paper
preliminaries
map-mrf
inference
notation
positive
integer
let
short-
hand
set
consider
set
dis-
crete
random
variables
taking
value
ﬁnite
label
set
set
use
denote
tuple
cartesian
product
individual
label
spaces
×i∈axi
ease
notation
use
xij
short-
hand
two
vector
use
denote
inner
product
map
let
graph
deﬁned
functions
deﬁning
energy
node
edge
labeling
variables
scope
goal
map
inference
ﬁnd
labeling
variables
minimizes
real-
valued
energy
function
variables
i.e
cid:0
cid:1
let
cid:88
cid:88
a∈v∪e
i∈v
cid:88
min
x∈xv
min
x∈xv
θij
techniques
developed
paper
naturally
applicable
higher-order
mrfs
well
however
simplify
exposition
restrict
pairwise
energy
functions
map
integer
program
map
inference
typi-
cally
set
integer
programming
problem
boolean
variables
node
edge
let
vector
indicator
variables
encoding
possible
con-
ﬁgurations
set
implies
takes
label
moreover
let
vector
holding
energies
possible
conﬁgura-
tions
vector
holding
entire
conﬁguration
using
notation
map
inference
integer
program
written
min
µij
s.t
θij
µij
cid:88
µij
µij
t∈xj
µij
i∈v
cid:88
cid:88
cid:88
cid:88
s∈xi
s∈xi
enforces
exactly
one
label
as-
signed
variable
assign-
ments
consistent
across
edges
con-
cise
use
denote
set
satisfy
three
constraints
thus
problem
written
concisely
cid:88
a∈v∪e
µ∈p
0,1
min
map
problem
known
np-hard
general
number
techniques
solve
linear
programming
relaxation
problem
given
relaxing
boolean
con-
straints
unit
interval
i.e
µij
thus
map
minimizes
energy
following
polytope
cid:8
cid:9
also
known
local
polytope
relaxation
map
known
tight
special
cases
like
tree-
graphs
binary
submodular
energies
meaning
optimal
vertex
local
polytope
in-
teger
special
cases
m-best
map
linear
program
let
revisit
m-best
map
formulation
let
denote
mth-best
map
thus
map
second-best
map
m-best
map
integer
program
given
argmin
µ∈p
0,1
s.t
a∈v∪e
cid:54
cid:88
map
integer
program
suggested
natu-
ral
relaxation
case
best
map
integer
program
due
exclusion
con-
straints
linear
constraints
fromer
globerson
introduced
concise
representation
polytope
called
assignment-excluding
local
poly-
tope
m−1
excludes
previous
solutions
help
additional
linear
inequalities
called
spanning-tree
inequalities
let
spanning
tree
inequalities
spanning
tree
let
set
spanning
trees
let
deﬁne
degree
node
cid:44
cid:88
µij
i∈v
spanning
tree
inequality
deﬁned
notice
moreover
shown
cid:54
thus
spanning
tree
inequality
separates
vertex
vertices
polytope
assignment-excluding
local
polytope
aelp
deﬁned
cid:88
cid:40
cid:12
cid:12
cid:12
cid:12
cid:41
m−1
thus
see
aelp
excludes
previous
solutions
help
spanning
tree
inequalities
recall
relaxation
local
polytope
tree-structured
mrfs
tight
also
shown
relaxation
aelp
tree
mrfs
tight
however
tree
mrfs
loopy
mrfs
aelp
guaranteed
tight
relaxation
eﬃcient
separation
oracle
note
tree
mrfs
single
spanning
tree
inequality
previous
solution
since
gen-
eral
graphs
may
exponentially
large
col-
lection
spanning
trees
e.g
nn−2
com-
plete
graphs
however
inequalities
need
explicitly
included
use
cutting-plane
algorithm
maintains
working
set
spanning
trees
cid:48
incrementally
adds
violated
in-
equality
cid:48
cid:48
argmaxt∈t
given
fromer
globerson
showed
separation
oracle
eﬃciently
implemented
maximum-weight
spanning
tree
algorithm
−µj·µm
edge
weights
given
wij
µij·µm
notice
algorithm
requires
solving
linear
pro-
gram
aelp
iteration
large
prob-
lems
arising
computer
vision
computational
bi-
ology
solving
standard
lp-solver
even
may
infeasible
next
section
present
proposed
message-passing
algorithm
solving
m-best
map
−µi·µm
m-best
map
lagrangian
relaxation
tree-mrf
let
ﬁrst
restrict
attention
tree-structured
mrfs
simple
enough
scenario
describe
main
elements
approach
discuss
general
case
section
tree
mrfs
single
spanning
tree
inequality
simplify
notation
refer
simply
m-best
map
written
cid:88
min
µ∈l
s.t
a∈v∪e
instead
solving
problem
pri-
mal
lp-solver
fromer
globerson
study
lagrangian
relaxation
formed
dualizing
spanning
tree
constraints
cid:88
a∈v∪e
m−1
cid:88
m=1
min
µ∈l
λmi
set
lagrange
multipliers
determine
weight
penalty
imposed
violating
spanning
tree
constraints
note
partial
lagrangian
dualized
spanning
tree
constraints
dualized
constraints
hidden
inside
local
polytope
key
idea
exploiting
structure
partial
lagrangian
immediately
exposes
structure
problem
primal
formulation
obfuscating
namely
spanning
tree
inequality
distributes
according
tree
structure
i.e
cid:40
cid:88
cid:16
cid:88
i∈v
cid:16
m−1
cid:88
m−1
cid:88
m=1
m=1
cid:17
cid:41
cid:17
µij
θij
λmµm
min
µ∈l
also
recall
tree
local
polytope
integral
vertices
thus
minimization
eﬃciently
performed
running
combina-
torial
optimization
algorithm
standard
two-pass
max-product
perturbed
mrf
need
solved
generic
solver
see
next
able
eﬃciently
evaluate
lagrangian
need
able
optimize
lagrangian
dual
5.1
projected
supergradient
ascent
lagrangian
dual
theory
lagrangian
duality
know
values
lower-bound
value
primal
problem
tightest
lower-
bound
obtained
solving
lagrangian
dual
problem
maxλ≥0
since
non-smooth
con-
cave
function
achieved
supergradi-
ent
ascent
algorithm
analogous
subgradient
de-
scent
minimizing
non-smooth
convex
functions
since
constrained
variable
follow
pro-
jected
supergradient
ascent
algorithm
iteratively
up-
dating
lagrange
multipliers
according
fol-
supergradient
step-size
projection
operator
projects
vector
onto
positive
orthant
sequence
multipliers
satisﬁes
t=0
projected
su-
lowing
update
rule
t+1
cid:2
αt∇f
cid:3
limt→∞
cid:80
pergradient
ascent
converges
optimum
lagrangian
dual
ﬁnd
supergradient
consider
follow-
ing
lemma
proved
supergradient
formulation
given
µm−1
cid:105
cid:104
optimal
primal
solution
current
setting
thus
computation
supergradient
done
dynamic
pro-
gramming
algorithm
evaluating
lagrangian
supergradient
update
procedure
intuitive
interpretation
recall
lagrangian
re-
laxation
minimizes
linear
combination
energy
value
spanning
tree
inequality
violates
one
weighting
given
spanning
tree
constraints
i.e
diﬀerent
previous
solution
supergradient
w.r.t
positive
cost
violating
con-
straint
increase
update
thus
encouraging
next
solution
t+1
satisfy
spanning
tree
constraints
conversely
constraints
strictly
satisﬁed
supergradient
negative
indicating
may
over-penalizing
violations
may
reduced
allow
lower
energy
solutions
tightness
lagrangian
relaxation
primal
problem
strong
duality
holds
thus
projected
supergradient
algorithm
described
exactly
solves
m-best
map
fromer
globerson
total
complexity
al-
gorithm
knl2
number
dual
ascent
iterations
number
nodes
largest
label
space
i.e
maxi
|xi|
m-best
map
lagrangian
relaxation
general
mrfs
section
build
basic
ideas
previous
section
develop
algorithm
general
graphs
may
contain
exponentially
many
span-
ning
trees
recall
m-best
map
general
graphs
given
cid:88
min
µ∈l
a∈v∪e
s.t
set
spanning
trees
lagrangian
formed
dualizing
span-
ning
tree
constraints
given
min
µ∈l
cid:88
a∈v∪e
m−1
cid:88
cid:88
m=1
t∈t
λmt
lemma
point-wise
minimum
linear
i.e
minµ
one
functions
supergradient
given
argminµ
notice
indeed
point-wise
minimum
linear
functions
mapping
lemma
see
two
main
concerns
prevent
directly
solving
lagrangian
relaxation
first
set
lagrange
multipliers
λmt
m−1
exponentially
large
second
graph
longer
tree
supergradient
longer
computed
max-product
address
concerns
next
subsections
index
dual
variable
corresponding
violated
inequality
i.e
t+1
index
argmax
t∈t
cid:88
wij
max-wt
span-tree
12a
figure
overview
dynamic
supergradient
method
figure
adapted
6.1
optimizing
exponentially
many
dual
variables
dynamic
supergradient
method
order
optimize
exponentially
large
set
dual
variables
follow
dynamic
supergradient
method
dsm
see
overview
intuitively
dsms
thought
dual-procedure
cutting-plane
algorithm
dsm
maintains
index
set
active
dual
variables
∪mj
inactive
dual
variables
ﬁxed
zero
i.e
λmj
visualized
fig
dsms
consist
three
kinds
operations
primal
block
given
current
dual
vari-
able
primal
block
evaluates
la-
grangian
ﬁnd
new
primal
point
constraint
management
block
given
current
primal
point
constraint
man-
agement
block
augments
index
set
active
dual
variables
get
new
index
set
op-
tionally
block
may
also
choose
drop
dual
variables
index
set
block
described
detail
dual
block
given
current
index
dual
block
constructs
dual
update
direction
supergradient
stepsize
produce
new
dual
variable
t+1
t+1
primal
dual
blocks
discussed
next
subsection
describe
supergra-
dient
may
eﬃciently
computed
general
graphs
describe
constraint
management
block
detail
develop
intuition
step
recall
complementary
slackness
condition
tells
given
pair
optimal
primal
dual
variables
thus
intuitively
active
set
must
focus
dual
variables
corresponding
violated
inequalities
use
maximum
violated
oracle
adds
active
set
wij
ˆµij
ˆµi
ˆµj
12b
note
process
dualized
version
cutting-plane
method
fromer
globerson
instead
adding
violated
spanning
tree
inequality
include
index
dual
variable
working
set
shown
dynamic
supergradient
method
maximum-violation-oracle
constraint
man-
agement
block
guaranteed
converge
op-
timum
lagrangian
relaxation
choice
stepsize
rules
standard
supergradient
methods
rigorous
proof
found
dsms
actually
allow
dual
variables
removed
active
set
well
certain
conditions
however
keep
exposition
simple
match
implementation
discuss
refer
reader
theory
allows
several
iterations
primal
dual
blocks
performed
constraint
man-
agement
step
found
crucial
practice
6.2
computing
supergradient
general
graphs
let
address
problem
computing
su-
pergradient
implementing
primal
dual
blocks
given
current
index
set
active
dual
vari-
ables
∪mj
supergradient
w.r.t
active
dual
variables
given
∇λmj
optimal
primal
solution
current
setting
case
tree-mrfs
could
compute
optimal
solution
via
dynamic
programming
general
mrfs
supergradient
computation
involves
solving
cid:16
cid:40
cid:88
cid:16
cid:88
i∈v
θij
cid:88
m−1
cid:88
cid:88
m−1
cid:88
m=1
j∈jm
m=1
j∈jm
cid:17
λmj
dtj
cid:41
cid:17
µij
λmjµm
min
µ∈l
graph
loops
evaluating
lagrangian
involves
solving
map
in-
ference
modiﬁed
potentials
thus
message-passing
algorithm
map
mplp
dual-
decomposition
max-sum
diﬀusion
may
primal
block
constraint
management
dual
block
primal
point
index
set
dual
point
t+1
used
solve
problem
however
unlike
two-pass
max-product
used
tree-mrfs
message-passing
algorithms
typically
require
many
hundreds
iterations
converge
running
it-
erations
step
supergradient
ascent
practice
become
prohibitively
slow
key
speeding
process
realize
partial
lagrangian
evaluating
diﬃcult
suggests
constraints
dualized
till
partial
lagrangian
becomes
tractable
precisely
using
ideas
dual-decomposition
literature
expanding
partial
lagrangian
order
expand
partial
lagrangian
ﬁrst
try
iden-
tify
tractable
tree-structured
subcomponents
spanning-tree
inequalities
already
tree
structured
convert
collection
tree-structured
factors
let
spanning-tree
cover
i.e
collection
spanning
trees
edge
appears
least
one
tree
approach
really
need
trees
spanning
describe
following
spanning-tree
cover
keep
notation
simple
slight
abuse
notation
use
denote
subset
trees
contain
edge
moreover
de-
compose
original
energy
function
collec-
tion
energy
functions
one
cid:88
tree
tree
cover
cid:88
θij
cid:88
t∈t
t∈t
15b
15a
t∈t
1|t
easily
satisﬁed
distributing
node
edge
energies
evenly
i.e
θij
thus
energies
specify
tree
decom-
position
let
assign
tree
copy
primal
variables
also
assign
spanning
tree
inequality
active
set
copy
primal
variables
µmj
finally
use
θmj
denote
node
energy
spanning
tree
factor
θmj
λmjµm
new
variables
write
exist-
ing
partial
lagrangian
denote
edge
energy
cid:44
λmj
cid:40
cid:88
t∈t
m−1
cid:88
cid:88
m=1
j∈jm
min
µ∈l
θmj
µmj
s.t
˜µi
16a
16b
formulation
partial
lagrangian
uses
global
variable
force
tree-structured
subprob-
lems
agree
labellings
nodes
thus
equivalent
earlier
formulation
however
expand
partial
lagrangian
dualizing
constraints
16b
cid:40
cid:88
cid:88
t∈t
m−1
cid:88
m=1
cid:88
cid:17
cid:41
j∈jm
˜µi
cid:88
δτi
cid:16
τ∈t
i∈v
θmj
µmj
min
µτ∈l
δτi
set
lagrangian
multipliers
dualized
equality
constraints
local
polytope
tree-structured
subproblems
notice
expanded
partial
lagrangian
com-
pletely
decouples
independent
minimizations
tree-structured
subproblems
cid:88
τ∈t
min
µτ∈l
cid:40
cid:88
i∈v
δτi
cid:17
cid:41
cid:16
cid:88
cid:19
δτi
˜µi
cid:18
cid:88
cid:88
τ∈t
i∈v
min
cid:80
i.e
cid:80
cid:44
cid:80
see
unconstrained
minimization
forces
constraint
lagrangian
variables
cid:80
i∈v
δτi
otherwise
la-
grangian
ﬁnite
value
denote
i∈v
δτi
set
τ∈t
τ∈t
feasible
lagrangian
multipliers
note
expanded
partial
lagrangian
ef-
ﬁciently
evaluated
running
two-pass
max-product
tree-structured
subproblems
number
tree-structured
subproblems
equal
|+|j|
i.e
size
spanning-tree
cover
upper
bounded
typically
small
constant
number
active
spanning
tree
inequalities
optimizing
expanded
partial
lagrangian
dual
problem
expanded
partial
lagrangian
given
maxλ≥0
δ∈∆
previous
sec-
tion
described
dual
partial
la-
grangian
optimized
dynamic
super-
gradient
method
optimizing
dual
expanded
lagrangian
similar
described
procedure
minor
modiﬁcation
dual
variable
always
stays
active
set
supergradient
w.r.t
given
∇δτi
ˆµτ
optimal
primal
solution
ˆµτ
tree
suproblem
current
setting
projection
step
onto
fairly
simple
in-
volves
satisfying
zero-sum
constraint
cid:41
t+1
cid:2
αt∇δf
cid:3
enforced
subtracting
mean
dual
vari-
ables
overall
dual
update
w.r.t
given
stepsize
zero-projection
operator
i.e
cid:80
1|δ|
cid:48
entire
algorithm
summarized
algorithm
call
algorithm
steelars
spanning
tree
inequality
lagrangian
relaxation
scheme
j∈v
cid:48
cid:80
output
m-best
map
algorithm
steelars
steelars
input
graph
instance
energy
vector
previous
solutions
optimum
solution
vector
optimum
dual
variables
algorithm
construct
tree-decomposition
converged
constraint
management
block
ˆµj
ˆµi
wij
max-weight
wij
ˆµij
argmaxt∈t
spanning
tree
t+1
index
cid:80
end
∪m∈
m−1
multiple
iteration
primal
dual
blocks
cid:48
θmj
θmj
mjµm
primal
block
ˆµτ
argminµτ∈l
two-
pass
max-product
end
dual
block
∇λmj
ˆµmj
∇δτi
ˆµτ
αt∇λf
αt∇δf
cid:105
cid:105
t+1
cid:104
t+1
cid:104
end
best
feasible
primal
far
ˆµ∗
running
average
ˆµτ
end
tightness
steelars
ﬁrst
glance
may
seem
like
expanded
partial
lagrangian
solves
weaker
relaxation
original
partial
la-
grangian
however
similar
argument
tree-mrf
case
problem
thus
strong
duality
holds
partial
lagrangians
achieve
optimum
implies
even
non-tree
mrfs
steelars
exactly
solves
m-best
map
fromer
globerson
introduce
new
approximation
gap
integer
program
moreover
guarantee
depend
choice
spanning-tree
cover
fact
tree
cover
even
non-spanning
may
used
finally
note
described
expanded
partial
lagrangian
terms
tree-structured
subproblems
however
eﬃcient
subproblem
may
used
e.g
submodular
subproblems
solved
graph-cuts
experiments
setup
tested
algorithm
three
scenarios
tree
mrfs
simplest
case
m-best
map
guaranteed
tight
moreover
single
spanning
tree
inequality
constraint
management
block
plays
role
2-label
submodular
mrfs
problems
map
guaranteed
tight
m-best
map
moreover
tree
decomposition
required
2-label
submodular
factor
may
eﬃciently
minimized
via
graph-cuts
general
4-label
loopy
mrfs
general
case
described
section
baselines
compared
algorithm
stripes
algorithm
fromer
globerson
lawler-nilsson
algorithm
16,18
bmmf
algo-
rithm
yanover
weiss
recall
stripes
directly
solve
m-best
map
rather
solves
sequence
2nd-best
map
lps
en-
capsulated
lawler-nilsson
partitioning
scheme
tradeoﬀ
eﬃciency
accu-
racy
would
eﬃcient
directly
solve
m-best
map
fractional
thus
partitioning
scheme
performs
better
note
steelars
could
also
encapsulated
lawler-
nilsson
partitioning
scheme
straightfor-
ward
manner
however
wish
study
per-
formance
lagrangian
relaxation
m-best
map
directly
leave
partitioning
scheme
extension
future
work
implementation
details
implementations
stripes
lawler-nilsson
provided
authors
bmmf
provided
au-
thors
stripes
lps
iteration
solved
using
gnu
lpk
library
steelars
implemented
matlab
max-product
written
c++
eﬃciency
experiments
performed
64-bit
8-core
intel
machine
12gb
ram
timing
reported
cputime
fol-
lowing
chose
stepsize
iteration
ηt+1
number
times
ob-
jective
value
decreased
one
iter-
ation
next
rule
convergence
guarantees
standard
decaying
rule
empirically
performs
much
better
integer
primal
extraction
steelars
dual-
ascent
algorithm
thus
always
maintains
feasible
dual
solution
necessarily
integer
primal
feasible
solution
interested
however
since
primal
block
repeatedly
called
computing
supergradient
simply
keep
track
best
integer
primal
feasible
solution
produced
far
output
end
algorithm
evaluation
compare
diﬀerent
algorithms
two
metrics
run-time
accuracy
solutions
returned
tree-mrfs
methods
guaran-
teed
return
exact
solutions
thus
simply
compare
run-times
general
mrfs
follow
protocol
measure
relative
accuracy
diﬀerent
methods
speciﬁcally
pool
solutions
returned
methods
note
top
solutions
pool
method
report
fraction
solutions
contributed
results
demonstrate
eﬀectiveness
steelars
primarily
terms
eﬃciency
show
since
steelars
message-passing
al-
gorithm
signiﬁcantly
faster
generic
solver
glpk
sometimes
orders
magnitude
even
though
guaranteed
converge
solu-
tion
steelars
naturally
scales
large
instances
previously
unsolvable
using
solvers
tree
mrfs
generated
synthetic
problems
sampling
random
spanning
trees
nodes
variable
could
take
labels
node
edge
potentials
sampled
standard
gaus-
sians
m-best
map
guaranteed
tight
thus
stripes
steelars
produces
precisely
answers
fig
shows
time
taken
algorithms
function
size
tree
averaged
samplings
parameters
note
x-axis
log-scale
increase
stripes
quickly
becomes
intractable
gplk
ultimately
running
memory
how-
ever
steelars
shows
much
better
behaviour
runtime
fig
also
shows
value
dual
best
feasible
integer
primal
produced
steelars
function
number
iterations
recall
steelars
iteration
corresponds
running
max-product
single
tree
fig
shows
generally
number
iterations
pretty
low
12.
note
since
tree
mrf
bmmf
would
require
call
compute
max-
marginals
partitioning
scheme
assuming
carefully
implemented
recognize
tree-graph
run
asynchronous
thus
steelars
much
slower
optimal
thing
tree
course
bmmf
would
also
require
multiple
iterations
synchronous
loopy
graphs
checked
next
2-label
submodular
mrfs
scenario
constructed
grids
variable
could
take
labels
sampled
node
edge
energies
gaussians
ensured
edge
energies
submodular
allows
use
graph-cuts
optimizing
submodular
factor
fig
shows
value
dual
best
feasible
integer
primal
function
number
iterations
sharp
falls
dual
correspond
constraint
manage-
ment
block
calling
separation
oracle
increment
working
set
iteration
involves
one
call
graph-cuts
calls
two-pass
max-product
notice
number
iterations
much
larger
tree
mrf
∼150
opposed
however
max-ﬂow
algorithms
general
implementation
particular
highly
eﬃ-
cient
fig
shows
run-time
algorithms
function
see
steelars
far
fastest
algorithms
becoming
in-
tractable
quickly
unfortunately
fig
shows
also
least
accurate
validating
choice
fromer
globerson
solve
directly
use
partitioning
scheme
instead
plan
follow
direction
interestingly
fig
seems
suggest
bmmf
performs
worse
stripes
even
though
bmmf
simply
involves
calls
loopy
believe
artifact
caused
fact
bmmf
im-
plementation
written
matlab
particularly
optimized
moreover
speciﬁc
ex-
periment
could
made
faster
computing
max-
marginals
via
approach
instead
loopy
high-level
key
diﬀerence
bmmf
steelars
analogous
diﬀerence
loopy
mplp
message-passing
al-
gorithms
one
solves
relaxation
provides
improving
lower-bounds
general
mrfs
scenario
constructed
grid
graphs
well
variable
could
take
4-labels
edge
energies
restricted
attractive
used
standard
two-tree
decom-
position
grid
graph
thus
iteration
supergradient
ascent
involved
calls
max-
product
observed
trends
similar
sub-
modular
mrfs
case
terms
number
iter-
ations
required
steelars
converge
relative
standing
w.r.t
baselines
case
time
vs.
primal
dual
vs.
it-
erations
primal
dual
vs.
it-
erations
zoomed
supergradient
it-
erations
figure
tree
mrf
time
vs.
primal
dual
vs.
it-
erations
primal
dual
vs.
it-
erations
zoomed
accuracy
solutions
figure
2-label
submodular
mrfs
specially
interesting
small
values
work
done
steelars
i.e
∼150
iteration
two-pass
solving
m-best
map
comparable
running
trw
13,24
map
moreover
could
run
steelars
300×
500
image
labelling
prob-
lem
similar
observation
could
even
solve
map
glpk
conclusions
conclusion
presented
ﬁrst
message-passing
algorithm
solving
relaxation
m-best
map
problem
discrete
undirected
graphical
mod-
els
approach
used
particular
lagrangian
relax-
ation
construct
partial
lagrangian
allowed
use
combinatorial
optimization
algorithms
handle
exponentially
large
set
constraints
used
dynamic
supergradient
scheme
essen-
tially
dual
procedure
cutting-plane
algorithm
message-passing
algorithm
retains
guar-
antees
formulation
fromer
glober-
son
orders
magnitude
faster
extracting
diverse
m-best
solutions
num-
ber
applications
especially
computer
vision
best
map
solutions
essentially
minor
perturba-
tions
concurrent
work
also
presented
solution
diverse
m-best
map
problem
given
measure
distance
two
solutions
block
solutions
within
distance-ball
previous
solutions
hope
algorithms
useful
practitioners
problem
size
prohibits
use
generic
lp-solvers
future
work
number
interesting
di-
rections
front
short-term
inter-
ested
encapsulating
steelars
inside
lawler-
nilsson
partitioning
scheme
similar
stripes
increase
accuracy
method
since
m-best
map
often
fractional
another
direction
tighten
e.g
using
techniques
proposed
sontag
would
also
interesting
compare
lp-relaxation
based
methods
like
stripes
steelars
heurisitic-search
methods
acknowledgements
thank
sebastian
nowozin
human
encyclopedia
optimization
pointing
literature
dynamic
subgradient
methods
amir
globerson
sharing
stripes
implementation
useful
discussions
danny
tarlow
discussions
helped
formalize
formulation
references
batra
yadollahpour
guzmn-rivera
diverse
m-best
solutions
shakhnarovich
markov
random
fields
eccv
2012
bertismas
tsitsiklis
introduction
linear
optimization
athena
scientiﬁc
1997
dechter
flerova
marinescu
search
algorithms
best
solutions
graphical
models
aaai
2012
101102103104105050100150number
nodestime
sec
steelarsstripes246810
cid:239
14000
cid:239
12000
cid:239
10000
cid:239
8000
cid:239
6000
cid:239
4000
cid:239
200002000
iterationsenergy
dualbest
feasible
primal246810
cid:239
200
cid:239
1000100200300
iterationsenergy
dualbest
feasible
primal11.522.533.544.555.5050100150log10
nodes
iteration1021030100200300400500600700number
nodestime
sec
steelarsstripesbmmfnilsson050100150
cid:239
250
cid:239
200
cid:239
150
cid:239
100
cid:239
50050
iterationsenergy
dualbest
feasible
primalseparation
oracle
0501001504567
iterationsenergy
dualbest
feasible
primal10110210320406080100number
nodesaccuracy
steelarsstripesbmmfnilsson
sontag
meltzer
globerson
jaakkola
weiss
tightening
relaxations
map
using
message
passing
uai
2008
wainwright
jaakkola
willsky
map
estimation
via
agreement
hyper
trees
message-
passing
linear-programming
approaches
trans
inf
th.
:3697–3717
2005
wainwright
jordan
graphical
models
exponential
families
variational
inference
foun-
dations
trends
machine
learning
1-2
:1–
305
2008
werner
linear
programming
approach
max-
sum
problem
review
pami
:1165–1179
2007
yanover
meltzer
weiss
linear
pro-
gramming
relaxations
belief
propagation
empirical
study
mach
learn
res.
7:1887–1907
2006
yanover
weiss
finding
prob-
able
conﬁgurations
using
loopy
belief
propagation
nips
2003
emiel
subgradient
methods
http
//www.optimization-online.org/db_html/
2008/08/2077.html
2008
sagastizabal
dynamic
optimization
online
emma
rollon
dechter
inference
schemes
best
solutions
soft
csps
proceedings
workshop
preferences
soft
constraints
2011
eppstein
finding
shortest
paths
siam
computing
:652–673
1998
fromer
globerson
view
best
map
problem
nips
2009
globerson
jaakkola
fixing
max-product
convergent
message
passing
algorithms
map
lp-
relaxations
nips
2007
guignard
lagrangean
relaxation
top
of-
ﬁcial
journal
spanish
society
statistics
operations
research
:151–200
2003
hamacher
queyranne
best
solutions
combinatorial
optimization
problems
annals
operations
research
4:123–143
1985
10.1007/bf02022039
kohli
torr
measuring
uncertainty
graph
cut
solutions
cviu
112
:30–38
2008
kolmogorov
zabih
energy
functions
minimized
via
graph
cuts
pami
:147–
159
2004
komodakis
paragios
tziritas
mrf
op-
timization
via
dual
decomposition
message-passing
revisited
iccv
2007
komodakis
paragios
tziritas
mrf
en-
ergy
minimization
beyond
via
dual
decomposi-
tion
pami
:531
–552
march
2011
koo
rush
collins
jaakkola
sontag
dual
decomposition
parsing
non-
projective
head
automata
conference
empirical
methods
natural
language
processing
2010
lawler
procedure
computing
best
solutions
discrete
optimization
problems
ap-
plication
shortest
path
problem
management
science
18:401–405
1972
natalia
flerova
dechter
bucket
mini-bucket
schemes
best
solutions
graph-
ical
models
ijcai
workshop
graph
structures
knowledge
representation
reasoning
2011
nilsson
eﬃcient
algorithm
ﬁnding
probable
conﬁgurations
probabilistic
expert
systems
statistics
computing
8:159–173
1998
10.1023/a:1008990218483
schlesinger
two-
dimensional
visual
signals
noisy
conditions
rus-
sian
kibernetika
4:113–130
1976
syntactic
analysis
seroussi
golmard
algorithm
directly
ﬁnding
probable
conﬁgurations
bayesian
networks
int
approx
reasoning
:205
233
1994
shimony
finding
maps
belief
networks
np-hard
artiﬁcial
intelligence
:399–410
au-
gust
1994
shor
minimization
methods
non-diﬀerentiable
functions
springer
series
computational
mathe-
matics
springer-verlag
1985
