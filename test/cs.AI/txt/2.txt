exploratory
gradient
boosting
reinforcement
learning
complex
domains
david
abel‡
alekh
agarwal†
fernando
diaz†
akshay
krishnamurthy†
robert
schapire†
‡department
computer
science
brown
university
providence
02912
microsoft
research
new
york
10011
abstract
high-dimensional
observations
complex
real-
world
dynamics
present
major
challenges
rein-
forcement
learning
function
approxima-
tion
exploration
address
challenges
two
complementary
techniques
first
develop
gradient-boosting
style
non-
parametric
function
approximator
learning
q-function
residuals
second
propose
exploration
strategy
inspired
principles
state
abstraction
information
acquisition
un-
der
uncertainty
demonstrate
empirical
ef-
fectiveness
techniques
ﬁrst
prelimi-
nary
check
two
standard
tasks
blackjack
n-chain
two
much
larger
realistic
tasks
high-dimensional
observation
spaces
speciﬁcally
introduce
two
benchmarks
built
within
game
minecraft
observa-
tions
pixel
arrays
agent
visual
ﬁeld
combination
two
algorithmic
techniques
per-
forms
competitively
standard
reinforcement-
learning
tasks
consistently
substantially
outperforming
baselines
two
tasks
high-
dimensional
observation
spaces
new
func-
tion
approximator
exploration
strategy
evalua-
tion
benchmarks
independent
interest
pursuit
reinforcement-learning
methods
scale
real-world
domains
introduction
many
real-world
domains
large
state
spaces
complex
dynamics
requiring
agent
reason
ex-
tremely
high-dimensional
observations
example
case
task
figure
agent
must
navigate
highest
location
using
raw
visual
input
developing
efﬁcient
effective
algorithms
envi-
ronments
critically
important
across
variety
domains
even
relatively
straightforward
tasks
like
one
cause
existing
approaches
ﬂounder
instance
sim-
ple
linear
function
approximation
scale
visual
in-
put
nonlinear
function
approximation
deep
learning
mnih
al.
2015
tends
use
relatively
simple
exploration
strategies
figure
visual
hill
climbing
agent
rewarded
navigating
higher
terrain
receiving
raw
visual
input
paper
propose
two
techniques
scaling
re-
inforcement
learning
domains
first
present
novel
non-parametric
function
approximation
scheme
based
gradient
boosting
friedman
2001
mason
al.
2000
method
meant
i.i.d
data
adapted
reinforcement
learning
approach
seems
several
merits
like
deep-learning
based
methods
mnih
al.
2015
succeed
learning
good
function
approximations
builds
powerful
learning
system
unlike
deep-learning
ap-
proaches
however
gradient
boosting
models
amenable
training
prediction
single
laptop
opposed
reliant
gpus
model
naturally
trained
residu-
als
recently
shown
helpful
even
deep
learning
literature
al.
2015
furthermore
boosting
rich
theoretical
foundation
supervised
learning
theory
could
plausibly
extended
reinforcement
learning
settings
future
work
second
contribution
give
complementary
ex-
ploration
tactic
inspired
principle
information
ac-
quisition
uncertainty
iauu
improves
uniform
exploration
incentivizing
novel
action
applica-
tions
extremely
simple
design
efﬁcient
use
data
demonstrate
new
algorithm
combining
techniques
called
generalized
exploratory
q-learning
geql
backbone
agent
facing
highly
com-
plex
tasks
raw
visual
observations
empirically
evaluate
techniques
two
standard
domains
blackjack
sutton
barto
1998
chain
strens
2000
two
much
larger
realis-
tic
tasks
high-dimensional
observation
spaces
latter
tasks
built
within
game
minecraft1
observations
pixel
arrays
agent
visual
ﬁeld
figure
minecraft
experiments
made
possible
new
artiﬁcial
intelligence
experimentation
aix
platform
describe
detail
ﬁnd
stan-
dard
tasks
technique
performs
competitively
two
large
high-dimensional
minecraft
tasks
method
consistently
quite
substantially
outperforms
baseline
related
work
literature
reinforcement
learning
vast
focus
related
results
speciﬁcally
func-
tion
approximation
exploration
strategies
general
introduction
see
sutton
barto
1998
function
approximation
important
technique
scal-
ing
reinforcement-learning
methods
complex
domains
linear
function
approximators
effective
many
problems
sutton
1984
complex
non-linear
models
function
approximation
often
demonstrate
stronger
perfor-
mance
many
challenging
domains
anderson
1986
tesauro
1994
unlike
recent
approaches
based
neu-
ral
network
architectures
mnih
al.
2015
adopt
gradient
boosted
regression
trees
friedman
2001
non-
parametric
class
regression
models
competitive
per-
formance
supervised
learning
tasks
although
similar
en-
semble
approaches
reinforcement
learning
ap-
plied
previous
work
marivate
littman
2013
assume
ﬁxed
set
independently-trained
agents
rather
boosting-style
ensemble
work
introduces
interleaving
boosting
iterations
data
collection
iterative
nature
approxima-
tion
resembles
ofﬂine
batch-style
training
fitted
iteration
ernst
al.
2005
q-learner
iteratively
ﬁxed
set
data
algorithm
differs
iteration
current
q-function
approximation
guides
subse-
quent
data
collection
results
used
drive
next
update
q-function
adaptive
data
collection
strategy
critical
exploration
problem
central
reinforcement
learning
experiments
interleaved
method
signiﬁcantly
outperforms
fitted
q-iteration
main
algorithmic
innovation
new
exploration
strategy
reinforcement
learning
function
approxima-
tion
approach
similar
work
state
abstrac-
tion
learning
agent
constructs
uses
compact
model
world
dietterich
2000
al.
2006
important
difference
algorithm
uses
compact
model
exploration
rather
exploration
policy
learning
consequently
model
compression
compromise
expressivity
learning
algorithm
still
learn
optimal
behavior
contrast
typi-
cal
state-abstraction
approaches
number
works
propose
exploration
tac-
tics
function
approximation
example
2015
train
model
predict
future
state
current
state
proposed
action
use
similarity
1https
//minecraft.net/
predicted
state
memory
bank
inform
exploration
decisions
another
approach
learn
dynamics
model
use
either
optimistic
estimates
xie
al.
2015
uncertainty
stadie
al.
2015
model
provide
exploration
bonuses
see
also
guez
al.
2012
lastly
exploration
strategies
theoretical
guar-
antees
domains
certain
metric
structure
kakade
al.
2003
structure
must
known
priori
unclear
construct
structure
general
consider
standard
geql
algorithm
section
present
new
model-free
reinforcement-
learning
algorithm
generalized
exploratory
q-learning
geql
includes
two
independent
complementary
components
new
function-approximation
scheme
based
gradient
boosting
new
exploration
tactic
based
model
compression
3.1
setting
discounted
model-free
reinforcement-learning
setting
agent
inter-
acts
environment
goal
accumulating
high
reward
time
step
agent
observes
state
might
represented
high-dimensional
vector
raw
visual
input
figure
agent
selects
action
whose
execution
modiﬁes
state
environment
typically
moving
agent
finally
agent
receives
real-valued
reward
process
either
repeats
indeﬁnitely
ﬁxed
number
actions
agent
goal
maximize
long-term
t=1
γt−1rt
process
typically
assumed
deﬁne
markov
deci-
sion
process
mdp
meaning
next
state
reached
st+1
ﬁxed
stochastic
function
depends
previous
state
action
executed
reward
similarly
depends
discounted
reward
cid:80
pre-speciﬁed
discount
factor
simplicity
assume
development
states
fact
fully
observable
however
many
real-
istic
settings
agent
observes
might
fully
deﬁne
underlying
state
words
environment
might
partially
observable
mdp
nevertheless
practice
may
often
reasonable
use
observations
actu-
ally
unobserved
states
especially
observations
rich
informative
alternatively
purpose
could
use
recent
past
window
observations
actions
even
entire
past
history
3.2
boosting-based
q-function
approximation
approach
based
q-learning
standard
tech-
nique
recall
optimal
cid:63
function
deﬁned
state-action
pair
expected
discounted
reward
trajectory
begins
state
ﬁrst
action
taken
subsequent
actions
chosen
optimally
maxi-
mize
expected
discounted
reward
general
condi-
tions
function
satisﬁes
cid:63
max
cid:48
cid:63
cid:48
cid:48
algorithm
geql
input
number
episodes
discount
factor
state-collapsing
function
learning
rate
schedule
output
policy
set
start
state
end
return
arg
maxa
choose
using
iauu
exploration
strategy
execute
observe
transition
si+1
end
set
minimize
regressors
αth
random
reward
cid:48
random
next
state
reached
action
executed
state
like
function-approximation
schemes
constructs
function
approximates
cid:63
attempting
observed
data
shown
algorithm
build
function
iter-
atively
series
episodes
using
gradient-boosting
ap-
proach
episode
ﬁrst
use
guide
behavior
agent
mainly
choosing
actions
seem
beneﬁ-
cial
according
occasionally
taking
exploration
steps
way
described
shortly
way
agent
observes
series
state-action-reward
tuples
simplicity
suppose
episode
ﬁxed
length
method
easily
handle
variable
length
episodes
next
step
use
observations
improve
speciﬁcally
algorithm
ﬁts
regressor
residuals
current
approximation
observed
one-step
look-ahead
standard
boosting
methods
chosen
approximately
minimize
max
cid:48
si+1
cid:48
functions
class
weak
regressors
typical
example
weak
regressors
might
chosen
regression
trees
highly
ﬂexible
effective
efﬁciently
trainable
mohan
al.
2011
computed
added
function
approximator
thus
also
updating
actions
selected
future
episodes
function-approximation
scheme
several
important
advantages
existing
approaches
first
using
non-
linear
base
model
regression
trees
agent
able
learn
complex
non-parametric
approximation
cid:63
function
crucial
settings
high-dimensional
observations
time
training
procedure
computationally
efﬁcient
updates
occur
batches
tra-
jectories
discarded
update
finally
in-
terleaving
data
collection
using
learned
policy
induced
residual
regression
data
collected
intu-
itively
improve
quality
informativeness
dataset
e−1
cid:88
i=1
thus
enabling
agent
effectively
accurately
ap-
proximate
optimal
cid:63
function
3.3
iauu
exploration
strategy
second
novel
component
algorithm
explo-
ration
technique
borrows
ideas
state-abstraction
literature
technique
uses
state-collapsing
function
maps
state
one
clusters
relatively
small
geql
function
trained
cluster-
ing
large
dataset
states
instance
using
k-means
algorithm
experiments
associating
state
nearest
cluster
center
ideally
optimality-
preserving
compression
would
used
char-
acterized
al.
2006
abstractions
always
easily
computable
main
idea
technique
keep
track
often
action
taken
states
clusters
choose
exploratory
steps
encourage
choice
actions
taken
less
often
current
clus-
ter
thus
episode
clusters
action
maintain
count
often
action
taken
states
cluster
i.e.
state
table
induces
gibbs
distribu-
tion
actions
deﬁned
a|s
exp
−ρm
temperature
parameter
controls
uni-
formity
distribution
concert
current
func-
tion
approximator
use
distribution
deﬁne
ran-
domized
choice
actions
step
episode
specif-
ically
current
state
probability
choose
act
greedily
selecting
action
maximizes
probability
take
exploration
step
sampling
a|s
call
exploration
strategy
in-
formation
acquisition
uncertainty
iauu
strategy
shares
beneﬁts
work
state
abstraction
without
suffering
drawbacks
main
advantage
state-collapsing
function
promotes
applying
infrequently-taken
actions
thereby
encouraging
agent
visit
new
regions
state
space
however
contrast
state-abstraction
literature
method
re-
mains
robust
misspeciﬁcation
state-collapsing
func-
tion
since
use
exploration
compromise
agent
ability
learn
optimal
behavior
fi-
nally
iauu
exploration
adds
minimal
computational
over-
head
space
needed
m|a|
additional
run-
ning
time
per
action
remains
|a|
iauu
exploration
tactic
attached
function
approximation
scheme
also
used
q-learning
tabular
setting
i.e.
maintained
explicitly
table
case
number
modiﬁcations
exploration
strategy
convergence
properties
q-learning
retained
one
option
modify
exploration
distribution
a|s
mix
vanishingly
small
amount
uniform
distribution
another
option
count
exploration
steps
updating
state-visitation
ta-
ble
cases
action
taken
state
inﬁnitely
often
property
sufﬁces
ensure
conver-
gence
q-learning
tabular
setting
experiments
standard
benchmarks
section
details
evaluation
two
standard
rein-
forcement
learning
benchmarks
blackjack
n-chain
4.1
blackjack
conducted
experiments
blackjack
domain
im-
plemented
exactly
deﬁned
section
5.1
sutton
barto
1998
experiment
test
hypothesis
geql
improve
standard
baselines
even
fairly
simple
domains
without
high-dimensional
visual
input
particular
blackjack
fully
observable
low
noise
low
dimension
short
episodes
small
action
space
algorithms
parameters
geql
used
depth-
regression
trees
weak
regressors
using
python
scikit-learn
package
buitinck
al.
2013
test
iso-
late
effectiveness
incremental
boosting
approach
henceforth
booster
compared
geql
three
baseline
function
approximators
q-learning
linear
approximator
linear
q-learning
batch
boosted
regression
approxi-
mator
similar
fitted
q-iteration
batchboost
q-learning
batch
random
forest
regression
ap-
proximator
forest
approximator
used
set
features
ex-
periments
two
batch-based
regression
approaches
trained
every
episodes
similar
geql
set
depth
regression
trees
batch
approaches
two
batchboost
forest
approximators
used
number
total
trees
geql
trained
batch
run
500
episodes
tree
based
approach
gets
total
500
trees
batchboost
forest
500
trees
retrained
episodes
worth
data
booster
adds
new
tree
every
episode
ran
function
approximators
-uniform
explo-
ration
iauu
exploration
across
experiments
set
0.4
0.15
0.95
decayed
episode
0/
0.04t
α0/
0.04t
state
clustering
function
used
iauu
learned
individual
task
randomly
sampling
states
via
random
policy
results
figure
shows
results
100
trials
run
500
episodes
results
indicate
500
episodes
minimal
learning
occurred
lin-
ear
approximator
gradient-boosting
approximator
able
learn
play
far
effectively
yielding
statistically
signiﬁcant
improvement
performance
two
batch
approximators
demonstrated
learning
though
gradient-boosting
approximator
far
outperformed
however
exploration
tactic
negligible
ef-
fect
domain
likely
due
small
action
space
two
actions
short
episodes
typically
one
two
actions
per
episode
brevity
episode
may
also
explain
linear
approximator
learned
little
many
episodes
4.2
n-chain
conducted
experiments
n-chain
domain
strens
2000
domain
states
numbered
figure
blackjack
running
average
per-episode
re-
ward
error
band
denoting
standard
errors
iauu
runs
legend
red
booster
blue
forest
green
batchboost
black
linear
solid
iauu
dashed
uni-
form
figure
n-chain
running
average
per-episode
reward
legend
iauu
solid
rmax
dashed
q-learning
dotted
two
actions
available
applying
forward
action
state
advances
agent
state
return
action
moves
agent
state
applica-
tions
forward
action
provide
zero
reward
states
except
transitions
state
provides
reward
100.
applications
return
action
receive
reward
tran-
sitions
actions
also
stochastic
proba-
bility
0.2
opposite
effect
task
poses
challenging
exploration
problem
since
agent
must
avoid
greedily
taking
return
action
learn
optimal
behavior
exploring
way
last
state
chain
used
typical
setting
task
algorithms
tabular
problem
evaluated
tabular
methods
used
tabular
q-learner
uni-
form
exploration
iauu
exploration
rmax
algo-
rithm
brafman
tennenholtz
2003
model
based
algorithm
strong
sample
complexity
guarantees
results
figure
displays
results
5000
trials
unsur-
prisingly
rmax
signiﬁcantly
outperformed
two
tabular
q-learning
strategies
used
since
designed
seek
state-action
applications
helping
quickly
discover
high
reward
end
chain
q-learning
uniform
exploration
hand
discover
high
reward
last
state
chain
exponentially
0100200300400500-0.6-0.5-0.4-0.3-0.2050100150200100150200250300350
small
probability
since
agent
favor
greedy
action
must
repeatedly
explore
advance
chain
iauu
exploration
two
extremes
general-
izes
extremely
large
state
spaces
unlike
rmax
pro-
vides
effective
exploration
-uniform
experiments
visual
domains
section
describes
empirical
evaluation
two
highly
challenging
problems
agent
must
reason
raw
rgb
images
used
minecraft
game
platform
provide
environments
two
tasks
minecraft
blocks
world
player
place
blocks
destroy
blocks
craft
objects
navigate
terrain
size
underlying
state
space
grows
exponentially
number
blocks
allowed
world
typically
or-
der
millions
making
planning
learning
infeasible
tabular
approaches
moreover
day
night
cycles
weather
cycles
dramatically
alter
visual
world
well
animals
roam
world
underwater
environ-
ments
size
state
space
complex
visual
elements
pose
signiﬁcant
challenges
learning
raw
vi-
sual
inputs
minecraft
previously
used
planning
research
abel
al.
2015
advocated
gen-
eral
artiﬁcial
intelligence
experimentation
platform
aluru
al.
2015
experimental
results
enabled
recently
de-
veloped
artiﬁcial
intelligence
experimentation
aix
plat-
form
designed
experimentation
within
minecraft
aix
provides
ﬂexible
easy-to-use
api
minecraft
en-
gine
allows
full
control
agent
including
ac-
tion
execution
perception
well
precise
design
minecraft
world
agent
operates
i.e
speciﬁc
block
placement
day
night
cycles
etc.
visual
grid
world
task
hand
crafted
grid
world
inspired
clas-
sical
reinforcement
learning
task
russell
norvig
1995
visual
hill
climbing
task
built
minecraft
random
world
generator
aix
running
minecraft
runs
around
frames
per
second
though
agents
execute
around
actions
per
second
5.1
visual
system
rich
world
minecraft
re-
inforcement
learning
agents
require
additional
preprocessing
raw
rgb
observation
employ
classical
computer
vi-
sion
techniques
preprocessing
raw
visual
images
minecraft
game
input
visual
pipeline
320
240
image
minecraft
agent
view
distracting
elements
toolbar
removed
implementation
details
use
data
ﬁve-minute
random
exploration
minecraft
world
agent
takes
random
action
every
time
step
speciﬁcally
every
20th
frame
received
game
engine
agent
performs
surf
key-point
detection
bay
al.
2008
stores
set
key
points
dataset
ﬁve
minutes
agent
performs
k-means
clustering
space
key
points
reduce
dimen-
sionality
key-point
space
interest
training
done
ofﬂine
experiments
conducted
vi-
sual
system
used
algorithms
system
trained
separately
task
task
new
frame
agent
receives
partitions
frame
grid
partition
agent
ﬁnds
key
point
similar
key-point
centers
computes
distance
distance
used
feature
cell
ultimate
feature
vector
concatenation
partition
key-point
distances
features
per
partitions
total
features
plus
bias
term
occupancy
grid
since
rgb
image
available
vision
system
based
agent
ﬁrst-person
perspective
see
figure
agent
immediate
surroundings
par-
tially
occluded
immediate
surroundings
crucial
decision
making
augment
agent
occupancy
grid
cells
agent
touching
occu-
pancy
grid
contains
corresponding
cell
adjacent
agent
contains
block
solid
e.g
dirt
grass
stone
etc
water
otherwise
binary
features
along
key
point
distances
vision
system
comprise
state
feature
vector
available
agent
state-collapsing
function
state-collapsing
func-
tion
see
section
3.3
train
another
k-means
instance
maps
given
state
object
lower
dimension
let
minecraft
agent
explore
another
ﬁve
minutes
saving
every
20th
frame
saved
frame
agent
computes
feature
vector
described
concatenates
occupancy
grid
agent
surrounding
cells
storing
vectors
data
set
ﬁve
minutes
agent
performs
k-means
data
set
features
reduce
feature
space
lower
dimension
training
also
done
ofﬂine
experiments
conducted
iauu
algorithms
use
state-collapsing
function
task
learning
iauu
agents
evaluate
state-collapsing
function
mapping
current
state
feature
vector
nearest
cluster
center
k-means
instance
5.2
visual
grid
world
ﬁrst
task
consider
visual
grid
world
task
environment
consists
grid
agent
always
starts
must
navigate
using
move-
ments
north
east
south
west
rotation
i.e
agent
always
facing
north
state
agent
ob-
serves
raw
bitmap
image
agent
view
figure
preprocessed
using
vision
system
augmented
occupancy
grid
reward
function
negation
agent
euclidean
distance
away
goal
ex-
ample
agent
distance
goal
agent
receives
reward
transitions
deterministic
op-
timal
policy
achieve
reward
roughly
−31
directly
proceeding
goal
function-approximation
schemes
algorithms
parameters
blackjack
used
four
linear
batchboost
forest
interleaved
gradient-
boosting
approach
denoted
booster
two
ex-
ploration
strategies
-uniform
iauu
regression
problems
solved
episode
linear
gradient
approximators
using
data
recent
episode
batchboost
forest
approximators
completely
retrained
every
ﬁve
episodes
using
recent
ﬁve
episodes
data
depth
number
figure
visual
grid
world
agent
rewarded
nav-
igating
blue
pillar
receiving
raw
visual
input
trees
tree-based
methods
set
two
total
number
episodes
100
parameters
set
blackjack
experiment
results
figure
shows
results
ﬁve
trials
100
episodes
episode
seconds
results
demonstrate
gradient-boosting
approxima-
tor
led
dramatically
faster
learning
task
linear
approximator
statistically
signiﬁcant
0.05
level
booster
also
outperforms
two
batch
approx-
imators
similar
statistical
signiﬁcance
apart
batchboost
baseline
uniform
exploration
still
underperforms
though
insigniﬁcantly
combina-
tion
gradient
boosting
iauu
exploration
best
average
performance
improvement
gradient
boost-
ing
-uniform
statistically
signiﬁcant
due
speed
minecraft
engine
scaling
experiments
challenging
nevertheless
results
clearly
show
geql
booster
iauu
exploration
major
im-
provement
previous
baselines
challenging
task
since
know
reward
optimal
policy
case
also
checked
reward
policy
learned
booster
end
100
episodes
found
average
rewards
booster
iauu
uniform
ex-
plorations
−34.22
−82.77
respectively
note
optimal
policy
gets
reward
close
−31
best
baseline
batchboost
uniform
exploration
picks
reward
around
−119.5
task
hence
conclude
geql
learns
signiﬁcantly
better
policy
base-
lines
task
5.3
visual
hill
climbing
second
task
call
visual
hill
climbing
espe-
cially
difﬁcult
core
variant
non-convex
op-
timization
environment
visual
grid
world
state
preprocessed
raw
rgb
bitmap
image
agent
view
along
occupancy
grid
agent
must
climb
highest
hill
ﬁnd
navigating
highly
com-
plex
world
includes
animals
lakes
rivers
trees
cav-
erns
clouds
pits
variety
different
terrain
types
example
snapshot
agent
task
pictured
figure
especially
challenging
exploration
problem
agent
view
restricted
ﬁnite
horizon
agent
figure
visual
grid
world
running
average
per-episode
reward
error
bands
iauu
versions
denoting
stan-
dard
errors
legend
red
booster
blue
forest
green
batchboost
black
linear
solid
iauu
dashed
uni-
form
see
one
direction
time
large
hills
may
also
partially
occluded
trees
animals
agent
arm
hills
scaling
hills
involves
steps
jumps
larger
agent
make
one
action
meaning
agent
scale
hill
even
gets
agent
may
move
forward
direction
facing
turn
left
degrees
turn
right
degrees
jump
two
units
perform
combination
jumps
two
units
moves
forward
agent
receives
reward
increas-
ing
elevation
units
reward
decreasing
elevation
units
small
reward
proportional
current
height
relative
sea
level
agent
ini-
tial
elevation
agent
reaches
elevation
receive
additional
1000
reward
transitions
deterministic
state
partially
observable
repeated
application
action
particular
observation
may
result
dramatically
different
future
observations
algorithms
parameters
used
algo-
rithms
parameter
settings
visual
grid
world
results
figure
displays
results
ten
trials
100
episodes
episode
exactly
seconds
results
indicate
gradient
booster
able
learn
far
better
policy
approximators
iauu
exploration
tactic
helps
indeed
gra-
dient
booster
non-negligible
learning
occur
given
complex
domain
extremely
promising
learn
reasonable
policy
i.e
agent
able
learn
policy
leads
positive
reward
suggesting
agent
climbing
substantial
amount
visualize
performance
agent
better
plot
elevation
proﬁle
policy
learned
booster
iauu
exploration
time
figure
notice
agent
barely
increases
elevation
initial
quarter
episodes
reliably
reaching
much
better
altitudes
last
quarter
indicating
identify
key
succeeding
task
020406080100-400-350-300-250-200-150-100
figure
visual
hill
climbing
elevation
proﬁle
elevation
throughout
episode
averaged
episodes
indicated
range
i.e
ﬁrst
second
third
fourth
episodes
booster
red
forest
black
iauu
exploration
elevations
respect
starting
elevation
elevation
effective
agent
well
understand
gulf
human
performance
exciting
proposition
future
work
acknowledgement
would
like
thank
katja
hoff-
man
matthew
johnson
david
bignell
tim
hutton
members
aix
platform
development
team
without
whose
support
work
would
possible
references
abel
al.
2015
david
abel
david
ellis
hershkowitz
gabriel
barth-maron
stephen
brawner
kevin
farrell
james
macglashan
stefanie
tellex
goal-based
ac-
international
conference
automated
tion
priors
planning
scheduling
icaps
2015
aluru
al.
2015
krishna
aluru
stefanie
tellex
john
oberlin
james
macglashan
minecraft
experi-
mental
world
robotics
aaai
fall
symposium
2015
anderson
1986
charles
william
anderson
learning
problem
solving
multilayer
connectionist
systems
phd
thesis
university
massachusetts
amherst
1986
bay
al.
2008
herbert
bay
andreas
ess
tinne
tuyte-
laars
luc
van
gool
speeded-up
robust
features
surf
computer
vision
image
understanding
2008
brafman
tennenholtz
2003
ronen
brafman
moshe
tennenholtz
r-max-a
general
polynomial
time
al-
gorithm
near-optimal
reinforcement
learning
journal
machine
learning
research
2003
buitinck
al.
2013
lars
buitinck
gilles
louppe
math-
ieu
blondel
fabian
pedregosa
andreas
mueller
olivier
grisel
vlad
niculae
peter
prettenhofer
alexandre
gram-
fort
jaques
grobler
api
design
machine
learning
software
experiences
scikit-learn
project
arxiv
preprint
arxiv:1309.0238
2013
dietterich
2000
thomas
dietterich
hierarchical
rein-
forcement
learning
maxq
value
function
decom-
position
journal
artiﬁcial
intelligence
research
2000
ernst
al.
2005
damien
ernst
pierre
geurts
louis
wehenkel
tree-based
batch
mode
reinforcement
learning
journal
machine
learning
research
2005
friedman
2001
jerome
friedman
greedy
function
ap-
proximation
gradient
boosting
machine
annals
statistics
2001.
figure
visual
hill
climbing
running
average
per-
episode
reward
error
bands
iauu
versions
denoting
standard
errors
legend
red
booster
blue
forest
green
batchboost
black
linear
solid
iauu
dashed
uniform
discussion
paper
described
novel
approaches
func-
tion
approximation
well
exploration
reinforcement
learning
evaluated
challenging
tasks
implemented
within
minecraft
encouraging
performance
meth-
ods
suggests
several
exciting
avenues
future
re-
search
empirically
performance
gradient
boost-
ing
coupled
favorable
computational
properties
ap-
pears
promising
would
interesting
compare
computationally-intensive
deep-learning
based
ap-
proaches
future
work
extending
existing
theory
gradient
boosting
supervised
reinforcement
learning
also
natural
question
terms
exploration
iauu
certainly
improves
-uniform
still
limited
state-collapsing
func-
tion
used
suboptimal
least-frequent
action
also
happens
bad
remains
challenging
ﬁnd
better
alternatives
tractable
reinforcement
learning
real-time
decisions
high-dimensional
observations
finally
minecraft
provides
attractive
framework
de-
velop
visual
versions
standard
tasks
show
two
ex-
amples
opportunity
translate
tasks
stress
highlight
various
learning
abilities
agent
020406080100-202461
25020406080-2024626
50020406080-2024651
75020406080-2024676
100020406080100-50050100150
xie
al.
2015
christopher
xie
sachin
patil
teodor
moldovan
sergey
levine
pieter
abbeel
model-
reinforcement
based
parametrized
physical
models
exploration
arxiv:1509.06824
2015.
learning
optimism-driven
guez
al.
2012
arthur
guez
david
silver
peter
dayan
efﬁcient
bayes-adaptive
reinforcement
learning
using
sample-based
search
advances
neural
infor-
mation
processing
systems
nips
2012
al.
2015
kaiming
xiangyu
zhang
shaoqing
ren
jian
sun
deep
residual
learning
image
recog-
nition
arxiv:1512.03385
2015
kakade
al.
2003
sham
kakade
michael
kearns
john
langford
exploration
metric
state
spaces
in-
ternational
conference
machine
learning
2003
al.
2006
lihong
thomas
walsh
michael
littman
towards
uniﬁed
theory
state
abstraction
mdps
international
symposium
artiﬁcial
intelli-
gence
mathematics
isaim
2006
marivate
littman
2013
vukosi
michael
littman
linearly
com-
bined
reinforcement-learning
agents
aaai
conference
late-breaking
papers
2013.
ensemble
marivate
mason
al.
2000
llew
mason
jonathan
baxter
peter
bartlett
marcus
frean
functional
gradient
tech-
niques
combining
hypotheses
advances
large
margin
classiﬁers
2000
mnih
al.
2015
volodymyr
mnih
koray
kavukcuoglu
david
silver
andrei
rusu
joel
veness
marc
belle-
mare
alex
graves
martin
riedmiller
andreas
fidje-
land
georg
ostrovski
human-level
control
deep
reinforcement
learning
nature
2015
mohan
al.
2011
ananth
mohan
zheng
chen
kil-
ian
weinberger
web-search
ranking
initialized
gradient
boosted
regression
trees
yahoo
learning
rank
challenge
2011
al.
2015
junhyuk
xiaoxiao
guo
honglak
lee
richard
lewis
satinder
singh
action-conditional
video
prediction
using
deep
networks
atari
games
advances
neural
information
processing
systems
nips
2015
russell
norvig
1995
stuart
russell
peter
norvig
artiﬁcial
intelligence
modern
approach
prentice-hall
englewood
cliffs
1995
stadie
al.
2015
bradly
stadie
sergey
levine
pieter
abbeel
incentivizing
exploration
reinforcement
learning
deep
predictive
models
arxiv:1507.00814
2015
strens
2000
malcolm
strens
bayesian
framework
international
conference
reinforcement
learning
machine
learning
icml
2000
sutton
barto
1998
richard
sutton
andrew
barto
reinforcement
learning
introduction
mit
press
1998
sutton
1984
richard
sutton
temporal
credit
assign-
ment
reinforcement
learning
phd
thesis
university
massachusetts
amherst
1984
tesauro
1994
gerald
tesauro
td-gammon
self-
teaching
backgammon
program
achieves
master-level
play
neural
computation
1994
