efficient
algorithm
estimating
state
sequences
imprecise
hidden
markov
models
jasper
bock
gert
cooman
abstract
present
efﬁcient
exact
algorithm
estimating
state
sequences
outputs
observations
imprecise
hidden
markov
models
ihmm
uncertainty
linking
one
state
next
linking
state
output
represented
using
coherent
lower
previsions
notion
independence
associate
credal
network
representing
ihmm
epistemic
irrelevance
consider
best
estim-
ates
state
sequences
walley–sen
maximal
sequences
posterior
joint
state
model
conditioned
observed
output
sequence
associated
gain
function
indicator
state
sequence
corresponds
generalises
ﬁnding
state
sequence
highest
posterior
probability
hmms
precise
transition
output
probabilities
phmms
argue
computational
complexity
worst
quadratic
length
markov
chain
cubic
number
states
essentially
linear
number
maximal
state
sequences
binary
ihmms
investigate
experimentally
number
maximal
state
sequences
depends
model
parameters
also
present
simple
toy
application
optical
character
recognition
demonstrating
algorithm
used
robustify
inferences
made
precise
probability
models
introduction
artiﬁcial
intelligence
probabilistic
graphical
models
becoming
increasingly
powerful
tool
amongst
hidden
markov
models
hmms
deﬁnitely
amongst
simplest
perhaps
also
amongst
popular
ones
important
application
hmms
involves
ﬁnding
sequence
hidden
states
highest
posterior
probability
observing
sequence
outputs
hmms
precise
local
transition
emission
probabilities
quite
efﬁcient
dynamic
programming
algorithm
due
viterbi
performing
task
imprecise-
probabilistic
local
models
coherent
lower
previsions
know
algorithm
literature
computational
complexity
comes
even
close
viterbi
paper
take
ﬁrst
steps
towards
remedying
situation
describe
imprecise
hidden
markov
models
special
cases
credal
trees
special
case
credal
networks
epistemic
irrelevance
section
show
particular
use
ideas
underlying
mepictir1
algorithm
involving
independent
natural
extension
marginal
extension
construct
conservative
joint
model
imprecise
local
transition
emission
models
also
derive
number
interesting
useful
formulas
construction
results
section
assume
basic
knowledge
theory
coherent
lower
previ-
sions
generalisation
classical
probability
allows
incomplete
speciﬁcation
probabilities
include
short
introduction
theory
section
section
explain
sequence
observations
leads
collection
so-
called
maximal
state
sequences
finding
seems
daunting
task
ﬁrst
search
space
grows
exponentially
length
markov
chain
however
key
words
phrases
hidden
markov
model
state
sequence
estimation
imprecise
probabilities
maximality
coherent
lower
previsions
1mepictir
message
passing
credal
trees
irrelevance
jasper
bock
gert
cooman
section
use
basic
formulas
found
section
derive
appropriate
version
bellman
principle
optimality
allows
exponential
reduction
search
space
using
number
additional
tricks
able
section
devise
estihmm2
algorithm
efﬁciently
constructs
maximal
state
sequences
prove
section
algorithm
essentially
linear
number
maximal
sequences
quadratic
length
chain
cubic
number
states
perceive
complexity
comparable
viterbi
algorithm
especially
realising
latter
makes
simplifying
step
resolving
ties
less
arbitrarily
order
produce
single
optimal
state
sequence
something
allow
algorithm
reasons
become
clear
section
consider
special
case
binary
ihmms
investigate
experi-
mentally
number
maximal
state
sequences
depends
model
parameters
comment
interesting
structures
emerge
give
heuristic
explanation
show
algorithm
efﬁciency
section
calculating
maximal
sequences
speciﬁc
ihmm
length
100.
conclude
section
simple
toy
application
optical
character
recognition
demonstrates
advantages
algorithm
gives
clear
indication
estihmm
algorithm
able
robustify
existing
viterbi
algorithm
intelligent
manner
order
make
main
argumentation
readable
possible
relegated
technical
proofs
appendix
freshening
coherent
lower
previsions
begin
basic
theory
coherent
lower
previsions
see
ref
in-depth
study
ref
recent
survey
coherent
lower
previsions
special
type
imprecise
probability
model
roughly
speaking
whereas
classical
probability
theory
assumes
subject
uncertainty
represented
single
probability
mass
function
theory
imprecise
probabilities
effectively
works
sets
possible
probability
mass
functions
thereby
allows
imprecision
well
indecision
modelled
represented
people
unfamiliar
theory
looking
way
robustifying
classical
theory
perhaps
easiest
way
understand
interpret
use
approach
consider
set
probability
mass
functions
deﬁned
discrete
set
possible
states
mass
function
associate
linear
prevision
expectation
operator
deﬁned
set
real-valued
maps
also
called
gamble
∑x∈x
expected
value
associated
probability
mass
function
deﬁne
lower
prevision
corresponds
set
following
lower
envelope
linear
previsions
inf
cid:8
cid:9
gambles
−inf
cid:8
−pp
cid:9
−inf
cid:8
cid:9
−pm
sup
cid:8
cid:9
similarly
deﬁne
upper
prevision
gambles
mostly
talk
lower
previsions
since
follows
conjugacy
relation
two
models
mathematically
equivalent
event
subset
set
possible
values
event
associate
indicator
gamble
assumes
value
2estimation
imprecise
hidden
markov
models
estimating
state
sequences
imprecise
hidden
markov
models
outside
call
inf
cid:26
x∈a
cid:27
lower
probability
event
similarly
upper
probability
shown
functional
satisﬁes
following
set
interesting
mathematical
properties
deﬁne
coherent
lower
prevision
min
real
non-negative
homogeneity
superadditivity
every
set
mass
functions
uniquely
deﬁnes
coherent
lower
prevision
general
converse
hold
however
limit
sets
mass
functions
closed
convex—which
makes
credal
sets—they
one-to-one
correspondence
coherent
lower
previsions
implies
use
theory
coherent
lower
previsions
tool
reasoning
closed
convex
sets
probability
mass
functions
longer
explicitly
refer
credal
sets
simply
talk
coherent
lower
previsions
useful
keep
mind
always
unique
credal
set
corresponds
coherent
lower
prevision
unique
credal
set
given
cid:8
cid:9
special
kind
imprecise
model
vacuous
lower
prevision
model
represents
complete
ignorance
therefore
set
possible
mass
functions
credal
set
shown
easily
every
corresponding
lower
prevision
given
min
conditional
lower
upper
previsions
extensions
classical
conditional
expectation
functionals
deﬁned
similar
intuitively
obvious
way
lower
envelopes
associated
sets
conditional
mass
functions
consider
variable
variable
conditional
lower
prevision
·|y
set
gambles
two-place
real-valued
function
gamble
f|y
gamble
whose
value
g|y
lower
prevision
conditional
event
lower
prevision
·|y
coherent—
satisﬁes
conditions
c1–c3—then
call
conditional
lower
prevision
·|y
separately
coherent
sometimes
useful
extend
domain
conditional
lower
prevision
·|y
letting
f|y
gambles
number
conditional
lower
previsions
involving
number
variables
must
separately
coherent
also
make
sure
satisfy
stringent
joint
coherence
requirement
explaining
detail
would
take
far
refer
ref
detailed
discussion
motivation
present
purposes
sufﬁces
say
joint
coherence
closely
related
making
sure
conditional
lower
previsions
lower
envelopes
associated
conditional
mass
functions
satisfy
bayes
rule
given
lower
prevision
x×y
corresponding
conditional
lower
prevision
·|y
jointly
coherent
uniquely
deﬁned
however
shown
ref
always
lies
so-called
natural
regular
extensions
using
natural
extension
conditional
coherent
lower
prevision
·|y
deﬁned
given
f|y
min
smallest
conservative
way
conditioning
lower
prevision
corresponds
conditioning
every
probability
mass
function
credal
set
observation
taking
lower
envelope
conditioned
mass
functions
using
regular
extension
conditional
coherent
lower
prevision
·|y
deﬁned
f|y
max
cid:8
cid:9
vacuous
thus
f|y
max
cid:8
cid:9
vacuous
jasper
bock
gert
cooman
gives
greatest
informative
conditional
lower
prevision
jointly
coherent
original
unconditional
lower
prevision
corresponds
taking
mass
functions
credal
set
cid:54
conditioning
observation
taking
lower
envelope
natural
regular
extension
coincide
different
latter
case
natural
extension
vacuous
regular
extension
usually
remains
informative
introduction
coherent
lower
previsions
interpreted
alternative
repres-
entation
closed
convex
sets
probability
mass
functions
approach
often
adopted
sensitivity
analysts
rather
intuitive
people
used
working
classical
probability
theory
sake
completeness
mention
coherent
lower
previsions
also
given
behavioural
interpretation
without
using
notion
probability
mass
function
lower
prevision
gamble
interpreted
supremum
acceptable
buying
price
subject
willing
pay
order
gain
possibly
negative
reward
outcome
experiment
determined
see
ref
information
regarding
interpretation
basic
notions
imprecise
hidden
markov
model
depicted
using
following
probabilistic
graphical
model
state
sequence
output
sequence
·|x1
·|xk−1
·|xn−1
·|x1
·|x2
·|xk
·|xn
figure
tree
representation
hidden
markov
model
natural
number
state
variables
assume
values
respective
ﬁnite
sets
output
variables
assume
values
respective
ﬁnite
sets
denote
generic
values
ˆxk
generic
values
ok.
3.1.
local
uncertainty
models
assume
following
local
uncertainty
models
variables
marginal
lower
prevision
deﬁned
set
real-valued
maps
gambles
subsequent
states
conditional
lower
prevision
·|xk−1
deﬁned
called
transition
model
order
maintain
uniformity
notation
also
denote
marginal
lower
prevision
conditional
lower
prevision
·|x0
denotes
variable
may
assume
single
value
whose
value
therefore
certain
gamble
fk|xk−1
interpreted
gamble
xk−1
whose
value
fk|zk−1
zk−1
xk−1
lower
prevision
gamble
conditional
xk−1
zk−1
addition
output
conditional
lower
prevision
·|xk
deﬁned
called
emission
model
gamble
gk|xk
interpreted
gamble
whose
value
gk|zk
lower
prevision
gamble
conditional
estimating
state
sequences
imprecise
hidden
markov
models
r=k
r=k
take
local
marginal
transition
emission
uncertainty
models
separately
coherent
recall
simply
means
lower
prevision
·|zk−1
coherent
unconditional
lower
prevision
every
zk−1
xk−1
·|zk
coherent
every
3.2.
interpretation
graphical
structure
assume
graphical
repres-
entation
figure
represents
following
irrelevance
assessments
conditional
mother
variable
non-parent
non-descendants
variable
tree
epistemic-
ally
irrelevant
variable
descendants
say
variable
epistemically
irrelevant
variable
observing
affect
beliefs
mathematically
stated
terms
lower
previsions
useful
introduce
mathematical
short-hand
notation
describing
joint
variables
tree
figure
cid:96
denote
tuple
xk+1
cid:96
cid:96
tuple
ok+1
cid:96
cid:96
cid:96
joint
variable
assume
values
set
cid:96
cid:96
cid:96
joint
variable
assume
values
set
cid:96
cid:96
generic
values
cid:96
denoted
cid:96
cid:96
generic
values
cid:96
cid:96
example
consider
variable
mother
variable
xk−1
figure
variables
k−2
k−1
non-parent
non-descendants
variables
xk+1
descendants
interpretation
graphical
structure
figure
implies
know
conditional
value
xk1
xk−1
additionally
learning
values
variables
xk−2
ok−1
change
beliefs
cid:7
epistemic
irrelevance
weaker
so-called
strong
independence
condition
usually
associated
credal
networks
name
usually
given
probabilistic
graphical
models
coherent
lower
previsions
local
uncertainty
models
recent
work
shown
using
weaker
condition
guarantees
efﬁcient
algorithm
exists
updating
beliefs
single
target
node
credal
tree
essentially
linear
number
nodes
tree
3.3.
joint
uncertainty
model
using
local
uncertainty
models
want
construct
global
model
joint
lower
prevision
variables
tree
joint
lower
prevision
jointly
coherent
local
models
encode
epistemic
irrelevance
assessments
encoded
tree
iii
small
conservative,3
possible
special
case
general
problem
credal
trees
discussed
solved
great
detail
ref
section
summarise
solution
ihmms
give
heuristic
justiﬁcation
refer
ref
proof
joint
model
present
indeed
conservative
lower
prevision
coherent
local
models
captures
epistemic
irrelevance
assessments
encoded
tree
proceed
recursive
manner
consider
xk−1
xk−1
consider
smallest
coherent
joint
lower
prevision
·|xk−1
variables
ihmm
depicted
figure
representing
subtree
tree
represented
figure
lower
prevision
·|xk−1
acting
marginal
model
ﬁrst
state
variable
note
global
model
looking
identiﬁed
conditional
lower
prevision
·|x0
reasons
given
section
3.1.
aim
develop
recursive
expressions
enable
construct
·|xk−1
pk+1
·|xk
using
expressions
eventually
yield
global
model
·|x0
ﬁrst
step
combine
joint
model
pk+1
·|xk
variables
xk+1
ok+1
deﬁned
xk+1
ok+1
—see
thick
dotted
lines
figure
local
3recall
point-wise
smaller
lower
previsions
correspond
larger
credal
sets
jasper
bock
gert
cooman
·|xk−1
qk+1
·|xk
xk+1
·|xk
ok+1
sk+1
·|xk+1
·|xk−1
·|xk
pk+1
·|xk
figure
subtree
ihmm
involving
variables
model
·|xk
variable
deﬁned
lead
joint
model
·|xk
variables
xk+1
deﬁned
xk+1
—see
semi-thick
dotted
lines
figure
trivial
since
must
·|xn
·|xn
cid:54
solution
less
obvious
joint
model
constructed
many
different
ways
impose
conditions
ﬁrst
condition
·|xk
separately
coherent
conditional
lower
prevision
jointly
coherent
marginal
models
pk+1
·|xk
·|xk
second
rather
obvious
condition
·|xk
coincide
pk+1
·|xk
·|xk
respective
domains
third
condition
model
capture
epistemic
irrelevance
assessments
encoded
tree
particular
state
conditional
two
variables
xk+1
ok+1
epistemically
independent
words
epistemically
irrelevant
one
another
model
meets
conditions
called
conditionally
independent
product
pk+1
·|xk
·|xk
generally
speaking
conditionally
independent
product
unique
call
point-wise
smallest
conservative
possible
conditionally
independent
products
always
exists
conditionally
independent
natural
extension
pk+1
·|xk
·|xk
denote
pk+1
·|xk
·|xk
summarising
·|xk
given
cid:40
·|xn
·|xk
pk+1
·|xk
·|xk
conditionally
independent
natural
extension
properties
studied
great
detail
ref
purposes
paper
sufﬁce
recall
study
that—very
much
like
independent
products
precise
probability
models—such
estimating
state
sequences
imprecise
hidden
markov
models
independent
natural
extensions
factorising
implies
particular
g|zk
gek
f|zk
|zk
cid:40
gpk+1
f|zk
|zk
g|zk
pk+1
f|zk
g|zk
pk+1
f|zk
g|zk
cid:12
pk+1
f|zk
pk+1
f|zk
pk+1
f|zk
xk+1
ok+1
non-negative
—we
call
gamble
non-negative
values
expression
ﬁrst
equality
actual
factorisation
property
second
equality
holds
·|xk
coincides
pk+1
·|xk
·|xk
respective
domains
third
equality
follows
conjugacy
relation
coherence
condition
fourth
used
shorthand
notation
cid:12
mmax
mmin
also
use
analogous
notation
cid:12
mnmax
mnmin
second
ﬁnal
step
combine
joint
model
·|xk
variables
xk+1
deﬁned
xk+1
local
model
·|xk−1
vari-
able
deﬁned
joint
model
·|xk−1
variables
deﬁned
shown
elsewhere
conservative
coherent
way
means
marginal
extension
also
known
law
iterated
lower
expectations
leads
·|xk−1
·|xk
|xk−1
allow
xk−1
range
xk−1
practical
purposes
useful
see
equivalent
xk+1
|zk
f|xk−1
·|xk−1
·|xk
|xk−1
cid:18
cid:12
cid:12
cid:12
xk−1
cid:19
zk∈xk
recall
expression
indicator
gamble
assumes
value
cid:54
3.4.
interesting
lower
upper
probabilities
without
much
trouble,4
use
equations
derive
following
expressions
number
interesting
lower
upper
probabilities
|zk−1
|zk−1
|zi
|zi−1
|zi
|zi−1
i=k
i=k
zk−1
xk−1
zk+1
|zk
|zk
i=k+1
|zi
|zi−1
|zi
|zi−1
zk+1
|zk
|zk
zk+1
xk+1
assume
throughout
i=k+1
4as
example
derive
equations
appendix
jasper
bock
gert
cooman
equivalently
local
upper
previsions
positive
sense
|zk−1
|zk
zk−1
xk−1
assumption
weak
restrictive
practical
purposes
imprecise-
probabilistic
local
models
usually
constructed
adding
margin
error
around
precise
model
thereby
making
upper
transition
probabilities
positive
construction
however
allow
lower
transition
probabilities
zero
something
happen
often
practical
problems
proposition
assumption
local
upper
previsions
positive
implies
|zk−1
|zk
zk−1
xk−1
estimating
states
outputs
hidden
markov
model
states
directly
observable
outputs
general
aim
use
outputs
estimate
states
concentrate
following
problem
suppose
observed
output
sequence
estimate
state
sequence
use
essentially
bayesian
approach
need
allow
fact
working
imprecise
rather
precise
probability
models
4.1.
updating
ihmm
ﬁrst
step
approach
consists
updating
condi-
tioning
joint
model
·|x0
observed
outputs
mentioned
section
unique
coherent
way
perform
updating
however
particular
problem
solving
paper
happens
makes
difference
updating
method
used
long
coherent
time
choose
use
least
conservative5
informative
coherent
updating
method
regular
extension
later
section
4.2
show
coherent
updating
method
yields
results
since
follows
positivity
assumption
proposition
regular
extension
leads
consider
updated
lower
prevision
·|o1
given
f|o1
max
cid:8
cid:9
gambles
using
coherence
joint
lower
prevision
hard
prove
strictly
decreasing
continuous
function
therefore
unique
zero
see
lemma
iii
appendix
consequence
f|o1
fact
hard
infer
strictly
decreasing
continuous
character
f|o1
sign
either
negative
positive
equal
zero
see
also
illustration
f|o1
5the
conservative
coherent
way
yields
vacuous
model
estimating
state
sequences
imprecise
hidden
markov
models
equation
crucial
importance
however
general
want
allow
zero
may
happen
allow
lower
transition
probabilities
zero
requiring
follows
positivity
assumption
proposition
generally
speaking
invalidate
second
equivalence
equation
turns
implication
limit
speciﬁc
type
gambles
form
ˆx1
still
prove
following
important
theorem
theorem
local
upper
previsions
positive
ˆx1
ˆx1
|o1
sign
ﬁxed
values
ˆx1
positive
negative
zero
4.2.
maximal
state
sequences
next
step
consists
using
posterior
model
·|o1
ﬁnd
best
estimates
state
sequence
bayesian
approach
usually
done
solving
decision-making
optimisation
problem
associate
gain
function
every
candidate
state
sequence
select
best
estimates
state
sequences
ˆx1
maximise
posterior
expected
gain
resulting
state
sequences
maximal
posterior
probability
generalise
decision-making
approach
towards
working
imprecise
probability
models
criterion
use
decide
estimates
optimal
given
gain
functions
walley–sen
maximality
maximality
number
desirable
properties
make
sure
works
well
optimisation
contexts
well-justiﬁed
behavioural
point
view
well
robustness
approach
shall
see
presently
express
strict
preference
cid:31
two
state
sequence
estimates
ˆx1
follows
ˆx1
cid:31
ˆx1
|o1
behavioural
interpretation
expresses
subject
lower
prevision
·|o1
disposed
pay
strictly
positive
amount
utility
replace
gain
associated
estimate
gain
associated
estimate
ˆx1
see
ref
section
3.9
details
alternatively
robustness
point
view
expresses
conditional
mass
function
·|o1
credal
set
associated
updated
lower
prevision
·|o1
state
sequence
ˆx1
posterior
probability
ˆx1
n|o1
strictly
higher
posterior
probability
n|o1
state
sequence
binary
relation
cid:31
thus
deﬁned
strict
partial
order
irreﬂexive
transitive
binary
relation
set
state
sequences
consider
estimate
ˆx1
optimal
undominated
maximal
strict
partial
order
ˆx1
opt
n|o1
∀x1
cid:54
cid:31
ˆx1
∀x1
ˆx1
|o1
∀x1
ˆx1
useful
last
equivalence
follows
theorem
summary
aim
paper
develop
efﬁcient
algorithm
ﬁnding
set
maximal
estimates
opt
n|o1
statement
section
4.1
coherent
updating
method
would
yield
results
regular
extension
justiﬁed
since
coherent
updating
unique
need
motivate
statement
special
case
use
regular
extension
update
model
optimal
estimates
given
special
case
however
ﬁnd
ˆx1
ˆx1
jasper
bock
gert
cooman
ﬁrst
inequality
follows
monotonicity
coherent
lower
previsions
consequence
therefore
ﬁnd
sequences
optimal
resulting
opt
n|o1
use
natural
extension
update
joint
model
optimal
state
sequences
still
given
ﬁnal
equivalence
would
longer
hold
uses
theorem
assumes
use
regular
extension
perform
updating
joint
model
however
special
case
natural
extension
deﬁnition
leads
updated
model
equal
vacuous
one
therefore
ﬁnd
ˆx1
ˆx1
|o1
min
ˆx1
implies
special
case
0—identical
found
regular
extension—natural
extension
also
results
sequences
optimal
meaning
opt
n|o1
thus
shown
special
case
set
optimal
sequences
regardless
whether
use
natural
regular
extension
update
joint
model
since
every
coherent
updating
method
lies
two
methods
opt
n|o1
depend
updating
method
long
coherent
coherent
updating
unique
thus
equal
regular
extension
thereby
making
result
trivial
case
therefore
conclude
results
paper
depend
particular
updating
method
chosen
long
coherent
instead
looking
maximal
state
sequences
one
could
also
use
decision
criteria
ﬁrst
approach
consider
could
consist
trying
ﬁnd
so-called
γ-maximin
state
sequences
maximise
posterior
lower
probability
argmax
n∈x1
|o1
well
known
γ-maximin
sequence
particular
guaranteed
also
maximal
sequence
ﬁnding
γ-maximin
sequences
seems
much
complicated
affair.6
course
know
maximal
solutions
could
determine
γ-maximin
solutions
comparing
posterior
lower
probabilities
far
see
however
calculating
seems
trivial
task
computational
point
view
expect
similar
computational
difﬁculties
yet
another
approach
also
con-
sidered
consists
ﬁnding
so-called
e-admissable
sequences
sequences
maximise
expected
gain
least
one
conditional
mass
function
·|o1
credal
set
associated
updated
lower
prevision
·|o1
similarly
γ-maximin
solutions
e-admissable
ones
also
known
contained
within
set
maximal
ones
constructing
main
reason
approach
efﬁcient
compared
ones
explicitly
calculate
value
lower
previsions
need
know
sign
thereby
allowing
work
directly
joint
model
instead
updated
model
4.3.
maximal
subsequences
shall
see
order
ﬁnd
set
maximal
estimates
useful
consider
general
sets
so-called
maximal
subsequences
zk−1
xk−1
deﬁne
opt
n|zk−1
ˆxk
opt
n|zk−1
∀xk
ˆxk
|zk−1
interpretation
sets
immediate
consider
following
part
original
ihmm
take
·|zk−1
marginal
model
ﬁrst
state
6private
communication
cassio
campos
estimating
state
sequences
imprecise
hidden
markov
models
·|zk−1
·|xr−1
·|xn−1
state
subsequence
output
subsequence
·|xk
·|xr
·|xn
argued
section
3.3
corresponding
joint
lower
prevision
precisely
·|zk−1
sequence
outputs
opt
n|zk−1
set
state
sequence
estimates
undominated
estimate
clear
set
opt
n|o1
eventually
looking
also
written
opt
n|z0
4.4.
useful
recursion
equations
fix
look
equation
see
useful
derive
manageable
expression
lower
prevision
ˆxk
|zk−1
easily
done
see
appendix
using
equa-
tions
together
algebraic
manipulations
consider
three
different
cases
ˆxk
using
notation
introduced
section
3.3
ˆxk
|zk−1
ˆxk
|zk−1
ˆxk
cid:12
pk+1
ok+1
xk+1
ˆxk+1
ˆxk
ˆxn
ˆxk
cid:54
ˆxn
|zn−1
ˆxk
|zk−1
ˆxk
ˆxk
|zk−1
deﬁne
zk+1
|zk
|zk
zk+1
|zk
|zk
i=k+1
i=k+1
|zi
|zi−1
|zi
|zi−1
given
sequence
states
found
simple
backward
recursion
zk+1
|zk
qk+1
zk+1
|zk
zk+1
|zk
qk+1
zk+1
|zk
starting
|zn
|zn
principle
optimality
determining
state
sequences
opt
n|o1
directly
using
equation
clearly
exponential
complexity
length
chain
going
take
dynamic
programming
approach
reducing
complexity
deriving
recursion
equation
sets
optimal
sub
sequences
opt
n|zk−1
jasper
bock
gert
cooman
theorem
principle
optimality
zk−1
xk−1
ˆxk
ˆxk
|zk−1
ˆxk
ˆxk
opt
n|zk−1
ˆxk+1
opt
xk+1
ˆxk
ok+1
immediate
consequence
ﬁnd
opt
n|zk−1
cand
n|zk−1
cand
n|zk−1
set
sequences
still
element
opt
n|zk−1
according
theorem
cid:19
cand
n|zk−1
cid:19
cid:18
cid:91
cid:18
cid:91
opt
xk+1
n|zk
ok+1
xk+1
zk∈posk
zk−1
/∈posk
zk−1
denotes
concatenation
state
sequences
set
states
posk
zk−1
deﬁned
posk
zk−1
|zk−1
|zk
equation
simpliﬁes
cand
n|zk−1
opt
xk+1
n|zk
ok+1
cid:91
zk∈xk
local
lower
previsions
positive
generally
true
general
case
considering
upper
previsions
required
positive
also
introduce
following
notation
candxk
n|zk−1
cand
n|zk−1
zk−1
xk−1
algorithm
finding
maximal
state
sequences
use
equation
devise
algorithm
constructing
set
opt
n|o1
maximal
state
sequences
recursive
manner
6.1.
initial
set-up
using
backward
recursion
begin
deﬁning
auxiliary
notions
first
consider
thresholds
cid:110
cid:111
ˆxk
|zk−1
ˆxk
xk|zk−1
min
zk−1
xk−1
ˆxk
next
deﬁne
αmax
max
n∈xk
zk=xk
max
max
n∈xk
zk=xk
using
equations
calculated
efﬁciently
using
following
backward
recursive
dynamic
programming
procedure
αmax
max
max
zk+1∈xk+1
k+1
zk+1
|xk
qk+1
zk+1
|xk
αmax
k+1
zk+1
qk+1
zk+1
|xk
αmax
|xk
max
zk+1∈xk+1
max
zk+1∈xk+1
k+1
zk+1
|xk
qk+1
zk+1
|xk
max
k+1
zk+1
qk+1
zk+1
|xk
max
|xk
max
zk+1∈xk+1
starting
estimating
state
sequences
imprecise
hidden
markov
models
|xn
max
|xn
αmax
finally
let
ˆxk|zk−1
max
αopt
xk∈xk
cid:54
ˆxk
max
ˆxk
xk|zk−1
zk−1
xk−1
ˆxk
ˆxk|zk−1
cal-
6.2.
reformulation
optimality
condition
turns
αopt
culated
equation
extremely
useful
proved
appendix
allow
signiﬁcantly
simplify
equation
follows
cid:110
ˆxk
cand
n|zk−1
ˆxk
αopt
cid:111
ˆxk|zk−1
opt
n|zk−1
reduces
opt
xn|zn−1
cid:8
ˆxn
ˆxn
αopt
ˆxn|zn−1
cid:9
6.3.
recursive
solution
method
aim
algorithm
determine
set
opt
n|o1
efﬁciently
recursively
opt
xn|zn−1
determined
straightforward
manner
every
zn−1
xn−1
using
criterion
example
consider
simple
binary
hmm
0,1
maximal
elements
simply
states
trivially
represented
could
example
ﬁnd
opt
xn|0
0,1
zn−1
opt
xn|1
zn−1
cid:7
next
let
run
backward
n−1
zk−1
xk−1
ﬁrst
build
set
cand
n|zk−1
using
deﬁnition
equation
results
previous
recursion
step
set
used
determine
opt
xk|zk−1
criterion
example
continue
discussion
example
zn−2
set
cand
xn−1
n|0
on−1
constructed
using
equation
instance
posn−1
0,1
reduces
equation
ﬁnd
cand
xn−1
n|0
on−1
zn−1
opt
xn|zn−1
cid:91
zn−1∈
0,1
0,1
00,01
00,01,10
applying
criterion
every
element
set
ﬁnd
set
opt
xn−1
n|0
on−1
instance
could
equal
00,10
zn−2
analoguous
method
cid:7
used
continuing
way
eventually
reach
yields
desired
set
maximal
sequences
opt
n|o1
opt
n|z0
possible
bottleneck
solution
lies
use
criterion
cri-
terion
already
much
efﬁcient
original
one
still
lead
exponential
complexity
set
cand
n|zk−1
number
elements
exponential
length
considered
sequences
therefore
present
method
avoids
checking
inequality
criterion
elements
cand
n|zk−1
ﬁrst
trick
consists
using
efﬁcient
data
structure
store
sets
optimal
sequences
simply
list
elements
could
also
list
optimal
sequences
would
imply
storing
information
multiple
times
since
parts
sequences
therefore
choose
represent
list
optimal
sequences
collection
tree
structures
way
trees
constructed
obvious
following
example
jasper
bock
gert
cooman
example
consider
following
set
sequences
00001000,00001010,00001110,00011110,10001010,10001110
representing
set
way
useful
information
gets
lost
memory
space
waisted
example
sequences
start
way
would
much
efﬁcient
store
common
subsequences
therefore
prefer
represent
set
collection
trees
depicted
cid:7
next
step
exploit
data
structure
order
apply
criterion
efﬁciently
start
constructing
set
cand
n|zk−1
representing
type
data
structure
example
consider
set
sequences
example
opt
xk+1
ok+1
n−8
since
length
sequences
suppose
already
constructed
set
previous
recursion
step
furthermore
sake
example
lets
assume
posk−1
posk−1
use
equation
construct
set
cand
n|0
cand
n|0
opt
xk+1
n|0
ok+1
xk+1
set
cand
n|0
consist
two
subsets
construct
separately
subset
opt
xk+1
n|0
ok+1
would
normally
take
quite
effort
compose
since
concatenate
individual
element
opt
xk+1
n|0
ok+1
however
using
representation
comes
adding
one
node
two
links
already
existing
data
structure
opt
xk+1
n|0
ok+1
opt
xk+1
n|0
ok+1
opt
xk+1
ok+1
conceptually
want
represent
set
xk+1
tree
would
look
like
ﬁgure
left
estimating
state
sequences
imprecise
hidden
markov
models
xk+1
xk+1
xk+1
xk+1
however
storing
way
computer
bad
idea
would
mean
constructing
complete
binary
tree
exponential
depth
tree
therefore
remember
set
sequences
represented
tree
without
actually
constructing
depicted
right
cand
n|0
opt
xk+1
ok+1
xk+1
set
cand
n|0
looking
trivially
constructed
joining
two
subsets
opt
xk+1
n|0
ok+1
xk+1
depicted
cid:7
follows
equation
data
structure
representing
opt
n|zk−1
contained
data
structure
representing
cand
n|zk−1
left
ﬁnd
subset
efﬁcient
manner
present
method
constructs
subset
cand
n|zk−1
prove
subset
indeed
opt
n|zk−1
s|zk−1
every
zk−1
xk−1
ﬁrst
deﬁne
αopt
zk|zk−1
deﬁned
equation
let
αopt
s|zk−1
recursively
deﬁned
αopt
s−1|zk−1
αopt
s|zk−1
αopt
every
k|zk−1
αopt
ss−1
os−1
|zs−1
|zs−1
optimal
tree
construction
following
method
select
subset
given
set
cand
n|zk−1
constructed
using
equation
first
every
check
whether
αmax
αopt
xk|zk−1
use
generic
notation
ˆxk
condition
satisﬁed
next
choose
arbitrary
ˆxk
check
every
xk+1
xk+1
non-empty
set
cand
ˆxk⊕xk+1
n|zk−1
following
condition
satisﬁed
k+1
xk+1
αopt
αmax
ˆxk
xk+1|zk−1
jasper
bock
gert
cooman
ˆxk
xk+1|zk−1
easily
calculated
using
equation
notice
αopt
ˆxk|zk−1
already
known
previous
recursion
step
denote
xk+1
xk+1
αopt
inequality
true
generically
ˆxk+1
concatenate
state
ˆxk
creating
set
state
sequences
ˆxk
k+1
every
ˆxk
previous
step
bundle
sets
obtaining
larger
set
state
sequences
ˆxk
k+1
next
step
consider
arbitrary
ˆxk
k+1
check
every
xk+2
xk+2
non-empty
set
cand
ˆxk
k+1⊕xk+2
n|zk−1
following
condition
satisﬁed
k+2
xk+2
αopt
αmax
ˆxk
k+1
xk+2|zk−1
ˆxk
k+1
xk+2|zk−1
calculated
easily
using
equation
since
αopt
ˆxk+1|zk−1
already
calculated
previous
step
denote
xk+2
xk+2
αopt
inequality
holds
generically
ˆxk+2
concatenate
ˆxk
k+1
creating
set
state
sequences
ˆxk
k+2
every
ˆxk
k+1
previous
step
bundle
sets
obtain
larger
set
state
sequences
ˆxk
k+2
clear
way
eventually
end
set
sequences
ˆxk
n−1
consider
arbitrary
ˆxk
n−1
check
every
non-empty
set
cand
ˆxk
n−1⊕xn
n|zk−1
following
condition
holds
αopt
ˆxk
n−1
xn|zk−1
denote
case
ˆxn
concatenate
ˆxk
n−1
creating
set
state
sequences
ˆxk
every
ˆxk
n−1
previous
step
bundle
sets
ﬁnally
obtain
set
state
sequences
ˆxk
subset
set
cand
n|zk−1
started
theorem
subset
cand
n|zk−1
obtained
using
optimal
tree
construction
equal
opt
n|zk−1
example
continue
example
following
optimal
tree
construction
start
checking
every
0,1
whether
αmax
xk|0
suppose
case
symbolise
giving
corresponding
nodes
representation
green
colour
leftmost
part
ﬁgure
follows
theorem
every
sequence
opt
n|0
either
start
since
set
ˆxk
0,1
example
course
trivial
set
ˆxk
would
would
obtained
non-trivial
result
every
sequence
opt
n|0
starts
represent
partial
information
set
opt
n|0
trivial
way
rightmost
part
ﬁgure
αopt
αmax
cand
n|0
opt
xk+1
ok+1
xk+1
next
step
need
check
criteria
every
ˆxk
found
previous
step
begin
ˆxk
start
looking
xk+1
set
cand
ˆxk⊕xk+1
n|0
cand00
n|0
simply
subset
sequences
cand
n|0
start
00.
tree
representation
cand
n|0
checking
whether
set
non-empty
comes
checking
node
ˆxk
daughter
value
since
estimating
state
sequences
imprecise
hidden
markov
models
k+1
αopt
k+1
αopt
ˆxk⊕xk+1|0
αopt
node
ˆxk
also
daughter
xk+1
αmax
00|0
indeed
case
need
check
whether
αmax
suppose
criterion
met
found
ﬁrst
subsequence
ˆxk
k+1
namely
00.
symbolise
ﬁgure
giving
child
xk+1
node
ˆxk
green
colour
01|0
daughter
gets
coloured
red
part
set
sequences
ˆxk
k+1
constructing
step
theorem
also
means
none
elements
opt
n|0
start
subsequence
01.
ˆxk
know
tree
representing
sequences
cand
n|0
start
complete
tree
explicitly
constructed
create
problem
since
need
tree
check
whether
cand1⊕xk+1
n|zk−1
non-empty
set
condition
trivially
met
xk+1
xk+1
completeness
set
cand1
n|zk−1
therefore
left
check
criterion
ˆxk
every
xk+1
0,1
xk+1
might
instance
ﬁnd
αmax
k+1
10|0
xk+1
might
ﬁnd
αmax
αopt
results
checks
summarised
leftmost
part
ﬁgure
corresponding
sequences
ˆxk
k+1
theorem
possible
starting
sequences
elements
opt
n|0
easily
stored
depicted
tree
representation
see
rightmost
part
following
ﬁgure
k+1
αopt
11|0
cand
n|0
opt
xk+1
n|0
ok+1
xk+1
keep
performing
steps
optimal
tree
construction
way
theorem
states
data
structure
built
checking
criteria
represents
set
opt
n|0
set
might
look
like
opt
n|0
figure
clarify
set
constructed
notice
indeed
never
explicitly
constructed
set
xk+1
tree
representation
since
every
time
reached
cid:7
red
node
descendants
node
constructed
jasper
bock
gert
cooman
cand
n|0
opt
xk+1
ok+1
xk+1
figure
clariﬁcation
construction
cand
n|0
6.4.
additional
comments
needed
order
produce
-functions
assessments
lower
upper
transition
emission
probabilities
|zk−1
|zk−1
|zk
|zk
zk−1
xk−1
ok.
conservative
coherent
models
·|xk−1
correspond
assessments
2-monotone
due
comonotone
additivity
implies
ˆxk
|zk−1
|zk−1
aqk
ˆxk
|zk−1
therefore
equation
leads
ˆxk
xk|zk−1
right-hand
side
smallest
possible
value
threshold
ˆxk
xk|zk−1
correspond-
ing
assessments
|zk−1
ˆxk
|zk−1
leading
conservative
inferences
therefore
largest
possible
sets
maximal
sequences
correspond
assessments
|zk−1
ˆxk
|zk−1
discussion
algorithm
complexity
7.1.
preparatory
calculations
begin
preparatory
calculations
quantit-
ies
equations
thresholds
ˆxk
xk|zk−1
equation
computa-
tional
complexity
clearly
cubic
number
states
linear
number
nodes
calculating
αmax
equations
linear
number
max
estimating
state
sequences
imprecise
hidden
markov
models
ˆxk|zk−1
nodes
quadratic
number
states
complexity
ﬁnding
αopt
equation
linear
number
nodes
cubic
number
states
7.2.
complexity
optimal
tree
construction
computational
complexity
optimal
tree
construction
less
trivial
let
start
noting
construction
essentially
consists
repeating
small
step
namely
adding
state
ˆxs
already
constructed
ˆxk
s−1.7
perform
step
sequence
ˆxk
s−1
ﬁrst
check
whether
cand
ˆxk
s−1⊕xs
n|zk−1
non-empty
done
constant
time
since
representation
reduces
step
checking
whether
node
daughter
ˆxs−1
data
structure
cand
ˆxk
s−1
n|zk−1
next
ˆxk
s−1
xs|zk−1
checking
indeed
case
need
check
αmax
two
criteria
every
called
performing
search
step
complexity
linear
number
states
meet
criteria
noted
ˆxs
concatenated
ˆxk
s−1
αopt
prove
performing
search
step
always
yield
least
one
ˆxs
concatenated
ˆxk
s−1
theorem
consider
arbitrary
sequence
ˆxk
s−1
created
performing
optimal
tree
construction
...
...
always
least
one
cand
ˆxk
s−1⊕xs
n|zk−1
non-empty
inequality
αmax
example
visual
representations
means
every
green
node
alway
least
one
green
child
implies
green
sequences
length
ˆxk
s−1
xs|zk−1
holds
αopt
situation
depicted
therefore
impossible
cid:7
next
notice
every
optimal
sequence
ˆxk
yielded
optimal
tree
construction
built
adding
extra
states
ˆxs
already
constructed
sequence
ˆxk
s−1
repeating
going
adding
state
means
performing
one
search
step
theorem
implies
performing
search
step
also
means
adding
least
one
state
therefore
constructing
one
maximal
sequence
ˆxk
never
take
search
steps
length
sequence
since
performing
one
search
step
linear
number
states
constructing
one
maximal
sequence
linear
length
sequence
number
states
determining
set
opt
n|zk−1
maximal
sequences
thus
linear
number
sequences
length
sequences
number
states
7.3.
recursive
construction
solutions
construct
opt
n|o1
recursively
let
run
ﬁxed
construct
set
opt
n|zk−1
every
zk−1
xk−1
means
optimal
tree
construction
already
shown
7if
identify
ˆxk
s−1
ˆxk
k−1
sequence
length
zero
jasper
bock
gert
cooman
constructing
set
linear
number
sequences
length
sequences
number
states
means
performing
recursive
construction
quadratic
length
sequences
quadratic
number
states
roughly
speaking8
linear
number
maximal
sequences
7.4.
general
complexity
complete
algorithm
consist
preparatory
calculations
recursive
construction
solutions
conclude
quadratic
number
nodes
cubic
number
states
roughly
speaking
linear
number
maximal
sequences
7.5.
comparison
viterbi
algorithm
precise
hmms
state
sequence
es-
timation
problem
solved
efﬁciently
viterbi
algorithm
whose
complexity
linear
number
nodes
quadratic
number
states
however
algorithm
emits
single
optimal
probable
state
sequence
even
cases
multiple
equally
probable
optimal
solutions
course
simpliﬁes
problem
would
content
giving
single
maximal
solution
ensuing
version
algorithm
would
complexity
similar
viterbi
allow
fair
comparison
viterbi
algorithm
would
need
alter
viterbi
algorithm
way
longer
resolves
ties
arbitrarily
emits
equally
probable
optimal
state
sequences
new
version
remain
linear
number
nodes
quadratic
number
states
also
added
complexity
easily
seen
noting
emitting
optimal
sequences
linear
number
thus
possibly
exponential
possible
solutions
would
example
equally
probable
complexity
time-consuming
part
algorithm
recursive
construction
solutions
difference
viterbi
approach
linear
quadratic
number
nodes
difference
come
ihmms
mutually
incomparable
solutions
whereas
phmms
optimal
solutions
indifferent
equally
probable
makes
sure
algorithm
phmms
requires
forward
loops
case
estihmm
algorithm
perform
optimal
tree
construction
believe
added
complexity
reasonable
price
pay
robustness
working
imprecise-probabilistic
models
offers
experiments
linear
complexity
number
maximal
sequences
probably
best
hope
also
see
able
ﬁnd
maximal
sequences
efﬁciently
provided
number
reasonably
small
say
tend
increase
exponentially
length
chain
algorithm
however
cleverly
designed
could
overcome
hurdle
number
maximal
sequences
important
study
behaviour
detail
order
take
closer
look
number
maximal
sequences
depends
transition
probabilities
model
evolves
let
imprecision
local
models
grow
shall
see
number
displays
interesting
behaviour
explained
even
predicted
extent
allow
easy
visualisation
limit
discussion
binary
ihmms
state
output
variables
assume
two
possible
values
say
8.1.
describing
binary
stationary
ihmm
ﬁrst
consider
binary
stationary
hmm
precise
transition
probabilities
going
one
state
next
completely
determined
numbers
unit
interval
probability
state
state
probability
state
state
pin
hmm
also
need
specify
marginal
probability
ﬁrst
state
two
emission
8for
every
constructing
set
opt
n|zk−1
linear
complexity
number
maximal
elements
stage
estimating
state
sequences
imprecise
hidden
markov
models
probabilities
probability
emitting
output
state
probability
emitting
output
state
binary
case
coherent
imprecise-probabilistic
models
found
con-
tamination
taking
convex
mixtures
precise
models
mixture
coefﬁcient
vacuous
model
mixture
coefﬁcient
leading
so-called
linear-vacuous
model
simplify
analysis
let
emission
model
remain
precise
use
mixture
coefﬁcient
marginal
transition
models
ranges
zero
one
evolve
precise
hmm
towards
ihmm
vacuous
marginal
transition
models
precise
emission
models
8.2.
explaining
basic
ideas
using
chain
length
two
examine
beha-
viour
ihmm
length
two
following
precise
probabilities
ﬁxed:9
0.1
0.8
0.3.
fixing
output
sequence
value
use
algorithm
calculate
corresponding
numbers
maximal
state
sequences
range
unit
interval
results
represented
conveniently
form
heat
plot
plots
correspond
output
sequence
o1:2
01.
number
maximal
state
sequences
clearly
de-
pends
transition
prob-
abilities
rather
large
parts
probab-
ility
space
coloured
white
get
single
maximal
sequence—as
would
hmms—
con-
tiguous
regions
see
higher
number
appear
present
example
binary
chain
length
two
highest
pos-
sible
number
maximal
se-
quences
course
four
dark
grey
area
three
maximal
sequences
two
light
grey
regions
plots
show
happens
let
increase
grey
areas
expand
number
maximal
sequences
increases
even
ﬁnd
small
area
coloured
black
four
possible
state
sequences
maximal
locally
due
relatively
high
imprecision
local
models
give
useful
robust
estimates
state
sequence
producing
output
sequence
o1:2
01.
small
areas
one
maximal
state
sequence
quite
small
seem
resemble
strips
narrow
lines
tends
zero
suggests
able
explain
least
qualitatively
areas
come
looking
compatible
precise
models
regions
ihmm
produces
different
maximal
mutually
incomparable
sequences
widened
versions
loci
indifference
precise
hmms
locus
indifference
mean
set
correspond
two
given
state
sequences
x1:2
ˆx1:2
equal
posterior
probability
x1:2|o1:2
ˆx1:2|o1:2
9this
choice
course
arbitrary
different
values
would
yield
comparable
results
jasper
bock
gert
cooman
provided
o1:2
x1:2
o1:2
ˆx1:2
o1:2
example
o1:2
ﬁnd
following
expressions
four
possible
state
sequences
00,01
01,01
10,01
11,01
equating
two
expressions
express
corresponding
two
state
sequences
equal
posterior
probability
since
resulting
equations
function
six
possible
combinations
deﬁnes
locus
indifference
depicted
lines
following
ﬁgure
parts
loci
depicted
blue
darker
bolder
monochrome
versions
pa-
per
demarcate
three
regions
state
sequences
optimal
highest
posterior
probability
happens
transition
models
become
impre-
cise
roughly
speaking
nearby
values
original
enter
picture
effectively
turning
loci
lines
in-
difference
bands
incom-
parability
emergence
re-
gions
two
max-
imal
sequences
seen
originate
loci
indif-
ference
compare
ﬁgure
loci
heat
plots
given
10−
00−11
8.3.
extending
argument
chain
length
three
chain
length
three
determine
loci
indifference
precise
models
completely
analogous
manner
use
marginal
model
emission
model
previous
example
resulting
lines
indifference
output
sequence
000
look
follows
estimating
state
sequences
imprecise
hidden
markov
models
compare
visualisation
num-
ber
maximal
elements
sequence
resemblance
quite
striking
example
black
areas
correspond
number
maximal
sequences
least
four
showing
algorithm
power
order
demonstrate
algorithm
indeed
quite
efﬁcient
let
determine
maximal
sequences
random
output
sequence
length
100.
consider
binary
stationary
hmm
presented
following
precise
marginal
emission
probabilities
0.1
0.98
0.01.
practical
applications
probability
output
variable
value
corresponding
hidden
state
variable
usually
quite
high
explains
chosen
close
respectively
contrast
previous
experiments
let
transition
probabilities
vary
following
values
0.6
0.5.
ihmm
use
determine
maximal
sequences
generated
mixing
precise
local
models
vacuous
one
using
mixture
coefﬁcient
marginal
transition
emission
models
figure
display
ﬁve
maximal
sequences
corresponding
highlighted
output
sequence
since
emission
probabilities
chosen
quite
accurate
surprise
output
sequence
one
maximal
sequences
addition
indicated
bold
face
state
values
differ
outputs
output
sequence
see
model
represents
indecision
values
state
variables
move
away
end
sequence
result
phenomenon
called
dilation
which—as
jasper
bock
gert
cooman
noted
another
paper
—tends
occur
inferences
credal
tree
proceed
leaves
towards
root
efﬁciency
algorithm
took
0.2
seconds
calculate
maximal
sequences.10
reason
could
done
fast
algorithm
linear
number
solutions
case
let
grow
example
number
maximal
sequences
output
sequence
764
determined
seconds
demonstrates
complexity
indeed
linear
number
solutions
algorithm
efﬁciently
calculate
maximal
sequences
even
long
output
sequences
10.
application
optical
character
recognition
ﬁrst
simple
toy
application
use
estihmm
al-
gorithm
try
detect
mistakes
words
written
word
re-
garded
hidden
sequence
generating
output
sequence
artiﬁcially
corrupting
word
way
simulate
observa-
tion
processes
perfectly
reliable
output
optical
character
recognition
ocr
device
leads
observed
output
sequences
may
contain
errors
try
detect
compare
results
viterbi
algorithm
show
algorithm
offers
robust
solution
10.1.
generating
hmm
local
uncertainty
model
must
identiﬁed
original
observed
letter
marginal
model
ﬁrst
letter
original
word
transition
model
·|xk−1
subsequent
letters
emission
model
·|xk
observed
letters
sake
simplicity
assume
stationarity
making
transition
emission
models
independent
identiﬁcation
local
models
ihmm
use
imprecise
dirichlet
model
idm
example
marginal
model
applying
idm
leads
following
simple
identiﬁc-
ation
∑z∈x
∑z∈x
counts
words
sample
text
ﬁrst
letter
positive
real
hyperparameter
expresses
degree
caution
inferences
example
let
transition
emission
models
proceed
similarly
counting
transitions
one
character
another
respectively
original
word
observation
process
way
obtain
lower
upper
transition
emission
probabilities
singletons
pointed
section
6.4
sufﬁce
run
algorithm
note
chosen
zero
local
models
would
become
precise
estihmm
algorithm
would
reduce
viterbi
algorithm
version
resolve
ties
arbitrarily
see
section
7.5
identiﬁcation
local
models
precise
hmm
use
similar
precise
dirichlet
model
approach
perks
prior
prior
strength
example
10running
python
programme
2012
macbookpro
figure
estimating
state
sequences
imprecise
hidden
markov
models
precise
marginal
model
leads
following
simple
identiﬁcation
|x|
number
states
s/|x|
∑z∈x
10.2.
results
let
ﬁrst
discuss
speciﬁc
example
difference
actual
results
obtained
using
viterbi
estihmm
algorithms
order
illustrate
important
advantage
latter
ocr
software
mistakenly
read
italian
word
quanto
ouanto
using
precise
model
viterbi
algorithm
correct
mistake
suggests
original
correct
word
duanto
estihmm
algorithm
hand
using
imprecise
model
returns
cuanto
duanto
fuanto
quanto
maximal
undominated
solutions
including
correct
one
course
would
still
pick
correct
solution
set
suggestions—for
example
using
dictionary
human
opinion—
using
estihmm
algorithm
managed
reduce
search
space
possible
ﬁve
letter
words
much
smaller
set
four
words
given
notice
solution
viterbi
algorithm
included
maximal
solutions
estihmm
returns
one
easily
prove
always
case
simulate
ocr
device
artiﬁcially
corrupted
ﬁrst
200
words
ﬁrst
canto
dante
divina
commedia
resulting
137
correctly
read
words
words
containing
errors
try
correct
errors
using
estihmm
viterbi
algorithm
compare
approaches
results
summarised
following
table
total
number
viterbi
correct
solution
wrong
solution
estihmm
correct
solution
included
correct
solution
included
total
number
200
100
correct
ocr
wrong
ocr
137
68.5
31.5
157
78.5
21.5
172
132
137
viterbi
algorithm
main
conclusion
applying
output
ocr
device
results
decreased
number
incorrect
words
number
correct
words
rises
68.5
78.5
however
viterbi
algorithm
also
introduces
new
errors
correctly
read
words
estihmm
algorithm
manages
suggest
original
correct
word
one
solutions
cases
assuming
able
detect
correct
word
percentage
correct
words
rises
68.5
applying
estihmm
algorithm
thereby
outperforming
viterbi
algorithm
almost
secondly
also
notice
estihmm
algorithm
never
introduced
new
errors
words
already
correct
course
since
estihmm
algorithm
allows
multiple
solutions
instead
single
one
surprise
manage
increase
amount
times
suggest
correct
solution
would
happen
even
added
random
extra
solutions
solution
viterbi
algorithm
giving
extra
solutions
seen
improvement
done
smartly
investigate
distinguish
cases
estihmm
algorithm
returns
single
solution
returns
multiple
solutions
look
viterbi
estihmm
algorithms
compare
two
cases
estihmm
algorithm
returned
single
solution
155
200
words
already
mentioned
single
solution
always
coincide
one
given
viterbi
algorithm
results
estihmm
viterbi
algorithms
summarised
following
table
jasper
bock
gert
cooman
estihmm
single
solutions
total
number
single
correct
solution
single
wrong
solution
total
number
155
100
134
86.5
13.5
correct
ocr
wrong
ocr
129
83.2
129
16.8
percentage
words
correctly
read
ocr
software
83.2
instead
global
68.5
result
estihmm
algorithm
single
solution
serves
indication
word
trying
correct
fairly
high
probability
already
correct
also
see
eventual
percentage
correct
words
86.5
slight
improvement
83.2
already
correct
applying
algorithms
next
look
remaining
words
estihmm
algorithm
returns
one
maximal
element
case
see
signiﬁcant
difference
results
viterbi
estihmm
algorithm
since
viterbi
algorithm
never
returned
one
solution.11
results
algorithms
listed
following
table
total
number
100
correct
ocr
wrong
ocr
17.8
82.2
84.4
15.6
total
number
estihmm
multiple
solutions
correct
solution
included
correct
solution
included
viterbi
correct
solution
wrong
solution
ﬁrst
important
conclusion
drawn
table
estihmm
indecisive
serves
rather
strong
indication
word
applying
algorithm
indeed
contain
errors
estihmm
algorithm
returns
multiple
solutions
original
word
incorrectly
read
ocr
software
82.2
cases
51.1
48.9
second
conclusion
related
ﬁrst
estihmm
indecisive
also
serves
indication
result
returned
viterbi
algorithm
less
reliable
percentage
correct
words
applying
viterbi
algorithm
dropped
51.1
contrast
global
percentage
78.5
estihmm
algorithm
however
still
gives
correct
word
one
solutions
84.4
cases
almost
high
global
percentage
set
given
estihmm
algorithm
contains
correct
solution
viterbi
algorithm
manages
pick
correct
solution
set
60.5
cases
see
estihmm
algorithm
seems
notice
dealing
difﬁcult
words
therefore
gives
multiple
solutions
decide
conclude
experiment
estihmm
usefully
applied
make
results
viterbi
algorithm
robust
gain
appreciation
likely
wrong
estihmm
algorithm
returns
multiple
solutions
serves
indication
robustness
issues
would
occur
solved
problem
viterbi
algorithm
case
estihmm
returns
multiple
solutions
decide
whereas
viterbi
algorithm
pick
one
set
fairly
arbitrary
way—depending
choice
prior—
thereby
increasing
amount
errors
made
advantage
method
detects
robustness
issues
leaving
option
solving
different
ways
ﬁrst
method
would
pick
correct
word
set
possible
solutions
non-arbitrary
way
current
application
could
done
using
dictionary
human
expert
another
method
dealing
robustness
issues
would
conclude
need
data
order
build
better
model
less
sensitive
choice
prior
applying
estihmm
algorithm
11in
theory
viterbi
algorithm
return
multiple
indifferent
solutions
practice
almost
never
estimating
state
sequences
imprecise
hidden
markov
models
using
new
model
could
check
whether
robustness
issues
satisfactorily
dealt
11.
conclusions
interpreting
graphical
structure
imprecise
hidden
markov
model
credal
network
epistemic
irrelevance
leads
efﬁcient
algorithm
ﬁnding
maximal
undominated
state
sequences
given
output
sequence
preliminary
simulations
show
even
transition
models
non-negligible
imprecision
number
maximal
elements
seems
reasonably
low
fairly
large
regions
parameter
space
high
numbers
maximal
elements
concentrated
fairly
small
regions
remains
seen
whether
observation
corroborated
deeper
theoretical
analysis
ﬁrst
simple
toy
application
clearly
shows
estihmm
algorithm
able
robustify
results
viterbi
algorithm
reduce
amount
wrong
conclusions
giving
extra
possible
solutions
intelligent
manner
adds
extra
solutions
speciﬁc
cases
viterbi
algorithm
robustness
issues
thereby
also
serving
indicator
reliability
result
given
viterbi
algorithm
interesting
avenue
research
would
compare
estihmm
algorithm
methods
also
try
robustify
viterbi
algorithm
although
methods
start
precise
model
introduce
safety
rather
imprecision
example
trying
ﬁnd
probable
solutions
practical
applications
similar
comparison
results
could
therefore
prove
interesting
leave
topic
future
research
clear
point
whether
ideas
similar
ones
discussed
could
used
derive
similarly
efﬁcient
algorithms
imprecise
hidden
markov
models
whose
graphical
structure
interpreted
credal
network
strong
independence
could
interesting
relevant
stringent
independence
condition
leads
joint
models
less
imprecise
therefore
produce
fewer
maximal
state
sequences
although
contained
solutions
acknowledgements
jasper
bock
ph.d.
fellow
research
foundation
flanders
fwo
ghent
university
developed
algorithm
described
context
master
thesis
close
cooperation
gert
cooman
acted
thesis
supervisor
present
article
describes
main
results
master
thesis
research
cooman
supported
sbo
project
060043
iwt-
vlaanderen
paper
beneﬁtted
discussions
marco
zaffalon
alessandro
antonucci
alessio
benavoli
cassio
campos
erik
quaeghebeur
filip
hermans
grateful
marco
zaffalon
providing
travel
funds
allowing
visit
idsia
discuss
practical
applications
references
richard
bellman
dynamic
programming
princeton
university
press
princeton
1957
fabio
cozman
credal
networks
artiﬁcial
intelligence
120:199–233
2000
campos
huete
moral
probability
intervals
tool
uncertain
reasoning
international
journal
uncertainty
fuzziness
knowledge-based
systems
2:167–196
1994
gert
cooman
filip
hermans
alessandro
antonucci
marco
zaffalon
epistemic
irrelevance
credal
nets
case
imprecise
markov
trees
international
journal
approximate
reasoning
51:1029–1052
2010
gert
cooman
enrique
miranda
marco
zaffalon
independent
natural
extension
artiﬁcial
intelligence
2010.
accepted
publication
gert
cooman
matthias
troffaes
dynamic
programming
deterministic
discrete-time
systems
uncertain
gain
international
journal
approximate
reasoning
39:257–278
2005
gert
cooman
matthias
troffaes
enrique
miranda
n-monotone
exact
functionals
journal
mathematical
analysis
applications
347:143–156
2008
jasper
bock
gert
cooman
nathan
huntley
matthias
troffaes
normal
form
backward
induction
decision
trees
coherent
lower
previsions
annals
operations
research
2010.
submitted
publication
enrique
miranda
survey
theory
coherent
lower
previsions
international
journal
approximate
reasoning
:628–658
january
2008
enrique
miranda
updating
coherent
lower
previsions
ﬁnite
spaces
fuzzy
sets
systems
160
:1286–
1307
january
2009
enrique
miranda
gert
cooman
marginal
extension
theory
coherent
lower
previsions
international
journal
approximate
reasoning
:188–225
september
2007
lawrence
rabiner
tutorial
hmm
selected
applications
speech
recognition
proceedings
ieee
:257–286
february
1989
matthias
troffaes
decision
making
uncertainty
using
imprecise
probabilities
international
journal
approximate
reasoning
:17–29
january
2007
andrew
viterbi
error
bounds
convolutional
codes
asymptotically
optimum
decoding
algorithm
ieee
transactions
information
theory
:260–269
1967
peter
walley
statistical
reasoning
imprecise
probabilities
chapman
hall
london
1991
peter
walley
inferences
multinomial
data
learning
bag
marbles
journal
royal
statistical
society
series
58:3–57
1996.
discussion
appendix
proofs
main
results
appendix
justify
formulas
give
proofs
proposition
theorems
2–5
frequently
use
terms
positive
negative
decreasing
increasing
therefore
start
clarifying
mean
say
positive
negative
non-negative
non-positive
call
real-valued
function
deﬁned
increasing
decreasing
iii
non-decreasing
non-increasing
proof
equation
zk−1
xk−1
infer
equation
|zk−1
|xk
|zk−1
cid:18
cid:19
cid:12
cid:12
cid:12
zk−1
xk∈xk
zk+1
|xk
zk+1
|zk
|zk−1
since
zk+1
|zk
see
transforms
|zk−1
zk+1
|zk
reformulated
|zk−1
|zk
pk+1
zk+1
ok+1
|zk
|zk−1
|zk
pk+1
zk+1
ok+1
|zk
take
account
equation
since
pk+1
zk+1
ok+1
|zk
repeating
steps
eventually
yields
equation
|zi−1
|zi
|zk−1
i=k
last
step
used
equality
|zn
|zn
cid:3
follows
equation
estimating
state
sequences
imprecise
hidden
markov
models
proof
equation
zk−1
xk−1
infer
conjugacy
equation
|zk−1
−pk
|zk−1
cid:18
−qk
|xk
|zk−1
−qk
−qk
zk+1
|zk
|zk−1
−qk
−ek
zk+1
|zk
|zk−1
zk+1
|xk
xk∈xk
cid:19
cid:12
cid:12
cid:12
zk−1
since
−ek
zk+1
|zk
zk+1
|zk
conjugacy
lemma
see
equation
transform
cid:0
zk+1
|zk
cid:1
|zk−1
−qk
|zk−1
zk+1
|zk
reformulated
−qk
|zk−1
|zk
pk+1
zk+1
ok+1
|zk
|zk−1
|zk
pk+1
zk+1
ok+1
|zk
|zk−1
|zk
pk+1
zk+1
ok+1
|zk
using
conjugacy
equation
since
pk+1
zk+1
ok+1
|zk
last
inequality
true
know
pk+1
zk+1
ok+1
|zk
−pk+1
zk+1
ok+1
|zk
conjugacy
pk+1
zk+1
ok+1
|zk
lemma
repeating
steps
eventually
yields
equation
|zk−1
|zi−1
|zi
i=k
last
step
used
equality
|zn
|zn
cid:3
follows
equation
conjugacy
lemma
consider
coherent
lower
prevision
min
max
proof
prove
inequalities
min
max
one
one
ﬁrst
one
follows
since
know
implies
using
conjugacy
last
equality
gamble
yields
min−
implies
max
−min−
cid:3
conclude
follows
applying
inequalities
cid:18
proof
proposition
observe
|xk−1
cid:19
cid:12
cid:12
cid:12
xk−1
ﬁrst
inequality
lemma
second
one
positivity
assumption
equation
element
equality
follows
∑zk
n∈xk
n∈xk
way
easily
prove
|xk
cid:18
zk+1
n∈xk+1
cid:19
cid:12
cid:12
cid:12
zk+1
k+1
cid:19
cid:12
cid:12
cid:12
xk−1
cid:19
cid:12
cid:12
cid:12
cid:18
cid:18
jasper
bock
gert
cooman
time
used
positivity
assumption
equation
last
inequal-
cid:3
ity
proof
theorem
consider
function
deﬁned
ˆx1
real
follows
equation
ˆx1
|o1
rightmost
zero
also
know
ˆx1
non-increasing
continuous
lemma
least
one
zero
lemma
hence
least
one
positive
zero
ˆx1
|o1
negative
zeroes
ˆx1
|o1
hence
proving
theorem
comes
proving
implies
since
turn
implies
ˆx1
|o1
prove
implication
consider
two
different
cases
case
ˆx1
real
ˆx1
cid:19
−εi
|z1
ˆx1
|x1
cid:18
ˆx2
|x1
cid:54
=x1
coefﬁcients
−εi
|z1
written
−εe1
|z1
conjugacy
makes
negative
decreasing
functions
since
|z1
positivity
assumption
proposition
coefﬁcient
ˆx2
|x1
consider
two
possible
cases
ˆx2
|x1
know
ˆx2
|x1
decreasing
function
lemma
therefore
argument
equation
decreases
pointwise
lemma
implies
decreasing
function
therefore
hand
ˆx2
|x1
know
lemma
cid:19
ˆx2
|x1
implying
−εi
|z1
cid:18
cid:0
z1∗
−εi
|z1∗
cid:1
−εe1
|z1∗
z1∗
expression
z1∗
arbitrary
cid:54
ﬁrst
two
inequalities
due
lemma
conjugacy
yield
equality
last
inequality
consequence
positivity
assumption
proposition
also
case
therefore
ﬁnd
cid:54
=x1
case
cid:54
ˆx1
real
ˆx1
ˆx1
|x1
cid:18
cid:19
|x1
ˆx1
ˆx2
ˆx1
−εi
|z1
cid:54
=x1
ˆx1
proof
case
ˆx1
already
shown
coefﬁcients
−εi
|z1
negative
decreasing
functions
together
lemma
implies
ˆx2
ˆx1
−εi
ˆx1
turn
lemma
vii
implies
ˆx2
ˆx1
decreasing
function
left
consider
coefﬁcient
|x1
two
possibilities
|x1
lemma
implies
|x1
decreasing
function
therefore
argument
equation
decreases
estimating
state
sequences
imprecise
hidden
markov
models
pointwise
lemma
implies
decreasing
function
therefore
hand
|x1
know
|x1
lemma
implying
ˆx1
ˆx2
ˆx1
ˆx1
−εi
ˆx1
−εe1
ˆx1
ˆx1
ﬁrst
two
inequalities
follow
lemma
conjugacy
yield
equality
last
inequality
consequence
positivity
assumption
proposition
cid:3
also
case
ﬁnd
lemma
let
coherent
lower
prevision
consider
real-valued
map
deﬁned
real
following
statements
hold
non-increasing
concave
continuous
least
one
zero
iii
decreasing
unique
zero
identically
zero
zero
f|y
negative
decreasing
f|y
decreasing
unique
zero
vii
negative
interval
also
decreasing
proof
start
proving
follows
directly
lemma
non-increasing
consider
concave
inequality
follows
subsequent
step
due
prove
continuous
consider
see
cid:12
inequality
follows
last
equality
due
conjugacy
hence
|µ2
µ1|p
proves
lipschitz
continuous
therefore
also
continuous
prove
notice
min
min
min
min
max
max
max
max
inequalities
consequence
lemma
last
equalities
follow
lemma
since
continuous
implies
existence
zero
min
max
property
iii
proved
considering
see
decreasing
since
jasper
bock
gert
cooman
ﬁrst
inequality
follows
last
equality
know
least
one
zero
must
unique
decreasing
prove
ﬁrst
note
also
implies
lemma
choose
min
min
max
max
time
using
lemma
conjugacy
conclude
proof
starts
noticing
f|y
f|y
due
deﬁnition
f|y
see
equation
fact
non-increasing
proof
already
shown
non-positive
allows
conclude
f|y
left
prove
decreasing
interval
f|y
contradiction
suppose
decreasing
interval
interval
since
zero
f|y
also
choose
existence
contradicts
concavity
established
prove
observe
lemma
implies
three
cases
considered
iii
exhaustive
mutually
exclusive
case
considered
iii
implies
decreasing
unique
zero
remains
prove
vii
repeating
argument
proof
see
negative
interval
cases
considered
iii
obtain
iii
decreasing
entire
domain
deﬁnitely
decreasing
cid:3
lemma
consider
coherent
lower
prevision
two
gambles
proof
start
pointwise
positive
min
therefore
min
using
ﬁrst
inequality
follows
therefore
whence
indeed
proof
analogous
min
implying
min
cid:3
proof
equation
let
ˆxk
ˆxk
since
ˆxk
implies
ˆxk
ˆxk
ok+1
xk+1
ˆxk+1
xk+1
ˆxk+1
turn
implies
ˆxk
|zk−1
xk+1
ˆxk+1
|xk
|zk−1
xk+1
ˆxk+1
|xk
|zk−1
|zk−1
cid:12
xk+1
ˆxk+1
|xk
|zk−1
|xk
cid:12
pk+1
xk+1
ˆxk+1
|xk
proving
equation
ﬁrst
equality
follows
equation
second
equality
holds
cid:54
implying
xk+1
ˆxk+1
|xk
estimating
state
sequences
imprecise
hidden
markov
models
xk+1
ˆxk+1
|xk
third
equality
follows
conjugacy
cid:3
last
one
follows
equation
proof
equation
since
ˆxn
lemma
yields
ˆxn
|zn−1
|zn−1
0|zn−1
cid:3
proof
equation
ˆxk
cid:54
ˆxk
|zk−1
ˆxk
|xk
|zk−1
xk+1
|xk
ˆxk
ˆxk+1
ˆxk
|zk−1
xk+1
|xk
ˆxk
ˆxk+1
ˆxk
|zk−1
ˆxk
ˆxk
|zk−1
proving
equation
reasons
equalities
hold
analogous
ones
cid:3
given
proof
equation
proof
theorem
fix
zk−1
xk−1
ˆxk
assume
ˆxk+1
opt
xk+1
ˆxk
ok+1
show
ˆxk
opt
n|zk−1
follows
assumption
pk+1
ok+1
xk+1
ˆxk+1
ˆxk
xk+1
xk+1
preﬁx
state
sequence
xk+1
state
ˆxk
form
state
sequence
implying
ˆxk
infer
equation
ˆxk
|zk−1
ˆxk
|zk−1
ˆxk
pk+1
ok+1
xk+1
ˆxk+1
ˆxk
tells
indeed
ˆxk
opt
n|zk−1
cid:3
proof
equations
first
consider
every
zn−1
xn−1
determine
opt
xn|zn−1
set
elements
ˆxn
ˆxn
ˆxn
|zn−1
∀xn
ˆxn
max
condition
equivalent
optimality
condition
taking
account
equations
show
condition
also
equivalent
∀xn
ˆxn
ˆxn
max
see
consider
two
different
cases
max
ˆxn
xn|zn−1
inequalities
max
trivially
satisﬁed
since
ˆxn
ˆxn
positivity
assumption
max
inequalities
equivalent
equation
ˆxn
ˆxn
|zn−1
ˆxn
max
ˆxn
xn|zn−1
max
ˆxn
ˆxn
|zn−1
cid:19
cid:12
cid:12
cid:12
zn−1
cid:18
ˆxn
ˆxn
ˆxn
max
max
ˆxn
max
ˆxn
xn|zn−1
ˆxn
xn|zn−1
ˆxn|zn−1
using
equation
equation
reformulated
ˆxn
αopt
completes
proof
equivalence
next
consider
fix
zk−1
xk−1
must
determine
opt
n|zk−1
know
principle
optimality
limit
candidate
optimal
sequences
ˆxk
set
cand
n|zk−1
consider
ˆxk
must
check
whether
ˆxk
|zk−1
see
equation
jasper
bock
gert
cooman
ˆxk
inequality
automatically
satisﬁed
indeed
ˆxk
posk
zk−1
infer
equation
ˆxk
|zk−1
ˆxk
equation
tells
ˆxk
|zk−1
ˆxk
posk
zk−1
know
equation
ˆxk+1
opt
xk+1
ˆxk
ok+1
implies
pk+1
ok+1
xk+1
ˆxk+1
ˆxk
hence
ˆxk
|zk−1
equation
means
limit
checking
inequality
cid:54
ˆxk
cid:54
ˆxk
must
check
whether
∀xk+1
xk+1
ˆxk
ˆxk
|zk−1
see
equation
equation
lemma
equivalent
max
ˆxk
ˆxk
|zk−1
ˆxn
αopt
ˆxk
n|zk−1
seen
equivalent
ˆxk
αopt
turn
seen
equivalent
ˆxk
max
ˆxk
xk|zk−1
using
course
reasoning
completely
analogous
one
used
case
since
inequality
must
hold
every
cid:54
ˆxk
infer
equation
must
ˆxk
αopt
ˆxk|zk−1
must
check
condition
candidate
sequences
ˆxk
cand
n|zk−1
proves
equation
cid:3
proof
theorem
proof
consists
two
parts
ﬁrst
prove
every
sequence
ˆxk
obtained
optimal
tree
construction
element
opt
n|zk−1
secondly
prove
sequence
part
set
sequences
obtained
optimal
tree
construction
element
opt
n|zk−1
let
start
proving
every
sequence
ˆxk
obtained
optimal
tree
construction
element
opt
n|zk−1
follows
last
step
optimal
tree
construction
every
ˆxk
constructed
set
element
cand
ˆxk
n|zk−1
therefore
equation
also
cand
n|zk−1
last
step
also
implies
ˆxk|zk−1
αmax
equation
repeated
use
equations
follows
equation
ˆxk
element
opt
n|zk−1
conclude
show
sequence
part
set
sequences
obtained
optimal
tree
construction
element
opt
n|zk−1
sequence
part
set
sequences
obtained
optimal
tree
construction
either
implies
element
cand
n|zk−1
s|zk−1
ﬁrst
case
follows
directly
equation
αmax
element
opt
n|zk−1
second
case
see
zk|zk−1
seen
αmax
zk|zk−1
repeated
use
equations
equivalent
αopt
follows
equation
element
opt
n|zk−1
cid:3
proof
theorem
proved
contradiction
conditions
would
fulﬁlled
optimal
tree
construction
would
stop
set
opt
n|zk−1
would
empty
contradiction
since
every
ﬁnite
partially
ordered
set
least
one
maximal
element
consider
equation
implies
least
one
αs−1
ˆxs−1⊕
sequence
αmax
s−1
ˆxs−1
prove
ﬁrst
state
sequence
meets
criteria
theorem
cand
ˆxk
s−1
n|zk−1
non-empty
set
αmax
inequality
αs−1
ˆxs−1
ˆxk
s−1⊕x∗
equivalent
αmax
meaning
know
ˆxk
s−1
found
using
optimal
tree
construction
implies
s−1
ˆxs−1
αopt
ˆxk
s−1|zk−1
follows
αopt
ˆxk
s−1|zk−1
seen
s|zk−1
equations
since
know
s|zk−1
αopt
s|zk−1
implies
αopt
αopt
equation
ﬁnd
αmax
ˆxk
s−1
satisﬁes
ﬁrst
criterium
αopt
αopt
estimating
state
sequences
imprecise
hidden
markov
models
prove
state
αmax
αmax
follows
lemma
αs−1
ˆxs−1
element
cand
n|zk−1
s−1
ˆxs−1
lemma
implies
ˆxk
s−1
also
satisﬁes
second
criterium
means
n|zk−1
non-empty
sufﬁces
equation
prove
set
cand
ˆxk
s−1⊕x∗
ˆxk
s−1
since
cand
ˆxk
s−1
n|zk−1
non-empty
least
one
ˆxk
s−1
element
cand
n|zk−1
furthermore
chosen
αs−1
ˆxs−1
element
cand
n|zk−1
cid:3
lemma
fix
zk−1
xk−1
ˆxk
s−1
s−1
choose
arbitrary
s−1
ˆxs−1
ˆxk
s−1⊕zs
belongs
cand
n|zk−1
ˆxk
s−1⊕x∗
belongs
cand
n|zk−1
proof
simplify
notations
proof
convenient
use
ˆxk−1
alternative
notation
zk−1
ˆxk−1
zk−1
opt
ˆxs−1
together
equation
implies
ˆxs−1
cand
xs−1
ˆxs−2
os−1
concludes
proof
consider
check
ˆxq
posq
ˆxq−1
see
deﬁnition
exists
denote
lowest
s−2
ˆxq∗
s−1⊕zs
case
equation
know
ˆxq∗
s−1⊕x∗
elements
cand
cid:0
xq∗
ˆxq∗−1
oq∗
belong
cand
cid:0
xq∗
ˆxq∗−1
oq∗
cand
cid:0
xq∗
ˆxq∗−1
oq∗
equations
also
know
ˆxq∗
s−1
cand
cid:0
xq∗
ˆxq∗−1
oq∗
cid:1
holds
ˆxq
posq
ˆxq−1
q∗−
cid:1
exists
ˆxq
posq
ˆxq−1
choose
follows
repeated
use
equations
ˆxs−1
belongs
cand
xs−1
ˆxs−2
os−1
already
know
ˆxs−1⊕x∗
cand
xs−1
ˆxs−2
os−1
ˆxq∗
s−1⊕zs
belong
s−1
ˆxq∗
s−1⊕x∗
cid:1
since
sequences
set
ˆxq∗
xq∗+1
s−1
notice
cand
n|zk−1
built
repeatedly
using
concludes
proof
cid:1
given
ˆxk
s−1
belongs
cand
n|zk−1
implies
ˆxq|
ˆxq−1
s−1
ˆxs−1
αs−1
ˆxs−1
ˆxq
s−1
αopt
αmax
ˆxt
s−1
furthermore
αs−1
ˆxs−1
equation
equation
tells
ˆxt
s−1
αs−1
ˆxs−1
know
αopt
ˆxq
s−1
ˆxq|
ˆxq−1
element
cand
cid:0
xq∗
ˆxq∗−1
oq∗
implies
since
cand
n|zk−1
built
repeatedly
using
equations
ˆxq∗
s−1
sequence
ˆxk
s−1
cid:3
αs−1
ˆxs−1
lemma
10.
consider
ˆxs−1
xs−1
αmax
proof
αs−1
ˆxs−1
belongs
cand
n|zk−1
concludes
proof
opt
ˆxs−1
cid:1
s−1
ˆxs−1
αmax
s−1
ˆxs−1
know
equation
αs−1
ˆxs−1
αs−1
ˆxs−1
therefore
equations
ss−1
os−1
ˆxs−1
ˆxs−1
ss−1
os−1
ˆxs−1
ˆxs−1
together
positivity
assumption
implies
ˆxs−1
ˆxs−1
jasper
bock
gert
cooman
ˆxs−1
ˆxs−1
ˆxs−1
conjugacy
implies
using
equation
see
concludes
proof
since
ˆxs−1
ˆxs−1
opt
ˆxs−1
equation
ˆxs
cid:3
|xs−1
