coco
platform
comparing
continuous
optimizers
black-box
setting
nikolaus
hansen1,2
anne
auger1,2
olaf
mersmann3
tea
tu≈°ar4
dimo
brockhoff4
1inria
research
centre
saclay
france
2universit√©
paris-saclay
lri
france
3tu
dortmund
university
chair
computational
statistics
germany
4inria
research
centre
lille
france
abstract
coco
platform
comparing
continuous
optimizers
black-box
setting
aims
automatizing
tedious
repetitive
task
benchmarking
numerical
optimization
al-
gorithms
greatest
possible
extent
present
rationals
behind
development
platform
general
proposition
guideline
towards
better
benchmarking
detail
underlying
fundamental
concepts
coco
deÔ¨Ånition
problem
idea
instances
relevance
target
values
runtime
central
performance
measure
finally
give
quick
overview
basic
code
structure
currently
available
test
suites
contents
introduction
1.1
coco
1.2
terminology
functions
instances
problems
runtime
target
values
3.1
restarts
simulated
restarts
3.2
aggregation
general
code
structure
test
suites
introduction
consider
continuous
black-box
optimization
search
problem
minimize
constraints
speciÔ¨Åcally
aim
Ô¨Ånd
quickly
possible
one
several
solutions
search
space
small
value
satisfy
constraints
generally
consider
time
number
calls
function
continuous
optimization
algorithm
also
known
solver
addresses
problem
assume
known
prior
knowledge
available
algorithm
considered
black-box
algorithm
query
solutions
get
respective
values
prerequisits
benchmarking
optimization
algorithms
seems
rather
simple
straightforward
task
run
algorithm
collection
problems
display
results
however
closer
inspection
benchmarking
turns
surprisingly
tedious
appears
difÔ¨Åcult
get
results
meaningfully
interpreted
beyond
standard
claim
one
algorithm
better
another
problems.1
offer
conceptual
guideline
benchmarking
continuous
optimization
algorithms
tries
address
challenge
implemented
within
coco
framework.2
coco
framework
provides
practical
means
automatized
benchmarking
procedure
installing
coco
shell
benchmarking
optimization
algorithm
say
function
fmin
scipy.optimize
python
becomes
simple3
shown
Ô¨Ågure
coco
framework
provides
interface
several
languages
benchmarked
optimizer
written
cur-
rently
c/c++
java
matlab/octave
python
several
benchmark
suites
testbeds
currently
written
data
logging
facilities
via
observer
data
post-processing
python
data
browsing
html
article
latex
templates
one
common
major
Ô¨Çaw
get
indication
much
better
algorithm
results
benchmarking
often
provide
indication
relevance
main
output
often
hundreds
tabulated
numbers
inter-
pretable
ordinal
ranking
scale
addressing
point
common
confusion
statistical
signiÔ¨Åcance
secondary
means
sufÔ¨Åcient
condition
relevance
confer
code
basis
github
api
documentation
implementation
details
see
also
example_experiment.py
runs
out-of-the-box
benchmarking
python
script
get
install
code
git
clone
https
//github.com/numbbo/coco.git
coco
python
do.py
run-python
python
do.py
install-postprocessing
install
python
experimental
module
cocoex
install
post-processing
get
coco
using
git
optional
run
example
shell
code-experiments/build/python/example_experiment.py
python
example_experiment.py
python
bbob_pproc
exdata/
...
open
ppdata/index.html
run
current
default
experiment
run
post-processing
browse
results
/usr/bin/env
python
python
script
benchmark
fmin
scipy.optimize
numpy.random
import
rand
import
cocoex
try
import
cocopp
except
importerror
import
bbob_pproc
cocopp
old
name
scipy.optimize
import
fmin
new
future
name
suite
cocoex.suite
bbob
year
2016
budget_multiply
1e4
use
1e1
even
quick
first
test
run
observer
cocoex.observer
bbob
result_folder
myoptimizer-on-bbob
suite
loop
problems
observer.observe
prepare
logging
necessary
data
fmin
p.initial_solution
disp=false
would
silence
fmin
output
p.final_target_hit
apply
restarts
desired
p.evaluations
p.dimension
budget_multiplier
fmin
p.lower_bounds
rand
p.dimension
rand
p.dimension
p.upper_bounds
p.lower_bounds
cocopp.main
'exdata/myoptimizer-on-bbob
invoke
data
post-processing
fig
shell
code
installation
coco
python
code
benchmark
fmin
bbob
suite
python
script
executed
Ô¨Åle
ppdata/index.html
used
browse
resulting
data
underlying
philosophy
coco
provide
everything
experimenters
need
setup
implement
want
benchmark
given
algorithm
implementation
properly
desired
side
effect
reusing
framework
data
collected
years
even
decades
effortlessly
compared.4
far
framework
successfully
used
benchmark
far
hundred
different
algorithms
dozens
researchers
example
see
access
data
submitted
bbob
2009
gecco
workshop
1.1
coco
appart
diminishing
time
burden
pitfalls
bugs
omissions
repetitive
coding
task
experimenters
aim
provide
conceptual
guideline
better
benchmarking
setup
guideline
following
deÔ¨Åning
features
benchmark
functions
used
black
boxes
algorithm
however
explicitly
known
scien-
tiÔ¨Åc
community
designed
comprehensible
allow
meaningful
interpretation
performance
results
difÔ¨Åcult
defeat
artiÔ¨Åcial
regularities
easily
intentionally
unintentionally
exploited
algorithm.5
scalable
input
dimension
whi1996
predeÔ¨Åned
budget
number
ùëì-evaluations
running
experiment
experimental
procedure
budget-free
han2016ex
single
performance
measure
used
thereafter
aggregated
displayed
several
ways
namely
runtime
measured
number
ùëì-evaluations
han2016perf
runtime
measure
advantages
independent
computational
platform
language
compiler
coding
styles
speciÔ¨Åc
experimental
conditions6
independent
measurement
speciÔ¨Åc
function
ob-
tained
relevant
meaningful
easily
interpretable
without
expert
domain
knowledge
quantitative
ratio
scale7
ste1946
assume
wide
range
values
aggregate
collection
values
meaningful
way8
missing
runtime
value
considered
possible
outcome
see
example
optimum
all-zeros
optima
placed
regular
grid
functions
separable
whi1996
objective
remain
comprehensible
makes
challenging
design
non-regular
functions
regularities
common
place
real-world
optimization
problems
remains
open
question
runtimes
measured
ùëì-evaluations
widely
comparable
designed
stay
experimental
procedure
han2016ex
includes
however
timing
experiment
records
internal
computational
effort
algorithm
cpu
wall
clock
time
opposed
ranking
algorithms
based
solution
quality
achieved
given
budget
caveat
arithmetic
average
dominated
large
values
compromise
informative
value
display
comprehensible
intuitive
informative
possible
believe
details
matter
aggregation
dimension
avoided
dimension
param-
eter
known
advance
used
algorithm
design
decisions
possible
without
signiÔ¨Åcant
drawbacks
functions
scalable
dimension
believe
however
process
algorithm
design
benchmarking
framework
like
coco
limitations
design
phase
usually
fewer
benchmark
functions
used
functions
measuring
tools
tailored
given
algorithm
design
question
overall
procedure
usually
rather
informal
interactive
rapid
iterations
benchmarking
framework
serves
conduct
formalized
validation
experiment
design
outcome
used
regression
testing
1.2
terminology
specify
terms
used
later
function
talk
objective
function
parametrized
mapping
scalable
input
space
usually
functions
parametrized
different
instances
function
available
e.g
translated
shifted
versions
problem
talk
problem
coco_problem_t
speciÔ¨Åc
function
instance
optimization
algorithm
run
problem
evaluated
returns
ùëì-value
-vector
case
ùëî-vector
context
performance
assessment
target
indicator-value
added
deÔ¨Åne
problem
problem
considered
solved
given
difÔ¨Åcult
available
target
obtained
runtime
deÔ¨Åne
runtime
run-length
hoo1998
number
evaluations
conducted
given
problem
prescribed
target
value
hit
also
referred
number
function
evaluations
ùëì-evaluations
runtime
central
performance
measure
suite
test-
benchmark-suite
collection
problems
typically
twenty
hun-
dred
number
objectives
Ô¨Åxed
functions
instances
problems
coco
framework
consider
functions
suite
distinguished
identi-
Ô¨Åer
functions
parametrized
input
dimension
in-
stance
number
think
index
continuous
parameter
vector
setting
parametrizes
among
others
things
translations
rotations
practice
discrete
identi-
Ô¨Åer
single
instantiations
parameters
given
varying
leads
variation
function
given
suite
fixing
function
deÔ¨Ånes
optimization
problem
presented
optimization
algorithm
problem
receives
index
suite
mapping
triple
single
number
formalization
suggests
differentiation
function
index
instance
index
purely
semantic
nature
semantics
however
important
display
interpret
results
interpret
varying
instance
parameter
natural
randomization
experiments9
order
generate
repetitions
function
average
away
irrelevant
aspects
function
deÔ¨Ånition
thereby
providing
generality
alleviates
problem
overÔ¨Åtting
fair
setup
prevents
intentional
unintentional
exploitation
irrelevant
artiÔ¨Åcial
function
properties
example
consider
absolute
location
optimum
deÔ¨Åning
function
feature
consequently
typical
coco
benchmark
suite
instances
randomized
search
space
trans-
lations
presented
optimizer.10
runtime
target
values
order
measure
runtime
algorithm
problem
establish
hitting
time
condition
prescribe
target
value
ùëì-value
generally
quality
indicator-value
han2016perf
bro2016
single
run
algorithm
reaches
surpasses
target
value
problem
say
solved
problem
successful.11
runtime
evaluation
count
target
value
reached
surpassed
Ô¨Årst
time
runtime
number
ùëì-evaluations
needed
solve
problem
.12
measured
runtimes
way
assess
performance
algorithm
observed
success
rates
generally
translated
runtimes
subset
problems
algorithm
hit
target
single
run
runtime
remains
undeÔ¨Åned
bounded
number
evaluations
unsuccessful
run
number
changing
sweeping
relevant
feature
problem
class
systematically
randomized
another
possible
usage
instance
parametrization
conducting
either
several
trials
instances
randomized
search
space
translations
randomized
initial
solution
equivalent
given
optimizer
behaves
translation
invariant
disregarding
domain
boundaries
reÔ¨Çecting
anytime
aspect
experimental
setup
use
term
problem
two
meanings
problem
algorithm
benchmarked
problem
algorithm
may
solve
hitting
target
runtime
may
fail
solve
problem
gives
raise
collection
dependent
problems
viewed
random
variables
events
given
independent
events
different
values
target
values
directly
linked
problem
leaving
burden
deÔ¨Åne
targets
designer
benchmark
suite
alternative
namely
present
obtained
indicator-values
results
leaves
rather
unsurmountable
burden
interpret
meaning
indicator
values
experimenter
Ô¨Ånal
audience
fortunately
automatized
generic
way
generate
target
values
observed
runtimes
so-called
run-
length
based
target
values
han2016perf
available
runtime
values
depends
budget
algorithm
explored
therefore
larger
budgets
preferable
however
come
expense
abandoning
reasonable
termination
conditions
instead
restarts
done
han2016ex
3.1
restarts
simulated
restarts
optimization
algorithm
bound
terminate
single-objective
case
return
rec-
ommended
solution
problem
.13
algorithm
solves
thereby
problems
independent
restarts
different
randomized
initial
solutions
simple
powerful
tool
increase
number
solved
problems
har1999
namely
increasing
number
ùë°-values
problem
solved.14
independent
restarts
tend
increase
success
rate
generally
change
performance
assess-
ment
successes
materialize
greater
runtimes
han2016perf
therefore
call
approach
budget-free
restarts
however
improve
reliability
comparability
precision
visibility
measured
results
han2016ex
simulated
restarts
han2010
han2016perf
used
determine
runtime
unsuccessful
runs
semantically
valid
interpret
different
instances
random
repetitions
resembling
bootstrapping
method
efr1994
face
unsolved
problem
draw
uniformly
random
new
Ô¨Ånd
instance
solved.15
eval-
uations
done
Ô¨Årst
unsolved
problem
subsequently
drawn
unsolved
problems
added
runtime
last
problem
considered
runtime
originally
unsolved
problem
method
applied
problem
instance
solved
available
least
one
problem
instance
solved
allows
directly
compare
algorithms
different
success
probabilities
3.2
aggregation
typical
benchmark
suite
consists
20‚Äì100
functions
5‚Äì15
instances
func-
tion
instance
100
targets
considered
performance
assessment
means
consider
least
100
100
100
150
000
runtimes
performance
assessment
make
amenable
experimenter
need
summarize
data
idea
behind
aggregation
make
statistical
summary
set
subset
problems
interest
assume
uniform
distribution
practical
perspective
means
simple
way
distinguish
problems
select
optimization
algorithm
speciÔ¨Åcally
use
anytime
scenario
consider
evaluation
evolving
quality
indicator
value
quality
indicator
always
deÔ¨Åned
given
problem
number
acquired
runtime
values
hitting
target
indicator
value
monotonously
increasing
used
budget
considered
random
variables
runtimes
independent
speciÔ¨Åcally
consider
problems
benchmarked
instances
targets
depend
instance
way
make
problems
comparable
accordingly
case
aggregation
single
algorithm
would
helpful
face
problem
similar
probability
aggregate
dimension
dimension
used
algorithm
selection
several
ways
aggregate
resulting
runtimes
empirical
cumulative
distribution
functions
ecdf
domain
optimization
ecdf
also
known
data
proÔ¨Åles
mor2009
prefer
simple
ecdf
inno-
vative
performance
proÔ¨Åles
mor2002
two
reasons
ecdf
depend
presented
algorithms
unconditionally
comparable
across
different
publica-
tions
let
distinguish
considered
algorithm
natural
way
easy
problems
difÔ¨Åcult
problems.16
usually
display
ecdf
log
scale
makes
area
curve
difference
area
two
curves
meaningful
conception
averaging
estimator
expected
runtime
average
runtime
often
plot-
ted
dimension
indicate
scaling
dimension
arithmetic
average
meaningful
underlying
distribution
values
similar
otherwise
average
log-runtimes
geometric
average
recommended
restarts
simulated
restarts
see
section
restarts
simulated
restarts
aggre-
gate
runtimes
literal
meaning
literally
deÔ¨Åned
hit
aggregate
however
time
data
eventually
supplement
applicable
missing
runtime
values
general
code
structure
code
basis
coco
code
consists
two
parts
experiments
part
deÔ¨Ånes
test
suites
allows
conduct
experiments
provides
output
data
code
base
written
wrapped
different
languages
currently
java
python
matlab/octave
amalgamation
technique
used
outputs
two
Ô¨Åles
coco.h
coco.c
sufÔ¨Åce
run
experiments
within
coco
framework
post-processing
part
processes
data
displays
resulting
runtimes
part
en-
tirely
written
python
heavily
depends
matplotlib
hun2007
test
suites
currently
coco
framework
provides
three
different
test
suites
bbob
contains
functions
Ô¨Åve
subgroups
han2009fun
reading
performance
proÔ¨Åle
question
immediately
crossing
ones
mind
often
whether
large
run-
time
difference
observed
mainly
one
algorithm
solves
problem
quickly
question
answered
proÔ¨Åle
advantage
data
proÔ¨Åles
disappears
using
run-length
based
target
values
han2016perf
bbob-noisy
contains
noisy
problems
three
subgroups
han2009noi
currently
im-
plemented
old
code
basis
bbob-biobj
contains
bi-objective
functions
subgroups
tus2016
acknowledgments
authors
would
like
thank
raymond
ros
steffen
finck
marc
schoenauer
petr
posik
dejan
tu≈°ar
many
invaluable
contributions
work
authors
also
acknowledge
support
grant
anr-12-monu-0009
numbbo
french
national
research
agency
references
bro2016
brockhoff
tu≈°ar
tu≈°ar
wagner
hansen
auger
2016
biobjective
performance
assessment
coco
platform
arxiv
e-prints
arxiv:1605.01746
han2016perf
hansen
auger
brockhoff
tu≈°ar
tu≈°ar
2016
coco
perfor-
han2010
mance
assessment
arxiv
e-prints
arxiv:1605.03560.
hansen
auger
ros
finck
posik
2010
comparing
results
algorithms
black-box
optimization
benchmarking
bbob-2009
workshop
proceedings
gecco
genetic
evolutionary
computation
conference
2010
acm
1689-1696
han2009fun
hansen
finck
ros
auger
2009
real-parameter
black-box
op-
timization
benchmarking
2009
noiseless
functions
deÔ¨Ånitions
research
report
rr-6829
inria
updated
february
2010
han2009noi
hansen
finck
ros
auger
2009
real-parameter
black-box
optimization
benchmarking
2009
noisy
functions
deÔ¨Ånitions
research
re-
port
rr-6869
inria
updated
february
2010
han2016ex
hansen
tu≈°ar
auger
brockhoff
mersmann
2016
coco
hun2007
efr1994
har1999
experimental
procedure
arxiv
e-prints
arxiv:1603.08776.
hunter
2007
matplotlib
graphics
environment
computing
sci-
ence
engineering
90-95.
efron
tibshirani
1994
introduction
bootstrap
crc
press
harik
lobo
1999
parameter-less
genetic
algorithm
pro-
ceedings
genetic
evolutionary
computation
conference
gecco
volume
pages
258-265.
acm
hoo1998
mor2009
mor2002
ste1946
tus2016
hoos
st√ºtzle
1998
evaluating
las
vegas
algorithms
pitfalls
remedies
proceedings
fourteenth
conference
uncertainty
artiÔ¨Å-
cial
intelligence
uai-98
pages
238-245.
mor√©
wild
2009
benchmarking
derivative-free
optimization
algo-
rithms
siam
optimization
:172-191.
dolan
mor√©
2002
benchmarking
optimization
software
per-
formance
proÔ¨Åles
mathematical
programming
91:201-213.
s.s.
stevens
1946
theory
scales
measurement
science
103
2684
677-680.
tu≈°ar
brockhoff
hansen
auger
2016
coco
bi-objective
black
box
optimization
benchmarking
bbob-biobj
test
suite
arxiv
e-prints
arxiv:1604.00359
whi1996
whitley
rana
dzubera
mathias
1996
evaluating
evolutionary
algorithms
artiÔ¨Åcial
intelligence
245-276
