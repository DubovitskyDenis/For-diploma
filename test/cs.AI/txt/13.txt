probabilistic
reasoning
via
deep
learning
neural
association
models
quan
liu†
hui
jiang‡
andrew
evdokimov‡
zhen-hua
ling†
xiaodan
zhu
cid:96
wei§
hu†§
national
engineering
laboratory
speech
language
information
processing
department
electrical
engineering
computer
science
york
university
canada
university
science
technology
china
hefei
anhui
china
cid:96
national
research
council
canada
ottawa
canada
iflytek
research
hefei
china
emails
quanliu
mail.ustc.edu.cn
cse.yorku.ca
ae2718
cse.yorku.ca
zhling
ustc.edu.cn
xiaodan
cse.yorku.ca
siwei
iﬂytek.com
yuhu
iﬂytek.com
abstract
paper
propose
new
deep
learning
approach
called
neural
association
model
nam
probabilistic
rea-
soning
artiﬁcial
intelligence
propose
use
neural
net-
works
model
association
two
events
do-
main
neural
networks
take
one
event
input
compute
conditional
probability
event
model
likely
two
events
associated
actual
meaning
conditional
probabilities
varies
applications
depends
models
trained
work
two
case
studies
investigated
two
nam
structures
namely
deep
neural
networks
dnn
relation-modulated
neural
nets
rmnn
several
probabilistic
reasoning
tasks
including
recognizing
textual
entailment
triple
classiﬁ-
cation
multi-relational
knowledge
bases
commonsense
reasoning
experimental
results
several
popular
datasets
derived
wordnet
freebase
conceptnet
demonstrated
dnns
rmnns
perform
equally
well
signiﬁcantly
outperform
conventional
methods
available
reasoning
tasks
moreover
com-
pared
dnns
rmnns
superior
knowledge
trans-
fer
pre-trained
model
quickly
extended
unseen
relation
observing
training
samples
prove
effectiveness
proposed
models
work
applied
nams
solving
challenging
winograd
schema
problems
experiments
conducted
set
problems
prove
proposed
models
potential
commonsense
reasoning
introduction
reasoning
important
topic
artiﬁcial
intelligence
attracted
considerable
attention
re-
search
effort
past
decades
mccarthy
1986
minsky
1988
mueller
2014
besides
traditional
logic
reasoning
probabilistic
reasoning
studied
an-
typical
genre
order
handle
knowledge
uncer-
tainty
reasoning
based
probability
theory
pearl
1988
neapolitan
2012
probabilistic
reasoning
used
predict
conditional
probability
e2|e1
one
event
given
another
event
state-of-the-art
methods
proba-
bilistic
reasoning
include
bayesian
networks
jensen
1996
markov
logic
networks
richardson
domingos
2006
graphical
models
koller
friedman
2009
taking
bayesian
networks
example
conditional
copyright
2015-2016.
probabilities
two
associated
events
calculated
posterior
probabilities
according
bayes
theorem
possible
events
modeled
pre-deﬁned
graph
struc-
ture
however
methods
quickly
become
intractable
practical
tasks
number
possible
events
usually
large
recent
years
distributed
representations
map
dis-
crete
language
units
continuous
vector
space
gained
signiﬁcant
popularity
along
development
neural
networks
bengio
2003
collobert
2011
mikolov
2013
main
beneﬁt
embedding
con-
tinuous
space
smoothness
property
helps
cap-
ture
semantic
relatedness
discrete
events
poten-
tially
generalizable
unseen
events
similar
ideas
knowledge
graph
embedding
proposed
repre-
sent
knowledge
bases
low-dimensional
continuous
space
bordes
2013
socher
2013
wang
2014
nickel
2015
using
smoothed
represen-
tation
possible
reason
relations
among
var-
ious
entities
however
human-like
reasoning
remains
extremely
challenging
problem
partially
requires
effective
encoding
world
knowledge
using
powerful
models
existing
kbs
quite
sparse
even
recently
created
large-scale
kbs
yago
nell
freebase
capture
fraction
world
knowledge
order
take
advantage
sparse
knowledge
bases
state-of-the-art
approaches
knowledge
graph
embed-
ding
usually
adopt
simple
linear
models
rescal
nickel
tresp
kriegel
2012
transe
bordes
2013
neural
tensor
networks
socher
2013
bowman
2013
although
deep
learning
techniques
achieve
great
pro-
gresses
many
domains
e.g
speech
image
lecun
bengio
hinton
2015
progress
commonsense
reasoning
seems
slow
paper
propose
use
deep
neural
networks
called
neural
association
model
nam
commonsense
reasoning
different
ex-
isting
linear
models
proposed
nam
model
uses
multi-
layer
nonlinear
activations
deep
neural
nets
model
association
conditional
probabilities
two
pos-
sible
events
proposed
nam
framework
symbolic
events
represented
low-dimensional
continuous
space
need
explicitly
specify
dependency
structure
among
events
required
bayesian
networks
deep
neural
networks
used
model
association
be-
tween
two
events
taking
one
event
input
compute
conditional
probability
another
event
computed
con-
ditional
probability
association
may
generalized
model
various
reasoning
problems
entailment
infer-
ence
relational
learning
causation
modelling
work
study
two
model
structures
nam
ﬁrst
model
standard
deep
neural
networks
dnn
second
model
uses
special
structure
called
relation
mod-
ulated
neural
nets
rmnn
experiments
several
proba-
bilistic
reasoning
tasks
including
recognizing
textual
entail-
ment
triple
classiﬁcation
multi-relational
kbs
com-
monsense
reasoning
demonstrated
dnns
rmnns
outperform
conventional
methods
more-
rmnn
model
shown
effective
knowl-
edge
transfer
learning
pre-trained
model
quickly
extended
new
relation
observing
training
samples
furthermore
also
apply
proposed
nam
models
challenging
commonsense
reasoning
problems
i.e.
recently
proposed
winograd
schemas
levesque
davis
morgenstern
2011
problems
viewed
alternative
turing
test
turing
1950
support
model
training
nam
propose
straight-
forward
method
collect
associated
cause-effect
pairs
large
unstructured
texts
pair
extraction
procedure
starts
constructing
vocabulary
thousands
common
verbs
adjectives
based
extracted
pairs
pa-
per
extends
nam
models
solve
winograd
schema
problems
achieves
accuracy
set
cause-
effect
examples
undoubtedly
realize
commonsense
rea-
soning
still
much
work
done
many
problems
solved
detailed
discussions
would
given
end
paper
motivation
association
events
paper
aims
model
association
relationships
be-
tween
events
using
neural
network
methods
make
clear
main
work
ﬁrst
describe
characteristics
events
possible
association
relationships
be-
tween
events
based
analysis
event
association
present
motivation
proposed
neural
association
models
commonsense
reasoning
main
characteris-
tics
events
following
massive
natural
situations
number
events
massive
means
association
space
model
large
sparse
events
occur
dialy
life
sparse
challenging
task
ideally
capture
similarities
different
events
time
association
events
appears
ev-
erywhere
consider
single
event
play
basketball
ex-
ample
shown
figure
single
event
would
asso-
ciate
many
events
person
plays
basketball
would
win
game
meanwhile
would
injured
cases
person
could
make
money
playing
basketball
well
moreover
know
person
plays
bas-
ketball
coached
regular
game
typical
associations
events
however
need
recognize
task
modeling
event
association
identical
performing
classiﬁcation
classiﬁcation
typically
map
event
feature
space
one
pre-deﬁned
ﬁnite
categories
classes
event
association
need
compute
association
probability
two
arbitrary
events
may
sample
pos-
sibly
inﬁnite
set
mapping
relationships
event
associ-
ation
would
many-to-many
e.g.
playing
basket-
ball
could
support
make
money
someone
makes
stock
trading
could
make
money
well
speciﬁcally
association
relationships
events
include
cause-
effect
spatial
temporal
paper
treats
general
relation
considering
sparseness
useful
kbs
figure
example
association
events
paper
believe
modeling
association
relationships
events
fundamental
work
com-
monsense
reasoning
could
model
event
associa-
tions
well
may
ability
solve
many
com-
monsense
reasoning
problems
considering
main
char-
acteristics
discrete
event
event
association
two
rea-
sons
given
describing
motivation
advantage
distributed
representation
methods
rep-
resenting
discrete
events
continuous
vector
space
pro-
vides
good
way
capture
similarities
dis-
crete
events
advantage
neural
network
methods
neural
net-
works
could
perform
universal
approximation
lin-
ear
models
easily
hornik
stinchcombe
white
1990
time
paper
takes
account
distributed
representation
neural
network
methods
data-hungry
artiﬁcial
intelligence
research
mining
large
sizes
useful
data
knowledge
model
learning
always
challenging
following
section
paper
presents
preliminary
work
data
collection
cor-
responding
experiments
made
solving
common-
sense
reasoning
problems
play
basketballwininjuredmake
moneybe
coacheddrink
waterstock
trading
neural
association
models
nam
paper
propose
use
nonlinear
model
namely
neural
association
model
probabilistic
reasoning
main
goal
use
neural
nets
model
association
probability
two
events
domain
i.e.
e2|e1
conditioning
possible
events
domain
projected
continuous
space
without
specifying
explicit
dependency
structure
among
following
ﬁrst
introduce
neural
association
mod-
els
nam
general
modeling
framework
probabilis-
tic
reasoning
next
describe
two
particular
nam
struc-
tures
modeling
typical
multi-relational
data
nam
general
figure
nam
framework
general
figure
shows
general
framework
nam
asso-
ciating
two
events
general
nam
frame-
work
events
ﬁrst
projected
low-dimension
continuous
space
deep
neural
networks
multi-layer
nonlinearity
used
model
likely
two
events
associated
neural
networks
take
embedding
one
event
antecedent
input
compute
condi-
tional
probability
e2|e1
event
conse-
quent
event
binary
true
false
nam
models
may
use
sigmoid
node
compute
e2|e1
takes
multiple
mutually
exclusive
values
use
softmax
nodes
e2|e1
may
need
use
multiple
embeddings
one
per
value
nams
explicitly
specify
different
events
actually
related
may
mutually
exclusive
contained
inter-
sected
nams
used
separately
compute
condi-
tional
probabilities
e2|e1
pair
events
task
actual
physical
meaning
condi-
tional
probabilities
e2|e1
varies
applications
depends
models
trained
table
lists
possible
applications
application
language
modeling
causal
reasoning
lexical
entailment
textual
entailment
knowledge
triple
classiﬁcation
cause
effect
table
applications
nams
language
modeling
antecedent
event
repre-
sentation
historical
context
consequent
event
next
word
takes
one
values
causal
rea-
soning
represent
cause
effect
respectively
example
eating
cheesy
cakes
happy
e2|e1
indicates
likely
may
cause
binary
true
false
event
model
may
add
nodes
model
different
ef-
fects
e.g.
cid:48
growing
fat
moreover
may
add
softmax
nodes
model
multi-valued
event
e.g.
cid:48
cid:48
happiness
scale
similarly
knowledge
triple
classiﬁcation
multi-relation
data
given
one
triple
consists
head
entity
sub-
ject
relation
predicate
binary
event
indicating
whether
tail
entity
object
true
false
finally
applications
recognizing
lexical
textual
entailment
may
deﬁned
premise
hy-
pothesis
generally
nams
used
model
inﬁnite
number
events
point
con-
tinuous
space
represents
possible
event
work
simplicity
consider
nams
ﬁnite
number
binary
events
formulation
easily
extended
general
cases
compared
traditional
methods
like
bayesian
net-
works
nams
employ
neural
nets
universal
approxima-
tor
directly
model
individual
pairwise
event
association
probabilities
without
relying
explicit
dependency
struc-
ture
therefore
nams
end-to-end
learned
purely
training
samples
without
strong
human
prior
knowl-
edge
potentially
scalable
real-world
tasks
cid:88
∈d+
cid:88
∈d−
learning
nams
assume
set
observed
examples
event
pairs
de-
noted
training
set
normally
includes
pos-
itive
negative
samples
denote
positive
sam-
ples
true
negative
samples
alse
independence
assumption
statistical
relational
learning
srl
getoor
2007
nickel
2015
log
likelihood
function
nam
model
expressed
follows
denotes
logistic
score
function
derived
nam
numerically
computes
conditional
probability
e2|e1
details
given
later
paper
stochastic
gradient
descent
sgd
methods
may
used
maximize
likeli-
hood
function
leading
maximum
likelihood
estimation
mle
nams
following
two
case
studies
consider
two
nam
structures
ﬁnite
number
output
nodes
model
e2|e1
pair
events
ﬁnite
number
binary
ﬁrst
model
typical
dnn
associates
antecedent
event
in-
put
consequent
event
output
present
an-
model
structure
called
relation-modulated
neural
nets
suitable
multi-relational
data
vector
spaceevent
e1vector
spaceevent
e2deep
neural
networksassociation
dnnspr
e2|e1
dnn
nams
ﬁrst
nam
structure
traditional
dnn
shown
figure
use
multi-relational
data
illus-
tration
given
triple
corre-
sponding
label
true
false
cast
compute
e2|e1
follows
figure
dnn
structure
nams
firstly
represent
head
entity
phrase
tail
en-
tity
phrase
two
embedding
vectors
similarly
relation
also
represented
low-dimensional
vector
call
relation
code
hereafter
secondly
combine
embeddings
head
entity
relation
feed
layer
dnn
input
dnn
consists
rectiﬁed
linear
relu
hidden
layers
nair
hinton
2010
input
feedforward
process
cid:96
cid:96
cid:96
cid:96
cid:96
···
cid:16
cid:96
cid:17
cid:16
cid:96
cid:17
cid:96
max
cid:96
···
cid:96
cid:96
represent
weight
matrix
bias
layer
cid:96
respectively
finally
propose
calculate
sigmoid
score
triple
association
probability
using
last
hidden
layer
output
tail
entity
vector
sigmoid
function
i.e.
1+e−x
network
parameters
nam
structure
repre-
sented
may
jointly
learned
maximizing
likelihood
function
cid:16
cid:17
relation-modulated
neural
networks
rmnn
particularly
multi-relation
data
following
idea
xue
2014
propose
use
so-called
relation-
modulated
neural
nets
rmnn
shown
figure
rmnn
uses
operations
dnns
project
entities
relations
low-dimensional
continuous
space
shown
figure
connect
knowledge-
speciﬁc
relation
code
hidden
layers
network
figure
relation-modulated
neural
networks
rmnn
shown
later
structure
superior
knowledge
trans-
fer
learning
tasks
therefore
layer
rmnns
in-
stead
using
linear
activation
signal
computed
previous
layer
cid:96
relation
code
follows
cid:96
cid:96
cid:96
cid:96
cid:96
1···
cid:96
cid:96
represent
normal
weight
matrix
relation-speciﬁc
weight
matrix
layer
cid:96
topmost
layer
calculate
ﬁnal
score
triple
using
relation
code
l+1
cid:17
cid:16
way
rmnn
parameters
including
jointly
learned
based
maximum
likelihood
estimation
rmnn
models
particularly
suitable
knowl-
edge
transfer
learning
pre-trained
model
quickly
extended
new
relation
observing
samples
relation
case
may
estimate
new
relation
code
based
available
new
samples
keeping
whole
network
unchanged
due
small
size
new
relation
code
reliably
estimated
small
number
new
samples
furthermore
model
perfor-
mance
original
relations
affected
since
model
original
relation
codes
changed
transfer
learning
experiments
section
evaluate
proposed
nam
models
various
reasoning
tasks
ﬁrst
describe
experimental
setup
report
results
several
reasoning
tasks
including
textual
entailment
recognition
triple
clas-
siﬁcation
multi-relational
kbs
commonsense
reasoning
knowledge
transfer
learning
experimental
setup
ﬁrst
introduce
common
experimental
set-
tings
used
experiments
entity
sentence
rep-
resentations
represent
composing
0relation
code
head
entity
vectortail
entity
vectorfscore
functionw
…………out
l+1
0new
relationhead
entity
vectorfw
…………b
tail
entity
vectorsv
head
head
l+1
0existed
relationshead
entity
vectorfw
…………b
tail
entity
vectorsv
head
head
l+1
transferingvector
spaceevent
e1vector
spaceevent
e2deep
neural
networksassociation
dnnsp
e2|e1
relation
vectorhead
entity
vectortail
entity
vectorfassociation
herew
…out
head
entity
vectortail
entity
vectorfassociation
herew
…out
l+1
relation
vector0relation
code
head
entity
vectortail
entity
vectorfscore
functionw
…………out
l+1
0new
relationhead
entity
vectorfw
…………b
tail
entity
vectorsv
head
head
l+1
0existed
relationshead
entity
vectorfw
…………b
tail
entity
vectorsv
head
head
l+1
transferingvector
spaceevent
e1vector
spaceevent
e2deep
neural
networksassociation
dnnsp
e2|e1
relation
vectorhead
entity
vectortail
entity
vectorfassociation
herew
…out
head
entity
vectortail
entity
vectorfassociation
herew
…out
l+1
relation
vector
word
vectors
socher
2013
word
vectors
initialized
pre-trained
skip-gram
mikolov
2013
word
embedding
model
trained
large
english
wikipedia
corpus
dimensions
word
embeddings
set
100
experiments
dimensions
relation
codes
set
50.
relation
codes
ran-
domly
initialized
network
structures
use
relu
nonlinear
activation
function
network
param-
eters
initialized
according
glorot
bengio
2010
meanwhile
since
number
training
examples
probabilistic
reasoning
tasks
relatively
small
adopt
dropout
approach
hinton
2012
train-
ing
process
avoid
over-ﬁtting
problem
learning
process
nams
need
use
negative
sam-
ples
automatically
generated
randomly
per-
cid:54
turbing
positive
triples
cid:96
cid:96
task
use
provided
development
set
tune
best
training
hyperparameters
example
tested
number
hidden
layers
among
initial
learning
rate
among
0.01
0.05
0.1
0.25
0.5
dropout
rate
among
0.1
0.2
0.3
0.4
finally
se-
lect
best
setting
based
performance
devel-
opment
set
ﬁnal
model
structure
uses
hidden
layers
learning
rate
dropout
rate
set
0.1
0.2
respectively
experiments
model
training
learning
rate
halved
performances
development
set
decreases
dnns
rmnns
trained
using
stochastic
gradient
descend
sgd
al-
gorithm
notice
nam
models
converge
quickly
epochs
recognizing
textual
entailment
understanding
entailment
contradiction
fundamental
language
understanding
conduct
experiments
popular
recognizing
textual
entailment
rte
task
aims
recognize
entailment
relationship
pair
english
sentences
experiment
use
snli
dataset
bowman
2015
conduct
2-class
rte
ex-
periments
entailment
contradiction
instances
labelled
entailment
converted
contradic-
tion
experiments
snli
dataset
contains
hundreds
thousands
training
examples
useful
train-
ing
nam
model
since
data
set
include
multi-
relational
data
investigate
dnn
structure
task
ﬁnal
nam
result
along
baseline
per-
formance
provided
bowman
2015
listed
ta-
ble
model
edit
distance
bowman
2015
classiﬁer
bowman
2015
lexical
resources
bowman
2015
dnn
accuracy
71.9
72.2
75.0
84.7
table
experimental
results
rte
task
results
see
proposed
dnn
based
nam
model
achieves
considerable
improvements
var-
ious
traditional
methods
indicates
better
model
entailment
relationship
natural
language
repre-
senting
sentences
continuous
space
conducting
prob-
abilistic
reasoning
deep
neural
networks
triple
classiﬁcation
multi-relational
kbs
section
evaluate
proposed
nam
models
two
popular
knowledge
triple
classiﬁcation
datasets
namely
wn11
fb13
socher
2013
derived
word-
net
freebase
predict
whether
new
triple
rela-
tions
hold
based
training
facts
database
wn11
dataset
contains
38,696
unique
entities
involving
different
relations
total
fb13
dataset
covers
relations
75,043
entities
table
summarizes
statis-
tics
two
datasets
dataset
wn11
fb13
ent
38,696
75,043
train
112,581
316,232
dev
2,609
5,908
test
10,544
23,733
table
statistics
kbs
triple
classiﬁcation
datasets
number
relations
ent
size
entity
set
goal
knowledge
triple
classiﬁcation
predict
whether
given
triple
correct
ﬁrst
use
training
data
learn
nam
models
after-
wards
use
development
set
tune
global
threshold
make
binary
decision
triple
classiﬁed
true
otherwise
false
ﬁnal
accuracy
calculated
based
many
triplets
test
set
classiﬁed
correctly
experimental
results
wn11
fb13
datasets
given
table
compare
two
nam
mod-
els
methods
reported
two
datasets
results
clearly
show
nam
methods
dnns
rmnns
achieve
comparable
performance
triple
classiﬁcation
tasks
yield
consistent
improvement
existing
methods
particular
rmnn
model
yields
3.7
1.9
absolute
improvements
pop-
ular
neural
tensor
networks
ntn
socher
2013
wn11
fb13
respectively
dnn
rmnn
mod-
els
much
smaller
ntn
number
parameters
scale
well
number
relation
types
increases
example
dnn
rmnn
models
wn11
7.8
millions
parameters
ntn
millions
although
rescal
transe
models
millions
parameters
wn11
size
goes
quickly
tasks
thousands
relation
types
addition
training
time
dnn
rmnn
much
shorter
ntn
transe
since
models
con-
verge
much
faster
example
obtained
least
times
speedup
ntn
wn11
commonsense
reasoning
similar
triple
classiﬁcation
task
socher
2013
work
use
conceptnet
liu
singh
2004
construct
new
commonsense
data
set
named
model
sme
bordes
2012
transe
bordes
2013
transh
wang
2014
transr
lin
2015
ntn
socher
2013
dnn
rmnn
wn11
70.0
75.9
78.8
85.9
86.2
89.3
89.9
fb13
avg
66.9
63.7
81.5
78.7
81.1
83.3
84.2
82.5
88.1
90.0
90.4
91.5
91.9
90.9
table
triple
classiﬁcation
accuracy
wn11
fb13
cn14
hereafter
building
cn14
ﬁrst
select
facts
conceptnet
related
typical
commonsense
re-
lations
e.g.
usedfor
capableof
see
figure
relations
randomly
divide
extracted
facts
three
sets
train
dev
test
finally
order
create
test
set
classiﬁcation
randomly
switch
entities
whole
vocabulary
correct
triples
get
total
test
triples
half
positive
samples
half
nega-
tive
examples
statistics
cn14
given
table
dataset
cn14
ent
159,135
train
200,198
dev
5,000
test
10,000
table
statistics
cn14
dataset
cn14
dataset
designed
answering
common-
sense
questions
like
camel
capable
journeying
across
desert
proposed
nam
models
answer
question
calculating
association
probability
e2|e1
camel
capable
journey
across
desert
paper
compare
two
nam
methods
pop-
ular
ntn
method
socher
2013
data
set
overall
results
given
table
see
nam
methods
outperform
ntn
task
dnn
rmnn
models
obtain
similar
performance
model
ntn
dnn
rmnn
positive
negative
82.7
84.5
85.1
86.5
86.9
87.1
total
84.6
85.7
86.1
table
accuracy
comparison
cn14
furthermore
show
classiﬁcation
accuracy
relations
cn14
rmnn
ntn
figure
show
accuracy
rmnn
varies
among
different
re-
lations
80.1
desires
93.5
createdby
no-
tice
commonsense
relations
desires
capa-
bleof
harder
others
like
createdby
causes-
desire
rmnn
overtakes
ntn
almost
relations
knowledge
transfer
learning
knowledge
transfer
various
domains
character-
istic
feature
crucial
cornerstone
human
learning
section
evaluate
proposed
nam
models
figure
accuracy
different
relations
cn14
figure
accuracy
test
set
new
rela-
tion
causesdesire
shown
function
used
training
samples
causesdesire
updating
relation
code
accuracy
original
relations
remains
85.7
knowledge
transfer
learning
scenario
adapt
pre-
trained
model
unseen
relation
train-
ing
samples
new
relation
randomly
select
relation
e.g.
causesdesire
cn14
experiment
relation
contains
4800
training
samples
480
test
samples
experiments
use
relations
cn14
train
baseline
nam
models
dnn
rmnn
transfer
learning
freeze
nam
parameters
including
weights
entity
repre-
sentations
learn
new
relation
code
causesde-
sire
given
samples
last
learned
relation
code
along
original
nam
models
used
classify
new
samples
causesdesire
test
set
obviously
transfer
learning
affect
model
performance
original
relations
models
changed
figure
shows
results
knowledge
transfer
learning
relation
causesdesire
increase
training
sam-
ples
gradually
result
shows
rmnn
performs
much
better
dnn
experiment
signiﬁ-
cantly
improve
rmnn
new
relation
5-20
total
training
samples
causesdesire
demon-
strates
structure
connect
relation
code
hidden
layers
leads
effective
learning
new
rela-
tion
codes
relatively
small
number
training
sam-
ples
next
also
test
aggressive
learning
strategy
transfer
learning
setting
simultaneously
up-
date
network
parameters
learning
6065707580859095100usedforcapableofhassubeventhasprerequisitehaspropertycausesmotivatedbygoalreceivesactioncausesdesiredesireshaslastsubeventcreatedbydesireofsymbolofntnrmnn70.0072.0074.0076.0078.0080.0082.0084.0086.0088.005
100
dnnrmnn
new
relation
code
results
shown
figure
strategy
obviously
improve
performance
new
relation
especially
add
training
sam-
ples
however
expected
performance
origi-
nal
relations
deteriorates
dnn
improves
perfor-
mance
new
relation
use
training
samples
94.6
however
performance
remaining
original
relations
drops
dramatically
85.6
75.5
rmnn
shows
advantage
dnn
transfer
learning
setting
accuracy
new
re-
lation
increases
77.9
90.8
accuracy
original
relations
drop
slightly
85.9
82.0
figure
transfer
learning
results
updating
network
parameters
left
ﬁgure
shows
results
new
relation
right
ﬁgure
shows
results
original
relations
extending
nams
winograd
schema
data
collection
previous
experiments
sections
tasks
already
contained
manually
constructed
training
data
how-
ever
many
cases
want
realize
ﬂexible
common-
sense
reasoning
real
world
conditions
obtaining
training
data
also
challenging
specif-
ically
since
proposed
neural
association
model
typ-
ical
deep
learning
technique
lack
training
data
would
make
difﬁcult
train
robust
model
therefore
paper
make
efforts
try
mine
use-
ful
data
model
training
ﬁrst
step
working
collecting
cause-effect
relationships
set
common
words
phrases
believe
type
knowledge
would
key
component
modeling
as-
sociation
relationships
discrete
events
section
describes
idea
automatic
cause-effect
pair
collection
well
data
collection
results
ﬁrst
introduce
common
vocabulary
created
query
generation
detailed
algorithm
cause-effect
pair
collection
presented
finally
following
sec-
tion
present
data
collection
results
common
vocabulary
query
generation
avoid
data
sparsity
problem
start
work
constructing
vocabulary
common
words
current
investigations
construct
vocabulary
con-
tains
7500
verbs
adjectives
shown
table
vocabulary
includes
3000
verb
words
2000
verb
phrases
2500
adjective
words
procedure
constructing
vocabulary
straightforward
ﬁrst
extract
words
phrases
divided
part-of-speech
tags
wordnet
miller
1995
conducting
part-of-speech
tagging
large
corpus
get
occurrence
frequencies
words
phrases
scanning
tagged
corpus
finally
sort
words
phrases
frequency
select
top
results
set
category
verb
words
verb
phrases
adjective
words
size
3000
2000
2500
table
common
vocabulary
constructed
mining
cause-
effect
event
pairs
query
generation
based
common
vocabulary
generate
search
queries
pairing
two
words
phrases
currently
focus
extracting
asso-
ciation
relationships
verbs
adjectives
even
small
vocabulary
search
space
large
7.5k
7.5k
leads
tens
millions
pairs
work
deﬁne
several
patterns
word
phrase
based
two
pop-
ular
semantic
dimensions
positive-negative
active-
passive
osgood
1952
using
verbs
rob
arrest
example
contains
patterns
i.e
active
posi-
tive
active
negative
passive
positive
passive
neg-
ative
therefore
query
formed
rob
arrest
would
contain
possible
dimensions
shown
figure
task
mining
cause-effect
relationships
two
words
phrases
becomes
task
getting
num-
ber
occurrences
possible
links
figure
typical
dimensions
typical
query
automatic
cause-effect
pair
collection
based
created
queries
section
present
procedures
extracting
cause-effect
pairs
large
un-
structured
texts
overall
system
framework
shown
figure
query
searching
goal
query
searching
ﬁnd
possible
sentences
may
contain
input
queries
since
number
queries
large
structure
queries
hashmap
conduct
string
matching
dur-
ing
text
scanning
detail
searching
program
starts
70.0075.0080.0085.0090.0095.00100.005
100
dnnrmnn70.0075.0080.0085.0090.0095.00100.005
100
dnnrmnntext
corpusvocabsentencesresults
rob
active
positiveactive
negativepassive
positivepassive
negative
arrest
active
positiveactive
negativepassive
positivepassive
negativeassociation
links
figure
automatic
pair
collection
system
framework
conducting
lemmatizing
part-of-speech
tagging
depen-
dency
parsing
source
corpus
scan
corpus
begining
end
dealing
sentence
try
ﬁnd
matched
words
phrases
using
hashmap
strategy
help
reduce
search
complexity
linear
size
corpus
proved
efﬁcient
experiments
subject-object
matching
based
dependency
pars-
ing
results
ﬁnd
one
phrase
query
would
check
whether
phrase
associated
least
one
subject
object
corresponding
sentence
time
record
whether
phrase
positive
negative
active
passive
moreover
helping
de-
cide
cause-effect
relationships
would
check
whether
phrase
linked
connective
words
typ-
ical
connective
words
used
work
ﬁnally
extract
cause-effect
pairs
design
simple
subject-object
matching
rule
similar
work
peng
khashabi
roth
2015
two
phrases
one
query
share
subject
relationship
be-
tween
straightforward
subject
one
phrase
object
phrase
need
ap-
ply
passive
pattern
phrase
related
object
subject-object
matching
idea
similar
work
pro-
posed
peng
khashabi
roth
2015
using
query
ar-
rest
rob
example
ﬁnd
sentence
tom
arrested
tom
robbed
man
obtain
depen-
dency
parsing
result
shown
figure
10.
verb
arrest
rob
share
subject
pattern
arrest
passive
add
occurrence
corresponding
as-
sociation
link
i.e
link
active
positive
pattern
rob
passive
positive
pattern
arrest
figure
dependency
parsing
result
sentence
tom
arrested
tom
robbed
man
data
collection
results
table
shows
corpora
used
collecting
cause-
effect
pairs
corresponding
data
collection
results
extract
approximately
240,000
pairs
different
corpora
corpus
gigaword
graff
2003
novels
zhu
2015
cbtest
hill
2015
bnc
burnard
1995
result
pairs
117,938
129,824
4,167
2,128
table
data
collection
results
different
corpora
winograd
schema
challenge
based
experiments
described
previous
sec-
tions
could
conclude
neural
association
model
potential
effective
commonsense
reason-
ing
evaluate
effectiveness
proposed
neural
association
model
paper
conduct
ex-
periments
solving
complex
winograd
schema
chal-
lenge
problems
levesque
davis
morgenstern
2011
morgenstern
davis
ortiz
2016
winograd
schema
commonsense
reasoning
task
proposed
recent
years
treated
alternative
turing
test
turing
1950
new
task
would
interesting
see
whether
neural
network
methods
suit-
able
solving
problem
section
describes
progress
made
attempting
meet
winograd
schema
challenge
making
clear
main
task
winograd
schema
ﬁrstly
introduce
high
level
afterwards
introduce
system
framework
well
corresponding
modules
proposed
au-
tomatically
solve
winograd
schema
problems
finally
experiments
discussions
human
annotated
cause-
effect
dataset
discussion
presented
winograd
schema
winograd
schema
evaluates
system
common-
sense
reasoning
ability
based
traditional
difﬁ-
cult
natural
language
processing
task
coreference
resolu-
tion
levesque
davis
morgenstern
2011
saba
2015
winograd
schema
problems
carefully
designed
task
easily
solved
without
commonsense
knowledge
fact
even
solution
traditional
coref-
erence
resolution
problems
relies
semantics
world
knowledge
rahman
2011
strube
2016
de-
scribing
detail
copy
words
levesque
davis
morgenstern
2011
small
reading
comprehension
test
involving
single
binary
question
two
examples
trophy
would
brown
suitcase
joan
made
sure
thank
susan
help
big
big
answer
trophy
answer
suitcase
given
given
help
answer
joan
answer
susan
correct
answers
obvious
human
beings
questions
corresponding
follow-
ing
four
features
text
corpusvocabsentencesresults
rob
active
positiveactive
negativepassive
positivepassive
negative
arrest
active
positiveactive
negativepassive
positivepassive
negativeassociation
links
two
parties
mentioned
sentence
noun
phrases
two
males
two
females
two
inanimate
ob-
jects
two
groups
people
objects
pronoun
possessive
adjective
used
sen-
tence
reference
one
parties
also
right
sort
second
party
case
males
he/him/his
females
she/her/her
inanimate
object
it/it/its
groups
they/them/their.
question
involves
determining
referent
pro-
noun
possessive
adjective
answer
always
ﬁrst
party
mentioned
sentence
repeated
sentence
clarity
answer
second
party
word
called
special
word
appears
sentence
possibly
question
re-
placed
another
word
called
alternate
word
every-
thing
still
makes
perfect
sense
answer
changes
solving
problems
easy
since
required
com-
monsense
knowledge
quite
difﬁcult
collect
following
sections
going
describe
work
solving
winograd
schema
problems
via
neural
network
methods
system
framework
paper
propose
commonsense
knowledge
required
many
winograd
schema
problems
could
for-
mulized
association
relationships
discrete
events
using
sentence
joan
made
sure
thank
susan
help
given
example
commonsense
knowledge
man
receives
help
thank
man
gives
help
believe
model-
ing
association
event
receive
help
thank
give
help
thank
make
decision
com-
paring
association
probability
thank|receive
help
thank|give
help
models
well
trained
inequality
thank|receive
help
get
thank|give
help
following
idea
propose
uti-
lize
data
constructed
previous
section
ex-
tend
nam
models
solving
problems
design
two
frameworks
training
nam
models
transmat-nam
design
apply
four
linear
trans-
formation
matrices
i.e.
matrices
active
positive
active
negative
passive
positive
passive
nega-
tive
transforming
cause
event
effect
event
use
nam
model
cause-
effect
association
relationship
cause
ef-
fect
events
figure
model
framework
transmat-nam
relationvec-nam
hand
conﬁgura-
tion
treat
typical
dimensions
shown
fig-
ure
distinct
relations
relation
vectors
corresponding
nam
models
currently
use
rmnn
structure
nam
figure
model
framework
relationvec-nam
dataset
available
labelling
training
nam
models
based
two
conﬁgura-
tions
straightforward
network
parameters
includ-
ing
relation
vectors
linear
transformation
matri-
ces
learned
standard
stochastic
gradient
descend
algorithm
experiments
section
introduce
current
experiments
solving
winograd
schema
problems
ﬁrst
se-
lect
cause-effect
dataset
constructed
standard
dataset
subsequently
experimental
setup
described
detail
presenting
experimental
results
discus-
sions
would
made
end
section
cause-effect
dataset
paper
based
http
//www.cs.nyu.edu/faculty/davise/papers/
winogradschemas/ws.html
labelled
cause-
effect
problems
among
278
available
questions
experiments
table
shows
typical
examples
problem
label
three
verb
adjective
phrases
corresponding
two
parities
pronoun
labelled
phrases
also
record
corresponding
patterns
word
respectively
using
word
lift
example
generate
lift
active
positive
pattern
lift
active
negative
pattern
lifted
passive
positive
pattern
lifted
passive
negative
pattern
example
sentence
man
lift
son
weak
identify
weak
lift
lifted
man
son
resspectively
commonsense
somebody
weak
would
likely
effect
lift
rather
lifted
main
work
nam
solving
problem
calculate
association
probability
phrases
experimental
setup
setup
nam
cause-
effect
task
similar
settings
previous
tasks
representing
phrases
neural
association
models
use
bag-of-word
bow
approach
composing
phrases
pre-trained
word
vectors
since
vocabu-
lary
use
experiment
contains
7500
com-
mon
verbs
adjectives
out-of-vocabulary
oov
words
phrases
based
bow
method
phrase
would
useless
words
contains
oov
paper
remove
testing
samples
useless
phrases
results
testing
cause-effect
samples
network
settings
set
embedding
size
causeeffectneural
association
modelcauseeffectneural
association
modelrelationtransformtransformcauseeffectneural
association
modelcauseeffectneural
association
modelrelationtransformtransform
schema
texts
man
lift
son
weak
man
lift
son
heavy
ﬁsh
ate
worm
tasty
ﬁsh
ate
worm
hungry
mary
tucked
daughter
anne
bed
could
sleep
mary
tucked
daughter
anne
bed
could
work
tom
threw
schoolbag
ray
reached
top
stairs
tom
threw
schoolbag
ray
reached
bottom
stairs
jackson
greatly
inﬂuenced
arnold
though
lived
two
centuries
earlier
jackson
greatly
inﬂuenced
arnold
though
lived
two
centuries
later
verb/adjective
verb/adjective
verb/adjective
weak
heavy
tasty
hungry
tuck
bed
tuck
bed
reach
top
reach
bottom
live
earlier
live
later
lift
lift
eat
eat
tucked
bed
tucked
bed
throw
throw
inﬂuence
inﬂuence
lifted
lifted
eaten
eaten
sleep
work
thrown
thrown
inﬂuenced
inﬂuenced
table
examples
cause-effect
dataset
labelled
winograd
schema
challenge
dimension
relation
vectors
50.
set
hidden
layers
nam
models
hidden
layer
sizes
set
100.
learning
rate
set
0.01
experiments
time
better
control
model
training
set
learning
rates
learning
embedding
matri-
ces
relation
vectors
0.025.
negative
sampling
important
model
training
task
transmat-nam
system
generate
negative
samples
randomly
selecting
different
patterns
respect
pattern
effect
event
posi-
tive
samples
example
positive
training
sample
hungry
active
positive
causes
eat
active
positive
may
generate
negative
samples
like
hungry
active
pos-
itive
causes
eat
passive
positive
hungry
active
positive
causes
eat
active
negative
relationvec-
nam
system
negative
sampling
method
much
straightforward
i.e.
randomly
select
different
ef-
fect
event
whole
vocabulary
example
shown
possible
negative
sample
would
hungry
ac-
tive
positive
causes
happy
active
positive
hungry
active
positive
causes
talk
active
positive
results
experimental
results
shown
table
10.
results
ﬁnd
proposed
nam
mod-
els
achieve
accuracy
cause-effect
dataset
constructed
winograd
schemas
speciﬁcally
relationvec-nam
system
performs
slightly
better
transmat-nam
system
model
accuracy
58.6
transmat-nam
relationvec-nam
61.4
table
results
nams
winograd
schema
cause-
effect
dataset
testing
results
ﬁnd
nam
performs
well
testing
examples
instance
call
phone
sce-
nario
proposed
nam
generates
corresponding
asso-
ciation
probabilities
follows
paul
tried
call
george
phone
suc-
cessful
successful
paul
successful|call
0.7299
george
successful|be
called
0.5430
answer
paul
paul
tried
call
george
phone
available
available
paul
available|call
0.6859
george
available|be
called
0.8306
answer
george
testing
examples
ﬁnd
model
answer
questions
correctly
calculating
associa-
tion
probabilities
probability
successful|call
probability
available|call
available|be
called
inequality
relationships
association
probabilities
reasonable
commonsense
examples
jim
yelled
kevin
upset
successful|be
called
simple
smaller
larger
upset
jim
yell|be
upset
0.9296
kevin
yelled|be
upset
0.8785
answer
jim
jim
comforted
kevin
upset
upset
jim
comfort|be
upset
0.0282
kevin
comforted|be
upset
0.5657
answer
kevin
example
also
conveys
commonsense
knowl-
edge
daily
life
know
somebody
upset
would
likely
yell
people
mean-
also
likely
would
comforted
people
conclusions
paper
proposed
neural
association
models
nam
probabilistic
reasoning
use
neural
networks
model
association
probabilities
two
events
domain
work
investigated
two
model
structures
namely
dnn
rmnn
nams
experi-
mental
results
several
reasoning
tasks
shown
dnns
rmnns
outperform
existing
meth-
ods
paper
also
reports
preliminary
results
use
nams
knowledge
transfer
learning
found
proposed
rmnn
model
quickly
adapted
new
relation
without
sacriﬁcing
performance
orig-
inal
relations
proving
effectiveness
nam
models
apply
solve
complex
commonsense
reasoning
problems
i.e.
winograd
schemas
levesque
davis
morgenstern
2011
support
model
training
task
propose
straightforward
method
col-
lect
associative
phrase
pairs
text
corpora
experiments
conducted
set
winograd
schema
problems
in-
dicated
neural
association
model
solve
prob-
lems
successfully
however
still
long
way
ﬁnally
achieving
automatic
commonsense
reasoning
acknowledgments
want
thank
prof.
gary
marcus
new
york
univer-
sity
useful
comments
commonsense
reasoning
also
want
thank
prof.
ernest
davis
dr.
leora
morgen-
stern
dr.
charles
ortiz
wonderful
organizations
making
ﬁrst
winograd
schema
challenge
happen
paper
supported
part
science
tech-
nology
development
anhui
province
china
grants
2014z02006
fundamental
research
funds
cen-
tral
universities
grant
wk2350000001
strate-
gic
priority
research
program
chinese
academy
sciences
grant
xdb02070006
references
bengio
2003
bengio
ducharme
vincent
janvin
2003.
neural
probabilistic
language
model
journal
machine
learning
research
3:1137–1155
bordes
2012
bordes
glorot
weston
bengio
2012.
joint
learning
words
meaning
rep-
resentations
open-text
semantic
parsing
proceedings
aistats
127–135
bordes
2013
bordes
usunier
garcia-duran
weston
yakhnenko
2013.
translating
em-
beddings
modeling
multi-relational
data
proceedings
nips
2787–2795
bowman
2015
bowman
angeli
potts
2015.
large
annotated
corpus
manning
arxiv
preprint
learning
natural
language
inference
arxiv:1508.05326
bowman
2013
bowman
2013.
recursive
neu-
ral
tensor
networks
learn
logical
reasoning
arxiv
preprint
arxiv:1312.6192
burnard
1995
burnard
1995.
users
reference
guide
british
national
corpus
version
1.0
collobert
2011
collobert
weston
bottou
karlen
kavukcuoglu
kuksa
2011.
natural
language
processing
almost
scratch
journal
machine
learning
research
12:2493–2537
getoor
2007
getoor
2007.
relational
learning
mit
press
glorot
bengio
2010
glorot
bengio
2010.
understanding
difﬁculty
training
deep
feedforward
neural
networks
proceedings
aistats
249–256
introduction
statistical
2015
1996
2012.
sutskever
deep
learning
introduction
srivastava
salakhutdinov
improving
neural
networks
prevent-
arxiv
preprint
graff
2003
graff
kong
chen
maeda
2003.
english
gigaword
linguistic
data
consortium
philadelphia
hill
2015
hill
bordes
chopra
we-
ston
2015.
goldilocks
principle
reading
children
books
explicit
memory
representations
arxiv
preprint
arxiv:1511.02301
hinton
2012
hinton
krizhevsky
ing
co-adaptation
feature
detectors
arxiv:1207.0580
hornik
stinchcombe
white
1990
hornik
stinch-
combe
white
1990.
universal
approximation
unknown
mapping
derivatives
using
multilayer
feedforward
networks
neural
networks
:551–560
jensen
1996
jensen
bayesian
networks
volume
210.
ucl
press
london
koller
friedman
2009
koller
friedman
2009.
probabilistic
graphical
models
principles
tech-
niques
mit
press
lecun
bengio
hinton
2015
lecun
bengio
nature
hinton
521
7553
:436–444
levesque
davis
morgenstern
2011
levesque
davis
morgenstern
2011.
winograd
schema
challenge
aaai
spring
symposium
logical
formaliza-
tions
commonsense
reasoning
lin
2015
lin
liu
sun
liu
zhu
2015.
learning
entity
relation
embeddings
knowledge
graph
completion
proceedings
aaai
liu
singh
2004
liu
singh
2004.
concept-
net
practical
commonsense
reasoning
toolkit
technol-
ogy
journal
:211–226
mccarthy
1986
mccarthy
1986.
applications
cir-
cumscription
formalizing
common-sense
knowledge
ar-
tiﬁcial
intelligence
:89–116
mikolov
2013
mikolov
chen
corrado
dean
2013.
efﬁcient
estimation
word
representa-
tions
vector
space
arxiv
preprint
arxiv:1301.3781
miller
1995
miller
1995.
wordnet
lexi-
cal
database
english
communications
acm
:39–41
minsky
1988
minsky
1988.
society
mind
simon
schuster
morgenstern
davis
ortiz
2016
morgenstern
davis
ortiz
2016.
planning
executing
evaluating
winograd
schema
challenge
magazine
:50–54
mueller
2014
mueller
2014.
commonsense
rea-
soning
event
calculus
based
approach
morgan
kauf-
mann
nair
hinton
2010
nair
hinton
2010.
rectiﬁed
linear
units
improve
restricted
boltzmann
ma-
chines
proceedings
icml
807–814
neapolitan
2012
neapolitan
2012.
probabilistic
rea-
soning
expert
systems
theory
algorithms
creates-
pace
independent
publishing
platform
nickel
2015
nickel
murphy
tresp
gabrilovich
2015.
review
relational
machine
learn-
ing
knowledge
graphs
arxiv
preprint
arxiv:1503.00759
nickel
tresp
kriegel
2012
nickel
tresp
kriegel
h.-p.
2012.
factorizing
yago
scalable
machine
learning
linked
data
proceedings
www
271–280
acm
osgood
1952
osgood
1952.
nature
mea-
surement
meaning
psychological
bulletin
:197
pearl
1988
pearl
1988.
probabilistic
reasoning
intel-
ligent
systems
networks
plausible
reasoning
peng
khashabi
roth
2015
peng
khashabi
roth
2015.
solving
hard
coreference
problems
urbana
51:61801
rahman
2011
rahman
2011.
coreference
resolution
world
knowledge
proceed-
ings
49th
annual
meeting
association
com-
putational
linguistics
human
language
technologies-
volume
814–824
association
computational
linguis-
tics
richardson
domingos
2006
richardson
domingos
2006.
markov
logic
networks
machine
learning
1-2
:107–136
saba
2015
saba
2015.
winograd
schema
chal-
lenge
socher
2013
socher
chen
manning
2013.
reasoning
neural
tensor
networks
proceedings
nips
knowledge
base
completion
926–934
strube
2016
strube
2016.
non
utility
se-
mantics
coreference
resolution
corbon
remix
naacl
2016
workshop
coreference
resolution
beyond
ontonotes
turing
1950
turing
1950.
computing
machinery
intelligence
mind
236
:433–460
wang
2014
wang
zhang
feng
chen
2014.
knowledge
graph
embedding
translating
hyperplanes
proceedings
aaai
1112–1119
citeseer
xue
2014
xue
abdel-hamid
jiang
dai
liu
2014.
fast
adaptation
deep
neural
network
based
discriminant
codes
speech
recognition
au-
dio
speech
language
processing
ieee/acm
trans
:1713–1725
zhu
2015
zhu
kiros
zemel
salakhutdi-
nov
urtasun
torralba
fidler
2015.
aligning
books
movies
towards
story-like
visual
ex-
planations
watching
movies
reading
books
pro-
ceedings
ieee
international
conference
computer
vision
19–27
