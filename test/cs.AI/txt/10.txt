comparing
human
automated
evaluation
open-ended
student
responses
questions
evolution
michael
wiser1,2
louise
mead1,2,3
james
smith1,2,3,4,5
robert
pennock1,2,4,6,7
2program
ecology
evolutionary
biology
behavior
michigan
state
university
east
lansing
usa
1beacon
center
study
evolution
action
3department
integrative
biology
michigan
state
university
east
lansing
usa
4lyman
briggs
college
michigan
state
university
lansing
usa
5department
entomology
michigan
state
university
east
lansing
usa
6department
philosophy
michigan
state
university
east
lansing
usa
7department
computer
science
engineering
michigan
state
university
east
lansing
usa
mwiser
msu.edu
abstract
written
responses
provide
wealth
data
understand-
ing
student
reasoning
topic
yet
time-
labor-
intensive
score
requiring
many
instructors
forego
except
limited
parts
summative
assessments
end
unit
course
recent
developments
machine
learning
produced
computational
methods
scoring
writ-
ten
responses
presence
absence
speciﬁc
concepts
compare
scores
one
particular
program
evograder
human
scoring
responses
structurally-
content-similar
questions
distinct
ones
program
trained
ﬁnd
substan-
tial
inter-rater
reliability
human
scoring
however
sufﬁcient
systematic
differences
remain
human
scoring
advise
using
scoring
formative
rather
summative
assessment
student
reasoning
background
central
importance
evolution
teaching
learn-
ing
biological
sciences
clearly
established
science
education
reform
states
1900
brewer
smith
2011
adequate
formative
assessment
instruments
administered
course
instruction
gauge
stu-
dent
understanding
reasoning
order
provide
feed-
back
future
instruction
instead
assign
grade
end
unit
measure
student
understanding
evo-
lutionary
concepts
bishop
anderson
1990
anderson
al.
2002
however
recently
rather
lim-
ited
nehm
schonfeld
2008
part
challenge
designing
effective
instrument
comes
fact
student
understanding
evolutionary
concepts
complex
constantly
changing
studies
ﬁnd
students
hold
scientiﬁcally
accurate
naive
non-scientiﬁc
ex-
planations
simultaneously
andrews
al.
2012
hiatt
al.
2013
accurately
identifying
alternative
conceptions
difﬁcult
rector
al.
2012
data
also
suggest
stu-
dents
reason
differently
experts
especially
response
different
contextual
elements
sample
questions
undergraduates
employ
naive
concepts
apply-
ing
explanations
natural
selection
plants
compared
animals
trait
loss
compared
trait
gain
unfamil-
iar
taxa
compared
familiar
taxa
nehm
2011
furthermore
ascertaining
meaning
student
responses
often
difﬁcult
one
study
found
percent
students
incorporated
lexically
ambiguous
language
responses
open
ended
questions
evolutionary
mech-
anisms
rector
al.
2012
despite
challenges
assessing
student
knowledge
important
particularly
evaluating
pedagogical
practices
designed
improve
student
understanding
effort
identify
effective
assessment
strategies
inves-
tigating
applicability
new
tool
evograder
mohar-
reri
al.
2014
open-ended
student
responses
provide
wealth
data
student
reasoning
unfortunately
also
time-
labor-intensive
score
one
study
found
took
average
four
minutes
human
grader
score
single
response
nine
ideas
analyze
study
moharreri
al.
2014
even
class
students
scoring
ﬁve
questions
would
take
ten
hours
quickly
becomes
prohibitive
instructor
wants
get
general
sense
student
understanding
formative
assessment
rapid
method
highly
desirable
appealing
potential
solution
problem
would
instructors
automated
system
sufﬁ-
ciently
sophisticated
evaluate
student
answers
open-ended
questions
course
simple
task
even
setting
aside
difﬁculty
parsing
open-ended
nat-
ural
language
responses
general
one
still
problem
interpreting
appropriateness
answers
re-
lation
content
knowledge
overarching
concepts
instance
science
teacher
may
want
know
whether
student
response
demonstrates
incorrect
naive
notions
whether
demonstrates
concrete
scientiﬁc
understanding
machine
learning
systems
begun
taking
ﬁrst
steps
accomplishing
difﬁcult
task
use
machine
learning
education
growing
interest
using
tools
techniques
machine
learning
classroom
environment
butler
al.
2014
fact
entire
book
written
using
machine
learning
educational
science
kidzi´nski
al.
2016
one
area
particular
interest
language
processing
machine
learning
techniques
used
classify
instructor
questions
according
bloom
taxon-
omy
yahya
al.
2013
perhaps
biggest
use
ma-
chine
learning
educational
environment
auto-
mated
scoring
student
writing
reviewed
nehm
al.
2012b
one
domain-speciﬁc
example
techniques
lan-
guage
processing
provided
web
portal
evograder
discussed
evograder
designed
assess
student
understanding
natural
selection
using
particular
set
questions
consisting
brief
scenario
asking
stu-
dents
biologist
would
explain
scenario
evo-
lutionary
change
patterns
study
seeks
measure
similar
scores
procedure
provides
human
scoring
questions
application
trained
written
style
evograder
evograder
http
//www.evograder.com
free
online
service
analyzes
open-ended
responses
ques-
tions
evolution
natural
selection
provides
users
formative
assessments
described
detail
moharreri
al.
2014
brief
description
follows
evograder
works
supervised
machine
learning
par-
ticipants
n=2,978
wrote
responses
acorns
assessment
items
nehm
al.
2012a
acorns-like
items
bishop
anderson
1990
generating
10,270
student
responses
items
consist
prompt
describing
short
scenario
relevant
natural
selection
ask
students
write
biologist
would
explain
situation
participants
spanned
many
different
levels
expertise
including
non-majors
un-
dergraduate
biology
anthropology
majors
graduate
stu-
dents
postdocs
faculty
evolutionary
science
response
scored
independently
two
human
raters
six
key
concepts
three
naive
ideas
see
box
consensus
scores
used
train
evo-
grader
based
supervised
machine
learning
tools
lightside
mayﬁeld
ros´e
2013
lightside
provides
feature
extraction
model
construction
model
valida-
tion
based
human-scored
responses
evograder
authors
chose
different
methods
optimize
scoring
algorithm
feature
extraction
scor-
ing
models
one
model
concept
considered
dictionary
words
used
particular
response
reduced
words
stems
removed
high
frequency
low
information
words
e.g.
also
in-
cluded
pairs
consecutive
words
e.g.
passing
removing
misclassiﬁed
data
see
moharreri
moharreri
al.
2014
table
details
feature
extraction
response
converted
set
vectors
containing
frequencies
words
word
pairs
vectors
passed
binary
classiﬁer
underwent
sequential
minimal
optimization
smo
platt
1999
models
smo
training
algo-
rithm
iteratively
assigned
weights
words
written
re-
sponses
model
able
match
human
scores
within
certain
margin
error
models
val-
idated
10-fold
cross-validation
using
data
generate
model
remaining
data
validate
repeating
procedure
total
times
data
used
validation
exactly
model
generation
times
authors
av-
eraged
models
get
ﬁnal
models
used
pro-
gram
assessing
whether
met
quality
benchmarks
accuracy
kappa
coefﬁcients
0.8
deﬁned
cre-
ators
adjusting
training
models
evograder
uses
validated
models
score
new
re-
sponses
web
users
users
must
upload
data
spe-
ciﬁc
format
portal
veriﬁes
data
format-
ted
correctly
evograder
evaluates
response
using
existing
validated
models
provides
machine
scored
data
downloadable
.csv
format
variety
web
visualizations
data
fig
methods
student
data
administered
pre-instruction
post-instruction
tests
consisting
two
questions
see
box
evolution
students
introductory
cell
molecular
biology
course
fall
semester
2014.
questions
asked
students
evolutionary
processes
occur
question
asks
evolutionary
gain
antibiotic
resistance
population
question
asks
evolutionary
loss
toxicity
mushroom
population
completed
pre-
post-test
responses
obtained
students
question
students
question
key
concepts
variation
muta-
tion/recombination/sex
differences
among
individuals
population
presence
causes
heritability
traits
genetic
basis
able
passed
parent
offspring
competition
situation
two
individ-
uals
struggle
get
resources
available
everyone
limited
resources
required
resources
survival
food
mates
water
etc
available
unlimited
amounts
differential
survival
differential
survival
and/or
repro-
duction
individuals
non-adaptive
ideas
genetic
drift
related
non-
adaptive
factors
contributing
evolutionary
change
naive
ideas
adapt
organisms/populations
adjust
acclimate
environment
need
organisms
gain
traits
advantage
response
need
goal
accomplish
something
use/disuse
traits
lost
gained
due
use
disuse
traits
human
evaluators
determined
whether
response
answered
question
asked
response
credit
given
key
concepts
example
consider
student
response
similar
kind
mutation
poi-
son
plants
eaten
able
reproduce
pass
thoses
sic
genes
future
generations
population
poisonous
mushrooms
would
soon
outnumber
non-poisonous
ones
since
poi-
sonous
mushrooms
less
likely
eaten
time
animals
would
learn
stay
away
teh
sic
mushroom
simply
sic
appearance
toxin
would
longer
needed
although
answer
demonstrates
adaptive
reasoning
origin
toxic
mushrooms
question
loss
toxin
population
origin
toxin
last
sentence
addresses
loss
toxin
demonstrate
key
concepts
figure
concept
maps
produced
evograder
pre-
instruction
upper
panel
post-instruction
lower
panel
analysis
question
see
box
sizes
circles
indicate
percentage
responses
scored
containing
concept
widths
lines
connecting
concepts
shows
fre-
quency
co-occurrence
concepts
evaluated
student
responses
two
prompts
box
question
explain
microbial
population
evolves
resistance
effects
antibiotic
question
species
mushroom
contains
chemical
toxic
mammals
would
biologists
explain
initial
occurrence
increase
frequency
number
individuals
population
longer
produce
toxin
scored
response
whether
contained
following
concepts
data
data
ﬁles
containing
student
responses
scoring
data
analysis
may
found
https
//github.com/
mjwiser/alife2016
scoring
responses
used
evograder
score
student
responses
two
open-
ended
questions
natural
selection
six
key
concepts
three
naive
ideas
see
box
two
human
graders
mjw
lsm
scored
student
responses
criteria
resolved
disagreement
among
humans
discussion
resulting
consensus
human
score
statistical
analysis
measured
inter-rater
reliability
irr
evo-
grader
scores
consensus
human
scores
question
outlined
hallgren
2012
interested
irr
speciﬁc
questions
combined
pre-and
post-instruction
responses
combined
data
set
computed
irr
question
whole
separately
key
concepts
naive
ideas
within
question
chose
compute
irr
individual
concept
separately
pre-
post-
instruction
questions
lower
statistical
power
examining
set
separately
increase
mul-
tiple
comparisons
would
necessitate
also
compared
evograder
human
consensus
scores
way
tailed
paired
t-tests
test
differences
number
key
concepts
naive
ideas
scored
conducted
statis-
tical
testing
version
3.2.3
core
team
2013
results
discussion
inter-rater
reliability
irr
evograder
con-
sensus
human
scoring
questions
good
values
0.63
antibiotic
resistance
question
0.55
mushroom
question
fig
means
half
total
variance
scoring
across
concepts
shared
among
raters
landis
koch
1977
suggest
irr
values
cohens
kappa
range
0.6
0.8
indicate
substantial
agreement
among
coders
values
be-
tween
0.4
0.6
indicate
moderate
agreement
landis
koch
1977
criteria
concepts
analyzed
together
irr
antibiotic
question
strong
irr
mushroom
question
moderate
examined
irr
separately
key
concepts
naive
ideas
fig
examine
whether
systematic
difference
two
concept
types
antibiotic
resistance
question
irr
notably
higher
key
concepts
naive
ideas
0.63
0.17
fact
conﬁdence
interval
naive
ideas
irr
overlaps
meaning
irr
statistically
signiﬁ-
cantly
different
ratings
assigned
random
con-
versely
irr
mushroom
question
consistent
across
key
concepts
naive
ideas
0.51
0.55
respec-
tively
showing
meaningful
difference
across
concept
type
account
differences
irr
one
thing
take
note
low
variation
given
raters
scoring
across
responses
little
sta-
tistical
power
detect
shared
variance
across
raters
thought
experiment
imagine
two
different
raters
assign
scores
yes
responses
even
two
raters
assigned
scores
randomly
two
raters
would
expected
agree
time
irr
analyses
take
account
expected
frequency
scoring
agreement
low
variance
across
responses
given
rater
negatively
affect
statistical
power
irr
anal-
yses
reﬂected
wide
conﬁdence
intervals
naive
ideas
particular
one
fewer
poten-
tial
naive
ideas
scored
since
three
naive
ideas
per
student
response
six
key
concepts
per
student
response
skew
responses
larger
impact
naive
ideas
antibiotic
resistance
ques-
tion
elsewhere
evograder
scored
entire
class
expressing
ﬁve
total
naive
ideas
antibiotic
ques-
tion
consensus
human
score
90.
part
general
trend
questions
human
consensus
score
differed
evograder
score
statisti-
cally
signiﬁcant
margin
even
correcting
multiple
comparisons
see
table
adjusted
p-values
0.05
questions
human
consensus
score
detected
naive
ideas
evograder
however
humans
de-
tected
key
concepts
evograder
an-
tibiotic
question
question
fewer
mushroom
question
question
several
factors
may
serve
lower
irr
ideal
levels
one
obvious
cause
mentioned
box
student
responses
demonstrate
reasoning
natural
se-
lection
answer
question
asked
cases
humans
credit
student
key
concepts
address
question
asked
evo-
grader
hand
screening
mech-
anism
analyzed
pre-
post-instruction
responses
jointly
expect
number
naive
ideas
expressed
decrease
instruction
expect
number
key
concepts
expressed
increase
instruction
instructional
effects
would
positive
outcome
students
may
reduce
variance
post-instructional
scoring
reducing
statistical
power
detect
shared
variance
account
difference
results
two
questions
two
potentially
salient
contextual
differences
questions
one
ﬁrst
question
gain
trait
second
loss
trait
two
two
questions
use
different
taxonomic
groups
examples
differences
shown
literature
important
student
reasoning
nehm
figure
inter-rater
reliability
questions
key
concepts
naive
ideas
pooled
within
question
plotted
values
cohen
kappa
error
bars
shown
conﬁdence
intervals
comparison
adj
antibiotic
antibiotic
5.779
2.604
mushroom
-2.806
mushroom
3.384
2.14
10−7
8.58
10−7
0.0113
0.00647
0.0453
0.0259
0.00117
0.00466
table
2-tailed
paired
t-tests
comparing
evograder
human
consensus
scoring
key
concepts
naive
ideas
negative
values
indicate
concepts
detected
evograder
positive
values
indicate
concepts
detected
humans
bonferroni
correction
used
generate
adj
values
2011
future
study
able
disentangle
factors
multifactorial
design
considers
multiple
taxonomic
groups
asks
gain
trait
loss
trait
question
within
conclusions
evograder
useful
tool
assessing
student
reasoning
natural
selection
even
questions
included
training
provides
reasonable
level
reliability
scoring
student
responses
open-ended
questions
sim-
ilar
style
acorns
assessment
however
figure
inter-rater
reliability
questions
broken
key
concepts
naive
ideas
plotted
values
cohen
kappa
error
bars
shown
conﬁdence
intervals
foolproof
study
evograder
credited
students
dis-
playing
key
concepts
fewer
naive
ideas
human
raters
particular
evograder
may
inaccu-
rately
credit
student
responses
address
spe-
ciﬁc
question
asked
evolutionary
reasoning
forma-
tive
assessments
valuable
tool
get
sense
student
responses
short
period
time
caution
using
evograder
assign
points
students
given
current
limitations
acknowledgments
thank
rohan
maddamsetti
emily
dolson
alex
lalejini
anya
vostinar
joshua
nahum
brian
goldman
charles
ofria
helpful
discussion
manuscript
preparation
work
supported
national
science
foundation
iuse
1432563
cooperative
agreement
dbi-0939454
opinions
ﬁndings
conclusions
recommendations
expressed
material
author
necessarily
reﬂect
views
national
science
foundation
references
anderson
fisher
norman
2002
development
evaluation
conceptual
inven-
tory
natural
selection
journal
research
sci-
ence
teaching
:952–978
andrews
price
mead
mcelhinny
thanukos
perez
herreid
terry
lemons
2012
biology
undergrad-
uates
misconceptions
genetic
drift
cbe-life
sciences
education
:248–259
bishop
anderson
1990
student
con-
ceptions
natural
selection
role
evolution
journal
research
science
teaching
27.
brewer
smith
2011
vision
change
undergraduate
biology
education
call
action
american
association
advancement
science
washington
butler
marsh
slavinsky
baraniuk
2014
integrating
cognitive
science
tech-
nology
improves
learning
stem
classroom
ed-
ucational
psychology
review
:331–340
hallgren
2012
computing
inter-rater
reliabil-
ity
observational
data
overview
tuto-
rial
tutorials
quantitative
methods
psychology
:23–34
hiatt
davis
trujillo
terry
french
price
perez
2013
getting
evo-
devo
concepts
challenges
students
learning
evolutionary
developmental
biology
cbe-life
sci-
ences
education
:494–508
kidzi´nski
giannakos
sampson
dillen-
bourg
2016
tutorial
machine
learning
educational
science
chang
kravcik
popescu
huang
kinshuk
chen
n.-s.
editors
state-of-the-art
future
directions
smart
learning
pages
453–459
springer
singapore
singa-
pore
landis
koch
1977
measurement
observer
agreement
categorical
data
biometrics
33.
mayﬁeld
ros´e
2013
open
source
machine
learning
text
handbook
automated
essay
eval-
uation
current
applications
new
directions
moharreri
nehm
2014
evograder
online
formative
assessment
tool
automatically
evaluating
written
evolutionary
explanations
evolu-
tion
education
outreach
:1–14
nehm
beggrow
opfer
2012a
reasoning
natural
selection
diagnosing
contextual
competency
using
acorns
instrument
american
biology
teacher
74.
nehm
2011
item
feature
effects
evolution
assessment
journal
research
science
teaching
48.
nehm
mayﬁeld
2012b
transform-
ing
biology
assessment
machine
learning
au-
tomated
scoring
written
evolutionary
explanations
journal
science
education
technology
21.
nehm
schonfeld
2008
measuring
knowl-
edge
natural
selection
comparison
cins
open-response
instrument
oral
interview
jour-
nal
research
science
teaching
45.
core
team
2013
language
environment
statistical
computing
rector
nehm
pearl
2012
learn-
ing
language
evolution
lexical
ambiguity
word
meaning
student
explanations
research
science
education
:1107–1133
states
1900
next
generation
science
standards
states
states
national
academies
press
yahya
osman
taleb
alattab
2013
analyzing
cognitive
level
classroom
questions
using
machine
learning
techniques
9th
international
conference
cognitive
science
97:587–595
