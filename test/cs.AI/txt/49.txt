scaling
decentralized
mdps
heuristic
search
jilles
dibangoye
christopher
amato
inria
loria
campus
scientiﬁque
239
54506
vandœuvre-l
es-nancy
france
jilles.dibangoye
inria.fr
computer
science
laboratory
massachusetts
institute
technology
cambridge
02139
usa
camato
csail.mit.edu
arnaud
doniec
universit´e
lille
nord
france
mines
douai
d´epartement
f-59500
douai
france
arnaud.doniec
mines-douai.fr
abstract
decentralized
partially
observable
markov
de-
cision
processes
dec-pomdps
rich
mod-
els
cooperative
decision-making
uncer-
tainty
often
intractable
solve
optimally
nexp-complete
transition
observa-
tion
independent
dec-mdp
general
subclass
shown
complexity
optimal
algorithms
subclass
still
inefﬁcient
practice
paper
ﬁrst
pro-
vide
updated
proof
optimal
policy
depend
histories
agents
local
observations
present
new
algorithm
based
heuristic
search
able
expand
search
nodes
using
constraint
opti-
mization
show
experimental
results
compar-
ing
approach
state-of-the-art
dec-
mdp
dec-pomdp
solvers
results
show
reduction
computation
time
in-
crease
scalability
multiple
orders
magni-
tude
number
benchmarks
introduction
substantial
progress
algorithms
mul-
tiagent
sequential
decision
making
represented
decen-
tralized
partially
observable
markov
decision
processes
dec-pomdps
algorithms
able
exploit
domain
structure
present
par-
ticularly
successful
unfortunately
general
dec-pomdp
problem
nexp-complete
even
methods
solve
moderately
sized
problems
op-
timally
decentralized
markov
decision
process
dec-mdp
independent
transitions
observations
represents
general
subclass
dec-pomdps
complexity
rather
nexp
algorithms
solv-
ing
dec-mdp
subclass
recently
proposed
approaches
often
solve
much
larger
problems
dec-pomdp
methods
solve
truly
large
problems
agents
paper
present
novel
algorithm
optimally
solving
dec-mdps
independent
transitions
obser-
vations
combines
heuristic
search
constraint
opti-
mization
show
one
cast
dec-mdp
in-
dependent
transitions
observations
continuous
de-
terministic
mdp
states
probability
distributions
states
original
dec-mdp
call
state
occupancy
distributions
allows
adapt
continu-
ous
mdp
techniques
solve
decentralized
mdps
following
insight
designed
algorithm
state
occupancy
exploration
performed
similarly
learning
real-time
policy
selection
ac-
cordance
decentralized
pomdp
techniques
result
approach
able
leverage
problem
structure
heuristics
limiting
space
policies
explored
bounding
value
efﬁciently
generating
policies
use
constraint
optimization
algorithm
termed
markov
policy
search
mps
shown
much
efﬁcient
algorithm
approach
used
dec-mdps
independent
transitions
observations
remainder
paper
organized
follows
first
provide
motivating
examples
utilizing
properties
dec-mdps
independent
transitions
observa-
tions
next
describe
dec-mdp
framework
discuss
related
work
present
theoretical
re-
sults
showing
optimal
policy
dec-mdps
independent
transitions
observations
depend
agent
histories
proven
offer
general
proof
permits
additional
in-
sights
next
describe
decentralized
markov
policy
search
algorithm
combines
constraint
optimization
heuristic
search
efﬁciently
produce
optimal
so-
lutions
dec-mdps
independent
transitions
ob-
servations
finally
present
empirical
evaluation
algorithm
respect
state-of-the-art
solvers
apply
decentralized
mdps
showing
ability
actions
motivated
real
problem
controlling
opera-
tion
multiple
space
exploration
rovers
ones
used
nasa
explore
surface
mars
distributed
sensor
net
surveillance
sensor
net
do-
main
team
stationary
moveable
uavs
satellites
sensors
must
coordinate
track
targets
sensors
independent
transitions
observa-
tions
particularly
suited
model
distributed
smart-grid
domains
application
aims
ﬁnding
optimal
schedules
amounts
generated
power
collection
generating
units
given
demands
operational
constraints
time
horizon
figure
meeting-grid
uncertainty
scenario
grid
inspired
seuken
zilberstein
background
related
work
solve
problems
multiple
orders
magnitude
larger
include
agents
motivating
examples
illustrate
characteristics
decentralized
partially
observable
markov
decision
processes
dec-pomdps
interested
consider
simple
two-agent
meeting-in-a-grid
uncertainty
domain
figure
scenario
two
agents
want
meet
soon
possi-
ble
two-dimensional
grid
world
agent
possible
actions
include
moving
north
south
west
east
staying
place
actions
given
agent
affect
agents
taking
action
agent
sense
information
case
cor-
responds
location
agent
par-
tial
information
insufﬁcient
determine
global
state
world
mainly
agents
per-
mitted
explicitly
communicate
local
locations
however
instantaneous
noise-free
communication
allowed
agents
partial
informa-
tion
together
would
reveal
true
state
world
i.e.
agents
joint
location
presence
joint
full
observability
property
differentiates
dec-mdps
dec-pomdps
generally
partially
observable
models
including
dec-pomdps
agents
partial
information
together
map
multiple
different
states
world
con-
sequence
decisions
models
depend
entire
past
histories
actions
observations
agents
ever
experienced
meeting-in-a-grid
uncer-
tainty
problem
since
transitions
observations
affected
agents
agent
decision
de-
pends
last
piece
partial
information
i.e.
agent
location
characteristics
appear
many
real-world
applications
including
mars
exploration
rovers
meeting-in-a-grid
domain
section
review
decentralized
mdp
model
assumptions
transition
observation
indepen-
dence
associated
notation
related
work
deﬁnition
decentralized
mdp
n-agent
decen-
tralized
mdp
consists
ﬁnite
set
···
states
cid:104
···
cid:105
denotes
set
local
ob-
servations
agent
ﬁnite
set
a1×
a2×···
joint
actions
cid:104
···
cid:105
set
local
actions
agent
transition
function
cid:48
denotes
probability
transiting
state
cid:104
cid:105
state
cid:48
cid:104
cid:48
cid:48
cid:48
cid:105
taking
joint
action
cid:104
cid:105
reward
function
cid:55
de-
notes
reward
received
executing
joint
action
state
noted
decentralized
mdps
distinguished
state
jointly
fully
observable
property
en-
sures
global
state
would
known
agents
shared
observations
given
step
i.e.
external
uncertainty
problem
follows
trivially
deﬁnition
states
observations
agent
dec-mdp
parameterized
initial
state
distribu-
tion
agents
operate
bounded
number
steps
typically
referred
problem
horizon
model
referred
ﬁnite-horizon
decentralized
mdp
solving
decentralized
mdp
given
planning
horizon
start
state
distribution
seen
ﬁnding
individual
policies
maximize
expected
cumulative
reward
steps
problem
3.1
additional
assumptions
interested
decentralized
mdps
exhibit
two
properties
ﬁrst
transition
independence
assump-
tion
local
observation
agent
depends
previous
local
observation
local
action
taken
agent
deﬁnition
transition
independent
assumption
n-agent
decentralized
mdp
said
transition
independent
transition
functions
cid:55
cid:55
cid:55
exists
local
cid:48
cid:48
cid:89
i=1
...
cid:104
cid:105
cid:48
cid:104
cid:48
cid:48
cid:48
cid:105
cid:104
cid:105
also
implicitly
assume
observation
independence
states
observation
function
agent
depend
dynamics
agents
cid:48
cid:48
cid:48
n|s
cid:48
i|s
assuming
dec-
cid:81
mdp
state
factored
local
observations
becomes
transition
independence
cid:48
i|zi
3.2
preliminary
deﬁnitions
notations
τ−1
goal
solving
dec-mdp
ﬁnd
decentral-
ized
deterministic
joint
policy
cid:104
cid:105
in-
dividual
policy
sequence
decision
rules
t−1
cid:105
addition
call
decentralized
decision
cid:104
rule
time
n-tuple
decision
rules
paper
distinguish
be-
tween
history-dependent
markov
decision
rules
history-dependent
decision
rule
time
maps
τ-step
local
action-observation
histories
cid:105
local
actions
cid:104
sequence
history-dependent
de-
cision
rules
deﬁnes
history-dependent
policy
contrast
markov
decision
rule
time
maps
local
observations
sequence
markov
decision
rules
deﬁnes
markov
policy
moreover
worth
notic-
ing
decentralized
markov
policies
exponentially
smaller
decentralized
history-dependent
ones
state
occupancy
another
important
notion
pa-
per
τ-th
state
occupancy
system
con-
trol
decentralized
markov
policy
cid:104
···
στ−1
cid:105
denoted
τ−1
starting
given
s|σ0
τ−1
moreover
cur-
rent
state
occupancy
depends
past
decentral-
ized
markov
policy
τ−1
previous
state
local
actions
στ−1
cid:48
cid:80
occupancy
ητ−1
decentralized
markov
decision
rule
στ−1
cid:48
ητ−1
following
update-rule
denoted
ητ−1
στ−1
sake
simplicity
also
denote
cid:52
state
occupancy
space
τ-th
horizon
standard
|s|-dimensional
simplex
distinction
belief
states
state
occupancy
may
thought
belief
state
differ-
ences
formally
belief
state
given
s|hτ
τ−1
be-
lief
states
information
agents
states
typ-
ically
conditioned
single
joint
action-observation
his-
tory
total
probability
property
s|hτ
τ−1
hτ|σ0
τ−1
overall
τ-th
state
occupancy
summarizes
infor-
mation
world
states
contained
belief
states
horizon
words
doubly
exponentially
joint
action-observation
histories
summarized
sin-
gle
state
occupancy
make
use
local
infor-
mation
cid:80
3.3
related
work
section
focus
approaches
solving
dec-
mdps
independent
transitions
observations
well
relevant
solution
methods
thorough
in-
troduction
solution
methods
dec-pomdps
reader
refer
becker
ﬁrst
describe
transition
observation
independent
dec-mdp
subclass
solve
optimally
approach
called
coverage
set
algo-
rithm
consists
three
main
steps
first
sets
augmented
mdps
created
incorporate
joint
reward
local
reward
functions
agent
best
re-
sponses
agent
policies
found
us-
ing
augmented
mdps
finally
joint
policy
highest
value
agents
best
responses
re-
turned
algorithm
optimal
keeps
track
complete
set
policy
candidates
agent
requiring
large
amount
time
memory
petrik
zilberstein
reformulated
coverage
set
al-
gorithm
bilinear
program
thereby
allowing
optimiza-
tion
approaches
utilized
bilinear
program
used
anytime
algorithm
providing
online
bounds
solution
quality
iteration
representation
also
better
able
take
advantage
sparse
joint
reward
distributions
representing
independent
rewards
linear
terms
compressing
joint
reward
matrix
results
greatly
increased
efﬁciency
many
cases
agents
rewards
often
depend
agents
bilin-
ear
program
still
inefﬁcient
due
lack
reward
sparsity
general
dec-pomdps
approximate
approaches
at-
tempted
scale
larger
problems
horizons
generating
full
set
policies
may
optimal
approaches
known
memory-bounded
algorithms
introduced
seuken
zilberstein
successively
reﬁned
memory-bounded
algorithms
sample
forward
bounded
number
belief
states
back
i.e.
generate
next
step
policies
one
decen-
tralized
history-dependent
policy
belief
state
avoid
explicit
enumeration
possible
policies
ku-
mar
zilberstein
perform
backup
solving
corresponding
constraint
optimization
problem
cop
represents
decentralized
backup
although
memory-bounded
techniques
suboptimal
decentral-
ized
backup
applied
exact
settings
demon-
strate
algorithm
speciﬁcally
decentralized
backup
build
horizon-τ
decentralized
policy
maximal
respect
belief
state
horizon-
policies
available
agent
associated
cop
given
set
vari-
ables
one
local
observation
agent
set
domains
domain
variables
corresponding
agent
set
horizon-
policies
available
agent
set
soft
constraints
one
joint
observation
soft
constraint
maps
assignments
real
values
intuitively
values
represent
expected
re-
ward
accrued
agents
together
perceive
given
joint
observation
follow
given
horizon-
decentral-
ized
policy
since
horizon-τ
decentralized
policies
consist
horizon-
policies
easy
see
maximizing
sum
soft
constraints
yields
maximal
horizon-τ
decentralized
policy
closer
model
nd-pomdp
framework
aims
modeling
multiagent
teamwork
agents
strong
locality
interaction
often
binary
interac-
tions
reward
model
domains
de-
composed
among
sets
agents
substan-
tial
body
work
extend
general
dec-pomdp
tech-
niques
discussed
exploit
locality
interac-
tion
nair
introduced
opti-
mal
algorithm
model
namely
general
optimal
algorithm
goa
domain
contain
bi-
nary
interactions
reason
expect
goa
out-
perform
general
dec-pomdp
algorithms
methods
use
similar
strategies
selecting
policy
candidates
how-
ever
domain
contains
primarily
binary
interac-
tions
generally
agent
rewards
dependent
many
agents
goa
likely
outper-
form
general
dec-pomdp
algorithms
worth
noting
nd-pomdps
transition
ob-
servation
independent
dec-mdps
make
assump-
tions
transition
observation
independence
make
different
assumptions
reward
model
partial
observability
speciﬁcally
nd-pomdps
as-
sume
reward
decomposed
sum
local
reward
models
sets
agents
reward
model
transition
observation
independent
dec-mdps
general
allowing
global
rewards
agents
i.e.
considering
agents
one
set
dec-mdps
assume
state
jointly
fully
observable
i.e.
state
fully
determined
combination
local
observa-
tions
agents
nd-pomdps
make
limiting
assumption
models
therefore
make
different
assumptions
address
complexity
choice
model
depends
assumptions
best
match
domain
be-
ing
solved
theoretical
properties
section
demonstrate
main
theoretical
results
paper
4.1
optimal
policies
decentralized
mdp
solver
aims
calculate
optimal
decentralized
policy
maximizes
expected
cu-
mulative
reward
arg
maxπ
cid:80
t−1
following
theorem
proves
decentralized
markov
policies
yield
optimal
performance
decentralized
mdps
independent
transitions
observations
goldman
established
optimality
markov
policy
agent
assumption
agents
choose
markov
policies
state
opti-
mality
markov
policies
agent
matter
teammates
policies
also
construct
proof
manner
directly
relates
policies
values
rather
information
sets
may
clear
readers
theorem
optimality
decentralized
markov
policies
dec-mdps
independent
transitions
observa-
tions
optimal
policies
agent
depend
local
state
agent
histories
local
policy
t−1
arg
maxa1
cid:80
proof
without
loss
generality
construct
proof
induction
two
agents
agent
perspec-
tive
ﬁrst
show
last
step
problem
agent
policy
depend
local
history
step
last
agent
t−1
t−1|h1
σ1∗
t−1
t−1
chooses
local
action
maximize
value
based
possible
local
his-
tories
agent
resulting
states
system
cid:104
based
transition
observation
independence
use
decentralized
policies
shown
t−1
due
space
limitations
t−1
t−1|h1
t−1
cid:105
t−1
t−1
t−1
t−1
t−1
t−1
t−1
arg
maxa1
cid:80
include
full
proof
claim
intuitively
holds
agent
receive
information
agents
local
histories
due
transition
indepen-
dence
therefore
represent
agent
policy
t−1
last
step
σ1∗
t−1
longer
depends
his-
tory
t−1
therefore
policy
last
step
either
agent
depend
history
allows
deﬁne
value
function
last
step
υt−1
induction
step
show
policy
step
depend
history
policy
step
also
depend
local
history
show
agent
perspective
represented
agent
policy
step
σ1∗
value
function
assumed
depend
history
τ|h1
transition
show
independence
represent
agent
policy
step
σ1∗
arg
maxa1
cid:80
arg
maxa1
cid:80
τ|h1
t−1
t−1
longer
depends
local
history
therefore
policy
either
agent
depend
local
history
step
problem
establish
sufﬁcient
statistic
selection
decentralized
markov
decision
rules
theorem
sufﬁcient
statistic
state
occupancy
sufﬁcient
statistic
decentralized
markov
decision
rules
cid:80
cid:80
cid:80
cid:80
proof
build
upon
proof
optimality
decen-
tralized
markov
policies
theorem
note
optimal
decentralized
markov
policy
starting
given
hτ|σ0
τ−1
arg
maxπ
substitution
hτ−1
aτ−1
plus
sum
pairs
hτ−1
aτ−1
yields
sτ|σ0
τ−1
arg
maxπ
sτ|σ0
τ−1
state
occupancy
dis-
denote
tribution
decentralized
markov
policy
produced
horizon
hence
arg
maxπ
sτ∈s
state
occupancy
summarizes
possible
joint
action-observation
histories
decentralized
markov
pol-
icy
produced
horizon
estimate
joint
deci-
sion
rule
thus
state
occupancy
sufﬁcient
statis-
tic
decentralized
markov
decision
rules
since
esti-
mates
depend
upon
state
occupancy
longer
possible
joint
observation-histories
cid:80
cid:80
states
belief
states
multi-agent
belief
states
suf-
ﬁcient
select
directly
actions
mdps
pomdps
decentralized
pomdps
respectively
mainly
be-
cause
statistics
summarize
information
world
states
single
agent
perspective
state
occupancy
instead
summarizes
information
world
states
perspective
team
agents
constrained
execute
policies
independently
setting
joint
actions
selected
independently
instead
selected
jointly
decentralized
markov
decision
rules
4.2
optimality
criterion
section
presents
optimality
criterion
based
policy
value
functions
ﬁrst
deﬁne
τ-th
expected
immediate
reward
func-
tion
cid:52
cid:55
given
es∼ητ
quantity
denotes
immediate
reward
taking
decision
rule
system
state
occupancy
τ-th
time
step
let
represent
expected
total
reward
de-
cision
making
horizon
policy
used
system
state
occupancy
ﬁrst
time
step
space
decentralized
markov
policies
expected
total
reward
given
···
say
decentralized
markov
policy
opti-
mal
total
reward
criterion
whenever
υπ∗
decentralized
markov
policies
following
bellman
principle
optimality
one
separate
problem
ﬁnding
optimal
policy
simpler
subproblems
subproblems
con-
sists
ﬁnding
policies
t−1
optimal
···
deﬁne
τ-th
value
function
υστ
cid:52
cid:55
control
decentral-
ized
markov
policy
t−1
follows
cid:104
cid:80
t−1
cid:105
υστ
t−1
υστ
quantity
υστ
denotes
expected
sum
rewards
attained
starting
state
occupancy
taking
one
joint
action
according
taking
next
joint
ac-
tion
according
slightly
abuse
nota-
tion
write
τ-th
value
function
control
unknown
decentralized
markov
policy
t−1
using
cid:52
cid:55
denote
space
bounded
value
functions
τ-th
horizon
decentralized
markov
decision
rule
deﬁne
linear
transformation
lστ
cid:55
lστ
τ-th
value
function
built
-th
value
function
follows
manding
exhaustive
backup
operation
lrta∗
algorithm
exhaustive
variant
use
maxστ
lστ
5.1
exhaustive
variant
setting
equations
denote
optimality
equa-
tions
worth
noting
decentralized
markov
policy
solution
cid:104
···
σt−1
cid:105
optimal-
ity
equations
greedy
respect
value
functions
υt−1
exhaustive
variant
consists
three
major
steps
initialization
step
line
backup
operation
step
line
update
step
lines
repeats
execu-
tion
steps
convergence
¯υ0
−υ0
point
-optimal
decentralized
markov
policy
found
markov
policy
search
···
section
compute
optimal
decentralized
markov
policy
cid:104
t−1
cid:105
given
initial
state
occupancy
planning
horizon
note
state
occupancies
used
calculate
heuristics
algorithm
ﬁnal
choices
step
depend
state
occupan-
cies
result
nonstationary
policy
agent
mapping
local
observations
actions
step
cast
decentralized
mdps
continuous
deterministic
mdps
states
state
occupancy
dis-
tributions
actions
decentralized
markov
policies
update-rules
στ−1
deﬁne
transitions
map-
pings
denote
reward
function
techniques
apply
continuous
deterministic
mdps
also
apply
decentralized
mdps
independent
transitions
observations
sake
efﬁciency
focus
optimal
techniques
exploit
initial
information
learning
real-time
lrta∗
algorithm
used
solve
deterministic
mdps
approach
updates
states
agents
actually
visit
planning
stage
therefore
suitable
continuous
state
spaces
algorithm
namely
markov
policy
search
mps
illus-
trates
adaptation
lrta∗
algorithm
solving
decentralized
mdps
independent
transitions
ob-
servations
mps
algorithm
relies
lower
upper
bounds
¯υτ
exact
value
functions
plan-
ning
horizons
use
following
deﬁnitions
q-value
functions
¯qτ
denote
rewards
accrued
taking
decision
rule
state
occupancy
following
policy
deﬁned
upper-bound
value
functions
remaining
planning
horizons
denote
set
stored
decentralized
markov
decision
rules
state
occupancy
thus
¯υτ
maxστ∈ψτ
¯qτ
represents
upper-bound
value
state
occupancy
formally
¯qτ
lστ
¯υτ
next
describe
two
variants
mps
algorithm
exhaustive
variant
replaces
states
state
occupancy
dis-
tributions
actions
decentralized
markov
decision
rules
lrta∗
algorithm
second
variant
uses
constraint
optimization
program
instead
memory
de-
algorithm
mps
algorithm
begin
initialize
bounds
¯υ0
mps-trial
mps-trial
begin
¯υτ
σgreedy
arg
maxστ
¯qτ
update
upper
bound
value
function
mps-trial
σgreedy
update
lower
bound
value
function
initialization
initialize
lower
bound
τ-th
value
function
decentralized
markov
pol-
icy
randomly
generated
policy
πrand
cid:104
σrand,0
σrand
cid:105
υσrand
...
σrand
initialize
upper
bound
¯υτ
τ-th
value
func-
tion
underlying
mdp
πmdp
cid:104
σmdp,0
σmdp
cid:105
¯υτ
υσmdp
...
σmdp
exhaustive
backup
operation
choose
decentral-
ized
markov
decision
rule
σgreedy
yields
high-
est
value
¯υτ
explicit
enumeration
possible
decentralized
markov
decision
rules
ﬁrst
store
decentralized
markov
decision
rules
visited
state
occupancy
together
corresponding
val-
ues
¯qτ
hence
greedy
decentralized
markov
decision
rule
σgreedy
arg
maxστ
¯qτ
state
oc-
cupancy
update
lower
upper
bounds
update
lower
bound
value
function
based
decentralized
markov
poli-
cies
πgreedy
cid:104
σgreedy,0
σgreedy
t−1
cid:105
selected
trial
πgreedy
yields
value
higher
cur-
rent
lower
bound
υπgreedy
set
υσgreedy
...
σgreedy
otherwise
leave
lower
bound
unchanged
update
upper
bound
value
function
based
decentralized
markov
deci-
sion
rules
σgreedy
-th
upper-bound
value
function
¯υτ
follows
¯υτ
lσgreedy
¯υτ
theoretical
guarantees
exhaustive
variant
mps
yields
advantages
drawbacks
one
hand
inherits
theoretical
guarantees
lrta∗
al-
gorithm
particular
terminates
decentralized
markov
policy
within
¯υ0
opti-
mal
decentralized
markov
policy
indeed
upper
bound
value
functions
¯υτ
never
underestimate
exact
value
state
occupancy
update
upper
bound
value
state
occupancy
based
upon
greedy
decision
rule
state
occupancy
hand
exhaustive
variant
algorithm
requires
exhaus-
tive
enumeration
possible
decentralized
markov
de-
cision
rules
backup
step
algorithm
line
mdp
techniques
exhaustive
enumeration
pro-
hibitive
since
action
space
often
manageable
de-
centralized
mdp
planning
however
space
de-
centralized
markov
decision
rules
increases
exponentially
increasing
observations
agents
ex-
haustive
variant
scale
problems
moderate
number
observations
local
states
two
agents
5.2
constraint
optimization
formulation
cid:80
overcome
memory
limitation
exhaustive
vari-
ant
use
constraint
optimization
instead
exhaus-
tive
backup
operation
precisely
constraint
op-
timization
program
returns
greedy
decentralized
markov
decision
rule
σgreedy
state
occupancy
visited
without
performing
exhaustive
enumeration
constraint
optimization
formulation
variables
associated
decision
rules
agents
local
observations
do-
action
space
main
variable
state
associated
single
soft
constraint
cid:55
assigns
value
cid:48
cid:48
υσmdp
...
σmdp
cid:48
joint
action
value
denotes
reward
ac-
crued
horizon
taking
joint
action
state
following
underlying
mdp
joint
policy
remaining
planning
horizons
decentralized
markov
decision
rule
also
associate
single
soft
constraint
assigns
value
¯qτ
¯qmdp
¯qmdp
lστ
υσmdp
...
σmdp
objective
con-
straint
optimization
model
ﬁnd
assignment
σgreedy
σgreedy
arg
maxστ
cid:80
actions
variables
aggregate
value
maximized
stated
formally
wish
ﬁnd
cid:80
better
understand
constraint
optimization
program
use
¯qmdp
instead
cid:80
note
deﬁnition
mapping
¯qmdp
¯qmdp
hence
get
σgreedy
arg
maxστ
¯qτ
thus
constraint
optimization
program
returns
decentralized
markov
deci-
sion
rule
highest
upper-bound
value
techniques
solve
constraint
optimization
formulation
abound
literature
constraint
programming
allowing
many
different
approaches
utilized
theoretical
guarantees
constraint
optimization
vari-
ant
yields
guarantees
exhaustive
variant
without
major
drawback
exhaustive
enumeration
decentralized
markov
decision
rules
instead
uses
constraint
optimization
formulation
returns
greedy
decentralized
markov
decision
rule
often
much
efﬁcient
exhaustive
enumeration
hence
retain
property
stopping
algorithm
time
solution
within
¯υ0
optimal
decentralized
markov
policy
comparison
cop
based
algorithms
rich
body
work
replaces
exhaustive
backup
operation
constraint
optimization
formulation
decentralized
control
settings
constraint
optimiza-
tion
programs
compute
decentralized
history-dependent
policy
given
belief
state
mps
also
takes
ad-
vantage
constraint
optimization
formulation
remains
fundamentally
different
difference
lies
cop
formulation
heuristic
search
existing
cop
based
algorithms
decentralized
control
authors
try
ﬁnd
best
assignment
sub-policies
histories
in-
stead
case
cop
formulation
aims
mapping
local
observations
local
actions
provides
consid-
erable
memory
time
savings
moreover
existing
algo-
rithms
proceed
backing
policies
backward
di-
rection
i.e.
last
step
ﬁrst
using
set
pre-selected
belief
states
contrast
mps
algorithm
proceeds
for-
ward
expanding
state
occupancy
distributions
se-
lecting
greedily
decision
rules
finally
mps
algorithm
returns
optimal
solution
whereas
cop
based
al-
gorithms
dec-pomdps
return
locally
optimal
so-
lutions
approximate
solutions
returned
algorithms
plan
centralized
be-
lief
states
constitute
sufﬁcient
statistic
dec-pomdps
dec-mdps
empirical
evaluations
evaluated
algorithm
using
several
benchmarks
decentralized
mdp
literature
benchmark
compared
algorithms
state-of-the-art
algorithms
solving
dec-mdps
dec-pomdps
note
compare
nd-pomdp
methods
since
benchmarks
allow
agents
interact
teammates
times
reason
expect
optimal
nd-
pomdp
method
goa
outperform
algorithms
presented
report
benchmark
optimal
value
together
running
time
seconds
different
planning
horizons
mps
variants
run
mac
osx
machine
2.4ghz
dual-core
intel
2gb
ram
available
solved
constraint
optimization
problems
using
aolib
library1
bilinear
programming
approach
listed
1the
aolib
library
available
following
website
mps
100
1000
100
1000
ice
ipg
blp
8848.7
1.27
6.00
28.6
exh
recycling
robot
|z|
|a|
0.016
154.94
0.090
185.71
216.47
0.111
0.124
247.24
0.151
278.01
0.156
308.78
3078.0
1.440
meeting
grid
|z|
|a|
0.0
0.13
0.43
0.89
1.49
4.68
94.26
994.2
10.0
34.7
192.8
571.2
1160.6
3938.5
600
0.00
0.02
0.37
4.38
cop
0.5
0.555
0.395
0.545
0.373
0.438
5.374
0.030
0.110
0.114
0.131
0.159
0.309
13.12
33.59
ice
12.55
ipg
blp
0.0
0.0
0.71
1.67
2.68
3.68
13.68
23.68
33.68
43.68
93.68
cop
meeting
8x8
grid
|z|
4096
|a|
5.05
6.20
13.16
13.76
16.94
18.66
38.37
52.39
59.70
74.28
214.73
100
navigation
mit
|z|
7225
|a|
47.322
321.26
180.70
400.48
1061.94
1236.11
100
0.0
0.0
14.28
34.97
54.92
154.93
85.85
ipg
ice
blp
3225.8
0.71
6.50
0.0
0.0
0.0
0.38
6.47
83.02
182.54
cop
navigation
isr
|z|
8100
|a|
2.39
3.19
4.31
13.43
54.16
194.66
100
1294.28
navigation
pentagon
|z|
9801
|a|
1.31
5.11
8.75
13.81
62.89
129.48
209.11
276.20
1033.70
0.0
0.0
0.0
0.38
4.82
19.73
39.18
57.75
76.39
0.99
6.01
4915.1
table
experimental
results
cop
exh
variants
mps
well
gmaa∗-ice
labeled
ice
ipg
blp
blp
run
2.8ghz
quad-core
intel
mac
2gb
ram
time
limit
hours
used
best
available
version
bilinear
program
approach
iterative
best
response
version
standard
pa-
rameters
generic
solution
method
perform
well
specialized
approaches
expect
results
differ
single
order
magnitude
compare
coverage
set
algorithm
bilinear
programming
methods
shown
efﬁcient
available
test
problems
provide
values
exhaustive
variant
exh
small
problems
constraint
optimization
formulation
cop
problems
tested
algorithms
six
bench-
marks
recycling
robot
meeting-in-a-grid
3x3
8x8
navigation
problems2
largest
hard-
est
benchmarks
could
ﬁnd
literature
com-
pare
algorithms
gmaa∗-ice
ipg
blp
gmaa∗-ice
heuristic
search
consistently
out-
performs
generic
exact
solvers
maa∗
ipg
algorithm
competitive
alternative
gmaa∗
approach
performs
well
problems
re-
duced
reachability
results
gmaa∗-ice
pro-
vided
matthijs
spaan
conducted
different
machine
similarly
results
ipg
col-
lected
different
machine
result
timing
results
gmaa∗-ice
ipg
directly
comparable
methods
likely
differ
small
constant
factor
would
obtained
test
machine
results
seen
table
benchmarks
cop
variant
mps
outperforms
algorithms
results
show
cop
variant
produces
opti-
http
//graphmod.ics.uci.edu/group/aolibwcsp/
http
//users.isr.ist.utl.pt/∼
mtjspaan/decpomdp/
2all
problem
deﬁnitions
available
following
website
mal
policies
much
less
time
tested
benchmarks
example
meeting
3x3
grid
problem
cop
variant
computed
optimal
policies
approximately
4358
4580
times
faster
gmaa∗-ice
blp
ipg
algorithms
respectively
also
note
cop
variant
useful
medium
large
domains
example
large
domains
exh
variant
ran
memory
cop
variant
com-
puted
optimal
solutions
horizons
100.
yet
exh
variant
compute
optimal
solution
small
problems
faster
cop
variant
instance
recycling
robot
horizon
1000
exh
variant
computed
optimal
solution
times
faster
cop
variant
mps
algorithm
due
overhead
constraint
optimization
formulation
lack
struc-
ture
utilized
many
different
reasons
results
mps
algorithm
outperforms
gmaa∗-ice
ipg
mainly
perform
policy
search
space
decen-
tralized
history-dependent
policies
instead
mps
algo-
rithm
performs
policy
search
space
decentral-
ized
markov
policies
exponentially
smaller
decentralized
history-dependent
policies
mps
outperforms
blp
algorithm
mainly
dimension
solution
representation
speciﬁcally
number
bilinear
terms
blp
approach
grows
polynomially
horizon
problem
causing
perform
well
large
problems
large
horizons
tightly
coupled
reward
values
continue
evaluation
mps
algorithm
ran-
domly
generated
instances
multiple
agents
ran-
dom
instances
built
upon
recycling
robot
problem
described
sutton
barto
given
models
associated
single
agent
choose
number
interaction
events
interaction
event
pair
joint
states
actions
reward
emax
randomly
chosen
structure
ties
agents
together
since
reward
model
decomposed
among
sub-
groups
agents
effort
provide
insight
degree
interaction
among
agents
distinguish
be-
tween
four
classes
depends
number
interaction
events
class
randomly
choose
emax
emax
denotes
number
joint
state
action
pairs
depicted
figure
constraint
formulation
allows
deal
larger
numbers
agents
calculated
op-
timal
value
functions
100
instances
class
reported
average
computational
time
cop
vari-
ant
able
scale
agents
horizon
000
seconds
could
also
produce
results
agents
000
seconds
using
powerful
machine
also
seen
increasing
number
interaction
events
problem
substantially
increase
amount
time
required
solve
prob-
lems
shows
even
dense
reward
matrices
approach
continue
perform
well
despite
high
running
time
mps
ﬁrst
generic
algorithm
scales
teams
two
agents
without
taking
advan-
tage
locality
interaction
example
blp
algorithm
currently
stands
solve
two-agent
problems
moreover
nd-pomdp
techniques
exploit
small
number
local
interactions
among
agents
scale
multiple
agents
problem
agents
interact
oretical
results
also
describe
novel
algorithm
combines
heuristic
search
constraint
optimization
efﬁciently
produce
optimal
solutions
class
problems
new
algorithm
termed
learning
markov
policy
mps
shown
scale
large
problems
planning
horizons
reducing
computation
time
mul-
tiple
orders
magnitude
previous
approaches
also
able
demonstrate
scalability
respect
number
agents
domains
agents
results
show
approach
could
applied
many
large
realistic
domains
future
plan
explore
extending
mps
al-
gorithm
classes
problems
larger
teams
agents
instance
may
able
produce
opti-
mal
solution
general
classes
dec-mdps
pro-
vide
approximate
results
dec-pomdps
extending
idea
occupancy
distribution
problems
furthermore
scalability
approach
larger
num-
bers
agents
encouraging
pursue
methods
increase
even
particular
think
ap-
proach
could
help
increase
number
agents
inter-
act
conjunction
structure
model
locality
interaction
nd-pomdps
sparse
joint
reward
matrices
bilinear
programming
approaches
acknowledgements
would
like
thank
frans
oliehoek
anony-
mous
reviewers
helpful
comments
initial
versions
paper
well
matthijs
spaan
marek
petrik
providing
algorithmic
results
code
respec-
tively
research
supported
part
afosr
muri
project
fa9550-091-0538
references
amato
dibangoye
zilberstein
in-
cremental
policy
generation
ﬁnite-horizon
dec-
international
pomdps
conference
automated
planning
scheduling
pages
2–9
thessaloniki
greece
2009.
proceedings
figure
mps
performance
increasing
number
agents
planning
horizon
randomized
in-
stances
recycling
robot
scenario
conclusion
future
work
aras
dutech
investigation
mathe-
matical
programming
ﬁnite
horizon
decentralized
pomdps
journal
artiﬁcial
intelligence
research
37:329–396
2010
astrom
optimal
control
markov
pro-
journal
cesses
incomplete
state
information
mathematical
analysis
applications
10:174–
205
1965.
paper
explores
new
theory
algorithms
solv-
ing
independent
transition
observation
dec-mdps
provide
new
proof
optimal
policies
depend
agent
histories
subclass
generalizing
previous
the-
barto
bradtke
singh
yee
gullapalli
pinette
learning
act
using
real-time
dynamic
programming
artiﬁcial
intelli-
gence
72:81–138
1995
0,01
0,1
100
1000
10000
100000
cpu
time
sec
classes
scalability
agents
agents
agents
agents
agents
agents
nair
tambe
yokoo
pynadath
marsella
taming
decentralized
pomdps
towards
efﬁcient
policy
computation
multiagent
proceedings
international
joint
settings
conference
artiﬁcial
intelligence
pages
705–711
2003
nair
varakantham
tambe
yokoo
networked
distributed
pomdps
synthesis
dis-
tributed
constraint
optimization
pomdps
proceedings
aaai
conference
artiﬁcial
in-
telligence
pages
133–139
2005
oliehoek
spaan
vlassis
op-
timal
approximate
q-value
functions
decen-
tralized
pomdps
journal
artiﬁcial
intelligence
research
32:289–353
2008
petrik
zilberstein
anytime
coordination
using
separable
bilinear
programs
proceedings
aaai
conference
artiﬁcial
intelligence
pages
750–755
2007
petrik
zilberstein
bilinear
programming
approach
multiagent
planning
journal
artiﬁ-
cial
intelligence
research
35:235–274
2009
putterman
markov
decision
processes
dis-
crete
stochastic
dynamic
programming
john
wiley
sons
new
york
1994
seuken
zilberstein
formal
models
al-
gorithms
decentralized
decision
making
un-
certainty
journal
autonomous
agents
multi-
agent
systems
:190–250
2008
spaan
oliehoek
amato
scal-
ing
optimal
heuristic
search
dec-pomdps
via
incremental
expansion
proceedings
inter-
national
joint
conference
artiﬁcial
intelligence
pages
2027–2032
2011
sutton
barto
reinforcement
learning
introduction
mit
press
cambridge
1998
zilberstein
washington
bernstein
a.-i
mouaddib
decision-theoretic
control
plan-
etary
rovers
advances
plan-based
control
robotic
agents.
pages
270–289
london
2002.
springer-verlag
becker
zilberstein
lesser
goldman
solving
transition
independent
decentral-
ized
markov
decision
processes
journal
artiﬁcial
intelligence
research
22:423–455
2004
bellman
dynamic
programming
princeton
university
press
1957
bernstein
amato
hansen
zil-
berstein
policy
iteration
decentralized
control
markov
decision
processes
journal
artiﬁcial
in-
telligence
research
34:89–132
2009
bernstein
givan
immerman
zil-
berstein
complexity
decentralized
control
markov
decision
processes
mathematics
opera-
tions
research
2002
dechter
constraint
optimization
constraint
processing
pages
363
397.
morgan
kaufmann
san
francisco
2003
dibangoye
a.-i
mouaddib
chaib-draa
point-based
incremental
pruning
heuristic
solv-
proceedings
ing
ﬁnite-horizon
dec-pomdps
international
joint
conference
autonomous
agents
multiagent
systems
budapest
hungary
2009
gallager
discrete
stochastic
processes
kluwer
academic
publishers
boston
1996
goldman
zilberstein
decentralized
con-
trol
cooperative
systems
categorization
com-
plexity
analysis
journal
artiﬁcial
intelligence
re-
search
22:143–174
2004
korf
real-time
heuristic
search
artiﬁcial
in-
telligence
2-3
:189–211
1990
kumar
zilberstein
constraint-based
dy-
namic
programming
decentralized
pomdps
structured
interactions
proceedings
interna-
tional
conference
autonomous
agents
multi-
agent
systems
pages
561–568
2009
kumar
zilberstein
point-based
backup
decentralized
pomdps
complexity
new
algo-
rithms
proceedings
international
confer-
ence
autonomous
agents
multiagent
systems
pages
1315–1322
toronto
canada
2010
kumar
zilberstein
toussaint
scalable
multiagent
planning
using
probabilistic
inference
proceedings
international
joint
conference
artiﬁcial
intelligence
pages
2140–2146
2011
lesser
ortiz
tambe
editors
dis-
tributed
sensor
networks
multiagent
perspective
volume
kluwer
academic
publishers
may
2003
